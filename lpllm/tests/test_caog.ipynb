{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c0e835",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/lpllm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The 'batch_size' argument of StaticCacheLen is deprecated and will be removed in v4.49. Use the more precisely named 'max_batch_size' argument instead.\n",
      "The 'batch_size' attribute of StaticCacheLen is deprecated and will be removed in v4.49. Use the more precisely named 'self.max_batch_size' attribute instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1])\n",
      "torch.Size([36, 16, 10, 128]) torch.Size([36, 16, 10, 128])\n",
      "18 2\n",
      "36 2\n",
      "tensor([2])\n",
      "36 3\n"
     ]
    }
   ],
   "source": [
    "from transformers.cache_utils import Cache, DynamicCache, StaticCache\n",
    "import torch, time\n",
    "from transformers import AutoConfig\n",
    "import sys\n",
    "from typing import Optional, Union\n",
    "sys.path.insert(0, '/mnt/zhengcf3/lpllm')\n",
    "from lpllm.lpllm import LPLLM\n",
    "from lpllm.StaticCacheLen import StaticCacheLen\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "            f\"/mnt/zhengcf3/models/deepseek-moe-16b-base\", trust_remote_code=True\n",
    "        )\n",
    "cache_kwargs = {}\n",
    "cache = StaticCacheLen(config, batch_size=36, max_cache_len=10, device=\"cpu\", dtype=torch.bfloat16)\n",
    "key_states = torch.randn(18, 16, 2, 128, dtype=torch.bfloat16, device=\"cpu\")\n",
    "value_states = torch.randn(18, 16, 2, 128, dtype=torch.bfloat16, device=\"cpu\")\n",
    "\n",
    "cache_position = torch.arange(key_states.shape[-2], device=\"cpu\")\n",
    "print(cache_position)\n",
    "cache_kwargs[\"cache_position\"] = cache_position\n",
    "start_batch = 0\n",
    "end_batch = 18\n",
    "# copy to first\n",
    "new_key_states, new_value_states=cache.update(key_states, value_states, 0, cache_kwargs, mode=\"update_batch\", batch_dim=(start_batch, end_batch))\n",
    "print(new_key_states.shape, new_value_states.shape)\n",
    "print(cache.get_batch_length(0), cache.get_seq_length(0))\n",
    "\n",
    "start_batch = cache.get_batch_length(0)\n",
    "end_batch = start_batch + key_states.shape[0]\n",
    "cache_position = torch.arange(key_states.shape[-2], device=\"cpu\")\n",
    "cache_kwargs[\"cache_position\"] = cache_position\n",
    "new_key_states, new_value_states=cache.update(key_states, value_states, 0, cache_kwargs, mode=\"update_batch\", batch_dim=(start_batch, end_batch))\n",
    "print(cache.get_batch_length(0), cache.get_seq_length(0))\n",
    "\n",
    "\n",
    "key_states = torch.randn(36, 16, 1, 128, dtype=torch.bfloat16, device=\"cpu\")\n",
    "value_states = torch.randn(36, 16, 1, 128, dtype=torch.bfloat16, device=\"cpu\")\n",
    "\n",
    "seq_length = cache.get_seq_length(0)\n",
    "cache_position = torch.arange(seq_length, seq_length+key_states.shape[-2], device=\"cpu\")\n",
    "print(cache_position)\n",
    "cache_kwargs[\"cache_position\"] = cache_position\n",
    "new_key_states, new_value_states=cache.update(key_states, value_states, 0, cache_kwargs, mode=\"update_seq\", batch_dim=(start_batch, end_batch))\n",
    "print(cache.get_batch_length(0), cache.get_seq_length(0))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbd6810",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "device=\"cuda:1\"\n",
    "weight_cache = torch.randn(1, 4096, 14336, dtype=torch.bfloat16, device=\"cpu\").pin_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6de37ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention_with_pinned_memory(\n",
    "    query: torch.Tensor,\n",
    "    key: torch.Tensor, \n",
    "    value: torch.Tensor,\n",
    "    output_tensor: Optional[torch.Tensor] = None,\n",
    "    attn_mask: Optional[torch.Tensor] = None,\n",
    "    dropout_p: float = 0.0,\n",
    "    is_causal: bool = False,\n",
    "    scale: Optional[float] = None,\n",
    "    enable_gqa: bool = False\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    修改版的 scaled_dot_product_attention，支持将结果直接写入预分配的 pinned memory。\n",
    "    \n",
    "    Args:\n",
    "        query: Query tensor of shape (..., L, E)\n",
    "        key: Key tensor of shape (..., S, E) \n",
    "        value: Value tensor of shape (..., S, Ev)\n",
    "        output_tensor: 预分配的 pinned memory tensor，用于存储结果。如果为 None，则创建新的 tensor\n",
    "        attn_mask: 可选的注意力掩码\n",
    "        dropout_p: Dropout 概率\n",
    "        is_causal: 是否使用因果掩码\n",
    "        scale: 缩放因子，如果为 None 则使用 1/sqrt(E)\n",
    "        enable_gqa: 是否启用分组查询注意力\n",
    "        \n",
    "    Returns:\n",
    "        注意力输出 tensor，如果提供了 output_tensor 则返回该 tensor\n",
    "    \"\"\"\n",
    "    L, S = query.size(-2), key.size(-2)\n",
    "    scale_factor = 1 / math.sqrt(query.size(-1)) if scale is None else scale\n",
    "    \n",
    "    # 计算输出形状\n",
    "    output_shape = query.shape[:-1] + (value.shape[-1],)\n",
    "    \n",
    "    # 如果提供了预分配的输出 tensor，验证其形状和类型\n",
    "    if output_tensor is not None:\n",
    "        # 注意：对于 GPU tensor，即使原始是 pinned memory，移动到 GPU 后也不再是 pinned\n",
    "        if output_tensor.shape != output_shape:\n",
    "            raise ValueError(f\"output_tensor 形状 {output_tensor.shape} 与期望形状 {output_shape} 不匹配\")\n",
    "        if output_tensor.dtype != query.dtype:\n",
    "            raise ValueError(f\"output_tensor 数据类型 {output_tensor.dtype} 与 query 数据类型 {query.dtype} 不匹配\")\n",
    "        if output_tensor.device != query.device:\n",
    "            raise ValueError(f\"output_tensor 设备 {output_tensor.device} 与 query 设备 {query.device} 不匹配\")\n",
    "    \n",
    "    # 创建注意力偏置\n",
    "    attn_bias = torch.zeros(L, S, dtype=query.dtype, device=query.device)\n",
    "    \n",
    "    if is_causal:\n",
    "        assert attn_mask is None, \"is_causal 和 attn_mask 不能同时使用\"\n",
    "        temp_mask = torch.ones(L, S, dtype=torch.bool, device=query.device).tril(diagonal=0)\n",
    "        attn_bias.masked_fill_(temp_mask.logical_not(), float(\"-inf\"))\n",
    "        attn_bias = attn_bias.to(query.dtype)\n",
    "\n",
    "    if attn_mask is not None:\n",
    "        if attn_mask.dtype == torch.bool:\n",
    "            attn_bias.masked_fill_(attn_mask.logical_not(), float(\"-inf\"))\n",
    "        else:\n",
    "            attn_bias += attn_mask\n",
    "\n",
    "    # 处理分组查询注意力\n",
    "    if enable_gqa:\n",
    "        key = key.repeat_interleave(query.size(-3)//key.size(-3), -3)\n",
    "        value = value.repeat_interleave(query.size(-3)//value.size(-3), -3)\n",
    "\n",
    "    # 计算注意力权重\n",
    "    attn_weight = query @ key.transpose(-2, -1) * scale_factor\n",
    "    attn_weight += attn_bias\n",
    "    attn_weight = torch.softmax(attn_weight, dim=-1)\n",
    "    \n",
    "    # 应用 dropout\n",
    "    if dropout_p > 0.0:\n",
    "        attn_weight = torch.dropout(attn_weight, dropout_p, train=True)\n",
    "    \n",
    "    # 计算最终输出\n",
    "    if output_tensor is not None:\n",
    "        # 直接写入预分配的 pinned memory\n",
    "        torch.matmul(attn_weight, value, out=output_tensor)\n",
    "        return output_tensor\n",
    "    else:\n",
    "        # 创建新的 tensor\n",
    "        return attn_weight @ value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3567852",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 960MB*3 = 2880MB\n",
    "time_start = time.time()\n",
    "for i in range(3*8):\n",
    "    gpu_weight_cache = weight_cache.to(device=\"cuda:1\")\n",
    "print(f\"time cost to cuda:1 {time.time() - time_start} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed106660",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "batch_size = 360\n",
    "device=\"cuda:1\"\n",
    "query_states = torch.randn(batch_size, 32, 1, 128, dtype=torch.bfloat16, device=\"cpu\").pin_memory()\n",
    "# 480MB, *4 = 1920MB\n",
    "key_states = torch.randn(batch_size, 8, 512, 128, dtype=torch.bfloat16, device=\"cpu\").pin_memory()\n",
    "value_states = torch.randn(batch_size, 8, 512, 128, dtype=torch.bfloat16, device=\"cpu\").pin_memory()\n",
    "# 960MB*3 = 2880MB\n",
    "time_start = time.time()\n",
    "for i in range(4):\n",
    "    key_states.to(device=\"cuda:1\")\n",
    "    value_states.to(device=\"cuda:1\")\n",
    "print(f\"time cost to cuda:1 {time.time() - time_start} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "120386dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key_states cpu\n",
      "time cost repeat kv 0.7527637481689453 s\n",
      "dot attn cost 0.071000 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "hidden_states_cache = torch.zeros(batch_size, 8, 4, 512, 128, dtype=torch.bfloat16, device=\"cpu\").pin_memory()\n",
    "# Copied from transformers.models.llama.modeling_llama.repeat_kv\n",
    "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n",
    "    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n",
    "    \"\"\"\n",
    "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
    "    if n_rep == 1:\n",
    "        return hidden_states\n",
    "    # 改为将hidden_states 复制到 hidden_states_cache 上\n",
    "    # for i in range(hidden_states_cache.shape[2]):\n",
    "        # 这行代码的作用是：将 hidden_states 的内容复制到 hidden_states_cache 的第 i 个位置（第三个维度）。\n",
    "        # 也就是说，对于 hidden_states_cache 的每个“副本”维度 i，都用同样的 hidden_states 填充，实现了在第3维上重复 hidden_states 的效果。\n",
    "        # hidden_states_cache[:, :, i, :, :] = hidden_states\n",
    "        # hidden_states_cache[:, :, i, :, :].index_copy_(3, torch.arange(hidden_states.shape[3]), hidden_states)\n",
    "    hidden_states_cache = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
    "    # hidden_states_cache = hidden_states.unsqueeze(2).expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
    "    return hidden_states_cache.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n",
    "def repeat_kv_optimized_v2(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    使用expand + reshape优化版本\n",
    "    \"\"\"\n",
    "    if n_rep == 1:\n",
    "        return hidden_states\n",
    "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
    "    # 在第1维上扩展n_rep倍\n",
    "    expanded = hidden_states.unsqueeze(2).expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
    "    return expanded.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n",
    "# attn_mask = torch.ones((480,512), dtype=torch.bfloat16, device=\"cpu\")\n",
    "print(f\"key_states {key_states.device}\")\n",
    "time_start = time.time()\n",
    "new_key_states = repeat_kv(key_states, 4)\n",
    "new_value_states = repeat_kv(value_states, 4)\n",
    "print(f\"time cost repeat kv {time.time() - time_start} s\")\n",
    "\n",
    "time_start = time.time()\n",
    "attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
    "    query_states,\n",
    "    new_key_states,\n",
    "    new_value_states,\n",
    "    attn_mask=None,\n",
    "    dropout_p=0.0,\n",
    "    # The q_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create a causal mask in case q_len == 1.\n",
    "    is_causal=False,\n",
    ")\n",
    "print(f\"dot attn cost {time.time()-time_start:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16bcff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测量移动整层layer需要的耗时\n",
    "import torch\n",
    "import time\n",
    "batch_size = 360\n",
    "device=\"cuda:1\"\n",
    "# hd = 16384\n",
    "# hdd= 6144\n",
    "hd = 14336\n",
    "hdd= 4096\n",
    "w0 = torch.randn(hd, hdd, dtype=torch.bfloat16, device=\"cpu\").pin_memory()\n",
    "w1 = torch.randn(hd, hdd, dtype=torch.bfloat16, device=\"cpu\").pin_memory()\n",
    "w2 = torch.randn(hd, hdd, dtype=torch.bfloat16, device=\"cpu\").pin_memory()\n",
    "\n",
    "time_start = time.time()\n",
    "for i in range(8):\n",
    "    w0_gpu = w0.to(device=\"cuda:1\")\n",
    "    w1_gpu = w1.to(device=\"cuda:1\")\n",
    "    w2_gpu = w2.to(device=\"cuda:1\")\n",
    "print(f\"time cost to cuda:1 {time.time() - time_start} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354baabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad5f78c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "# 1440MB\n",
    "src = torch.randn(360, 32, 512, 128, dtype=torch.float16, device=\"cpu\")\n",
    "dst = torch.randn(360, 32, 512, 128, dtype=torch.float16, device=\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22d04075",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "key = torch.randn(360, 8, 512, 128, dtype=torch.float16, device=\"cpu\")\n",
    "key_dst = torch.randn(360, 8, 512, 128, dtype=torch.float16, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ab24e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "src_pin = src.pin_memory()\n",
    "dst_pin = dst.pin_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2aeb5aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time cost pin copy 0.10234665870666504 s\n",
      "time cost 0.07175183296203613 s\n",
      "time cost pin copy 0.08554410934448242 s\n",
      "time cost pin copy 0.06585931777954102 s\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "for i in range(4):\n",
    "    dst[:, i*8:i*8+8, :, :] = key\n",
    "print(f\"time cost pin copy {time.time()-time_start} s\")\n",
    "\n",
    "time_start = time.time()\n",
    "for i in range(1):\n",
    "    dst.copy_(src)\n",
    "print(f\"time cost {time.time()-time_start} s\")\n",
    "\n",
    "time_start = time.time()\n",
    "for i in range(1):\n",
    "    dst_pin.copy_(src_pin)\n",
    "print(f\"time cost pin copy {time.time()-time_start} s\")\n",
    "\n",
    "time_start = time.time()\n",
    "for i in range(4):\n",
    "    key_dst.copy_(key)\n",
    "print(f\"time cost pin copy {time.time()-time_start} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62a7ab63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key_states cpu\n",
      "time cost repeat kv 0.11017584800720215 s\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "from typing import Optional\n",
    "import math\n",
    "from torch.nn.attention import sdpa_kernel, SDPBackend\n",
    "def scaled_dot_product_attention_with_pinned_memory(\n",
    "    query: torch.Tensor,\n",
    "    key: torch.Tensor, \n",
    "    value: torch.Tensor,\n",
    "    output_tensor: Optional[torch.Tensor] = None,\n",
    "    attn_mask: Optional[torch.Tensor] = None,\n",
    "    dropout_p: float = 0.0,\n",
    "    is_causal: bool = False,\n",
    "    scale: Optional[float] = None,\n",
    "    enable_gqa: bool = False\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    修改版的 scaled_dot_product_attention，支持将结果直接写入预分配的 pinned memory。\n",
    "    \n",
    "    Args:\n",
    "        query: Query tensor of shape (..., L, E)\n",
    "        key: Key tensor of shape (..., S, E) \n",
    "        value: Value tensor of shape (..., S, Ev)\n",
    "        output_tensor: 预分配的 pinned memory tensor，用于存储结果。如果为 None，则创建新的 tensor\n",
    "        attn_mask: 可选的注意力掩码\n",
    "        dropout_p: Dropout 概率\n",
    "        is_causal: 是否使用因果掩码\n",
    "        scale: 缩放因子，如果为 None 则使用 1/sqrt(E)\n",
    "        enable_gqa: 是否启用分组查询注意力\n",
    "        \n",
    "    Returns:\n",
    "        注意力输出 tensor，如果提供了 output_tensor 则返回该 tensor\n",
    "    \"\"\"\n",
    "    L, S = query.size(-2), key.size(-2)\n",
    "    scale_factor = 1 / math.sqrt(query.size(-1)) if scale is None else scale\n",
    "    \n",
    "    # 计算输出形状\n",
    "    output_shape = query.shape[:-1] + (value.shape[-1],)\n",
    "    \n",
    "    # 如果提供了预分配的输出 tensor，验证其形状和类型\n",
    "    if output_tensor is not None:\n",
    "        # 注意：对于 GPU tensor，即使原始是 pinned memory，移动到 GPU 后也不再是 pinned\n",
    "        if output_tensor.shape != output_shape:\n",
    "            raise ValueError(f\"output_tensor 形状 {output_tensor.shape} 与期望形状 {output_shape} 不匹配\")\n",
    "        if output_tensor.dtype != query.dtype:\n",
    "            raise ValueError(f\"output_tensor 数据类型 {output_tensor.dtype} 与 query 数据类型 {query.dtype} 不匹配\")\n",
    "        if output_tensor.device != query.device:\n",
    "            raise ValueError(f\"output_tensor 设备 {output_tensor.device} 与 query 设备 {query.device} 不匹配\")\n",
    "    \n",
    "    # 创建注意力偏置\n",
    "    attn_bias = torch.zeros(L, S, dtype=query.dtype, device=query.device)\n",
    "    \n",
    "    if is_causal:\n",
    "        assert attn_mask is None, \"is_causal 和 attn_mask 不能同时使用\"\n",
    "        temp_mask = torch.ones(L, S, dtype=torch.bool, device=query.device).tril(diagonal=0)\n",
    "        attn_bias.masked_fill_(temp_mask.logical_not(), float(\"-inf\"))\n",
    "        attn_bias = attn_bias.to(query.dtype)\n",
    "\n",
    "    if attn_mask is not None:\n",
    "        if attn_mask.dtype == torch.bool:\n",
    "            attn_bias.masked_fill_(attn_mask.logical_not(), float(\"-inf\"))\n",
    "        else:\n",
    "            attn_bias += attn_mask\n",
    "\n",
    "    # 处理分组查询注意力\n",
    "    if enable_gqa:\n",
    "        key = key.repeat_interleave(query.size(-3)//key.size(-3), -3)\n",
    "        value = value.repeat_interleave(query.size(-3)//value.size(-3), -3)\n",
    "\n",
    "    # 计算注意力权重\n",
    "    attn_weight = query @ key.transpose(-2, -1) * scale_factor\n",
    "    attn_weight += attn_bias\n",
    "    attn_weight = torch.softmax(attn_weight, dim=-1)\n",
    "    \n",
    "    # 应用 dropout\n",
    "    if dropout_p > 0.0:\n",
    "        attn_weight = torch.dropout(attn_weight, dropout_p, train=True)\n",
    "    \n",
    "    # 计算最终输出\n",
    "    if output_tensor is not None:\n",
    "        # 直接写入预分配的 pinned memory\n",
    "        torch.matmul(attn_weight, value, out=output_tensor)\n",
    "        return output_tensor\n",
    "    else:\n",
    "        # 创建新的 tensor\n",
    "        return attn_weight @ value\n",
    "\n",
    "batch_size = 90\n",
    "device=\"cuda:1\"\n",
    "query_states = torch.randn(batch_size, 32, 1, 128, dtype=torch.bfloat16, device=\"cpu\").pin_memory()\n",
    "# 480MB, *4 = 1920MB\n",
    "key_states = torch.randn(batch_size, 8, 512, 128, dtype=torch.bfloat16, device=\"cpu\").pin_memory()\n",
    "value_states = torch.randn(batch_size, 8, 512, 128, dtype=torch.bfloat16, device=\"cpu\").pin_memory()\n",
    "\n",
    "hidden_states_cache = torch.zeros(batch_size, 8, 4, 512, 128, dtype=torch.bfloat16, device=\"cpu\").pin_memory()\n",
    "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n",
    "    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n",
    "    \"\"\"\n",
    "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
    "    if n_rep == 1: \n",
    "        return hidden_states\n",
    "    # 改为将hidden_states 复制到 hidden_states_cache 上\n",
    "    # for i in range(hidden_states_cache.shape[2]):\n",
    "        # 这行代码的作用是：将 hidden_states 的内容复制到 hidden_states_cache 的第 i 个位置（第三个维度）。\n",
    "        # 也就是说，对于 hidden_states_cache 的每个“副本”维度 i，都用同样的 hidden_states 填充，实现了在第3维上重复 hidden_states 的效果。\n",
    "        # hidden_states_cache[:, :, i, :, :].index_copy_(\n",
    "        #     2, torch.arange(hidden_states.shape[2]), hidden_states\n",
    "        # )\n",
    "        # hidden_states_cache[:, :, i, :, :] = hidden_states\n",
    "        # hidden_states_cache[:, :, i, :, :].index_copy_(3, torch.arange(hidden_states.shape[3]), hidden_states)\n",
    "    hidden_states_cache = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
    "    # hidden_states_cache = hidden_states.unsqueeze(2).expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
    "    return hidden_states_cache.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n",
    "print(f\"key_states {key_states.device}\")\n",
    "time_start = time.time()\n",
    "torch.cuda.nvtx.range_push(\"repeat kv\")\n",
    "new_key_states = repeat_kv(key_states, 4)\n",
    "new_value_states = repeat_kv(value_states, 4)\n",
    "torch.cuda.nvtx.range_pop()\n",
    "\n",
    "# if torch.isnan(key_states).any():\n",
    "#     print(\"NaN detected in key_states\")\n",
    "# if torch.isnan(value_states).any():\n",
    "#     print(\"NaN detected in value_states\")\n",
    "# torch.cuda.synchronize()\n",
    "print(f\"time cost repeat kv {time.time() - time_start} s\")\n",
    "\n",
    "attn_output_c = torch.randn(batch_size, 32, 1,  128, dtype=torch.bfloat16, device=\"cpu\")\n",
    "\n",
    "# torch.cuda.nvtx.range_push(\"move same attn_output\")\n",
    "# time_start = time.time()\n",
    "# attn_output_c = attn_output_c.pin_memory()\n",
    "# ag = attn_output_c.to(device=\"cuda:1\")\n",
    "# print(f\"move same attn_output time cost {time.time()-time_start} s\")\n",
    "# torch.cuda.nvtx.range_pop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7addc876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dot attn cost 0.005899 seconds\n",
      "torch.Size([90, 32, 1, 128])\n",
      "dot attn pin cost 0.006668 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.cuda.nvtx.range_push(\"dot attn\")\n",
    "time_start = time.time()\n",
    "# with sdpa_kernel([SDPBackend.CUDNN_ATTENTION, SDPBackend.MATH, SDPBackend.EFFICIENT_ATTENTION, SDPBackend.FLASH_ATTENTION]):\n",
    "attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
    "    query_states,\n",
    "    new_key_states,\n",
    "    new_value_states,\n",
    "    attn_mask=None,\n",
    "    dropout_p=0.0,\n",
    "    enable_gqa = False,\n",
    "    # The q_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create a causal mask in case q_len == 1.\n",
    "    is_causal=False,\n",
    ")\n",
    "# torch.cuda.synchronize()\n",
    "print(f\"dot attn cost {time.time()-time_start:.6f} seconds\")\n",
    "# print(attn_output.shape)\n",
    "# torch.cuda.nvtx.range_push(\"clarify to pin\")\n",
    "# attn_output_pin = attn_output.pin_memory()\n",
    "# torch.cuda.nvtx.range_pop()\n",
    "# torch.cuda.nvtx.range_push(\"move\")\n",
    "# attn_output_gpu = attn_output_pin.to(device=\"cuda:1\")\n",
    "# torch.cuda.nvtx.range_pop()\n",
    "# torch.cuda.nvtx.range_pop()\n",
    "# print(f\"dot attn and move cost {time.time()-time_start:.6f} seconds\")\n",
    "\n",
    "attn_output_c = torch.empty_like(query_states, device=\"cpu\", dtype=torch.bfloat16, pin_memory=True)\n",
    "torch.cuda.nvtx.range_push(\"dot attn pin\")\n",
    "def func_pin():\n",
    "    time_start = time.time()\n",
    "    torch.cuda.nvtx.range_push(\"scaled dot product\")\n",
    "    attn_output = scaled_dot_product_attention_with_pinned_memory(\n",
    "        query_states,\n",
    "        new_key_states,\n",
    "        new_value_states,\n",
    "        output_tensor=attn_output_c,\n",
    "        attn_mask=None,\n",
    "        dropout_p=0.0,\n",
    "        # The q_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create a causal mask in case q_len == 1.\n",
    "        is_causal=True,\n",
    "    )\n",
    "    print(attn_output.shape)\n",
    "    attn_output.to(device=\"cuda:1\")\n",
    "    torch.cuda.nvtx.range_pop()\n",
    "    print(f\"dot attn pin cost {time.time()-time_start:.6f} seconds\")\n",
    "func_pin()\n",
    "torch.cuda.nvtx.range_pop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef05fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sllm_store.transformers import save_model\n",
    "\n",
    "# # Load a model from HuggingFace model hub.\n",
    "# import torch\n",
    "# from transformers import AutoModelForCausalLM\n",
    "# model = AutoModelForCausalLM.from_pretrained('/mnt/zhengcf3/models/deepseek-moe-16b-base', torch_dtype=torch.float16, trust_remote_code=True)\n",
    "\n",
    "# # Replace './models' with your local path.\n",
    "# save_model(model, '/mnt/zhengcf3/models/models/deepseek-moe-16b-base')\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "# Add the current directory to Python path\n",
    "sys.path.insert(0, '/mnt/zhengcf3/lpllm')\n",
    "from lpllm.lpllm import LPLLM\n",
    "from lpllm.StaticCacheLen import StaticCacheLen\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "from transformers.cache_utils import Cache, DynamicCache, StaticCache\n",
    "\n",
    "\n",
    "import time\n",
    "storage_path = \"/mnt/zhengcf3/models/models\"\n",
    "tdevice = \"cuda:1\"\n",
    "\n",
    "# model_name = \"deepseek-moe-16b-base\"\n",
    "model_name = \"Mixtral-8x7B\"\n",
    "model_path = f\"/mnt/zhengcf3/models/models/{model_name}\"\n",
    "\n",
    "\n",
    "lp = LPLLM(model_name, tdevice, storage_path, pool_size=8, chunk_size_mb=2048)\n",
    "config = lp.config\n",
    "\n",
    "time_start = time.time()\n",
    "lp.start_load_into_gpu(0)\n",
    "lp.wait_load_into_gpu(0)\n",
    "print(f\"load into gpu cost {time.time() - time_start} s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lpllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
