DEBUG 10-15 15:26:27 lpllm.py:267] other_tensor_data_index: {'model.embed_tokens.weight': (0, 262144000), 'model.norm.weight': (93143433216, 8192), 'lm_head.weight': (93143441408, 262144000)}
DEBUG 10-15 15:26:27 lpllm.py:275] cuda memory needed for all qkv related 2684878848, for single layer 2902540288
DEBUG 10-15 15:26:27 lpllm.py:282] cuda memory needed for all model 524296192
DEBUG 10-15 15:26:28 client.py:72] load_into_gpu: Mixtral-8x7B, 0eb6746c-70d2-48fa-b3c3-731ead327d07
INFO 10-15 15:26:28 client.py:113] Model loaded: Mixtral-8x7B, 0eb6746c-70d2-48fa-b3c3-731ead327d07
INFO 10-15 15:26:28 client.py:117] confirm_model_loaded: Mixtral-8x7B, 0eb6746c-70d2-48fa-b3c3-731ead327d07
INFO 10-15 15:26:28 client.py:125] Model loaded
here pin
INFO 10-15 15:26:28 pinpool.py:31] Initializing PinnedMemoryPool with 8GB total, allocating in 2048MB chunks...
DEBUG 10-15 15:26:28 pinpool.py:43] Allocated chunk 1: 1073741824 elements (2048.0 MB)
DEBUG 10-15 15:26:29 pinpool.py:43] Allocated chunk 2: 1073741824 elements (2048.0 MB)
DEBUG 10-15 15:26:30 pinpool.py:43] Allocated chunk 3: 1073741824 elements (2048.0 MB)
DEBUG 10-15 15:26:31 pinpool.py:43] Allocated chunk 4: 1073741824 elements (2048.0 MB)
INFO 10-15 15:26:31 pinpool.py:55] Successfully allocated 4 chunks, total 4294967296 elements (8192.0 MB) in 3.619s
DEBUG 10-15 15:26:31 lpllm.py:2206] GPU2CPU thread started, id 1297477
INFO 10-15 15:26:31 lpllm.py:2363] CPU compute thread started, id 1297478
DEBUG 10-15 15:26:31 lpllm.py:2483] CPU2GPU thread started, id 1297479
torch.Size([960, 512])
init kv cache time cost 11.54201054573059 s
当前程序内存占用: 76063.03 MB
DEBUG 10-15 15:26:43 lpllm.py:600] input_ids shape torch.Size([960, 512]) orig_shape for decoders one chunk: torch.Size([120, 512, 4096])
DEBUG 10-15 15:26:43 lpllm.py:601] chunk_batch_size: 60 hidden_states_chunk shape and value: torch.Size([60, 512, 4096])
DEBUG 10-15 15:26:43 lpllm.py:602] attention_mask shape and value: None
DEBUG 10-15 15:26:43 lpllm.py:848] hidden_states_chunks1 shape: torch.Size([60, 512, 4096]), hidden_states_chunks1 length: 1
DEBUG 10-15 15:26:43 lpllm.py:849] hidden_states_chunks2 shape: torch.Size([60, 512, 4096]), hidden_states_chunks2 length: 1
DEBUG 10-15 15:26:43 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:26:43 client.py:72] load_into_gpu: Mixtral-8x7B, 813e26c9-a539-4953-ac74-b9b2d9009039
INFO 10-15 15:26:43 client.py:113] Model loaded: Mixtral-8x7B, 813e26c9-a539-4953-ac74-b9b2d9009039
DEBUG 10-15 15:26:43 lpllm.py:1743] restore layer func cost 0.0010838508605957031 s
INFO 10-15 15:26:43 client.py:117] confirm_model_loaded: Mixtral-8x7B, 813e26c9-a539-4953-ac74-b9b2d9009039
INFO 10-15 15:26:43 client.py:125] Model loaded
DEBUG 10-15 15:26:43 lpllm.py:422] prepare layer cost 0.2740199565887451 s
DEBUG 10-15 15:26:43 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed650>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x003\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x96\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:26:43 client.py:72] load_into_gpu: Mixtral-8x7B, 491ecbfe-63ec-44db-8834-17ec9754f5b6
INFO 10-15 15:26:43 client.py:113] Model loaded: Mixtral-8x7B, 491ecbfe-63ec-44db-8834-17ec9754f5b6
DEBUG 10-15 15:26:43 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:26:43 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:26:43 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:26:44 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:26:44 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:26:44 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:44 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:44 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:44 lpmodule.py:506]         ...,
DEBUG 10-15 15:26:44 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:44 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:44 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:26:44 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:26:44 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:26:44 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:26:44 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:26:44 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:44 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:44 lpllm.py:924] 
DEBUG 10-15 15:26:44 lpllm.py:924] decoder loop j: 1 cur_layer_idx: 0 layer_attn_idx: 0 layer_mlp_idx: 0
DEBUG 10-15 15:26:44 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:26:44 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:26:44 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:26:44 lpllm.py:2265] GPU2CPU move cost 0.001409 seconds
DEBUG 10-15 15:26:44 lpmodule.py:340] layer_idx 0 attn layer_idx: 0
DEBUG 10-15 15:26:44 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:26:44 lpmodule.py:364] decoder_attn_batch update batch_dim 0-60
DEBUG 10-15 15:26:44 StaticCacheLen.py:84] static cache update layer_idx: 0, start_batch: 0, end_batch: 60
DEBUG 10-15 15:26:44 lpmodule.py:368] update for kv cache 0-60 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:26:44 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:26:44 lpmodule.py:374] update past key value cost 0.023816 seconds
DEBUG 10-15 15:26:44 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:26:44 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:26:44 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:26:44 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:44 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:44 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:44 lpmodule.py:506]         ...,
DEBUG 10-15 15:26:44 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:44 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:44 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:26:44 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:26:44 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:26:44 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:26:44 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:26:44 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:44 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:44 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:26:44 lpmodule.py:399] repeat qkv cost 0.041507 seconds
DEBUG 10-15 15:26:44 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:26:44 lpmodule.py:433] dot attn cost 0.047044 seconds
DEBUG 10-15 15:26:44 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:26:44 lpmodule.py:444] time cost move to cuda:1 0.13675737380981445 s
DEBUG 10-15 15:26:44 lpllm.py:2283] CPU attn cost 0.280426 seconds if batch True
DEBUG 10-15 15:26:44 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:26:44 lpllm.py:2294] CPU compute cost 0.282138 seconds
DEBUG 10-15 15:26:44 lpllm.py:2312] free cost 0.000290 seconds
DEBUG 10-15 15:26:44 lpllm.py:2265] GPU2CPU move cost 0.000647 seconds
DEBUG 10-15 15:26:44 lpmodule.py:340] layer_idx 0 attn layer_idx: 0
DEBUG 10-15 15:26:44 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:26:44 lpmodule.py:364] decoder_attn_batch update batch_dim 60-120
DEBUG 10-15 15:26:44 StaticCacheLen.py:84] static cache update layer_idx: 0, start_batch: 60, end_batch: 120
DEBUG 10-15 15:26:44 lpmodule.py:368] update for kv cache 60-120 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:26:44 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:26:44 lpmodule.py:374] update past key value cost 0.004990 seconds
DEBUG 10-15 15:26:44 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:26:44 lpmodule.py:227] layer idx 0
DEBUG 10-15 15:26:44 lpmodule.py:399] repeat qkv cost 0.035924 seconds
DEBUG 10-15 15:26:44 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:26:44 lpmodule.py:433] dot attn cost 0.034828 seconds
DEBUG 10-15 15:26:44 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:26:44 lpmodule.py:444] time cost move to cuda:1 0.033167123794555664 s
DEBUG 10-15 15:26:44 lpllm.py:2283] CPU attn cost 0.150235 seconds if batch True
DEBUG 10-15 15:26:44 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:26:44 lpllm.py:2294] CPU compute cost 0.151138 seconds
DEBUG 10-15 15:26:44 lpllm.py:2312] free cost 0.000108 seconds
DEBUG 10-15 15:26:45 lpllm.py:498] reset update attn
DEBUG 10-15 15:26:45 lpllm.py:1774] update state cost 3.552436828613281e-05 s
DEBUG 10-15 15:26:45 lpllm.py:1743] restore layer func cost 0.00040268898010253906 s
DEBUG 10-15 15:26:45 lpllm.py:511] restore layer cost 0.0006749629974365234 s
DEBUG 10-15 15:26:45 lpllm.py:512] reset_next_layer_need: layer_attn_idx=1, layer_mlp_idx=0, j_loc=1
DEBUG 10-15 15:26:45 lpllm.py:1037] reset layer cost 0.0007483959197998047 s
DEBUG 10-15 15:26:45 lpllm.py:1038] have reset next layer layer_attn 1 layer_mlp 0 
DEBUG 10-15 15:26:45 lpllm.py:1044] j: 1 waiting the layer with layer_idx 1 before wait time 1.2423577308654785 s
INFO 10-15 15:26:45 client.py:117] confirm_model_loaded: Mixtral-8x7B, 491ecbfe-63ec-44db-8834-17ec9754f5b6
INFO 10-15 15:26:45 client.py:125] Model loaded
DEBUG 10-15 15:26:45 lpllm.py:1048] j: load cost 1.2446208000183105 s waiting cost 0.002247333526611328 s
DEBUG 10-15 15:26:45 lpllm.py:924] 
DEBUG 10-15 15:26:45 lpllm.py:924] decoder loop j: 2 cur_layer_idx: 1 layer_attn_idx: 1 layer_mlp_idx: 0
DEBUG 10-15 15:26:45 lpllm.py:933] start load next layer cur_layer_idx: 2
DEBUG 10-15 15:26:45 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed6e0>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x004\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x98\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:26:45 client.py:72] load_into_gpu: Mixtral-8x7B, 7703ef41-9f45-4ad6-8fdc-4225c21f81f8
INFO 10-15 15:26:45 client.py:113] Model loaded: Mixtral-8x7B, 7703ef41-9f45-4ad6-8fdc-4225c21f81f8
DEBUG 10-15 15:26:45 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:26:45 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:26:45 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:26:45 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:26:45 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:26:45 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:45 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:45 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:45 lpmodule.py:506]         ...,
DEBUG 10-15 15:26:45 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:45 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:45 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:26:45 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:26:45 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:26:45 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:26:45 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:26:45 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:45 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:45 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:26:45 lpllm.py:2265] GPU2CPU move cost 0.000648 seconds
DEBUG 10-15 15:26:45 lpmodule.py:340] layer_idx 1 attn layer_idx: 1
DEBUG 10-15 15:26:45 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:26:45 lpmodule.py:364] decoder_attn_batch update batch_dim 0-60
DEBUG 10-15 15:26:45 StaticCacheLen.py:84] static cache update layer_idx: 1, start_batch: 0, end_batch: 60
DEBUG 10-15 15:26:45 lpmodule.py:368] update for kv cache 0-60 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:26:45 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:26:45 lpmodule.py:374] update past key value cost 0.006396 seconds
DEBUG 10-15 15:26:45 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:26:45 lpmodule.py:227] layer idx 0
DEBUG 10-15 15:26:45 lpmodule.py:399] repeat qkv cost 0.033304 seconds
DEBUG 10-15 15:26:45 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:26:45 lpmodule.py:433] dot attn cost 0.033947 seconds
DEBUG 10-15 15:26:45 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:26:45 lpmodule.py:444] time cost move to cuda:1 0.026479244232177734 s
DEBUG 10-15 15:26:45 lpllm.py:2283] CPU attn cost 0.137515 seconds if batch True
DEBUG 10-15 15:26:45 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:26:45 lpllm.py:2294] CPU compute cost 0.138454 seconds
DEBUG 10-15 15:26:45 lpllm.py:2312] free cost 0.000090 seconds
DEBUG 10-15 15:26:45 lpllm.py:503] reset update experts
DEBUG 10-15 15:26:45 lpllm.py:1774] update state cost 1.811981201171875e-05 s
DEBUG 10-15 15:26:45 lpllm.py:1743] restore layer func cost 0.0008780956268310547 s
DEBUG 10-15 15:26:45 lpllm.py:511] restore layer cost 0.0011348724365234375 s
DEBUG 10-15 15:26:45 lpllm.py:512] reset_next_layer_need: layer_attn_idx=1, layer_mlp_idx=1, j_loc=2
DEBUG 10-15 15:26:45 lpllm.py:1037] reset layer cost 0.0012056827545166016 s
DEBUG 10-15 15:26:45 lpllm.py:1038] have reset next layer layer_attn 1 layer_mlp 1 
DEBUG 10-15 15:26:45 lpllm.py:924] 
DEBUG 10-15 15:26:45 lpllm.py:924] decoder loop j: 3 cur_layer_idx: 1 layer_attn_idx: 1 layer_mlp_idx: 1
DEBUG 10-15 15:26:45 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:26:45 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:26:45 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:26:45 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:26:45 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:26:45 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:45 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:45 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:45 lpmodule.py:506]         ...,
DEBUG 10-15 15:26:45 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:45 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:45 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:26:45 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:26:45 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:26:45 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:26:45 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:26:45 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:45 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:45 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:26:45 lpllm.py:2265] GPU2CPU move cost 0.000386 seconds
DEBUG 10-15 15:26:45 lpmodule.py:340] layer_idx 1 attn layer_idx: 1
DEBUG 10-15 15:26:45 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:26:45 lpmodule.py:364] decoder_attn_batch update batch_dim 60-120
DEBUG 10-15 15:26:45 StaticCacheLen.py:84] static cache update layer_idx: 1, start_batch: 60, end_batch: 120
DEBUG 10-15 15:26:45 lpmodule.py:368] update for kv cache 60-120 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:26:45 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:26:45 lpmodule.py:374] update past key value cost 0.005164 seconds
DEBUG 10-15 15:26:45 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:26:45 lpmodule.py:227] layer idx 1
DEBUG 10-15 15:26:45 lpmodule.py:399] repeat qkv cost 0.035279 seconds
DEBUG 10-15 15:26:45 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:26:45 lpmodule.py:433] dot attn cost 0.034374 seconds
DEBUG 10-15 15:26:45 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:26:45 lpmodule.py:444] time cost move to cuda:1 0.027744770050048828 s
DEBUG 10-15 15:26:45 lpllm.py:2283] CPU attn cost 0.143122 seconds if batch True
DEBUG 10-15 15:26:45 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:26:45 lpllm.py:2294] CPU compute cost 0.143793 seconds
DEBUG 10-15 15:26:45 lpllm.py:2312] free cost 0.000096 seconds
DEBUG 10-15 15:26:45 lpllm.py:498] reset update attn
DEBUG 10-15 15:26:45 lpllm.py:1774] update state cost 1.9073486328125e-05 s
DEBUG 10-15 15:26:45 lpllm.py:1743] restore layer func cost 0.00039315223693847656 s
DEBUG 10-15 15:26:45 lpllm.py:511] restore layer cost 0.0006449222564697266 s
DEBUG 10-15 15:26:45 lpllm.py:512] reset_next_layer_need: layer_attn_idx=2, layer_mlp_idx=1, j_loc=3
DEBUG 10-15 15:26:45 lpllm.py:1037] reset layer cost 0.0007157325744628906 s
DEBUG 10-15 15:26:45 lpllm.py:1038] have reset next layer layer_attn 2 layer_mlp 1 
DEBUG 10-15 15:26:45 lpllm.py:1044] j: 3 waiting the layer with layer_idx 2 before wait time 0.44008922576904297 s
INFO 10-15 15:26:45 client.py:117] confirm_model_loaded: Mixtral-8x7B, 7703ef41-9f45-4ad6-8fdc-4225c21f81f8
INFO 10-15 15:26:45 client.py:125] Model loaded
DEBUG 10-15 15:26:45 lpllm.py:1048] j: load cost 0.4415261745452881 s waiting cost 0.0014204978942871094 s
DEBUG 10-15 15:26:45 lpllm.py:924] 
DEBUG 10-15 15:26:45 lpllm.py:924] decoder loop j: 4 cur_layer_idx: 2 layer_attn_idx: 2 layer_mlp_idx: 1
DEBUG 10-15 15:26:45 lpllm.py:933] start load next layer cur_layer_idx: 3
DEBUG 10-15 15:26:45 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:26:45 client.py:72] load_into_gpu: Mixtral-8x7B, 0bfcc394-5f24-4677-8c9f-f4007b89eadd
INFO 10-15 15:26:45 client.py:113] Model loaded: Mixtral-8x7B, 0bfcc394-5f24-4677-8c9f-f4007b89eadd
DEBUG 10-15 15:26:45 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:26:45 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:26:45 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:26:45 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:26:45 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:26:45 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:45 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:45 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:45 lpmodule.py:506]         ...,
DEBUG 10-15 15:26:45 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:45 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:45 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:26:45 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:26:45 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:26:45 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:26:45 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:26:45 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:45 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:45 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:26:45 lpllm.py:2265] GPU2CPU move cost 0.000501 seconds
DEBUG 10-15 15:26:45 lpmodule.py:340] layer_idx 2 attn layer_idx: 2
DEBUG 10-15 15:26:45 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:26:45 lpmodule.py:364] decoder_attn_batch update batch_dim 0-60
DEBUG 10-15 15:26:45 StaticCacheLen.py:84] static cache update layer_idx: 2, start_batch: 0, end_batch: 60
DEBUG 10-15 15:26:45 lpmodule.py:368] update for kv cache 0-60 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:26:45 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:26:45 lpmodule.py:374] update past key value cost 0.005832 seconds
DEBUG 10-15 15:26:45 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:26:45 lpmodule.py:227] layer idx 1
DEBUG 10-15 15:26:45 lpmodule.py:399] repeat qkv cost 0.036942 seconds
DEBUG 10-15 15:26:45 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:26:45 lpmodule.py:433] dot attn cost 0.034794 seconds
DEBUG 10-15 15:26:45 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:26:45 lpmodule.py:444] time cost move to cuda:1 0.027023792266845703 s
DEBUG 10-15 15:26:45 lpllm.py:2283] CPU attn cost 0.144384 seconds if batch True
DEBUG 10-15 15:26:45 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:26:45 lpllm.py:2294] CPU compute cost 0.145229 seconds
DEBUG 10-15 15:26:45 lpllm.py:2312] free cost 0.000092 seconds
DEBUG 10-15 15:26:45 lpllm.py:503] reset update experts
DEBUG 10-15 15:26:45 lpllm.py:1774] update state cost 3.266334533691406e-05 s
DEBUG 10-15 15:26:45 lpllm.py:1743] restore layer func cost 0.0008804798126220703 s
DEBUG 10-15 15:26:45 lpllm.py:511] restore layer cost 0.0011365413665771484 s
DEBUG 10-15 15:26:45 lpllm.py:512] reset_next_layer_need: layer_attn_idx=2, layer_mlp_idx=2, j_loc=4
DEBUG 10-15 15:26:45 lpllm.py:1037] reset layer cost 0.0012087821960449219 s
DEBUG 10-15 15:26:45 lpllm.py:1038] have reset next layer layer_attn 2 layer_mlp 2 
DEBUG 10-15 15:26:45 lpllm.py:924] 
DEBUG 10-15 15:26:45 lpllm.py:924] decoder loop j: 5 cur_layer_idx: 2 layer_attn_idx: 2 layer_mlp_idx: 2
DEBUG 10-15 15:26:45 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:26:45 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:26:45 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:26:45 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:26:45 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:26:45 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:45 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:45 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:45 lpmodule.py:506]         ...,
DEBUG 10-15 15:26:45 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:45 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:45 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:26:45 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:26:45 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:26:45 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:26:45 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:26:45 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:45 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:45 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:26:45 lpllm.py:2265] GPU2CPU move cost 0.000492 seconds
DEBUG 10-15 15:26:45 lpmodule.py:340] layer_idx 2 attn layer_idx: 2
DEBUG 10-15 15:26:45 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:26:45 lpmodule.py:364] decoder_attn_batch update batch_dim 60-120
DEBUG 10-15 15:26:45 StaticCacheLen.py:84] static cache update layer_idx: 2, start_batch: 60, end_batch: 120
DEBUG 10-15 15:26:45 lpmodule.py:368] update for kv cache 60-120 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:26:45 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:26:45 lpmodule.py:374] update past key value cost 0.005146 seconds
DEBUG 10-15 15:26:45 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:26:45 lpmodule.py:399] repeat qkv cost 0.032388 seconds
DEBUG 10-15 15:26:45 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:26:45 lpmodule.py:227] layer idx 2
DEBUG 10-15 15:26:45 lpmodule.py:433] dot attn cost 0.038892 seconds
DEBUG 10-15 15:26:45 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:26:45 lpmodule.py:444] time cost move to cuda:1 0.027805805206298828 s
DEBUG 10-15 15:26:45 lpllm.py:2283] CPU attn cost 0.144416 seconds if batch True
DEBUG 10-15 15:26:45 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:26:45 lpllm.py:2294] CPU compute cost 0.145196 seconds
DEBUG 10-15 15:26:45 lpllm.py:2312] free cost 0.000120 seconds
DEBUG 10-15 15:26:45 lpllm.py:498] reset update attn
DEBUG 10-15 15:26:45 lpllm.py:1774] update state cost 3.552436828613281e-05 s
DEBUG 10-15 15:26:45 lpllm.py:1743] restore layer func cost 0.00038504600524902344 s
DEBUG 10-15 15:26:45 lpllm.py:511] restore layer cost 0.0006480216979980469 s
DEBUG 10-15 15:26:45 lpllm.py:512] reset_next_layer_need: layer_attn_idx=3, layer_mlp_idx=2, j_loc=5
DEBUG 10-15 15:26:45 lpllm.py:1037] reset layer cost 0.0007190704345703125 s
DEBUG 10-15 15:26:45 lpllm.py:1038] have reset next layer layer_attn 3 layer_mlp 2 
DEBUG 10-15 15:26:45 lpllm.py:1044] j: 5 waiting the layer with layer_idx 3 before wait time 0.4433445930480957 s
INFO 10-15 15:26:45 client.py:117] confirm_model_loaded: Mixtral-8x7B, 0bfcc394-5f24-4677-8c9f-f4007b89eadd
INFO 10-15 15:26:45 client.py:125] Model loaded
DEBUG 10-15 15:26:45 lpllm.py:1048] j: load cost 0.4450104236602783 s waiting cost 0.001649618148803711 s
DEBUG 10-15 15:26:45 lpllm.py:924] 
DEBUG 10-15 15:26:45 lpllm.py:924] decoder loop j: 6 cur_layer_idx: 3 layer_attn_idx: 3 layer_mlp_idx: 2
DEBUG 10-15 15:26:45 lpllm.py:933] start load next layer cur_layer_idx: 4
DEBUG 10-15 15:26:45 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed650>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x003\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x96\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:26:45 client.py:72] load_into_gpu: Mixtral-8x7B, 7140bcf2-653a-4d74-8d8c-a6c05abc42d0
INFO 10-15 15:26:45 client.py:113] Model loaded: Mixtral-8x7B, 7140bcf2-653a-4d74-8d8c-a6c05abc42d0
DEBUG 10-15 15:26:45 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:26:45 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:26:45 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:26:45 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:26:45 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:26:45 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:45 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:45 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:45 lpmodule.py:506]         ...,
DEBUG 10-15 15:26:45 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:45 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:45 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:26:45 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:26:45 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:26:45 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:26:45 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:26:45 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:45 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:45 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:26:45 lpllm.py:2265] GPU2CPU move cost 0.000601 seconds
DEBUG 10-15 15:26:45 lpmodule.py:340] layer_idx 3 attn layer_idx: 3
DEBUG 10-15 15:26:45 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:26:45 lpmodule.py:364] decoder_attn_batch update batch_dim 0-60
DEBUG 10-15 15:26:45 StaticCacheLen.py:84] static cache update layer_idx: 3, start_batch: 0, end_batch: 60
DEBUG 10-15 15:26:45 lpmodule.py:368] update for kv cache 0-60 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:26:45 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:26:45 lpmodule.py:374] update past key value cost 0.006444 seconds
DEBUG 10-15 15:26:45 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:26:45 lpmodule.py:227] layer idx 2
DEBUG 10-15 15:26:45 lpmodule.py:399] repeat qkv cost 0.033524 seconds
DEBUG 10-15 15:26:45 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:26:46 lpmodule.py:433] dot attn cost 0.055538 seconds
DEBUG 10-15 15:26:46 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:26:46 lpmodule.py:444] time cost move to cuda:1 0.02736186981201172 s
DEBUG 10-15 15:26:46 lpllm.py:2283] CPU attn cost 0.161019 seconds if batch True
DEBUG 10-15 15:26:46 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:26:46 lpllm.py:2294] CPU compute cost 0.161927 seconds
DEBUG 10-15 15:26:46 lpllm.py:2312] free cost 0.000090 seconds
DEBUG 10-15 15:26:46 lpllm.py:503] reset update experts
DEBUG 10-15 15:26:46 lpllm.py:1774] update state cost 1.7404556274414062e-05 s
DEBUG 10-15 15:26:46 lpllm.py:1743] restore layer func cost 0.0008616447448730469 s
DEBUG 10-15 15:26:46 lpllm.py:511] restore layer cost 0.0011010169982910156 s
DEBUG 10-15 15:26:46 lpllm.py:512] reset_next_layer_need: layer_attn_idx=3, layer_mlp_idx=3, j_loc=6
DEBUG 10-15 15:26:46 lpllm.py:1037] reset layer cost 0.001177072525024414 s
DEBUG 10-15 15:26:46 lpllm.py:1038] have reset next layer layer_attn 3 layer_mlp 3 
DEBUG 10-15 15:26:46 lpllm.py:924] 
DEBUG 10-15 15:26:46 lpllm.py:924] decoder loop j: 7 cur_layer_idx: 3 layer_attn_idx: 3 layer_mlp_idx: 3
DEBUG 10-15 15:26:46 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:26:46 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:26:46 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:26:46 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:26:46 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:26:46 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:46 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:46 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:46 lpmodule.py:506]         ...,
DEBUG 10-15 15:26:46 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:46 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:46 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:26:46 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:26:46 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:26:46 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:26:46 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:26:46 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:46 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:46 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:26:46 lpllm.py:2265] GPU2CPU move cost 0.000690 seconds
DEBUG 10-15 15:26:46 lpmodule.py:340] layer_idx 3 attn layer_idx: 3
DEBUG 10-15 15:26:46 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:26:46 lpmodule.py:364] decoder_attn_batch update batch_dim 60-120
DEBUG 10-15 15:26:46 StaticCacheLen.py:84] static cache update layer_idx: 3, start_batch: 60, end_batch: 120
DEBUG 10-15 15:26:46 lpmodule.py:368] update for kv cache 60-120 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:26:46 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:26:46 lpmodule.py:374] update past key value cost 0.006593 seconds
DEBUG 10-15 15:26:46 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:26:46 lpmodule.py:399] repeat qkv cost 0.035073 seconds
DEBUG 10-15 15:26:46 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:26:46 lpmodule.py:227] layer idx 3
DEBUG 10-15 15:26:46 lpmodule.py:433] dot attn cost 0.034669 seconds
DEBUG 10-15 15:26:46 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:26:46 lpmodule.py:444] time cost move to cuda:1 0.027144432067871094 s
DEBUG 10-15 15:26:46 lpllm.py:2283] CPU attn cost 0.141491 seconds if batch True
DEBUG 10-15 15:26:46 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:26:46 lpllm.py:2294] CPU compute cost 0.142507 seconds
DEBUG 10-15 15:26:46 lpllm.py:2312] free cost 0.000093 seconds
DEBUG 10-15 15:26:46 lpllm.py:498] reset update attn
DEBUG 10-15 15:26:46 lpllm.py:1774] update state cost 3.528594970703125e-05 s
DEBUG 10-15 15:26:46 lpllm.py:1743] restore layer func cost 0.00038743019104003906 s
DEBUG 10-15 15:26:46 lpllm.py:511] restore layer cost 0.0006458759307861328 s
DEBUG 10-15 15:26:46 lpllm.py:512] reset_next_layer_need: layer_attn_idx=4, layer_mlp_idx=3, j_loc=7
DEBUG 10-15 15:26:46 lpllm.py:1037] reset layer cost 0.0007178783416748047 s
DEBUG 10-15 15:26:46 lpllm.py:1038] have reset next layer layer_attn 4 layer_mlp 3 
DEBUG 10-15 15:26:46 lpllm.py:1044] j: 7 waiting the layer with layer_idx 4 before wait time 0.5549640655517578 s
INFO 10-15 15:26:46 client.py:117] confirm_model_loaded: Mixtral-8x7B, 7140bcf2-653a-4d74-8d8c-a6c05abc42d0
INFO 10-15 15:26:46 client.py:125] Model loaded
DEBUG 10-15 15:26:46 lpllm.py:1048] j: load cost 0.5566980838775635 s waiting cost 0.0017173290252685547 s
DEBUG 10-15 15:26:46 lpllm.py:924] 
DEBUG 10-15 15:26:46 lpllm.py:924] decoder loop j: 8 cur_layer_idx: 4 layer_attn_idx: 4 layer_mlp_idx: 3
DEBUG 10-15 15:26:46 lpllm.py:933] start load next layer cur_layer_idx: 5
DEBUG 10-15 15:26:46 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed6e0>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x004\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x98\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:26:46 client.py:72] load_into_gpu: Mixtral-8x7B, 5ce4d1c8-6445-4fa0-8b2e-203d59335802
INFO 10-15 15:26:46 client.py:113] Model loaded: Mixtral-8x7B, 5ce4d1c8-6445-4fa0-8b2e-203d59335802
DEBUG 10-15 15:26:46 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:26:46 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:26:46 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:26:46 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:26:46 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:26:46 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:46 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:46 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:46 lpmodule.py:506]         ...,
DEBUG 10-15 15:26:46 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:46 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:46 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:26:46 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:26:46 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:26:46 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:26:46 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:26:46 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:46 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:46 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:26:46 lpllm.py:2265] GPU2CPU move cost 0.000409 seconds
DEBUG 10-15 15:26:46 lpmodule.py:340] layer_idx 4 attn layer_idx: 4
DEBUG 10-15 15:26:46 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:26:46 lpmodule.py:364] decoder_attn_batch update batch_dim 0-60
DEBUG 10-15 15:26:46 StaticCacheLen.py:84] static cache update layer_idx: 4, start_batch: 0, end_batch: 60
DEBUG 10-15 15:26:46 lpmodule.py:368] update for kv cache 0-60 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:26:46 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:26:46 lpmodule.py:374] update past key value cost 0.005374 seconds
DEBUG 10-15 15:26:46 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:26:46 lpmodule.py:227] layer idx 3
DEBUG 10-15 15:26:46 lpmodule.py:399] repeat qkv cost 0.035055 seconds
DEBUG 10-15 15:26:46 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:26:46 lpmodule.py:433] dot attn cost 0.049310 seconds
DEBUG 10-15 15:26:46 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:26:46 lpmodule.py:444] time cost move to cuda:1 0.026973485946655273 s
DEBUG 10-15 15:26:46 lpllm.py:2283] CPU attn cost 0.154956 seconds if batch True
DEBUG 10-15 15:26:46 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:26:46 lpllm.py:2294] CPU compute cost 0.155675 seconds
DEBUG 10-15 15:26:46 lpllm.py:2312] free cost 0.000095 seconds
DEBUG 10-15 15:26:46 lpllm.py:503] reset update experts
DEBUG 10-15 15:26:46 lpllm.py:1774] update state cost 3.504753112792969e-05 s
DEBUG 10-15 15:26:46 lpllm.py:1743] restore layer func cost 0.0008475780487060547 s
DEBUG 10-15 15:26:46 lpllm.py:511] restore layer cost 0.0010912418365478516 s
DEBUG 10-15 15:26:46 lpllm.py:512] reset_next_layer_need: layer_attn_idx=4, layer_mlp_idx=4, j_loc=8
DEBUG 10-15 15:26:46 lpllm.py:1037] reset layer cost 0.0011632442474365234 s
DEBUG 10-15 15:26:46 lpllm.py:1038] have reset next layer layer_attn 4 layer_mlp 4 
DEBUG 10-15 15:26:46 lpllm.py:924] 
DEBUG 10-15 15:26:46 lpllm.py:924] decoder loop j: 9 cur_layer_idx: 4 layer_attn_idx: 4 layer_mlp_idx: 4
DEBUG 10-15 15:26:46 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:26:46 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:26:46 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:26:46 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:26:46 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:26:46 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:46 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:46 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:46 lpmodule.py:506]         ...,
DEBUG 10-15 15:26:46 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:46 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:46 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:26:46 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:26:46 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:26:46 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:26:46 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:26:46 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:46 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:46 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:26:46 lpllm.py:2265] GPU2CPU move cost 0.000419 seconds
DEBUG 10-15 15:26:46 lpmodule.py:340] layer_idx 4 attn layer_idx: 4
DEBUG 10-15 15:26:46 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:26:46 lpmodule.py:364] decoder_attn_batch update batch_dim 60-120
DEBUG 10-15 15:26:46 StaticCacheLen.py:84] static cache update layer_idx: 4, start_batch: 60, end_batch: 120
DEBUG 10-15 15:26:46 lpmodule.py:368] update for kv cache 60-120 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:26:46 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:26:46 lpmodule.py:374] update past key value cost 0.005030 seconds
DEBUG 10-15 15:26:46 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:26:46 lpmodule.py:227] layer idx 4
DEBUG 10-15 15:26:46 lpmodule.py:399] repeat qkv cost 0.033130 seconds
DEBUG 10-15 15:26:46 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:26:46 lpmodule.py:433] dot attn cost 0.045068 seconds
DEBUG 10-15 15:26:46 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:26:46 lpmodule.py:444] time cost move to cuda:1 0.0270078182220459 s
DEBUG 10-15 15:26:46 lpllm.py:2283] CPU attn cost 0.148576 seconds if batch True
DEBUG 10-15 15:26:46 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:26:46 lpllm.py:2294] CPU compute cost 0.149325 seconds
DEBUG 10-15 15:26:46 lpllm.py:2312] free cost 0.000096 seconds
DEBUG 10-15 15:26:46 lpllm.py:498] reset update attn
DEBUG 10-15 15:26:46 lpllm.py:1774] update state cost 2.0742416381835938e-05 s
DEBUG 10-15 15:26:46 lpllm.py:1743] restore layer func cost 0.00038123130798339844 s
DEBUG 10-15 15:26:46 lpllm.py:511] restore layer cost 0.0006184577941894531 s
DEBUG 10-15 15:26:46 lpllm.py:512] reset_next_layer_need: layer_attn_idx=5, layer_mlp_idx=4, j_loc=9
DEBUG 10-15 15:26:46 lpllm.py:1037] reset layer cost 0.0006928443908691406 s
DEBUG 10-15 15:26:46 lpllm.py:1038] have reset next layer layer_attn 5 layer_mlp 4 
DEBUG 10-15 15:26:46 lpllm.py:1044] j: 9 waiting the layer with layer_idx 5 before wait time 0.4621860980987549 s
INFO 10-15 15:26:46 client.py:117] confirm_model_loaded: Mixtral-8x7B, 5ce4d1c8-6445-4fa0-8b2e-203d59335802
INFO 10-15 15:26:46 client.py:125] Model loaded
DEBUG 10-15 15:26:46 lpllm.py:1048] j: load cost 0.4640052318572998 s waiting cost 0.0018019676208496094 s
DEBUG 10-15 15:26:46 lpllm.py:924] 
DEBUG 10-15 15:26:46 lpllm.py:924] decoder loop j: 10 cur_layer_idx: 5 layer_attn_idx: 5 layer_mlp_idx: 4
DEBUG 10-15 15:26:46 lpllm.py:933] start load next layer cur_layer_idx: 6
DEBUG 10-15 15:26:46 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:26:46 client.py:72] load_into_gpu: Mixtral-8x7B, 7c596391-0499-440e-adaf-a8cde0a81bb0
INFO 10-15 15:26:46 client.py:113] Model loaded: Mixtral-8x7B, 7c596391-0499-440e-adaf-a8cde0a81bb0
DEBUG 10-15 15:26:46 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:26:46 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:26:46 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:26:46 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:26:46 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:26:46 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:46 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:46 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:46 lpmodule.py:506]         ...,
DEBUG 10-15 15:26:46 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:46 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:46 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:26:46 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:26:46 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:26:46 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:26:46 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:26:46 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:46 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:46 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:26:46 lpllm.py:2265] GPU2CPU move cost 0.000666 seconds
DEBUG 10-15 15:26:46 lpmodule.py:340] layer_idx 5 attn layer_idx: 5
DEBUG 10-15 15:26:46 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:26:46 lpmodule.py:364] decoder_attn_batch update batch_dim 0-60
DEBUG 10-15 15:26:46 StaticCacheLen.py:84] static cache update layer_idx: 5, start_batch: 0, end_batch: 60
DEBUG 10-15 15:26:46 lpmodule.py:368] update for kv cache 0-60 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:26:46 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:26:46 lpmodule.py:374] update past key value cost 0.005574 seconds
DEBUG 10-15 15:26:46 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:26:47 lpmodule.py:227] layer idx 4
DEBUG 10-15 15:26:47 lpmodule.py:399] repeat qkv cost 0.033530 seconds
DEBUG 10-15 15:26:47 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:26:47 lpmodule.py:433] dot attn cost 0.050566 seconds
DEBUG 10-15 15:26:47 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:26:47 lpmodule.py:444] time cost move to cuda:1 0.02714824676513672 s
DEBUG 10-15 15:26:47 lpllm.py:2283] CPU attn cost 0.154258 seconds if batch True
DEBUG 10-15 15:26:47 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:26:47 lpllm.py:2294] CPU compute cost 0.155243 seconds
DEBUG 10-15 15:26:47 lpllm.py:2312] free cost 0.000098 seconds
DEBUG 10-15 15:26:47 lpllm.py:503] reset update experts
DEBUG 10-15 15:26:47 lpllm.py:1774] update state cost 1.811981201171875e-05 s
DEBUG 10-15 15:26:47 lpllm.py:1743] restore layer func cost 0.0008521080017089844 s
DEBUG 10-15 15:26:47 lpllm.py:511] restore layer cost 0.0010962486267089844 s
DEBUG 10-15 15:26:47 lpllm.py:512] reset_next_layer_need: layer_attn_idx=5, layer_mlp_idx=5, j_loc=10
DEBUG 10-15 15:26:47 lpllm.py:1037] reset layer cost 0.001168966293334961 s
DEBUG 10-15 15:26:47 lpllm.py:1038] have reset next layer layer_attn 5 layer_mlp 5 
DEBUG 10-15 15:26:47 lpllm.py:924] 
DEBUG 10-15 15:26:47 lpllm.py:924] decoder loop j: 11 cur_layer_idx: 5 layer_attn_idx: 5 layer_mlp_idx: 5
DEBUG 10-15 15:26:47 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:26:47 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:26:47 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:26:47 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:26:47 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:26:47 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:47 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:47 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:47 lpmodule.py:506]         ...,
DEBUG 10-15 15:26:47 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:47 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:47 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:26:47 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:26:47 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:26:47 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:26:47 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:26:47 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:47 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:47 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:26:47 lpllm.py:2265] GPU2CPU move cost 0.000407 seconds
DEBUG 10-15 15:26:47 lpmodule.py:340] layer_idx 5 attn layer_idx: 5
DEBUG 10-15 15:26:47 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:26:47 lpmodule.py:364] decoder_attn_batch update batch_dim 60-120
DEBUG 10-15 15:26:47 StaticCacheLen.py:84] static cache update layer_idx: 5, start_batch: 60, end_batch: 120
DEBUG 10-15 15:26:47 lpmodule.py:368] update for kv cache 60-120 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:26:47 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:26:47 lpmodule.py:374] update past key value cost 0.006056 seconds
DEBUG 10-15 15:26:47 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:26:47 lpmodule.py:227] layer idx 5
DEBUG 10-15 15:26:47 lpmodule.py:399] repeat qkv cost 0.034740 seconds
DEBUG 10-15 15:26:47 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:26:47 lpmodule.py:433] dot attn cost 0.035679 seconds
DEBUG 10-15 15:26:47 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:26:47 lpmodule.py:444] time cost move to cuda:1 0.027072906494140625 s
DEBUG 10-15 15:26:47 lpllm.py:2283] CPU attn cost 0.140861 seconds if batch True
DEBUG 10-15 15:26:47 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:26:47 lpllm.py:2294] CPU compute cost 0.141554 seconds
DEBUG 10-15 15:26:47 lpllm.py:2312] free cost 0.000091 seconds
DEBUG 10-15 15:26:47 lpllm.py:498] reset update attn
DEBUG 10-15 15:26:47 lpllm.py:1774] update state cost 3.719329833984375e-05 s
DEBUG 10-15 15:26:47 lpllm.py:1743] restore layer func cost 0.0003895759582519531 s
DEBUG 10-15 15:26:47 lpllm.py:511] restore layer cost 0.0006487369537353516 s
DEBUG 10-15 15:26:47 lpllm.py:512] reset_next_layer_need: layer_attn_idx=6, layer_mlp_idx=5, j_loc=11
DEBUG 10-15 15:26:47 lpllm.py:1037] reset layer cost 0.0007181167602539062 s
DEBUG 10-15 15:26:47 lpllm.py:1038] have reset next layer layer_attn 6 layer_mlp 5 
DEBUG 10-15 15:26:47 lpllm.py:1044] j: 11 waiting the layer with layer_idx 6 before wait time 0.43897390365600586 s
INFO 10-15 15:26:47 client.py:117] confirm_model_loaded: Mixtral-8x7B, 7c596391-0499-440e-adaf-a8cde0a81bb0
INFO 10-15 15:26:47 client.py:125] Model loaded
DEBUG 10-15 15:26:47 lpllm.py:1048] j: load cost 0.44051146507263184 s waiting cost 0.0015208721160888672 s
DEBUG 10-15 15:26:47 lpllm.py:924] 
DEBUG 10-15 15:26:47 lpllm.py:924] decoder loop j: 12 cur_layer_idx: 6 layer_attn_idx: 6 layer_mlp_idx: 5
DEBUG 10-15 15:26:47 lpllm.py:933] start load next layer cur_layer_idx: 7
DEBUG 10-15 15:26:47 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed650>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x003\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x96\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:26:47 client.py:72] load_into_gpu: Mixtral-8x7B, 849df47b-103b-45f9-9cc9-7e5c85106a5a
INFO 10-15 15:26:47 client.py:113] Model loaded: Mixtral-8x7B, 849df47b-103b-45f9-9cc9-7e5c85106a5a
DEBUG 10-15 15:26:47 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:26:47 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:26:47 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:26:47 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:26:47 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:26:47 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:47 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:47 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:47 lpmodule.py:506]         ...,
DEBUG 10-15 15:26:47 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:47 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:47 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:26:47 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:26:47 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:26:47 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:26:47 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:26:47 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:47 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:47 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:26:47 lpllm.py:2265] GPU2CPU move cost 0.000762 seconds
DEBUG 10-15 15:26:47 lpmodule.py:340] layer_idx 6 attn layer_idx: 6
DEBUG 10-15 15:26:47 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:26:47 lpmodule.py:364] decoder_attn_batch update batch_dim 0-60
DEBUG 10-15 15:26:47 StaticCacheLen.py:84] static cache update layer_idx: 6, start_batch: 0, end_batch: 60
DEBUG 10-15 15:26:47 lpmodule.py:368] update for kv cache 0-60 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:26:47 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:26:47 lpmodule.py:374] update past key value cost 0.005813 seconds
DEBUG 10-15 15:26:47 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:26:47 lpmodule.py:227] layer idx 5
DEBUG 10-15 15:26:47 lpmodule.py:399] repeat qkv cost 0.034496 seconds
DEBUG 10-15 15:26:47 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:26:47 lpmodule.py:433] dot attn cost 0.041463 seconds
DEBUG 10-15 15:26:47 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:26:47 lpmodule.py:444] time cost move to cuda:1 0.027973175048828125 s
DEBUG 10-15 15:26:47 lpllm.py:2283] CPU attn cost 0.150362 seconds if batch True
DEBUG 10-15 15:26:47 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:26:47 lpllm.py:2294] CPU compute cost 0.151446 seconds
DEBUG 10-15 15:26:47 lpllm.py:2312] free cost 0.000089 seconds
DEBUG 10-15 15:26:47 lpllm.py:503] reset update experts
DEBUG 10-15 15:26:47 lpllm.py:1774] update state cost 3.24249267578125e-05 s
DEBUG 10-15 15:26:47 lpllm.py:1743] restore layer func cost 0.0008420944213867188 s
DEBUG 10-15 15:26:47 lpllm.py:511] restore layer cost 0.0010559558868408203 s
DEBUG 10-15 15:26:47 lpllm.py:512] reset_next_layer_need: layer_attn_idx=6, layer_mlp_idx=6, j_loc=12
DEBUG 10-15 15:26:47 lpllm.py:1037] reset layer cost 0.0011284351348876953 s
DEBUG 10-15 15:26:47 lpllm.py:1038] have reset next layer layer_attn 6 layer_mlp 6 
DEBUG 10-15 15:26:47 lpllm.py:924] 
DEBUG 10-15 15:26:47 lpllm.py:924] decoder loop j: 13 cur_layer_idx: 6 layer_attn_idx: 6 layer_mlp_idx: 6
DEBUG 10-15 15:26:47 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:26:47 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:26:47 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:26:47 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:26:47 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:26:47 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:47 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:47 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:47 lpmodule.py:506]         ...,
DEBUG 10-15 15:26:47 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:47 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:47 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:26:47 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:26:47 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:26:47 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:26:47 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:26:47 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:47 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:47 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:26:47 lpllm.py:2265] GPU2CPU move cost 0.000634 seconds
DEBUG 10-15 15:26:47 lpmodule.py:340] layer_idx 6 attn layer_idx: 6
DEBUG 10-15 15:26:47 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:26:47 lpmodule.py:364] decoder_attn_batch update batch_dim 60-120
DEBUG 10-15 15:26:47 StaticCacheLen.py:84] static cache update layer_idx: 6, start_batch: 60, end_batch: 120
DEBUG 10-15 15:26:47 lpmodule.py:368] update for kv cache 60-120 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:26:47 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:26:47 lpmodule.py:374] update past key value cost 0.005768 seconds
DEBUG 10-15 15:26:47 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:26:47 lpmodule.py:227] layer idx 6
DEBUG 10-15 15:26:47 lpmodule.py:399] repeat qkv cost 0.033336 seconds
DEBUG 10-15 15:26:47 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:26:47 lpmodule.py:433] dot attn cost 0.034735 seconds
DEBUG 10-15 15:26:47 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:26:47 lpmodule.py:444] time cost move to cuda:1 0.027856111526489258 s
DEBUG 10-15 15:26:47 lpllm.py:2283] CPU attn cost 0.142272 seconds if batch True
DEBUG 10-15 15:26:47 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:26:47 lpllm.py:2294] CPU compute cost 0.143194 seconds
DEBUG 10-15 15:26:47 lpllm.py:2312] free cost 0.000088 seconds
DEBUG 10-15 15:26:47 lpllm.py:498] reset update attn
DEBUG 10-15 15:26:47 lpllm.py:1774] update state cost 1.71661376953125e-05 s
DEBUG 10-15 15:26:47 lpllm.py:1743] restore layer func cost 0.00038313865661621094 s
DEBUG 10-15 15:26:47 lpllm.py:511] restore layer cost 0.0005562305450439453 s
DEBUG 10-15 15:26:47 lpllm.py:512] reset_next_layer_need: layer_attn_idx=7, layer_mlp_idx=6, j_loc=13
DEBUG 10-15 15:26:47 lpllm.py:1037] reset layer cost 0.0006256103515625 s
DEBUG 10-15 15:26:47 lpllm.py:1038] have reset next layer layer_attn 7 layer_mlp 6 
DEBUG 10-15 15:26:47 lpllm.py:1044] j: 13 waiting the layer with layer_idx 7 before wait time 0.4356682300567627 s
INFO 10-15 15:26:47 client.py:117] confirm_model_loaded: Mixtral-8x7B, 849df47b-103b-45f9-9cc9-7e5c85106a5a
INFO 10-15 15:26:47 client.py:125] Model loaded
DEBUG 10-15 15:26:47 lpllm.py:1048] j: load cost 0.43712782859802246 s waiting cost 0.0014433860778808594 s
DEBUG 10-15 15:26:47 lpllm.py:924] 
DEBUG 10-15 15:26:47 lpllm.py:924] decoder loop j: 14 cur_layer_idx: 7 layer_attn_idx: 7 layer_mlp_idx: 6
DEBUG 10-15 15:26:47 lpllm.py:933] start load next layer cur_layer_idx: 8
DEBUG 10-15 15:26:47 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed6e0>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x004\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x98\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:26:47 client.py:72] load_into_gpu: Mixtral-8x7B, c30f37bd-f3d5-4575-b4ba-c2981a4f9e7e
INFO 10-15 15:26:47 client.py:113] Model loaded: Mixtral-8x7B, c30f37bd-f3d5-4575-b4ba-c2981a4f9e7e
DEBUG 10-15 15:26:47 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:26:47 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:26:47 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:26:47 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:26:47 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:26:47 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:47 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:47 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:47 lpmodule.py:506]         ...,
DEBUG 10-15 15:26:47 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:47 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:47 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:26:47 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:26:47 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:26:47 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:26:47 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:26:47 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:47 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:47 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:26:47 lpllm.py:2265] GPU2CPU move cost 0.000391 seconds
DEBUG 10-15 15:26:47 lpmodule.py:340] layer_idx 7 attn layer_idx: 7
DEBUG 10-15 15:26:47 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:26:47 lpmodule.py:364] decoder_attn_batch update batch_dim 0-60
DEBUG 10-15 15:26:47 StaticCacheLen.py:84] static cache update layer_idx: 7, start_batch: 0, end_batch: 60
DEBUG 10-15 15:26:47 lpmodule.py:368] update for kv cache 0-60 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:26:47 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:26:47 lpmodule.py:374] update past key value cost 0.005465 seconds
DEBUG 10-15 15:26:47 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:26:47 lpmodule.py:227] layer idx 6
DEBUG 10-15 15:26:47 lpmodule.py:399] repeat qkv cost 0.032501 seconds
DEBUG 10-15 15:26:47 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:26:47 lpmodule.py:433] dot attn cost 0.043935 seconds
DEBUG 10-15 15:26:47 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:26:47 lpmodule.py:444] time cost move to cuda:1 0.028039216995239258 s
DEBUG 10-15 15:26:47 lpllm.py:2283] CPU attn cost 0.151389 seconds if batch True
DEBUG 10-15 15:26:47 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:26:47 lpllm.py:2294] CPU compute cost 0.152022 seconds
DEBUG 10-15 15:26:47 lpllm.py:2312] free cost 0.000090 seconds
DEBUG 10-15 15:26:48 lpllm.py:503] reset update experts
DEBUG 10-15 15:26:48 lpllm.py:1774] update state cost 1.52587890625e-05 s
DEBUG 10-15 15:26:48 lpllm.py:1743] restore layer func cost 0.0008339881896972656 s
DEBUG 10-15 15:26:48 lpllm.py:511] restore layer cost 0.0010106563568115234 s
DEBUG 10-15 15:26:48 lpllm.py:512] reset_next_layer_need: layer_attn_idx=7, layer_mlp_idx=7, j_loc=14
DEBUG 10-15 15:26:48 lpllm.py:1037] reset layer cost 0.001079559326171875 s
DEBUG 10-15 15:26:48 lpllm.py:1038] have reset next layer layer_attn 7 layer_mlp 7 
DEBUG 10-15 15:26:48 lpllm.py:924] 
DEBUG 10-15 15:26:48 lpllm.py:924] decoder loop j: 15 cur_layer_idx: 7 layer_attn_idx: 7 layer_mlp_idx: 7
DEBUG 10-15 15:26:48 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:26:48 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:26:48 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:26:48 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:26:48 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:26:48 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:48 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:48 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:48 lpmodule.py:506]         ...,
DEBUG 10-15 15:26:48 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:48 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:48 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:26:48 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:26:48 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:26:48 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:26:48 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:26:48 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:48 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:48 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:26:48 lpllm.py:2265] GPU2CPU move cost 0.000520 seconds
DEBUG 10-15 15:26:48 lpmodule.py:340] layer_idx 7 attn layer_idx: 7
DEBUG 10-15 15:26:48 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:26:48 lpmodule.py:364] decoder_attn_batch update batch_dim 60-120
DEBUG 10-15 15:26:48 StaticCacheLen.py:84] static cache update layer_idx: 7, start_batch: 60, end_batch: 120
DEBUG 10-15 15:26:48 lpmodule.py:368] update for kv cache 60-120 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:26:48 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:26:48 lpmodule.py:374] update past key value cost 0.005301 seconds
DEBUG 10-15 15:26:48 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:26:48 lpmodule.py:227] layer idx 7
DEBUG 10-15 15:26:48 lpmodule.py:399] repeat qkv cost 0.032728 seconds
DEBUG 10-15 15:26:48 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:26:48 lpmodule.py:433] dot attn cost 0.032828 seconds
DEBUG 10-15 15:26:48 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:26:48 lpmodule.py:444] time cost move to cuda:1 0.02713298797607422 s
DEBUG 10-15 15:26:48 lpllm.py:2283] CPU attn cost 0.140014 seconds if batch True
DEBUG 10-15 15:26:48 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:26:48 lpllm.py:2294] CPU compute cost 0.140822 seconds
DEBUG 10-15 15:26:48 lpllm.py:2312] free cost 0.000090 seconds
DEBUG 10-15 15:26:48 lpllm.py:498] reset update attn
DEBUG 10-15 15:26:48 lpllm.py:1774] update state cost 1.7404556274414062e-05 s
DEBUG 10-15 15:26:48 lpllm.py:1743] restore layer func cost 0.00037980079650878906 s
DEBUG 10-15 15:26:48 lpllm.py:511] restore layer cost 0.0005512237548828125 s
DEBUG 10-15 15:26:48 lpllm.py:512] reset_next_layer_need: layer_attn_idx=8, layer_mlp_idx=7, j_loc=15
DEBUG 10-15 15:26:48 lpllm.py:1037] reset layer cost 0.0006203651428222656 s
DEBUG 10-15 15:26:48 lpllm.py:1038] have reset next layer layer_attn 8 layer_mlp 7 
DEBUG 10-15 15:26:48 lpllm.py:1044] j: 15 waiting the layer with layer_idx 8 before wait time 0.44236230850219727 s
INFO 10-15 15:26:48 client.py:117] confirm_model_loaded: Mixtral-8x7B, c30f37bd-f3d5-4575-b4ba-c2981a4f9e7e
INFO 10-15 15:26:48 client.py:125] Model loaded
DEBUG 10-15 15:26:48 lpllm.py:1048] j: load cost 0.4441385269165039 s waiting cost 0.0017597675323486328 s
DEBUG 10-15 15:26:48 lpllm.py:924] 
DEBUG 10-15 15:26:48 lpllm.py:924] decoder loop j: 16 cur_layer_idx: 8 layer_attn_idx: 8 layer_mlp_idx: 7
DEBUG 10-15 15:26:48 lpllm.py:933] start load next layer cur_layer_idx: 9
DEBUG 10-15 15:26:48 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:26:48 client.py:72] load_into_gpu: Mixtral-8x7B, c87d54df-7112-49a6-875d-dc992866a495
INFO 10-15 15:26:48 client.py:113] Model loaded: Mixtral-8x7B, c87d54df-7112-49a6-875d-dc992866a495
DEBUG 10-15 15:26:48 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:26:48 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:26:48 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:26:48 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:26:48 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:26:48 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:48 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:48 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:48 lpmodule.py:506]         ...,
DEBUG 10-15 15:26:48 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:48 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:48 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:26:48 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:26:48 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:26:48 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:26:48 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:26:48 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:48 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:48 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:26:48 lpllm.py:2265] GPU2CPU move cost 0.000648 seconds
DEBUG 10-15 15:26:48 lpmodule.py:340] layer_idx 8 attn layer_idx: 8
DEBUG 10-15 15:26:48 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:26:48 lpmodule.py:364] decoder_attn_batch update batch_dim 0-60
DEBUG 10-15 15:26:48 StaticCacheLen.py:84] static cache update layer_idx: 8, start_batch: 0, end_batch: 60
DEBUG 10-15 15:26:48 lpmodule.py:368] update for kv cache 0-60 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:26:48 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:26:48 lpmodule.py:374] update past key value cost 0.005883 seconds
DEBUG 10-15 15:26:48 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:26:48 lpmodule.py:227] layer idx 7
DEBUG 10-15 15:26:48 lpmodule.py:399] repeat qkv cost 0.032363 seconds
DEBUG 10-15 15:26:48 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:26:48 lpmodule.py:433] dot attn cost 0.035550 seconds
DEBUG 10-15 15:26:48 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:26:48 lpmodule.py:444] time cost move to cuda:1 0.10308003425598145 s
DEBUG 10-15 15:26:48 lpllm.py:2283] CPU attn cost 0.220179 seconds if batch True
DEBUG 10-15 15:26:48 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:26:48 lpllm.py:2294] CPU compute cost 0.221141 seconds
DEBUG 10-15 15:26:48 lpllm.py:2312] free cost 0.000096 seconds
DEBUG 10-15 15:26:48 lpllm.py:503] reset update experts
DEBUG 10-15 15:26:48 lpllm.py:1774] update state cost 3.0994415283203125e-05 s
DEBUG 10-15 15:26:48 lpllm.py:1743] restore layer func cost 0.0008423328399658203 s
DEBUG 10-15 15:26:48 lpllm.py:511] restore layer cost 0.001018524169921875 s
DEBUG 10-15 15:26:48 lpllm.py:512] reset_next_layer_need: layer_attn_idx=8, layer_mlp_idx=8, j_loc=16
DEBUG 10-15 15:26:48 lpllm.py:1037] reset layer cost 0.0010902881622314453 s
DEBUG 10-15 15:26:48 lpllm.py:1038] have reset next layer layer_attn 8 layer_mlp 8 
DEBUG 10-15 15:26:48 lpllm.py:924] 
DEBUG 10-15 15:26:48 lpllm.py:924] decoder loop j: 17 cur_layer_idx: 8 layer_attn_idx: 8 layer_mlp_idx: 8
DEBUG 10-15 15:26:48 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:26:48 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:26:48 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:26:48 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:26:48 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:26:48 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:48 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:48 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:48 lpmodule.py:506]         ...,
DEBUG 10-15 15:26:48 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:48 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:48 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:26:48 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:26:48 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:26:48 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:26:48 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:26:48 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:48 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:48 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:26:48 lpllm.py:2265] GPU2CPU move cost 0.000491 seconds
DEBUG 10-15 15:26:48 lpmodule.py:340] layer_idx 8 attn layer_idx: 8
DEBUG 10-15 15:26:48 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:26:48 lpmodule.py:364] decoder_attn_batch update batch_dim 60-120
DEBUG 10-15 15:26:48 StaticCacheLen.py:84] static cache update layer_idx: 8, start_batch: 60, end_batch: 120
DEBUG 10-15 15:26:48 lpmodule.py:368] update for kv cache 60-120 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:26:48 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:26:48 lpmodule.py:374] update past key value cost 0.005124 seconds
DEBUG 10-15 15:26:48 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:26:48 lpmodule.py:227] layer idx 8
DEBUG 10-15 15:26:48 lpmodule.py:399] repeat qkv cost 0.034121 seconds
DEBUG 10-15 15:26:48 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:26:48 lpmodule.py:433] dot attn cost 0.034798 seconds
DEBUG 10-15 15:26:48 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:26:48 lpmodule.py:444] time cost move to cuda:1 0.02718353271484375 s
DEBUG 10-15 15:26:48 lpllm.py:2283] CPU attn cost 0.141720 seconds if batch True
DEBUG 10-15 15:26:48 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:26:48 lpllm.py:2294] CPU compute cost 0.142482 seconds
DEBUG 10-15 15:26:48 lpllm.py:2312] free cost 0.000084 seconds
DEBUG 10-15 15:26:48 lpllm.py:498] reset update attn
DEBUG 10-15 15:26:48 lpllm.py:1774] update state cost 3.266334533691406e-05 s
DEBUG 10-15 15:26:48 lpllm.py:1743] restore layer func cost 0.0003876686096191406 s
DEBUG 10-15 15:26:48 lpllm.py:511] restore layer cost 0.0005495548248291016 s
DEBUG 10-15 15:26:48 lpllm.py:512] reset_next_layer_need: layer_attn_idx=9, layer_mlp_idx=8, j_loc=17
DEBUG 10-15 15:26:48 lpllm.py:1037] reset layer cost 0.0006194114685058594 s
DEBUG 10-15 15:26:48 lpllm.py:1038] have reset next layer layer_attn 9 layer_mlp 8 
DEBUG 10-15 15:26:48 lpllm.py:1044] j: 17 waiting the layer with layer_idx 9 before wait time 0.6778759956359863 s
INFO 10-15 15:26:48 client.py:117] confirm_model_loaded: Mixtral-8x7B, c87d54df-7112-49a6-875d-dc992866a495
INFO 10-15 15:26:48 client.py:125] Model loaded
DEBUG 10-15 15:26:48 lpllm.py:1048] j: load cost 0.6795186996459961 s waiting cost 0.0016260147094726562 s
DEBUG 10-15 15:26:48 lpllm.py:924] 
DEBUG 10-15 15:26:48 lpllm.py:924] decoder loop j: 18 cur_layer_idx: 9 layer_attn_idx: 9 layer_mlp_idx: 8
DEBUG 10-15 15:26:48 lpllm.py:933] start load next layer cur_layer_idx: 10
DEBUG 10-15 15:26:48 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed650>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x003\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x96\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:26:48 client.py:72] load_into_gpu: Mixtral-8x7B, 81d5dc07-fd55-4d5d-944b-b1839d98d3ae
INFO 10-15 15:26:48 client.py:113] Model loaded: Mixtral-8x7B, 81d5dc07-fd55-4d5d-944b-b1839d98d3ae
DEBUG 10-15 15:26:48 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:26:48 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:26:48 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:26:48 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:26:48 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:26:48 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:48 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:48 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:48 lpmodule.py:506]         ...,
DEBUG 10-15 15:26:48 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:48 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:48 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:26:48 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:26:48 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:26:48 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:26:48 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:26:48 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:48 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:48 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:26:48 lpllm.py:2265] GPU2CPU move cost 0.000491 seconds
DEBUG 10-15 15:26:48 lpmodule.py:340] layer_idx 9 attn layer_idx: 9
DEBUG 10-15 15:26:48 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:26:48 lpmodule.py:364] decoder_attn_batch update batch_dim 0-60
DEBUG 10-15 15:26:48 StaticCacheLen.py:84] static cache update layer_idx: 9, start_batch: 0, end_batch: 60
DEBUG 10-15 15:26:48 lpmodule.py:368] update for kv cache 0-60 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:26:48 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:26:48 lpmodule.py:374] update past key value cost 0.005927 seconds
DEBUG 10-15 15:26:48 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:26:49 lpmodule.py:227] layer idx 8
DEBUG 10-15 15:26:49 lpmodule.py:399] repeat qkv cost 0.034090 seconds
DEBUG 10-15 15:26:49 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:26:49 lpmodule.py:433] dot attn cost 0.038342 seconds
DEBUG 10-15 15:26:49 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:26:49 lpmodule.py:444] time cost move to cuda:1 0.02782273292541504 s
DEBUG 10-15 15:26:49 lpllm.py:2283] CPU attn cost 0.148842 seconds if batch True
DEBUG 10-15 15:26:49 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:26:49 lpllm.py:2294] CPU compute cost 0.149606 seconds
DEBUG 10-15 15:26:49 lpllm.py:2312] free cost 0.000087 seconds
DEBUG 10-15 15:26:49 lpllm.py:503] reset update experts
DEBUG 10-15 15:26:49 lpllm.py:1774] update state cost 1.4543533325195312e-05 s
DEBUG 10-15 15:26:49 lpllm.py:1743] restore layer func cost 0.0008375644683837891 s
DEBUG 10-15 15:26:49 lpllm.py:511] restore layer cost 0.001005411148071289 s
DEBUG 10-15 15:26:49 lpllm.py:512] reset_next_layer_need: layer_attn_idx=9, layer_mlp_idx=9, j_loc=18
DEBUG 10-15 15:26:49 lpllm.py:1037] reset layer cost 0.0010766983032226562 s
DEBUG 10-15 15:26:49 lpllm.py:1038] have reset next layer layer_attn 9 layer_mlp 9 
DEBUG 10-15 15:26:49 lpllm.py:924] 
DEBUG 10-15 15:26:49 lpllm.py:924] decoder loop j: 19 cur_layer_idx: 9 layer_attn_idx: 9 layer_mlp_idx: 9
DEBUG 10-15 15:26:49 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:26:49 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:26:49 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:26:49 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:26:49 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:26:49 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:49 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:49 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:49 lpmodule.py:506]         ...,
DEBUG 10-15 15:26:49 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:49 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:49 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:26:49 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:26:49 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:26:49 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:26:49 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:26:49 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:49 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:49 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:26:49 lpllm.py:2265] GPU2CPU move cost 0.000302 seconds
DEBUG 10-15 15:26:49 lpmodule.py:340] layer_idx 9 attn layer_idx: 9
DEBUG 10-15 15:26:49 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:26:49 lpmodule.py:364] decoder_attn_batch update batch_dim 60-120
DEBUG 10-15 15:26:49 StaticCacheLen.py:84] static cache update layer_idx: 9, start_batch: 60, end_batch: 120
DEBUG 10-15 15:26:49 lpmodule.py:368] update for kv cache 60-120 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:26:49 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:26:49 lpmodule.py:374] update past key value cost 0.004831 seconds
DEBUG 10-15 15:26:49 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:26:49 lpmodule.py:399] repeat qkv cost 0.032112 seconds
DEBUG 10-15 15:26:49 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:26:49 lpmodule.py:227] layer idx 9
DEBUG 10-15 15:26:49 lpmodule.py:433] dot attn cost 0.037497 seconds
DEBUG 10-15 15:26:49 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:26:49 lpmodule.py:444] time cost move to cuda:1 0.027498483657836914 s
DEBUG 10-15 15:26:49 lpllm.py:2283] CPU attn cost 0.142419 seconds if batch True
DEBUG 10-15 15:26:49 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:26:49 lpllm.py:2294] CPU compute cost 0.142944 seconds
DEBUG 10-15 15:26:49 lpllm.py:2312] free cost 0.000085 seconds
DEBUG 10-15 15:26:49 lpllm.py:498] reset update attn
DEBUG 10-15 15:26:49 lpllm.py:1774] update state cost 1.811981201171875e-05 s
DEBUG 10-15 15:26:49 lpllm.py:1743] restore layer func cost 0.00038242340087890625 s
DEBUG 10-15 15:26:49 lpllm.py:511] restore layer cost 0.0005533695220947266 s
DEBUG 10-15 15:26:49 lpllm.py:512] reset_next_layer_need: layer_attn_idx=10, layer_mlp_idx=9, j_loc=19
DEBUG 10-15 15:26:49 lpllm.py:1037] reset layer cost 0.0006244182586669922 s
DEBUG 10-15 15:26:49 lpllm.py:1038] have reset next layer layer_attn 10 layer_mlp 9 
DEBUG 10-15 15:26:49 lpllm.py:1044] j: 19 waiting the layer with layer_idx 10 before wait time 0.4544045925140381 s
INFO 10-15 15:26:49 client.py:117] confirm_model_loaded: Mixtral-8x7B, 81d5dc07-fd55-4d5d-944b-b1839d98d3ae
INFO 10-15 15:26:49 client.py:125] Model loaded
DEBUG 10-15 15:26:49 lpllm.py:1048] j: load cost 0.45623159408569336 s waiting cost 0.0018117427825927734 s
DEBUG 10-15 15:26:49 lpllm.py:924] 
DEBUG 10-15 15:26:49 lpllm.py:924] decoder loop j: 20 cur_layer_idx: 10 layer_attn_idx: 10 layer_mlp_idx: 9
DEBUG 10-15 15:26:49 lpllm.py:933] start load next layer cur_layer_idx: 11
DEBUG 10-15 15:26:49 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed6e0>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x004\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x98\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:26:49 client.py:72] load_into_gpu: Mixtral-8x7B, 3b004e99-b6a4-4f1d-99aa-7947ca517331
INFO 10-15 15:26:49 client.py:113] Model loaded: Mixtral-8x7B, 3b004e99-b6a4-4f1d-99aa-7947ca517331
DEBUG 10-15 15:26:49 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:26:49 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:26:49 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:26:49 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:26:49 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:26:49 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:49 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:49 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:49 lpmodule.py:506]         ...,
DEBUG 10-15 15:26:49 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:49 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:49 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:26:49 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:26:49 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:26:49 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:26:49 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:26:49 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:49 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:49 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:26:49 lpllm.py:2265] GPU2CPU move cost 0.000385 seconds
DEBUG 10-15 15:26:49 lpmodule.py:340] layer_idx 10 attn layer_idx: 10
DEBUG 10-15 15:26:49 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:26:49 lpmodule.py:364] decoder_attn_batch update batch_dim 0-60
DEBUG 10-15 15:26:49 StaticCacheLen.py:84] static cache update layer_idx: 10, start_batch: 0, end_batch: 60
DEBUG 10-15 15:26:49 lpmodule.py:368] update for kv cache 0-60 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:26:49 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:26:49 lpmodule.py:374] update past key value cost 0.005609 seconds
DEBUG 10-15 15:26:49 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:26:49 lpmodule.py:227] layer idx 9
DEBUG 10-15 15:26:49 lpmodule.py:399] repeat qkv cost 0.033802 seconds
DEBUG 10-15 15:26:49 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:26:49 lpmodule.py:433] dot attn cost 0.035916 seconds
DEBUG 10-15 15:26:49 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:26:49 lpmodule.py:444] time cost move to cuda:1 0.032666683197021484 s
DEBUG 10-15 15:26:49 lpllm.py:2283] CPU attn cost 0.146904 seconds if batch True
DEBUG 10-15 15:26:49 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:26:49 lpllm.py:2294] CPU compute cost 0.147544 seconds
DEBUG 10-15 15:26:49 lpllm.py:2312] free cost 0.000086 seconds
DEBUG 10-15 15:26:49 lpllm.py:503] reset update experts
DEBUG 10-15 15:26:49 lpllm.py:1774] update state cost 3.147125244140625e-05 s
DEBUG 10-15 15:26:49 lpllm.py:1743] restore layer func cost 0.0008306503295898438 s
DEBUG 10-15 15:26:49 lpllm.py:511] restore layer cost 0.0010242462158203125 s
DEBUG 10-15 15:26:49 lpllm.py:512] reset_next_layer_need: layer_attn_idx=10, layer_mlp_idx=10, j_loc=20
DEBUG 10-15 15:26:49 lpllm.py:1037] reset layer cost 0.0010988712310791016 s
DEBUG 10-15 15:26:49 lpllm.py:1038] have reset next layer layer_attn 10 layer_mlp 10 
DEBUG 10-15 15:26:49 lpllm.py:924] 
DEBUG 10-15 15:26:49 lpllm.py:924] decoder loop j: 21 cur_layer_idx: 10 layer_attn_idx: 10 layer_mlp_idx: 10
DEBUG 10-15 15:26:49 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:26:49 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:26:49 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:26:49 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:26:49 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:26:49 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:49 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:49 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:49 lpmodule.py:506]         ...,
DEBUG 10-15 15:26:49 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:49 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:49 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:26:49 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:26:49 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:26:49 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:26:49 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:26:49 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:49 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:49 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:26:49 lpllm.py:2265] GPU2CPU move cost 0.000496 seconds
DEBUG 10-15 15:26:49 lpmodule.py:340] layer_idx 10 attn layer_idx: 10
DEBUG 10-15 15:26:49 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:26:49 lpmodule.py:364] decoder_attn_batch update batch_dim 60-120
DEBUG 10-15 15:26:49 StaticCacheLen.py:84] static cache update layer_idx: 10, start_batch: 60, end_batch: 120
DEBUG 10-15 15:26:49 lpmodule.py:368] update for kv cache 60-120 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:26:49 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:26:49 lpmodule.py:374] update past key value cost 0.005349 seconds
DEBUG 10-15 15:26:49 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:26:49 lpmodule.py:227] layer idx 10
DEBUG 10-15 15:26:49 lpmodule.py:399] repeat qkv cost 0.032194 seconds
DEBUG 10-15 15:26:49 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:26:49 lpmodule.py:433] dot attn cost 0.033047 seconds
DEBUG 10-15 15:26:49 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:26:49 lpmodule.py:444] time cost move to cuda:1 0.02672100067138672 s
DEBUG 10-15 15:26:49 lpllm.py:2283] CPU attn cost 0.136964 seconds if batch True
DEBUG 10-15 15:26:49 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:26:49 lpllm.py:2294] CPU compute cost 0.137743 seconds
DEBUG 10-15 15:26:49 lpllm.py:2312] free cost 0.000089 seconds
DEBUG 10-15 15:26:49 lpllm.py:498] reset update attn
DEBUG 10-15 15:26:49 lpllm.py:1774] update state cost 3.314018249511719e-05 s
DEBUG 10-15 15:26:49 lpllm.py:1743] restore layer func cost 0.00038170814514160156 s
DEBUG 10-15 15:26:49 lpllm.py:511] restore layer cost 0.0005459785461425781 s
DEBUG 10-15 15:26:49 lpllm.py:512] reset_next_layer_need: layer_attn_idx=11, layer_mlp_idx=10, j_loc=21
DEBUG 10-15 15:26:49 lpllm.py:1037] reset layer cost 0.0006155967712402344 s
DEBUG 10-15 15:26:49 lpllm.py:1038] have reset next layer layer_attn 11 layer_mlp 10 
DEBUG 10-15 15:26:49 lpllm.py:1044] j: 21 waiting the layer with layer_idx 11 before wait time 0.4867849349975586 s
INFO 10-15 15:26:49 client.py:117] confirm_model_loaded: Mixtral-8x7B, 3b004e99-b6a4-4f1d-99aa-7947ca517331
INFO 10-15 15:26:49 client.py:125] Model loaded
DEBUG 10-15 15:26:49 lpllm.py:1048] j: load cost 0.48840999603271484 s waiting cost 0.0016083717346191406 s
DEBUG 10-15 15:26:49 lpllm.py:924] 
DEBUG 10-15 15:26:49 lpllm.py:924] decoder loop j: 22 cur_layer_idx: 11 layer_attn_idx: 11 layer_mlp_idx: 10
DEBUG 10-15 15:26:49 lpllm.py:933] start load next layer cur_layer_idx: 12
DEBUG 10-15 15:26:49 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:26:49 client.py:72] load_into_gpu: Mixtral-8x7B, 006c4e99-7345-4832-9854-15b365fd59a7
INFO 10-15 15:26:49 client.py:113] Model loaded: Mixtral-8x7B, 006c4e99-7345-4832-9854-15b365fd59a7
DEBUG 10-15 15:26:49 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:26:49 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:26:49 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:26:49 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:26:49 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:26:49 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:49 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:49 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:49 lpmodule.py:506]         ...,
DEBUG 10-15 15:26:49 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:49 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:49 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:26:49 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:26:49 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:26:49 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:26:49 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:26:49 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:49 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:49 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:26:49 lpllm.py:2265] GPU2CPU move cost 0.000490 seconds
DEBUG 10-15 15:26:49 lpmodule.py:340] layer_idx 11 attn layer_idx: 11
DEBUG 10-15 15:26:49 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:26:49 lpmodule.py:364] decoder_attn_batch update batch_dim 0-60
DEBUG 10-15 15:26:49 StaticCacheLen.py:84] static cache update layer_idx: 11, start_batch: 0, end_batch: 60
DEBUG 10-15 15:26:49 lpmodule.py:368] update for kv cache 0-60 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:26:49 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:26:49 lpmodule.py:374] update past key value cost 0.008383 seconds
DEBUG 10-15 15:26:49 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:26:49 lpmodule.py:227] layer idx 10
DEBUG 10-15 15:26:49 lpmodule.py:399] repeat qkv cost 0.034722 seconds
DEBUG 10-15 15:26:49 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:26:50 lpmodule.py:433] dot attn cost 0.049429 seconds
DEBUG 10-15 15:26:50 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:26:50 lpmodule.py:444] time cost move to cuda:1 0.02886056900024414 s
DEBUG 10-15 15:26:50 lpllm.py:2283] CPU attn cost 0.160280 seconds if batch True
DEBUG 10-15 15:26:50 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:26:50 lpllm.py:2294] CPU compute cost 0.161046 seconds
DEBUG 10-15 15:26:50 lpllm.py:2312] free cost 0.000090 seconds
DEBUG 10-15 15:26:50 lpllm.py:503] reset update experts
DEBUG 10-15 15:26:50 lpllm.py:1774] update state cost 1.5974044799804688e-05 s
DEBUG 10-15 15:26:50 lpllm.py:1743] restore layer func cost 0.000843048095703125 s
DEBUG 10-15 15:26:50 lpllm.py:511] restore layer cost 0.0010356903076171875 s
DEBUG 10-15 15:26:50 lpllm.py:512] reset_next_layer_need: layer_attn_idx=11, layer_mlp_idx=11, j_loc=22
DEBUG 10-15 15:26:50 lpllm.py:1037] reset layer cost 0.0011060237884521484 s
DEBUG 10-15 15:26:50 lpllm.py:1038] have reset next layer layer_attn 11 layer_mlp 11 
DEBUG 10-15 15:26:50 lpllm.py:924] 
DEBUG 10-15 15:26:50 lpllm.py:924] decoder loop j: 23 cur_layer_idx: 11 layer_attn_idx: 11 layer_mlp_idx: 11
DEBUG 10-15 15:26:50 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:26:50 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:26:50 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:26:50 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:26:50 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:26:50 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:50 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:50 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:50 lpmodule.py:506]         ...,
DEBUG 10-15 15:26:50 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:50 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:50 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:26:50 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:26:50 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:26:50 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:26:50 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:26:50 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:50 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:50 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:26:50 lpllm.py:2265] GPU2CPU move cost 0.000643 seconds
DEBUG 10-15 15:26:50 lpmodule.py:340] layer_idx 11 attn layer_idx: 11
DEBUG 10-15 15:26:50 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:26:50 lpmodule.py:364] decoder_attn_batch update batch_dim 60-120
DEBUG 10-15 15:26:50 StaticCacheLen.py:84] static cache update layer_idx: 11, start_batch: 60, end_batch: 120
DEBUG 10-15 15:26:50 lpmodule.py:368] update for kv cache 60-120 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:26:50 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:26:50 lpmodule.py:374] update past key value cost 0.005504 seconds
DEBUG 10-15 15:26:50 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:26:50 lpmodule.py:227] layer idx 11
DEBUG 10-15 15:26:50 lpmodule.py:399] repeat qkv cost 0.033995 seconds
DEBUG 10-15 15:26:50 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:26:50 lpmodule.py:433] dot attn cost 0.036409 seconds
DEBUG 10-15 15:26:50 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:26:50 lpmodule.py:444] time cost move to cuda:1 0.02719283103942871 s
DEBUG 10-15 15:26:50 lpllm.py:2283] CPU attn cost 0.141407 seconds if batch True
DEBUG 10-15 15:26:50 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:26:50 lpllm.py:2294] CPU compute cost 0.142341 seconds
DEBUG 10-15 15:26:50 lpllm.py:2312] free cost 0.000089 seconds
DEBUG 10-15 15:26:50 lpllm.py:498] reset update attn
DEBUG 10-15 15:26:50 lpllm.py:1774] update state cost 1.7642974853515625e-05 s
DEBUG 10-15 15:26:50 lpllm.py:1743] restore layer func cost 0.0003769397735595703 s
DEBUG 10-15 15:26:50 lpllm.py:511] restore layer cost 0.0005424022674560547 s
DEBUG 10-15 15:26:50 lpllm.py:512] reset_next_layer_need: layer_attn_idx=12, layer_mlp_idx=11, j_loc=23
DEBUG 10-15 15:26:50 lpllm.py:1037] reset layer cost 0.0006093978881835938 s
DEBUG 10-15 15:26:50 lpllm.py:1038] have reset next layer layer_attn 12 layer_mlp 11 
DEBUG 10-15 15:26:50 lpllm.py:1044] j: 23 waiting the layer with layer_idx 12 before wait time 0.4451587200164795 s
INFO 10-15 15:26:50 client.py:117] confirm_model_loaded: Mixtral-8x7B, 006c4e99-7345-4832-9854-15b365fd59a7
INFO 10-15 15:26:50 client.py:125] Model loaded
DEBUG 10-15 15:26:50 lpllm.py:1048] j: load cost 0.4469280242919922 s waiting cost 0.0017547607421875 s
DEBUG 10-15 15:26:50 lpllm.py:924] 
DEBUG 10-15 15:26:50 lpllm.py:924] decoder loop j: 24 cur_layer_idx: 12 layer_attn_idx: 12 layer_mlp_idx: 11
DEBUG 10-15 15:26:50 lpllm.py:933] start load next layer cur_layer_idx: 13
DEBUG 10-15 15:26:50 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed650>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x003\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x96\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:26:50 client.py:72] load_into_gpu: Mixtral-8x7B, 3cc84712-fa4a-4ffe-9d85-8b904ee90159
INFO 10-15 15:26:50 client.py:113] Model loaded: Mixtral-8x7B, 3cc84712-fa4a-4ffe-9d85-8b904ee90159
DEBUG 10-15 15:26:50 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:26:50 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:26:50 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:26:50 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:26:50 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:26:50 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:50 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:50 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:50 lpmodule.py:506]         ...,
DEBUG 10-15 15:26:50 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:50 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:50 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:26:50 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:26:50 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:26:50 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:26:50 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:26:50 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:50 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:50 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:26:50 lpllm.py:2265] GPU2CPU move cost 0.000499 seconds
DEBUG 10-15 15:26:50 lpmodule.py:340] layer_idx 12 attn layer_idx: 12
DEBUG 10-15 15:26:50 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:26:50 lpmodule.py:364] decoder_attn_batch update batch_dim 0-60
DEBUG 10-15 15:26:50 StaticCacheLen.py:84] static cache update layer_idx: 12, start_batch: 0, end_batch: 60
DEBUG 10-15 15:26:50 lpmodule.py:368] update for kv cache 0-60 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:26:50 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:26:50 lpmodule.py:374] update past key value cost 0.005609 seconds
DEBUG 10-15 15:26:50 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:26:50 lpmodule.py:227] layer idx 11
DEBUG 10-15 15:26:50 lpmodule.py:399] repeat qkv cost 0.033480 seconds
DEBUG 10-15 15:26:50 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:26:50 lpmodule.py:433] dot attn cost 0.049083 seconds
DEBUG 10-15 15:26:50 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:26:50 lpmodule.py:444] time cost move to cuda:1 0.027280092239379883 s
DEBUG 10-15 15:26:50 lpllm.py:2283] CPU attn cost 0.153293 seconds if batch True
DEBUG 10-15 15:26:50 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:26:50 lpllm.py:2294] CPU compute cost 0.154050 seconds
DEBUG 10-15 15:26:50 lpllm.py:2312] free cost 0.000086 seconds
DEBUG 10-15 15:26:50 lpllm.py:503] reset update experts
DEBUG 10-15 15:26:50 lpllm.py:1774] update state cost 3.1948089599609375e-05 s
DEBUG 10-15 15:26:50 lpllm.py:1743] restore layer func cost 0.0008401870727539062 s
DEBUG 10-15 15:26:50 lpllm.py:511] restore layer cost 0.001005411148071289 s
DEBUG 10-15 15:26:50 lpllm.py:512] reset_next_layer_need: layer_attn_idx=12, layer_mlp_idx=12, j_loc=24
DEBUG 10-15 15:26:50 lpllm.py:1037] reset layer cost 0.0010738372802734375 s
DEBUG 10-15 15:26:50 lpllm.py:1038] have reset next layer layer_attn 12 layer_mlp 12 
DEBUG 10-15 15:26:50 lpllm.py:924] 
DEBUG 10-15 15:26:50 lpllm.py:924] decoder loop j: 25 cur_layer_idx: 12 layer_attn_idx: 12 layer_mlp_idx: 12
DEBUG 10-15 15:26:50 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:26:50 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:26:50 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:26:50 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:26:50 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:26:50 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:50 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:50 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:50 lpmodule.py:506]         ...,
DEBUG 10-15 15:26:50 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:50 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:50 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:26:50 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:26:50 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:26:50 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:26:50 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:26:50 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:50 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:50 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:26:50 lpllm.py:2265] GPU2CPU move cost 0.000303 seconds
DEBUG 10-15 15:26:50 lpmodule.py:340] layer_idx 12 attn layer_idx: 12
DEBUG 10-15 15:26:50 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:26:50 lpmodule.py:364] decoder_attn_batch update batch_dim 60-120
DEBUG 10-15 15:26:50 StaticCacheLen.py:84] static cache update layer_idx: 12, start_batch: 60, end_batch: 120
DEBUG 10-15 15:26:50 lpmodule.py:368] update for kv cache 60-120 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:26:50 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:26:50 lpmodule.py:374] update past key value cost 0.005100 seconds
DEBUG 10-15 15:26:50 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:26:50 lpmodule.py:227] layer idx 12
DEBUG 10-15 15:26:50 lpmodule.py:399] repeat qkv cost 0.033184 seconds
DEBUG 10-15 15:26:50 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:26:50 lpmodule.py:433] dot attn cost 0.052544 seconds
DEBUG 10-15 15:26:50 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:26:50 lpmodule.py:444] time cost move to cuda:1 0.02785468101501465 s
DEBUG 10-15 15:26:50 lpllm.py:2283] CPU attn cost 0.158223 seconds if batch True
DEBUG 10-15 15:26:50 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:26:50 lpllm.py:2294] CPU compute cost 0.158764 seconds
DEBUG 10-15 15:26:50 lpllm.py:2312] free cost 0.000090 seconds
DEBUG 10-15 15:26:50 lpllm.py:498] reset update attn
DEBUG 10-15 15:26:50 lpllm.py:1774] update state cost 3.2901763916015625e-05 s
DEBUG 10-15 15:26:50 lpllm.py:1743] restore layer func cost 0.0003788471221923828 s
DEBUG 10-15 15:26:50 lpllm.py:511] restore layer cost 0.0005471706390380859 s
DEBUG 10-15 15:26:50 lpllm.py:512] reset_next_layer_need: layer_attn_idx=13, layer_mlp_idx=12, j_loc=25
DEBUG 10-15 15:26:50 lpllm.py:1037] reset layer cost 0.0006158351898193359 s
DEBUG 10-15 15:26:50 lpllm.py:1038] have reset next layer layer_attn 13 layer_mlp 12 
DEBUG 10-15 15:26:50 lpllm.py:1044] j: 25 waiting the layer with layer_idx 13 before wait time 0.4450266361236572 s
INFO 10-15 15:26:50 client.py:117] confirm_model_loaded: Mixtral-8x7B, 3cc84712-fa4a-4ffe-9d85-8b904ee90159
INFO 10-15 15:26:50 client.py:125] Model loaded
DEBUG 10-15 15:26:50 lpllm.py:1048] j: load cost 0.4467504024505615 s waiting cost 0.0017082691192626953 s
DEBUG 10-15 15:26:50 lpllm.py:924] 
DEBUG 10-15 15:26:50 lpllm.py:924] decoder loop j: 26 cur_layer_idx: 13 layer_attn_idx: 13 layer_mlp_idx: 12
DEBUG 10-15 15:26:50 lpllm.py:933] start load next layer cur_layer_idx: 14
DEBUG 10-15 15:26:50 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed6e0>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x004\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x98\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:26:50 client.py:72] load_into_gpu: Mixtral-8x7B, 73463787-8dc2-41d1-95a2-11134640c986
INFO 10-15 15:26:50 client.py:113] Model loaded: Mixtral-8x7B, 73463787-8dc2-41d1-95a2-11134640c986
DEBUG 10-15 15:26:50 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:26:50 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:26:50 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:26:50 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:26:50 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:26:50 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:50 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:50 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:50 lpmodule.py:506]         ...,
DEBUG 10-15 15:26:50 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:50 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:50 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:26:50 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:26:50 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:26:50 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:26:50 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:26:50 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:50 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:50 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:26:50 lpllm.py:2265] GPU2CPU move cost 0.000240 seconds
DEBUG 10-15 15:26:50 lpmodule.py:340] layer_idx 13 attn layer_idx: 13
DEBUG 10-15 15:26:50 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:26:50 lpmodule.py:364] decoder_attn_batch update batch_dim 0-60
DEBUG 10-15 15:26:50 StaticCacheLen.py:84] static cache update layer_idx: 13, start_batch: 0, end_batch: 60
DEBUG 10-15 15:26:50 lpmodule.py:368] update for kv cache 0-60 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:26:50 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:26:50 lpmodule.py:374] update past key value cost 0.005034 seconds
DEBUG 10-15 15:26:50 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:26:50 lpmodule.py:227] layer idx 12
DEBUG 10-15 15:26:50 lpmodule.py:399] repeat qkv cost 0.034419 seconds
DEBUG 10-15 15:26:50 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:26:50 lpmodule.py:433] dot attn cost 0.035125 seconds
DEBUG 10-15 15:26:50 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:26:50 lpmodule.py:444] time cost move to cuda:1 0.026758909225463867 s
DEBUG 10-15 15:26:50 lpllm.py:2283] CPU attn cost 0.141177 seconds if batch True
DEBUG 10-15 15:26:50 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:26:50 lpllm.py:2294] CPU compute cost 0.141626 seconds
DEBUG 10-15 15:26:50 lpllm.py:2312] free cost 0.000072 seconds
DEBUG 10-15 15:26:51 lpllm.py:503] reset update experts
DEBUG 10-15 15:26:51 lpllm.py:1774] update state cost 3.123283386230469e-05 s
DEBUG 10-15 15:26:51 lpllm.py:1743] restore layer func cost 0.0009744167327880859 s
DEBUG 10-15 15:26:51 lpllm.py:511] restore layer cost 0.0011441707611083984 s
DEBUG 10-15 15:26:51 lpllm.py:512] reset_next_layer_need: layer_attn_idx=13, layer_mlp_idx=13, j_loc=26
DEBUG 10-15 15:26:51 lpllm.py:1037] reset layer cost 0.001216888427734375 s
DEBUG 10-15 15:26:51 lpllm.py:1038] have reset next layer layer_attn 13 layer_mlp 13 
DEBUG 10-15 15:26:51 lpllm.py:924] 
DEBUG 10-15 15:26:51 lpllm.py:924] decoder loop j: 27 cur_layer_idx: 13 layer_attn_idx: 13 layer_mlp_idx: 13
DEBUG 10-15 15:26:51 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:26:51 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:26:51 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:26:51 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:26:51 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:26:51 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:51 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:51 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:51 lpmodule.py:506]         ...,
DEBUG 10-15 15:26:51 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:51 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:51 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:26:51 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:26:51 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:26:51 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:26:51 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:26:51 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:51 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:51 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:26:51 lpllm.py:2265] GPU2CPU move cost 0.000312 seconds
DEBUG 10-15 15:26:51 lpmodule.py:340] layer_idx 13 attn layer_idx: 13
DEBUG 10-15 15:26:51 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:26:51 lpmodule.py:364] decoder_attn_batch update batch_dim 60-120
DEBUG 10-15 15:26:51 StaticCacheLen.py:84] static cache update layer_idx: 13, start_batch: 60, end_batch: 120
DEBUG 10-15 15:26:51 lpmodule.py:368] update for kv cache 60-120 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:26:51 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:26:51 lpmodule.py:374] update past key value cost 0.004961 seconds
DEBUG 10-15 15:26:51 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:26:51 lpmodule.py:227] layer idx 13
DEBUG 10-15 15:26:51 lpmodule.py:399] repeat qkv cost 0.033982 seconds
DEBUG 10-15 15:26:51 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:26:51 lpmodule.py:433] dot attn cost 0.038485 seconds
DEBUG 10-15 15:26:51 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:26:51 lpmodule.py:444] time cost move to cuda:1 0.026416778564453125 s
DEBUG 10-15 15:26:51 lpllm.py:2283] CPU attn cost 0.143806 seconds if batch True
DEBUG 10-15 15:26:51 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:26:51 lpllm.py:2294] CPU compute cost 0.144347 seconds
DEBUG 10-15 15:26:51 lpllm.py:2312] free cost 0.000072 seconds
DEBUG 10-15 15:26:51 lpllm.py:498] reset update attn
DEBUG 10-15 15:26:51 lpllm.py:1774] update state cost 1.7642974853515625e-05 s
DEBUG 10-15 15:26:51 lpllm.py:1743] restore layer func cost 0.00041961669921875 s
DEBUG 10-15 15:26:51 lpllm.py:511] restore layer cost 0.0005791187286376953 s
DEBUG 10-15 15:26:51 lpllm.py:512] reset_next_layer_need: layer_attn_idx=14, layer_mlp_idx=13, j_loc=27
DEBUG 10-15 15:26:51 lpllm.py:1037] reset layer cost 0.0006513595581054688 s
DEBUG 10-15 15:26:51 lpllm.py:1038] have reset next layer layer_attn 14 layer_mlp 13 
DEBUG 10-15 15:26:51 lpllm.py:1044] j: 27 waiting the layer with layer_idx 14 before wait time 0.4840545654296875 s
INFO 10-15 15:26:51 client.py:117] confirm_model_loaded: Mixtral-8x7B, 73463787-8dc2-41d1-95a2-11134640c986
INFO 10-15 15:26:51 client.py:125] Model loaded
DEBUG 10-15 15:26:51 lpllm.py:1048] j: load cost 0.48591136932373047 s waiting cost 0.0018391609191894531 s
DEBUG 10-15 15:26:51 lpllm.py:924] 
DEBUG 10-15 15:26:51 lpllm.py:924] decoder loop j: 28 cur_layer_idx: 14 layer_attn_idx: 14 layer_mlp_idx: 13
DEBUG 10-15 15:26:51 lpllm.py:933] start load next layer cur_layer_idx: 15
DEBUG 10-15 15:26:51 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:26:51 client.py:72] load_into_gpu: Mixtral-8x7B, eb6fd88a-c068-4cb8-b78c-1f1f9dd83560
INFO 10-15 15:26:51 client.py:113] Model loaded: Mixtral-8x7B, eb6fd88a-c068-4cb8-b78c-1f1f9dd83560
DEBUG 10-15 15:26:51 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:26:51 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:26:51 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:26:51 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:26:51 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:26:51 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:51 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:51 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:51 lpmodule.py:506]         ...,
DEBUG 10-15 15:26:51 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:51 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:51 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:26:51 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:26:51 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:26:51 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:26:51 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:26:51 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:51 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:51 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:26:51 lpllm.py:2265] GPU2CPU move cost 0.000493 seconds
DEBUG 10-15 15:26:51 lpmodule.py:340] layer_idx 14 attn layer_idx: 14
DEBUG 10-15 15:26:51 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:26:51 lpmodule.py:364] decoder_attn_batch update batch_dim 0-60
DEBUG 10-15 15:26:51 StaticCacheLen.py:84] static cache update layer_idx: 14, start_batch: 0, end_batch: 60
DEBUG 10-15 15:26:51 lpmodule.py:368] update for kv cache 0-60 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:26:51 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:26:51 lpmodule.py:374] update past key value cost 0.005861 seconds
DEBUG 10-15 15:26:51 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:26:51 lpmodule.py:227] layer idx 13
DEBUG 10-15 15:26:51 lpmodule.py:399] repeat qkv cost 0.033891 seconds
DEBUG 10-15 15:26:51 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:26:51 lpmodule.py:433] dot attn cost 0.038135 seconds
DEBUG 10-15 15:26:51 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:26:51 lpmodule.py:444] time cost move to cuda:1 0.025782346725463867 s
DEBUG 10-15 15:26:51 lpllm.py:2283] CPU attn cost 0.141080 seconds if batch True
DEBUG 10-15 15:26:51 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:26:51 lpllm.py:2294] CPU compute cost 0.141855 seconds
DEBUG 10-15 15:26:51 lpllm.py:2312] free cost 0.000091 seconds
DEBUG 10-15 15:26:51 lpllm.py:503] reset update experts
DEBUG 10-15 15:26:51 lpllm.py:1774] update state cost 3.504753112792969e-05 s
DEBUG 10-15 15:26:51 lpllm.py:1743] restore layer func cost 0.0009844303131103516 s
DEBUG 10-15 15:26:51 lpllm.py:511] restore layer cost 0.001251220703125 s
DEBUG 10-15 15:26:51 lpllm.py:512] reset_next_layer_need: layer_attn_idx=14, layer_mlp_idx=14, j_loc=28
DEBUG 10-15 15:26:51 lpllm.py:1037] reset layer cost 0.001329183578491211 s
DEBUG 10-15 15:26:51 lpllm.py:1038] have reset next layer layer_attn 14 layer_mlp 14 
DEBUG 10-15 15:26:51 lpllm.py:924] 
DEBUG 10-15 15:26:51 lpllm.py:924] decoder loop j: 29 cur_layer_idx: 14 layer_attn_idx: 14 layer_mlp_idx: 14
DEBUG 10-15 15:26:51 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:26:51 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:26:51 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:26:51 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:26:51 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:26:51 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:51 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:51 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:51 lpmodule.py:506]         ...,
DEBUG 10-15 15:26:51 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:51 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:51 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:26:51 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:26:51 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:26:51 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:26:51 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:26:51 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:51 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:51 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:26:51 lpllm.py:2265] GPU2CPU move cost 0.000660 seconds
DEBUG 10-15 15:26:51 lpmodule.py:340] layer_idx 14 attn layer_idx: 14
DEBUG 10-15 15:26:51 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:26:51 lpmodule.py:364] decoder_attn_batch update batch_dim 60-120
DEBUG 10-15 15:26:51 StaticCacheLen.py:84] static cache update layer_idx: 14, start_batch: 60, end_batch: 120
DEBUG 10-15 15:26:51 lpmodule.py:368] update for kv cache 60-120 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:26:51 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:26:51 lpmodule.py:374] update past key value cost 0.005197 seconds
DEBUG 10-15 15:26:51 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:26:51 lpmodule.py:227] layer idx 14
DEBUG 10-15 15:26:51 lpmodule.py:399] repeat qkv cost 0.033989 seconds
DEBUG 10-15 15:26:51 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:26:51 lpmodule.py:433] dot attn cost 0.034835 seconds
DEBUG 10-15 15:26:51 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:26:51 lpmodule.py:444] time cost move to cuda:1 0.02585434913635254 s
DEBUG 10-15 15:26:51 lpllm.py:2283] CPU attn cost 0.136991 seconds if batch True
DEBUG 10-15 15:26:51 lpllm.py:2292] deal attn result cost 0.000003 seconds
DEBUG 10-15 15:26:51 lpllm.py:2294] CPU compute cost 0.137985 seconds
DEBUG 10-15 15:26:51 lpllm.py:2312] free cost 0.000091 seconds
DEBUG 10-15 15:26:51 lpllm.py:498] reset update attn
DEBUG 10-15 15:26:51 lpllm.py:1774] update state cost 3.5762786865234375e-05 s
DEBUG 10-15 15:26:51 lpllm.py:1743] restore layer func cost 0.0004169940948486328 s
DEBUG 10-15 15:26:51 lpllm.py:511] restore layer cost 0.0007011890411376953 s
DEBUG 10-15 15:26:51 lpllm.py:512] reset_next_layer_need: layer_attn_idx=15, layer_mlp_idx=14, j_loc=29
DEBUG 10-15 15:26:51 lpllm.py:1037] reset layer cost 0.0007939338684082031 s
DEBUG 10-15 15:26:51 lpllm.py:1038] have reset next layer layer_attn 15 layer_mlp 14 
DEBUG 10-15 15:26:51 lpllm.py:1044] j: 29 waiting the layer with layer_idx 15 before wait time 0.44204258918762207 s
INFO 10-15 15:26:51 client.py:117] confirm_model_loaded: Mixtral-8x7B, eb6fd88a-c068-4cb8-b78c-1f1f9dd83560
INFO 10-15 15:26:51 client.py:125] Model loaded
DEBUG 10-15 15:26:51 lpllm.py:1048] j: load cost 0.44408130645751953 s waiting cost 0.002021312713623047 s
DEBUG 10-15 15:26:51 lpllm.py:924] 
DEBUG 10-15 15:26:51 lpllm.py:924] decoder loop j: 30 cur_layer_idx: 15 layer_attn_idx: 15 layer_mlp_idx: 14
DEBUG 10-15 15:26:51 lpllm.py:933] start load next layer cur_layer_idx: 16
DEBUG 10-15 15:26:51 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed650>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x003\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x96\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:26:51 client.py:72] load_into_gpu: Mixtral-8x7B, c55ad4b8-57cd-4bbb-a5d8-dc4ad53d1329
INFO 10-15 15:26:51 client.py:113] Model loaded: Mixtral-8x7B, c55ad4b8-57cd-4bbb-a5d8-dc4ad53d1329
DEBUG 10-15 15:26:51 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:26:51 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:26:51 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:26:51 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:26:51 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:26:51 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:51 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:51 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:51 lpmodule.py:506]         ...,
DEBUG 10-15 15:26:51 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:51 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:51 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:26:51 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:26:51 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:26:51 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:26:51 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:26:51 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:51 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:51 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:26:51 lpllm.py:2265] GPU2CPU move cost 0.001154 seconds
DEBUG 10-15 15:26:51 lpmodule.py:340] layer_idx 15 attn layer_idx: 15
DEBUG 10-15 15:26:51 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:26:51 lpmodule.py:364] decoder_attn_batch update batch_dim 0-60
DEBUG 10-15 15:26:51 StaticCacheLen.py:84] static cache update layer_idx: 15, start_batch: 0, end_batch: 60
DEBUG 10-15 15:26:51 lpmodule.py:368] update for kv cache 0-60 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:26:51 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:26:51 lpmodule.py:374] update past key value cost 0.005408 seconds
DEBUG 10-15 15:26:51 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:26:51 lpmodule.py:399] repeat qkv cost 0.031919 seconds
DEBUG 10-15 15:26:51 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:26:51 lpmodule.py:227] layer idx 14
DEBUG 10-15 15:26:51 lpmodule.py:433] dot attn cost 0.039041 seconds
DEBUG 10-15 15:26:51 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:26:51 lpmodule.py:444] time cost move to cuda:1 0.02606344223022461 s
DEBUG 10-15 15:26:51 lpllm.py:2283] CPU attn cost 0.141158 seconds if batch True
DEBUG 10-15 15:26:51 lpllm.py:2292] deal attn result cost 0.000003 seconds
DEBUG 10-15 15:26:51 lpllm.py:2294] CPU compute cost 0.142657 seconds
DEBUG 10-15 15:26:51 lpllm.py:2312] free cost 0.000108 seconds
DEBUG 10-15 15:26:51 lpllm.py:503] reset update experts
DEBUG 10-15 15:26:51 lpllm.py:1774] update state cost 2.288818359375e-05 s
DEBUG 10-15 15:26:51 lpllm.py:1743] restore layer func cost 0.0013973712921142578 s
DEBUG 10-15 15:26:51 lpllm.py:511] restore layer cost 0.0017042160034179688 s
DEBUG 10-15 15:26:51 lpllm.py:512] reset_next_layer_need: layer_attn_idx=15, layer_mlp_idx=15, j_loc=30
DEBUG 10-15 15:26:51 lpllm.py:1037] reset layer cost 0.001821279525756836 s
DEBUG 10-15 15:26:51 lpllm.py:1038] have reset next layer layer_attn 15 layer_mlp 15 
DEBUG 10-15 15:26:51 lpllm.py:924] 
DEBUG 10-15 15:26:51 lpllm.py:924] decoder loop j: 31 cur_layer_idx: 15 layer_attn_idx: 15 layer_mlp_idx: 15
DEBUG 10-15 15:26:51 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:26:51 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:26:51 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:26:51 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:26:51 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:26:51 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:51 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:51 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:51 lpmodule.py:506]         ...,
DEBUG 10-15 15:26:51 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:51 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:51 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:26:51 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:26:51 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:26:51 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:26:51 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:26:51 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:51 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:51 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:26:51 lpllm.py:2265] GPU2CPU move cost 0.000769 seconds
DEBUG 10-15 15:26:51 lpmodule.py:340] layer_idx 15 attn layer_idx: 15
DEBUG 10-15 15:26:51 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:26:51 lpmodule.py:364] decoder_attn_batch update batch_dim 60-120
DEBUG 10-15 15:26:51 StaticCacheLen.py:84] static cache update layer_idx: 15, start_batch: 60, end_batch: 120
DEBUG 10-15 15:26:51 lpmodule.py:368] update for kv cache 60-120 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:26:51 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:26:51 lpmodule.py:374] update past key value cost 0.005438 seconds
DEBUG 10-15 15:26:51 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:26:52 lpmodule.py:227] layer idx 15
DEBUG 10-15 15:26:52 lpmodule.py:399] repeat qkv cost 0.033257 seconds
DEBUG 10-15 15:26:52 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:26:52 lpmodule.py:433] dot attn cost 0.032000 seconds
DEBUG 10-15 15:26:52 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:26:52 lpmodule.py:444] time cost move to cuda:1 0.03120589256286621 s
DEBUG 10-15 15:26:52 lpllm.py:2283] CPU attn cost 0.153889 seconds if batch True
DEBUG 10-15 15:26:52 lpllm.py:2292] deal attn result cost 0.000003 seconds
DEBUG 10-15 15:26:52 lpllm.py:2294] CPU compute cost 0.155059 seconds
DEBUG 10-15 15:26:52 lpllm.py:2312] free cost 0.000107 seconds
DEBUG 10-15 15:26:52 lpllm.py:498] reset update attn
DEBUG 10-15 15:26:52 lpllm.py:1774] update state cost 2.0503997802734375e-05 s
DEBUG 10-15 15:26:52 lpllm.py:1743] restore layer func cost 0.0004246234893798828 s
DEBUG 10-15 15:26:52 lpllm.py:511] restore layer cost 0.0006871223449707031 s
DEBUG 10-15 15:26:52 lpllm.py:512] reset_next_layer_need: layer_attn_idx=16, layer_mlp_idx=15, j_loc=31
DEBUG 10-15 15:26:52 lpllm.py:1037] reset layer cost 0.000762939453125 s
DEBUG 10-15 15:26:52 lpllm.py:1038] have reset next layer layer_attn 16 layer_mlp 15 
DEBUG 10-15 15:26:52 lpllm.py:1044] j: 31 waiting the layer with layer_idx 16 before wait time 0.4560375213623047 s
INFO 10-15 15:26:52 client.py:117] confirm_model_loaded: Mixtral-8x7B, c55ad4b8-57cd-4bbb-a5d8-dc4ad53d1329
INFO 10-15 15:26:52 client.py:125] Model loaded
DEBUG 10-15 15:26:52 lpllm.py:1048] j: load cost 0.45796799659729004 s waiting cost 0.0019145011901855469 s
DEBUG 10-15 15:26:52 lpllm.py:924] 
DEBUG 10-15 15:26:52 lpllm.py:924] decoder loop j: 32 cur_layer_idx: 16 layer_attn_idx: 16 layer_mlp_idx: 15
DEBUG 10-15 15:26:52 lpllm.py:933] start load next layer cur_layer_idx: 17
DEBUG 10-15 15:26:52 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed6e0>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x004\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x98\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:26:52 client.py:72] load_into_gpu: Mixtral-8x7B, d0a2c64b-2dcb-4852-a2b8-cd78fc28f9eb
INFO 10-15 15:26:52 client.py:113] Model loaded: Mixtral-8x7B, d0a2c64b-2dcb-4852-a2b8-cd78fc28f9eb
DEBUG 10-15 15:26:52 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:26:52 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:26:52 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:26:52 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:26:52 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:26:52 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:52 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:52 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:52 lpmodule.py:506]         ...,
DEBUG 10-15 15:26:52 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:52 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:52 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:26:52 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:26:52 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:26:52 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:26:52 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:26:52 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:52 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:52 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:26:52 lpllm.py:2265] GPU2CPU move cost 0.000446 seconds
DEBUG 10-15 15:26:52 lpmodule.py:340] layer_idx 16 attn layer_idx: 16
DEBUG 10-15 15:26:52 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:26:52 lpmodule.py:364] decoder_attn_batch update batch_dim 0-60
DEBUG 10-15 15:26:52 StaticCacheLen.py:84] static cache update layer_idx: 16, start_batch: 0, end_batch: 60
DEBUG 10-15 15:26:52 lpmodule.py:368] update for kv cache 0-60 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:26:52 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:26:52 lpmodule.py:374] update past key value cost 0.005525 seconds
DEBUG 10-15 15:26:52 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:26:52 lpmodule.py:227] layer idx 15
DEBUG 10-15 15:26:52 lpmodule.py:399] repeat qkv cost 0.034121 seconds
DEBUG 10-15 15:26:52 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:26:52 lpmodule.py:433] dot attn cost 0.033370 seconds
DEBUG 10-15 15:26:52 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:26:52 lpmodule.py:444] time cost move to cuda:1 0.030106067657470703 s
DEBUG 10-15 15:26:52 lpllm.py:2283] CPU attn cost 0.149538 seconds if batch True
DEBUG 10-15 15:26:52 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:26:52 lpllm.py:2294] CPU compute cost 0.150256 seconds
DEBUG 10-15 15:26:52 lpllm.py:2312] free cost 0.000106 seconds
DEBUG 10-15 15:26:52 lpllm.py:503] reset update experts
DEBUG 10-15 15:26:52 lpllm.py:1774] update state cost 3.3855438232421875e-05 s
DEBUG 10-15 15:26:52 lpllm.py:1743] restore layer func cost 0.0010755062103271484 s
DEBUG 10-15 15:26:52 lpllm.py:511] restore layer cost 0.0013365745544433594 s
DEBUG 10-15 15:26:52 lpllm.py:512] reset_next_layer_need: layer_attn_idx=16, layer_mlp_idx=16, j_loc=32
DEBUG 10-15 15:26:52 lpllm.py:1037] reset layer cost 0.001413583755493164 s
DEBUG 10-15 15:26:52 lpllm.py:1038] have reset next layer layer_attn 16 layer_mlp 16 
DEBUG 10-15 15:26:52 lpllm.py:924] 
DEBUG 10-15 15:26:52 lpllm.py:924] decoder loop j: 33 cur_layer_idx: 16 layer_attn_idx: 16 layer_mlp_idx: 16
DEBUG 10-15 15:26:52 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:26:52 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:26:52 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:26:52 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:26:52 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:26:52 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:52 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:52 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:52 lpmodule.py:506]         ...,
DEBUG 10-15 15:26:52 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:52 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:52 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:26:52 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:26:52 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:26:52 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:26:52 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:26:52 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:52 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:52 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:26:52 lpllm.py:2265] GPU2CPU move cost 0.000783 seconds
DEBUG 10-15 15:26:52 lpmodule.py:340] layer_idx 16 attn layer_idx: 16
DEBUG 10-15 15:26:52 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:26:52 lpmodule.py:364] decoder_attn_batch update batch_dim 60-120
DEBUG 10-15 15:26:52 StaticCacheLen.py:84] static cache update layer_idx: 16, start_batch: 60, end_batch: 120
DEBUG 10-15 15:26:52 lpmodule.py:368] update for kv cache 60-120 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:26:52 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:26:52 lpmodule.py:374] update past key value cost 0.005733 seconds
DEBUG 10-15 15:26:52 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:26:52 lpmodule.py:227] layer idx 16
DEBUG 10-15 15:26:52 lpmodule.py:399] repeat qkv cost 0.035311 seconds
DEBUG 10-15 15:26:52 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:26:52 lpmodule.py:433] dot attn cost 0.035569 seconds
DEBUG 10-15 15:26:52 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:26:52 lpmodule.py:444] time cost move to cuda:1 0.02981281280517578 s
DEBUG 10-15 15:26:52 lpllm.py:2283] CPU attn cost 0.154228 seconds if batch True
DEBUG 10-15 15:26:52 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:26:52 lpllm.py:2294] CPU compute cost 0.155412 seconds
DEBUG 10-15 15:26:52 lpllm.py:2312] free cost 0.000106 seconds
DEBUG 10-15 15:26:52 lpllm.py:498] reset update attn
DEBUG 10-15 15:26:52 lpllm.py:1774] update state cost 2.0742416381835938e-05 s
DEBUG 10-15 15:26:52 lpllm.py:1743] restore layer func cost 0.000518798828125 s
DEBUG 10-15 15:26:52 lpllm.py:511] restore layer cost 0.0007734298706054688 s
DEBUG 10-15 15:26:52 lpllm.py:512] reset_next_layer_need: layer_attn_idx=17, layer_mlp_idx=16, j_loc=33
DEBUG 10-15 15:26:52 lpllm.py:1037] reset layer cost 0.0008697509765625 s
DEBUG 10-15 15:26:52 lpllm.py:1038] have reset next layer layer_attn 17 layer_mlp 16 
DEBUG 10-15 15:26:52 lpllm.py:1044] j: 33 waiting the layer with layer_idx 17 before wait time 0.44469714164733887 s
INFO 10-15 15:26:52 client.py:117] confirm_model_loaded: Mixtral-8x7B, d0a2c64b-2dcb-4852-a2b8-cd78fc28f9eb
INFO 10-15 15:26:52 client.py:125] Model loaded
DEBUG 10-15 15:26:52 lpllm.py:1048] j: load cost 0.4464867115020752 s waiting cost 0.0017695426940917969 s
DEBUG 10-15 15:26:52 lpllm.py:924] 
DEBUG 10-15 15:26:52 lpllm.py:924] decoder loop j: 34 cur_layer_idx: 17 layer_attn_idx: 17 layer_mlp_idx: 16
DEBUG 10-15 15:26:52 lpllm.py:933] start load next layer cur_layer_idx: 18
DEBUG 10-15 15:26:52 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:26:52 client.py:72] load_into_gpu: Mixtral-8x7B, 63b8fe2c-c5da-4c7e-817d-a27450335a68
INFO 10-15 15:26:52 client.py:113] Model loaded: Mixtral-8x7B, 63b8fe2c-c5da-4c7e-817d-a27450335a68
DEBUG 10-15 15:26:52 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:26:52 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:26:52 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:26:52 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:26:52 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:26:52 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:52 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:52 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:52 lpmodule.py:506]         ...,
DEBUG 10-15 15:26:52 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:52 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:52 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:26:52 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:26:52 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:26:52 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:26:52 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:26:52 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:52 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:52 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:26:52 lpllm.py:2265] GPU2CPU move cost 0.000638 seconds
DEBUG 10-15 15:26:52 lpmodule.py:340] layer_idx 17 attn layer_idx: 17
DEBUG 10-15 15:26:52 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:26:52 lpmodule.py:364] decoder_attn_batch update batch_dim 0-60
DEBUG 10-15 15:26:52 StaticCacheLen.py:84] static cache update layer_idx: 17, start_batch: 0, end_batch: 60
DEBUG 10-15 15:26:52 lpmodule.py:368] update for kv cache 0-60 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:26:52 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:26:52 lpmodule.py:374] update past key value cost 0.005861 seconds
DEBUG 10-15 15:26:52 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:26:52 lpmodule.py:227] layer idx 16
DEBUG 10-15 15:26:52 lpmodule.py:399] repeat qkv cost 0.033086 seconds
DEBUG 10-15 15:26:52 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:26:52 lpmodule.py:433] dot attn cost 0.034215 seconds
DEBUG 10-15 15:26:52 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:26:52 lpmodule.py:444] time cost move to cuda:1 0.026861190795898438 s
DEBUG 10-15 15:26:52 lpllm.py:2283] CPU attn cost 0.140930 seconds if batch True
DEBUG 10-15 15:26:52 lpllm.py:2292] deal attn result cost 0.000003 seconds
DEBUG 10-15 15:26:52 lpllm.py:2294] CPU compute cost 0.141992 seconds
DEBUG 10-15 15:26:52 lpllm.py:2312] free cost 0.000115 seconds
DEBUG 10-15 15:26:52 lpllm.py:503] reset update experts
DEBUG 10-15 15:26:52 lpllm.py:1774] update state cost 2.0265579223632812e-05 s
DEBUG 10-15 15:26:52 lpllm.py:1743] restore layer func cost 0.0010263919830322266 s
DEBUG 10-15 15:26:52 lpllm.py:511] restore layer cost 0.0012791156768798828 s
DEBUG 10-15 15:26:52 lpllm.py:512] reset_next_layer_need: layer_attn_idx=17, layer_mlp_idx=17, j_loc=34
DEBUG 10-15 15:26:52 lpllm.py:1037] reset layer cost 0.0013909339904785156 s
DEBUG 10-15 15:26:52 lpllm.py:1038] have reset next layer layer_attn 17 layer_mlp 17 
DEBUG 10-15 15:26:52 lpllm.py:924] 
DEBUG 10-15 15:26:52 lpllm.py:924] decoder loop j: 35 cur_layer_idx: 17 layer_attn_idx: 17 layer_mlp_idx: 17
DEBUG 10-15 15:26:52 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:26:52 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:26:52 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:26:52 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:26:52 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:26:52 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:52 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:52 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:52 lpmodule.py:506]         ...,
DEBUG 10-15 15:26:52 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:52 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:52 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:26:52 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:26:52 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:26:52 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:26:52 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:26:52 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:52 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:52 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:26:52 lpllm.py:2265] GPU2CPU move cost 0.000645 seconds
DEBUG 10-15 15:26:52 lpmodule.py:340] layer_idx 17 attn layer_idx: 17
DEBUG 10-15 15:26:52 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:26:52 lpmodule.py:364] decoder_attn_batch update batch_dim 60-120
DEBUG 10-15 15:26:52 StaticCacheLen.py:84] static cache update layer_idx: 17, start_batch: 60, end_batch: 120
DEBUG 10-15 15:26:52 lpmodule.py:368] update for kv cache 60-120 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:26:52 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:26:52 lpmodule.py:374] update past key value cost 0.005964 seconds
DEBUG 10-15 15:26:52 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:26:52 lpmodule.py:227] layer idx 17
DEBUG 10-15 15:26:52 lpmodule.py:399] repeat qkv cost 0.032566 seconds
DEBUG 10-15 15:26:52 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:26:52 lpmodule.py:433] dot attn cost 0.034174 seconds
DEBUG 10-15 15:26:52 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:26:52 lpmodule.py:444] time cost move to cuda:1 0.027172327041625977 s
DEBUG 10-15 15:26:53 lpllm.py:2283] CPU attn cost 0.144181 seconds if batch True
DEBUG 10-15 15:26:53 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:26:53 lpllm.py:2294] CPU compute cost 0.145236 seconds
DEBUG 10-15 15:26:53 lpllm.py:2312] free cost 0.000137 seconds
DEBUG 10-15 15:26:53 lpllm.py:498] reset update attn
DEBUG 10-15 15:26:53 lpllm.py:1774] update state cost 3.719329833984375e-05 s
DEBUG 10-15 15:26:53 lpllm.py:1743] restore layer func cost 0.0004954338073730469 s
DEBUG 10-15 15:26:53 lpllm.py:511] restore layer cost 0.0007944107055664062 s
DEBUG 10-15 15:26:53 lpllm.py:512] reset_next_layer_need: layer_attn_idx=18, layer_mlp_idx=17, j_loc=35
DEBUG 10-15 15:26:53 lpllm.py:1037] reset layer cost 0.0008690357208251953 s
DEBUG 10-15 15:26:53 lpllm.py:1038] have reset next layer layer_attn 18 layer_mlp 17 
DEBUG 10-15 15:26:53 lpllm.py:1044] j: 35 waiting the layer with layer_idx 18 before wait time 0.4495713710784912 s
INFO 10-15 15:26:53 client.py:117] confirm_model_loaded: Mixtral-8x7B, 63b8fe2c-c5da-4c7e-817d-a27450335a68
INFO 10-15 15:26:53 client.py:125] Model loaded
DEBUG 10-15 15:26:53 lpllm.py:1048] j: load cost 0.45130443572998047 s waiting cost 0.00171661376953125 s
DEBUG 10-15 15:26:53 lpllm.py:924] 
DEBUG 10-15 15:26:53 lpllm.py:924] decoder loop j: 36 cur_layer_idx: 18 layer_attn_idx: 18 layer_mlp_idx: 17
DEBUG 10-15 15:26:53 lpllm.py:933] start load next layer cur_layer_idx: 19
DEBUG 10-15 15:26:53 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed650>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x003\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x96\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:26:53 client.py:72] load_into_gpu: Mixtral-8x7B, 9f4deb26-1049-43e9-bdcb-fdb4545e4673
INFO 10-15 15:26:53 client.py:113] Model loaded: Mixtral-8x7B, 9f4deb26-1049-43e9-bdcb-fdb4545e4673
DEBUG 10-15 15:26:53 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:26:53 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:26:53 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:26:53 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:26:53 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:26:53 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:53 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:53 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:53 lpmodule.py:506]         ...,
DEBUG 10-15 15:26:53 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:53 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:53 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:26:53 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:26:53 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:26:53 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:26:53 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:26:53 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:53 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:53 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:26:53 lpllm.py:2265] GPU2CPU move cost 0.001184 seconds
DEBUG 10-15 15:26:53 lpmodule.py:340] layer_idx 18 attn layer_idx: 18
DEBUG 10-15 15:26:53 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:26:53 lpmodule.py:364] decoder_attn_batch update batch_dim 0-60
DEBUG 10-15 15:26:53 StaticCacheLen.py:84] static cache update layer_idx: 18, start_batch: 0, end_batch: 60
DEBUG 10-15 15:26:53 lpmodule.py:368] update for kv cache 0-60 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:26:53 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:26:53 lpmodule.py:374] update past key value cost 0.006176 seconds
DEBUG 10-15 15:26:53 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:26:53 lpmodule.py:227] layer idx 17
DEBUG 10-15 15:26:53 lpmodule.py:399] repeat qkv cost 0.032753 seconds
DEBUG 10-15 15:26:53 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:26:53 lpmodule.py:433] dot attn cost 0.035805 seconds
DEBUG 10-15 15:26:53 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:26:53 lpmodule.py:444] time cost move to cuda:1 0.02788567543029785 s
DEBUG 10-15 15:26:53 lpllm.py:2283] CPU attn cost 0.148148 seconds if batch True
DEBUG 10-15 15:26:53 lpllm.py:2292] deal attn result cost 0.000004 seconds
DEBUG 10-15 15:26:53 lpllm.py:2294] CPU compute cost 0.149992 seconds
DEBUG 10-15 15:26:53 lpllm.py:2312] free cost 0.000179 seconds
DEBUG 10-15 15:26:53 lpllm.py:503] reset update experts
DEBUG 10-15 15:26:53 lpllm.py:1774] update state cost 4.7206878662109375e-05 s
DEBUG 10-15 15:26:53 lpllm.py:1743] restore layer func cost 0.0012333393096923828 s
DEBUG 10-15 15:26:53 lpllm.py:511] restore layer cost 0.001585245132446289 s
DEBUG 10-15 15:26:53 lpllm.py:512] reset_next_layer_need: layer_attn_idx=18, layer_mlp_idx=18, j_loc=36
DEBUG 10-15 15:26:53 lpllm.py:1037] reset layer cost 0.0017139911651611328 s
DEBUG 10-15 15:26:53 lpllm.py:1038] have reset next layer layer_attn 18 layer_mlp 18 
DEBUG 10-15 15:26:53 lpllm.py:924] 
DEBUG 10-15 15:26:53 lpllm.py:924] decoder loop j: 37 cur_layer_idx: 18 layer_attn_idx: 18 layer_mlp_idx: 18
DEBUG 10-15 15:26:53 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:26:53 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:26:53 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:26:53 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:26:53 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:26:53 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:53 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:53 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:53 lpmodule.py:506]         ...,
DEBUG 10-15 15:26:53 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:53 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:53 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:26:53 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:26:53 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:26:53 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:26:53 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:26:53 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:53 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:53 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:26:53 lpllm.py:2265] GPU2CPU move cost 0.000853 seconds
DEBUG 10-15 15:26:53 lpmodule.py:340] layer_idx 18 attn layer_idx: 18
DEBUG 10-15 15:26:53 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:26:53 lpmodule.py:364] decoder_attn_batch update batch_dim 60-120
DEBUG 10-15 15:26:53 StaticCacheLen.py:84] static cache update layer_idx: 18, start_batch: 60, end_batch: 120
DEBUG 10-15 15:26:53 lpmodule.py:368] update for kv cache 60-120 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:26:53 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:26:53 lpmodule.py:374] update past key value cost 0.005778 seconds
DEBUG 10-15 15:26:53 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:26:53 lpmodule.py:227] layer idx 18
DEBUG 10-15 15:26:53 lpmodule.py:399] repeat qkv cost 0.031923 seconds
DEBUG 10-15 15:26:53 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:26:53 lpmodule.py:433] dot attn cost 0.035327 seconds
DEBUG 10-15 15:26:53 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:26:53 lpmodule.py:444] time cost move to cuda:1 0.029407501220703125 s
DEBUG 10-15 15:26:53 lpllm.py:2283] CPU attn cost 0.145917 seconds if batch True
DEBUG 10-15 15:26:53 lpllm.py:2292] deal attn result cost 0.000003 seconds
DEBUG 10-15 15:26:53 lpllm.py:2294] CPU compute cost 0.147229 seconds
DEBUG 10-15 15:26:53 lpllm.py:2312] free cost 0.000104 seconds
DEBUG 10-15 15:26:53 lpllm.py:498] reset update attn
DEBUG 10-15 15:26:53 lpllm.py:1774] update state cost 2.0265579223632812e-05 s
DEBUG 10-15 15:26:53 lpllm.py:1743] restore layer func cost 0.0004284381866455078 s
DEBUG 10-15 15:26:53 lpllm.py:511] restore layer cost 0.0007126331329345703 s
DEBUG 10-15 15:26:53 lpllm.py:512] reset_next_layer_need: layer_attn_idx=19, layer_mlp_idx=18, j_loc=37
DEBUG 10-15 15:26:53 lpllm.py:1037] reset layer cost 0.0008058547973632812 s
DEBUG 10-15 15:26:53 lpllm.py:1038] have reset next layer layer_attn 19 layer_mlp 18 
DEBUG 10-15 15:26:53 lpllm.py:1044] j: 37 waiting the layer with layer_idx 19 before wait time 0.448167085647583 s
INFO 10-15 15:26:53 client.py:117] confirm_model_loaded: Mixtral-8x7B, 9f4deb26-1049-43e9-bdcb-fdb4545e4673
INFO 10-15 15:26:53 client.py:125] Model loaded
DEBUG 10-15 15:26:53 lpllm.py:1048] j: load cost 0.44966769218444824 s waiting cost 0.0014836788177490234 s
DEBUG 10-15 15:26:53 lpllm.py:924] 
DEBUG 10-15 15:26:53 lpllm.py:924] decoder loop j: 38 cur_layer_idx: 19 layer_attn_idx: 19 layer_mlp_idx: 18
DEBUG 10-15 15:26:53 lpllm.py:933] start load next layer cur_layer_idx: 20
DEBUG 10-15 15:26:53 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed6e0>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x004\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x98\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:26:53 client.py:72] load_into_gpu: Mixtral-8x7B, c7d5dd34-8d37-4b11-b5c5-156d46da5ad6
INFO 10-15 15:26:53 client.py:113] Model loaded: Mixtral-8x7B, c7d5dd34-8d37-4b11-b5c5-156d46da5ad6
DEBUG 10-15 15:26:53 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:26:53 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:26:53 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:26:53 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:26:53 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:26:53 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:53 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:53 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:53 lpmodule.py:506]         ...,
DEBUG 10-15 15:26:53 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:53 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:53 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:26:53 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:26:53 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:26:53 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:26:53 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:26:53 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:53 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:53 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:26:53 lpllm.py:2265] GPU2CPU move cost 0.000690 seconds
DEBUG 10-15 15:26:53 lpmodule.py:340] layer_idx 19 attn layer_idx: 19
DEBUG 10-15 15:26:53 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:26:53 lpmodule.py:364] decoder_attn_batch update batch_dim 0-60
DEBUG 10-15 15:26:53 StaticCacheLen.py:84] static cache update layer_idx: 19, start_batch: 0, end_batch: 60
DEBUG 10-15 15:26:53 lpmodule.py:368] update for kv cache 0-60 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:26:53 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:26:53 lpmodule.py:374] update past key value cost 0.005429 seconds
DEBUG 10-15 15:26:53 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:26:53 lpmodule.py:227] layer idx 18
DEBUG 10-15 15:26:53 lpmodule.py:399] repeat qkv cost 0.034315 seconds
DEBUG 10-15 15:26:53 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:26:53 lpmodule.py:433] dot attn cost 0.035565 seconds
DEBUG 10-15 15:26:53 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:26:53 lpmodule.py:444] time cost move to cuda:1 0.026926755905151367 s
DEBUG 10-15 15:26:53 lpllm.py:2283] CPU attn cost 0.144146 seconds if batch True
DEBUG 10-15 15:26:53 lpllm.py:2292] deal attn result cost 0.000003 seconds
DEBUG 10-15 15:26:53 lpllm.py:2294] CPU compute cost 0.145207 seconds
DEBUG 10-15 15:26:53 lpllm.py:2312] free cost 0.000098 seconds
DEBUG 10-15 15:26:53 lpllm.py:503] reset update experts
DEBUG 10-15 15:26:53 lpllm.py:1774] update state cost 1.7642974853515625e-05 s
DEBUG 10-15 15:26:53 lpllm.py:1743] restore layer func cost 0.0009806156158447266 s
DEBUG 10-15 15:26:53 lpllm.py:511] restore layer cost 0.0012273788452148438 s
DEBUG 10-15 15:26:53 lpllm.py:512] reset_next_layer_need: layer_attn_idx=19, layer_mlp_idx=19, j_loc=38
DEBUG 10-15 15:26:53 lpllm.py:1037] reset layer cost 0.0013206005096435547 s
DEBUG 10-15 15:26:53 lpllm.py:1038] have reset next layer layer_attn 19 layer_mlp 19 
DEBUG 10-15 15:26:53 lpllm.py:924] 
DEBUG 10-15 15:26:53 lpllm.py:924] decoder loop j: 39 cur_layer_idx: 19 layer_attn_idx: 19 layer_mlp_idx: 19
DEBUG 10-15 15:26:53 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:26:53 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:26:53 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:26:53 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:26:53 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:26:53 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:53 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:53 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:53 lpmodule.py:506]         ...,
DEBUG 10-15 15:26:53 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:53 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:53 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:26:53 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:26:53 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:26:53 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:26:53 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:26:53 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:53 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:53 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:26:53 lpllm.py:2265] GPU2CPU move cost 0.000666 seconds
DEBUG 10-15 15:26:53 lpmodule.py:340] layer_idx 19 attn layer_idx: 19
DEBUG 10-15 15:26:53 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:26:53 lpmodule.py:364] decoder_attn_batch update batch_dim 60-120
DEBUG 10-15 15:26:53 StaticCacheLen.py:84] static cache update layer_idx: 19, start_batch: 60, end_batch: 120
DEBUG 10-15 15:26:53 lpmodule.py:368] update for kv cache 60-120 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:26:53 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:26:53 lpmodule.py:374] update past key value cost 0.005823 seconds
DEBUG 10-15 15:26:53 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:26:53 lpmodule.py:227] layer idx 19
DEBUG 10-15 15:26:53 lpmodule.py:399] repeat qkv cost 0.035227 seconds
DEBUG 10-15 15:26:53 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:26:53 lpmodule.py:433] dot attn cost 0.038425 seconds
DEBUG 10-15 15:26:53 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:26:53 lpmodule.py:444] time cost move to cuda:1 0.027483701705932617 s
DEBUG 10-15 15:26:53 lpllm.py:2283] CPU attn cost 0.149593 seconds if batch True
DEBUG 10-15 15:26:53 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:26:53 lpllm.py:2294] CPU compute cost 0.150630 seconds
DEBUG 10-15 15:26:53 lpllm.py:2312] free cost 0.000098 seconds
DEBUG 10-15 15:26:53 lpllm.py:498] reset update attn
DEBUG 10-15 15:26:53 lpllm.py:1774] update state cost 3.790855407714844e-05 s
DEBUG 10-15 15:26:53 lpllm.py:1743] restore layer func cost 0.00042700767517089844 s
DEBUG 10-15 15:26:53 lpllm.py:511] restore layer cost 0.0007121562957763672 s
DEBUG 10-15 15:26:53 lpllm.py:512] reset_next_layer_need: layer_attn_idx=20, layer_mlp_idx=19, j_loc=39
DEBUG 10-15 15:26:53 lpllm.py:1037] reset layer cost 0.0007915496826171875 s
DEBUG 10-15 15:26:53 lpllm.py:1038] have reset next layer layer_attn 20 layer_mlp 19 
DEBUG 10-15 15:26:53 lpllm.py:1044] j: 39 waiting the layer with layer_idx 20 before wait time 0.446730375289917 s
INFO 10-15 15:26:53 client.py:117] confirm_model_loaded: Mixtral-8x7B, c7d5dd34-8d37-4b11-b5c5-156d46da5ad6
INFO 10-15 15:26:53 client.py:125] Model loaded
DEBUG 10-15 15:26:53 lpllm.py:1048] j: load cost 0.4484524726867676 s waiting cost 0.001705169677734375 s
DEBUG 10-15 15:26:53 lpllm.py:924] 
DEBUG 10-15 15:26:53 lpllm.py:924] decoder loop j: 40 cur_layer_idx: 20 layer_attn_idx: 20 layer_mlp_idx: 19
DEBUG 10-15 15:26:53 lpllm.py:933] start load next layer cur_layer_idx: 21
DEBUG 10-15 15:26:53 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:26:53 client.py:72] load_into_gpu: Mixtral-8x7B, 1ccfcda0-f5ad-4abd-a9ae-b2eae65438cb
INFO 10-15 15:26:53 client.py:113] Model loaded: Mixtral-8x7B, 1ccfcda0-f5ad-4abd-a9ae-b2eae65438cb
DEBUG 10-15 15:26:53 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:26:53 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:26:53 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:26:53 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:26:53 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:26:53 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:53 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:53 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:53 lpmodule.py:506]         ...,
DEBUG 10-15 15:26:53 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:53 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:53 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:26:53 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:26:53 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:26:53 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:26:53 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:26:53 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:53 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:53 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:26:53 lpllm.py:2265] GPU2CPU move cost 0.000464 seconds
DEBUG 10-15 15:26:53 lpmodule.py:340] layer_idx 20 attn layer_idx: 20
DEBUG 10-15 15:26:53 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:26:53 lpmodule.py:364] decoder_attn_batch update batch_dim 0-60
DEBUG 10-15 15:26:54 StaticCacheLen.py:84] static cache update layer_idx: 20, start_batch: 0, end_batch: 60
DEBUG 10-15 15:26:54 lpmodule.py:368] update for kv cache 0-60 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:26:54 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:26:54 lpmodule.py:374] update past key value cost 0.005532 seconds
DEBUG 10-15 15:26:54 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:26:54 lpmodule.py:227] layer idx 19
DEBUG 10-15 15:26:54 lpmodule.py:399] repeat qkv cost 0.032987 seconds
DEBUG 10-15 15:26:54 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:26:54 lpmodule.py:433] dot attn cost 0.038720 seconds
DEBUG 10-15 15:26:54 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:26:54 lpmodule.py:444] time cost move to cuda:1 0.027680158615112305 s
DEBUG 10-15 15:26:54 lpllm.py:2283] CPU attn cost 0.145617 seconds if batch True
DEBUG 10-15 15:26:54 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:26:54 lpllm.py:2294] CPU compute cost 0.146442 seconds
DEBUG 10-15 15:26:54 lpllm.py:2312] free cost 0.000106 seconds
DEBUG 10-15 15:26:54 lpllm.py:503] reset update experts
DEBUG 10-15 15:26:54 lpllm.py:1774] update state cost 3.409385681152344e-05 s
DEBUG 10-15 15:26:54 lpllm.py:1743] restore layer func cost 0.0010182857513427734 s
DEBUG 10-15 15:26:54 lpllm.py:511] restore layer cost 0.0012798309326171875 s
DEBUG 10-15 15:26:54 lpllm.py:512] reset_next_layer_need: layer_attn_idx=20, layer_mlp_idx=20, j_loc=40
DEBUG 10-15 15:26:54 lpllm.py:1037] reset layer cost 0.0013568401336669922 s
DEBUG 10-15 15:26:54 lpllm.py:1038] have reset next layer layer_attn 20 layer_mlp 20 
DEBUG 10-15 15:26:54 lpllm.py:924] 
DEBUG 10-15 15:26:54 lpllm.py:924] decoder loop j: 41 cur_layer_idx: 20 layer_attn_idx: 20 layer_mlp_idx: 20
DEBUG 10-15 15:26:54 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:26:54 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:26:54 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:26:54 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:26:54 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:26:54 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:54 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:54 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:54 lpmodule.py:506]         ...,
DEBUG 10-15 15:26:54 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:54 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:54 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:26:54 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:26:54 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:26:54 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:26:54 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:26:54 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:54 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:54 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:26:54 lpllm.py:2265] GPU2CPU move cost 0.000715 seconds
DEBUG 10-15 15:26:54 lpmodule.py:340] layer_idx 20 attn layer_idx: 20
DEBUG 10-15 15:26:54 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:26:54 lpmodule.py:364] decoder_attn_batch update batch_dim 60-120
DEBUG 10-15 15:26:54 StaticCacheLen.py:84] static cache update layer_idx: 20, start_batch: 60, end_batch: 120
DEBUG 10-15 15:26:54 lpmodule.py:368] update for kv cache 60-120 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:26:54 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:26:54 lpmodule.py:374] update past key value cost 0.005456 seconds
DEBUG 10-15 15:26:54 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:26:54 lpmodule.py:227] layer idx 20
DEBUG 10-15 15:26:54 lpmodule.py:399] repeat qkv cost 0.036489 seconds
DEBUG 10-15 15:26:54 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:26:54 lpmodule.py:433] dot attn cost 0.032299 seconds
DEBUG 10-15 15:26:54 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:26:54 lpmodule.py:444] time cost move to cuda:1 0.0278778076171875 s
DEBUG 10-15 15:26:54 lpllm.py:2283] CPU attn cost 0.144689 seconds if batch True
DEBUG 10-15 15:26:54 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:26:54 lpllm.py:2294] CPU compute cost 0.145823 seconds
DEBUG 10-15 15:26:54 lpllm.py:2312] free cost 0.000108 seconds
DEBUG 10-15 15:26:54 lpllm.py:498] reset update attn
DEBUG 10-15 15:26:54 lpllm.py:1774] update state cost 3.552436828613281e-05 s
DEBUG 10-15 15:26:54 lpllm.py:1743] restore layer func cost 0.0004246234893798828 s
DEBUG 10-15 15:26:54 lpllm.py:511] restore layer cost 0.0007016658782958984 s
DEBUG 10-15 15:26:54 lpllm.py:512] reset_next_layer_need: layer_attn_idx=21, layer_mlp_idx=20, j_loc=41
DEBUG 10-15 15:26:54 lpllm.py:1037] reset layer cost 0.0007765293121337891 s
DEBUG 10-15 15:26:54 lpllm.py:1038] have reset next layer layer_attn 21 layer_mlp 20 
DEBUG 10-15 15:26:54 lpllm.py:1044] j: 41 waiting the layer with layer_idx 21 before wait time 0.4523661136627197 s
INFO 10-15 15:26:54 client.py:117] confirm_model_loaded: Mixtral-8x7B, 1ccfcda0-f5ad-4abd-a9ae-b2eae65438cb
INFO 10-15 15:26:54 client.py:125] Model loaded
DEBUG 10-15 15:26:54 lpllm.py:1048] j: load cost 0.4541158676147461 s waiting cost 0.0017337799072265625 s
DEBUG 10-15 15:26:54 lpllm.py:924] 
DEBUG 10-15 15:26:54 lpllm.py:924] decoder loop j: 42 cur_layer_idx: 21 layer_attn_idx: 21 layer_mlp_idx: 20
DEBUG 10-15 15:26:54 lpllm.py:933] start load next layer cur_layer_idx: 22
DEBUG 10-15 15:26:54 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed650>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x003\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x96\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:26:54 client.py:72] load_into_gpu: Mixtral-8x7B, 27a80fb3-1f34-4d1d-8e22-e3a4724e647c
INFO 10-15 15:26:54 client.py:113] Model loaded: Mixtral-8x7B, 27a80fb3-1f34-4d1d-8e22-e3a4724e647c
DEBUG 10-15 15:26:54 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:26:54 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:26:54 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:26:54 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:26:54 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:26:54 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:54 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:54 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:54 lpmodule.py:506]         ...,
DEBUG 10-15 15:26:54 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:54 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:54 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:26:54 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:26:54 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:26:54 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:26:54 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:26:54 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:54 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:54 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:26:54 lpllm.py:2265] GPU2CPU move cost 0.000652 seconds
DEBUG 10-15 15:26:54 lpmodule.py:340] layer_idx 21 attn layer_idx: 21
DEBUG 10-15 15:26:54 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:26:54 lpmodule.py:364] decoder_attn_batch update batch_dim 0-60
DEBUG 10-15 15:26:54 StaticCacheLen.py:84] static cache update layer_idx: 21, start_batch: 0, end_batch: 60
DEBUG 10-15 15:26:54 lpmodule.py:368] update for kv cache 0-60 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:26:54 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:26:54 lpmodule.py:374] update past key value cost 0.005881 seconds
DEBUG 10-15 15:26:54 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:26:54 lpmodule.py:227] layer idx 20
DEBUG 10-15 15:26:54 lpmodule.py:399] repeat qkv cost 0.034828 seconds
DEBUG 10-15 15:26:54 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:26:54 lpmodule.py:433] dot attn cost 0.036929 seconds
DEBUG 10-15 15:26:54 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:26:54 lpmodule.py:444] time cost move to cuda:1 0.02846360206604004 s
DEBUG 10-15 15:26:54 lpllm.py:2283] CPU attn cost 0.147413 seconds if batch True
DEBUG 10-15 15:26:54 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:26:54 lpllm.py:2294] CPU compute cost 0.148425 seconds
DEBUG 10-15 15:26:54 lpllm.py:2312] free cost 0.000098 seconds
DEBUG 10-15 15:26:54 lpllm.py:503] reset update experts
DEBUG 10-15 15:26:54 lpllm.py:1774] update state cost 3.4809112548828125e-05 s
DEBUG 10-15 15:26:54 lpllm.py:1743] restore layer func cost 0.0010352134704589844 s
DEBUG 10-15 15:26:54 lpllm.py:511] restore layer cost 0.0013098716735839844 s
DEBUG 10-15 15:26:54 lpllm.py:512] reset_next_layer_need: layer_attn_idx=21, layer_mlp_idx=21, j_loc=42
DEBUG 10-15 15:26:54 lpllm.py:1037] reset layer cost 0.0013844966888427734 s
DEBUG 10-15 15:26:54 lpllm.py:1038] have reset next layer layer_attn 21 layer_mlp 21 
DEBUG 10-15 15:26:54 lpllm.py:924] 
DEBUG 10-15 15:26:54 lpllm.py:924] decoder loop j: 43 cur_layer_idx: 21 layer_attn_idx: 21 layer_mlp_idx: 21
DEBUG 10-15 15:26:54 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:26:54 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:26:54 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:26:54 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:26:54 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:26:54 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:54 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:54 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:54 lpmodule.py:506]         ...,
DEBUG 10-15 15:26:54 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:54 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:54 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:26:54 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:26:54 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:26:54 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:26:54 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:26:54 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:54 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:54 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:26:54 lpllm.py:2265] GPU2CPU move cost 0.000449 seconds
DEBUG 10-15 15:26:54 lpmodule.py:340] layer_idx 21 attn layer_idx: 21
DEBUG 10-15 15:26:54 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:26:54 lpmodule.py:364] decoder_attn_batch update batch_dim 60-120
DEBUG 10-15 15:26:54 StaticCacheLen.py:84] static cache update layer_idx: 21, start_batch: 60, end_batch: 120
DEBUG 10-15 15:26:54 lpmodule.py:368] update for kv cache 60-120 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:26:54 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:26:54 lpmodule.py:374] update past key value cost 0.005259 seconds
DEBUG 10-15 15:26:54 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:26:54 lpmodule.py:227] layer idx 21
DEBUG 10-15 15:26:54 lpmodule.py:399] repeat qkv cost 0.033169 seconds
DEBUG 10-15 15:26:54 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:26:54 lpmodule.py:433] dot attn cost 0.035808 seconds
DEBUG 10-15 15:26:54 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:26:54 lpmodule.py:444] time cost move to cuda:1 0.02968740463256836 s
DEBUG 10-15 15:26:54 lpllm.py:2283] CPU attn cost 0.145717 seconds if batch True
DEBUG 10-15 15:26:54 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:26:54 lpllm.py:2294] CPU compute cost 0.146451 seconds
DEBUG 10-15 15:26:54 lpllm.py:2312] free cost 0.000096 seconds
DEBUG 10-15 15:26:54 lpllm.py:498] reset update attn
DEBUG 10-15 15:26:54 lpllm.py:1774] update state cost 2.0742416381835938e-05 s
DEBUG 10-15 15:26:54 lpllm.py:1743] restore layer func cost 0.0004248619079589844 s
DEBUG 10-15 15:26:54 lpllm.py:511] restore layer cost 0.0006701946258544922 s
DEBUG 10-15 15:26:54 lpllm.py:512] reset_next_layer_need: layer_attn_idx=22, layer_mlp_idx=21, j_loc=43
DEBUG 10-15 15:26:54 lpllm.py:1037] reset layer cost 0.0007648468017578125 s
DEBUG 10-15 15:26:54 lpllm.py:1038] have reset next layer layer_attn 22 layer_mlp 21 
DEBUG 10-15 15:26:54 lpllm.py:1044] j: 43 waiting the layer with layer_idx 22 before wait time 0.45105695724487305 s
INFO 10-15 15:26:54 client.py:117] confirm_model_loaded: Mixtral-8x7B, 27a80fb3-1f34-4d1d-8e22-e3a4724e647c
INFO 10-15 15:26:54 client.py:125] Model loaded
DEBUG 10-15 15:26:54 lpllm.py:1048] j: load cost 0.45267772674560547 s waiting cost 0.0016036033630371094 s
DEBUG 10-15 15:26:54 lpllm.py:924] 
DEBUG 10-15 15:26:54 lpllm.py:924] decoder loop j: 44 cur_layer_idx: 22 layer_attn_idx: 22 layer_mlp_idx: 21
DEBUG 10-15 15:26:54 lpllm.py:933] start load next layer cur_layer_idx: 23
DEBUG 10-15 15:26:54 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed6e0>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x004\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x98\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:26:54 client.py:72] load_into_gpu: Mixtral-8x7B, a14087db-a9e0-48d7-ac11-dfce6174f7f8
INFO 10-15 15:26:54 client.py:113] Model loaded: Mixtral-8x7B, a14087db-a9e0-48d7-ac11-dfce6174f7f8
DEBUG 10-15 15:26:54 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:26:54 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:26:54 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:26:54 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:26:54 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:26:54 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:54 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:54 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:54 lpmodule.py:506]         ...,
DEBUG 10-15 15:26:54 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:54 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:54 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:26:54 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:26:54 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:26:54 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:26:54 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:26:54 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:54 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:54 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:26:54 lpllm.py:2265] GPU2CPU move cost 0.000927 seconds
DEBUG 10-15 15:26:54 lpmodule.py:340] layer_idx 22 attn layer_idx: 22
DEBUG 10-15 15:26:54 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:26:54 lpmodule.py:364] decoder_attn_batch update batch_dim 0-60
DEBUG 10-15 15:26:54 StaticCacheLen.py:84] static cache update layer_idx: 22, start_batch: 0, end_batch: 60
DEBUG 10-15 15:26:54 lpmodule.py:368] update for kv cache 0-60 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:26:54 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:26:54 lpmodule.py:374] update past key value cost 0.006222 seconds
DEBUG 10-15 15:26:54 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:26:54 lpmodule.py:227] layer idx 21
DEBUG 10-15 15:26:54 lpmodule.py:399] repeat qkv cost 0.031838 seconds
DEBUG 10-15 15:26:54 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:26:55 lpmodule.py:433] dot attn cost 0.040560 seconds
DEBUG 10-15 15:26:55 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:26:55 lpmodule.py:444] time cost move to cuda:1 0.030817508697509766 s
DEBUG 10-15 15:26:55 lpllm.py:2283] CPU attn cost 0.150471 seconds if batch True
DEBUG 10-15 15:26:55 lpllm.py:2292] deal attn result cost 0.000003 seconds
DEBUG 10-15 15:26:55 lpllm.py:2294] CPU compute cost 0.151848 seconds
DEBUG 10-15 15:26:55 lpllm.py:2312] free cost 0.000104 seconds
DEBUG 10-15 15:26:55 lpllm.py:503] reset update experts
DEBUG 10-15 15:26:55 lpllm.py:1774] update state cost 2.0503997802734375e-05 s
DEBUG 10-15 15:26:55 lpllm.py:1743] restore layer func cost 0.0009961128234863281 s
DEBUG 10-15 15:26:55 lpllm.py:511] restore layer cost 0.0012450218200683594 s
DEBUG 10-15 15:26:55 lpllm.py:512] reset_next_layer_need: layer_attn_idx=22, layer_mlp_idx=22, j_loc=44
DEBUG 10-15 15:26:55 lpllm.py:1037] reset layer cost 0.0013370513916015625 s
DEBUG 10-15 15:26:55 lpllm.py:1038] have reset next layer layer_attn 22 layer_mlp 22 
DEBUG 10-15 15:26:55 lpllm.py:924] 
DEBUG 10-15 15:26:55 lpllm.py:924] decoder loop j: 45 cur_layer_idx: 22 layer_attn_idx: 22 layer_mlp_idx: 22
DEBUG 10-15 15:26:55 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:26:55 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:26:55 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:26:55 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:26:55 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:26:55 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:55 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:55 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:55 lpmodule.py:506]         ...,
DEBUG 10-15 15:26:55 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:55 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:55 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:26:55 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:26:55 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:26:55 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:26:55 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:26:55 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:55 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:55 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:26:55 lpllm.py:2265] GPU2CPU move cost 0.000477 seconds
DEBUG 10-15 15:26:55 lpmodule.py:340] layer_idx 22 attn layer_idx: 22
DEBUG 10-15 15:26:55 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:26:55 lpmodule.py:364] decoder_attn_batch update batch_dim 60-120
DEBUG 10-15 15:26:55 StaticCacheLen.py:84] static cache update layer_idx: 22, start_batch: 60, end_batch: 120
DEBUG 10-15 15:26:55 lpmodule.py:368] update for kv cache 60-120 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:26:55 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:26:55 lpmodule.py:374] update past key value cost 0.005091 seconds
DEBUG 10-15 15:26:55 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:26:55 lpmodule.py:227] layer idx 22
DEBUG 10-15 15:26:55 lpmodule.py:399] repeat qkv cost 0.033109 seconds
DEBUG 10-15 15:26:55 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:26:55 lpmodule.py:433] dot attn cost 0.046718 seconds
DEBUG 10-15 15:26:55 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:26:55 lpmodule.py:444] time cost move to cuda:1 0.02845001220703125 s
DEBUG 10-15 15:26:55 lpllm.py:2283] CPU attn cost 0.155536 seconds if batch True
DEBUG 10-15 15:26:55 lpllm.py:2292] deal attn result cost 0.000003 seconds
DEBUG 10-15 15:26:55 lpllm.py:2294] CPU compute cost 0.156340 seconds
DEBUG 10-15 15:26:55 lpllm.py:2312] free cost 0.000112 seconds
DEBUG 10-15 15:26:55 lpllm.py:498] reset update attn
DEBUG 10-15 15:26:55 lpllm.py:1774] update state cost 3.6716461181640625e-05 s
DEBUG 10-15 15:26:55 lpllm.py:1743] restore layer func cost 0.00044417381286621094 s
DEBUG 10-15 15:26:55 lpllm.py:511] restore layer cost 0.0007028579711914062 s
DEBUG 10-15 15:26:55 lpllm.py:512] reset_next_layer_need: layer_attn_idx=23, layer_mlp_idx=22, j_loc=45
DEBUG 10-15 15:26:55 lpllm.py:1037] reset layer cost 0.0007770061492919922 s
DEBUG 10-15 15:26:55 lpllm.py:1038] have reset next layer layer_attn 23 layer_mlp 22 
DEBUG 10-15 15:26:55 lpllm.py:1044] j: 45 waiting the layer with layer_idx 23 before wait time 0.4878809452056885 s
INFO 10-15 15:26:55 client.py:117] confirm_model_loaded: Mixtral-8x7B, a14087db-a9e0-48d7-ac11-dfce6174f7f8
INFO 10-15 15:26:55 client.py:125] Model loaded
DEBUG 10-15 15:26:55 lpllm.py:1048] j: load cost 0.48960375785827637 s waiting cost 0.0017066001892089844 s
DEBUG 10-15 15:26:55 lpllm.py:924] 
DEBUG 10-15 15:26:55 lpllm.py:924] decoder loop j: 46 cur_layer_idx: 23 layer_attn_idx: 23 layer_mlp_idx: 22
DEBUG 10-15 15:26:55 lpllm.py:933] start load next layer cur_layer_idx: 24
DEBUG 10-15 15:26:55 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:26:55 client.py:72] load_into_gpu: Mixtral-8x7B, 07e414a2-5526-422f-a017-0884e204ac12
INFO 10-15 15:26:55 client.py:113] Model loaded: Mixtral-8x7B, 07e414a2-5526-422f-a017-0884e204ac12
DEBUG 10-15 15:26:55 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:26:55 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:26:55 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:26:55 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:26:55 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:26:55 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:55 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:55 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:55 lpmodule.py:506]         ...,
DEBUG 10-15 15:26:55 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:55 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:55 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:26:55 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:26:55 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:26:55 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:26:55 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:26:55 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:55 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:55 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:26:55 lpllm.py:2265] GPU2CPU move cost 0.000722 seconds
DEBUG 10-15 15:26:55 lpmodule.py:340] layer_idx 23 attn layer_idx: 23
DEBUG 10-15 15:26:55 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:26:55 lpmodule.py:364] decoder_attn_batch update batch_dim 0-60
DEBUG 10-15 15:26:55 StaticCacheLen.py:84] static cache update layer_idx: 23, start_batch: 0, end_batch: 60
DEBUG 10-15 15:26:55 lpmodule.py:368] update for kv cache 0-60 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:26:55 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:26:55 lpmodule.py:374] update past key value cost 0.006057 seconds
DEBUG 10-15 15:26:55 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:26:55 lpmodule.py:227] layer idx 22
DEBUG 10-15 15:26:55 lpmodule.py:399] repeat qkv cost 0.033742 seconds
DEBUG 10-15 15:26:55 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:26:55 lpmodule.py:433] dot attn cost 0.031516 seconds
DEBUG 10-15 15:26:55 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:26:55 lpmodule.py:444] time cost move to cuda:1 0.027833938598632812 s
DEBUG 10-15 15:26:55 lpllm.py:2283] CPU attn cost 0.141662 seconds if batch True
DEBUG 10-15 15:26:55 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:26:55 lpllm.py:2294] CPU compute cost 0.142790 seconds
DEBUG 10-15 15:26:55 lpllm.py:2312] free cost 0.000113 seconds
DEBUG 10-15 15:26:55 lpllm.py:503] reset update experts
DEBUG 10-15 15:26:55 lpllm.py:1774] update state cost 1.9788742065429688e-05 s
DEBUG 10-15 15:26:55 lpllm.py:1743] restore layer func cost 0.0009748935699462891 s
DEBUG 10-15 15:26:55 lpllm.py:511] restore layer cost 0.0012154579162597656 s
DEBUG 10-15 15:26:55 lpllm.py:512] reset_next_layer_need: layer_attn_idx=23, layer_mlp_idx=23, j_loc=46
DEBUG 10-15 15:26:55 lpllm.py:1037] reset layer cost 0.0013072490692138672 s
DEBUG 10-15 15:26:55 lpllm.py:1038] have reset next layer layer_attn 23 layer_mlp 23 
DEBUG 10-15 15:26:55 lpllm.py:924] 
DEBUG 10-15 15:26:55 lpllm.py:924] decoder loop j: 47 cur_layer_idx: 23 layer_attn_idx: 23 layer_mlp_idx: 23
DEBUG 10-15 15:26:55 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:26:55 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:26:55 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:26:55 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:26:55 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:26:55 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:55 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:55 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:55 lpmodule.py:506]         ...,
DEBUG 10-15 15:26:55 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:55 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:55 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:26:55 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:26:55 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:26:55 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:26:55 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:26:55 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:55 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:55 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:26:55 lpllm.py:2265] GPU2CPU move cost 0.000492 seconds
DEBUG 10-15 15:26:55 lpmodule.py:340] layer_idx 23 attn layer_idx: 23
DEBUG 10-15 15:26:55 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:26:55 lpmodule.py:364] decoder_attn_batch update batch_dim 60-120
DEBUG 10-15 15:26:55 StaticCacheLen.py:84] static cache update layer_idx: 23, start_batch: 60, end_batch: 120
DEBUG 10-15 15:26:55 lpmodule.py:368] update for kv cache 60-120 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:26:55 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:26:55 lpmodule.py:374] update past key value cost 0.004836 seconds
DEBUG 10-15 15:26:55 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:26:55 lpmodule.py:227] layer idx 23
DEBUG 10-15 15:26:55 lpmodule.py:399] repeat qkv cost 0.033248 seconds
DEBUG 10-15 15:26:55 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:26:55 lpmodule.py:433] dot attn cost 0.044397 seconds
DEBUG 10-15 15:26:55 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:26:55 lpmodule.py:444] time cost move to cuda:1 0.027422428131103516 s
DEBUG 10-15 15:26:55 lpllm.py:2283] CPU attn cost 0.153074 seconds if batch True
DEBUG 10-15 15:26:55 lpllm.py:2292] deal attn result cost 0.000003 seconds
DEBUG 10-15 15:26:55 lpllm.py:2294] CPU compute cost 0.153894 seconds
DEBUG 10-15 15:26:55 lpllm.py:2312] free cost 0.000109 seconds
DEBUG 10-15 15:26:55 lpllm.py:498] reset update attn
DEBUG 10-15 15:26:55 lpllm.py:1774] update state cost 2.0742416381835938e-05 s
DEBUG 10-15 15:26:55 lpllm.py:1743] restore layer func cost 0.0004279613494873047 s
DEBUG 10-15 15:26:55 lpllm.py:511] restore layer cost 0.0006780624389648438 s
DEBUG 10-15 15:26:55 lpllm.py:512] reset_next_layer_need: layer_attn_idx=24, layer_mlp_idx=23, j_loc=47
DEBUG 10-15 15:26:55 lpllm.py:1037] reset layer cost 0.0007669925689697266 s
DEBUG 10-15 15:26:55 lpllm.py:1038] have reset next layer layer_attn 24 layer_mlp 23 
DEBUG 10-15 15:26:55 lpllm.py:1044] j: 47 waiting the layer with layer_idx 24 before wait time 0.44927263259887695 s
INFO 10-15 15:26:55 client.py:117] confirm_model_loaded: Mixtral-8x7B, 07e414a2-5526-422f-a017-0884e204ac12
INFO 10-15 15:26:55 client.py:125] Model loaded
DEBUG 10-15 15:26:55 lpllm.py:1048] j: load cost 0.4509134292602539 s waiting cost 0.0016245841979980469 s
DEBUG 10-15 15:26:55 lpllm.py:924] 
DEBUG 10-15 15:26:55 lpllm.py:924] decoder loop j: 48 cur_layer_idx: 24 layer_attn_idx: 24 layer_mlp_idx: 23
DEBUG 10-15 15:26:55 lpllm.py:933] start load next layer cur_layer_idx: 25
DEBUG 10-15 15:26:55 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed650>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x003\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x96\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:26:55 client.py:72] load_into_gpu: Mixtral-8x7B, 7bbd94c1-d79f-4880-a496-9dff3799e2b9
INFO 10-15 15:26:55 client.py:113] Model loaded: Mixtral-8x7B, 7bbd94c1-d79f-4880-a496-9dff3799e2b9
DEBUG 10-15 15:26:55 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:26:55 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:26:55 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:26:55 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:26:55 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:26:55 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:55 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:55 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:55 lpmodule.py:506]         ...,
DEBUG 10-15 15:26:55 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:55 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:55 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:26:55 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:26:55 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:26:55 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:26:55 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:26:55 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:55 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:55 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:26:55 lpllm.py:2265] GPU2CPU move cost 0.000691 seconds
DEBUG 10-15 15:26:55 lpmodule.py:340] layer_idx 24 attn layer_idx: 24
DEBUG 10-15 15:26:55 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:26:55 lpmodule.py:364] decoder_attn_batch update batch_dim 0-60
DEBUG 10-15 15:26:55 StaticCacheLen.py:84] static cache update layer_idx: 24, start_batch: 0, end_batch: 60
DEBUG 10-15 15:26:55 lpmodule.py:368] update for kv cache 0-60 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:26:55 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:26:55 lpmodule.py:374] update past key value cost 0.005887 seconds
DEBUG 10-15 15:26:55 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:26:55 lpmodule.py:227] layer idx 23
DEBUG 10-15 15:26:55 lpmodule.py:399] repeat qkv cost 0.033351 seconds
DEBUG 10-15 15:26:55 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:26:55 lpmodule.py:433] dot attn cost 0.034636 seconds
DEBUG 10-15 15:26:55 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:26:55 lpmodule.py:444] time cost move to cuda:1 0.02762460708618164 s
DEBUG 10-15 15:26:55 lpllm.py:2283] CPU attn cost 0.142586 seconds if batch True
DEBUG 10-15 15:26:55 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:26:55 lpllm.py:2294] CPU compute cost 0.143634 seconds
DEBUG 10-15 15:26:55 lpllm.py:2312] free cost 0.000095 seconds
DEBUG 10-15 15:26:56 lpllm.py:503] reset update experts
DEBUG 10-15 15:26:56 lpllm.py:1774] update state cost 3.5762786865234375e-05 s
DEBUG 10-15 15:26:56 lpllm.py:1743] restore layer func cost 0.0010292530059814453 s
DEBUG 10-15 15:26:56 lpllm.py:511] restore layer cost 0.0012996196746826172 s
DEBUG 10-15 15:26:56 lpllm.py:512] reset_next_layer_need: layer_attn_idx=24, layer_mlp_idx=24, j_loc=48
DEBUG 10-15 15:26:56 lpllm.py:1037] reset layer cost 0.001377105712890625 s
DEBUG 10-15 15:26:56 lpllm.py:1038] have reset next layer layer_attn 24 layer_mlp 24 
DEBUG 10-15 15:26:56 lpllm.py:924] 
DEBUG 10-15 15:26:56 lpllm.py:924] decoder loop j: 49 cur_layer_idx: 24 layer_attn_idx: 24 layer_mlp_idx: 24
DEBUG 10-15 15:26:56 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:26:56 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:26:56 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:26:56 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:26:56 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:26:56 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:56 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:56 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:56 lpmodule.py:506]         ...,
DEBUG 10-15 15:26:56 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:56 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:56 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:26:56 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:26:56 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:26:56 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:26:56 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:26:56 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:56 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:56 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:26:56 lpllm.py:2265] GPU2CPU move cost 0.000700 seconds
DEBUG 10-15 15:26:56 lpmodule.py:340] layer_idx 24 attn layer_idx: 24
DEBUG 10-15 15:26:56 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:26:56 lpmodule.py:364] decoder_attn_batch update batch_dim 60-120
DEBUG 10-15 15:26:56 StaticCacheLen.py:84] static cache update layer_idx: 24, start_batch: 60, end_batch: 120
DEBUG 10-15 15:26:56 lpmodule.py:368] update for kv cache 60-120 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:26:56 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:26:56 lpmodule.py:374] update past key value cost 0.005342 seconds
DEBUG 10-15 15:26:56 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:26:56 lpmodule.py:227] layer idx 24
DEBUG 10-15 15:26:56 lpmodule.py:399] repeat qkv cost 0.033811 seconds
DEBUG 10-15 15:26:56 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:26:56 lpmodule.py:433] dot attn cost 0.032990 seconds
DEBUG 10-15 15:26:56 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:26:56 lpmodule.py:444] time cost move to cuda:1 0.027841806411743164 s
DEBUG 10-15 15:26:56 lpllm.py:2283] CPU attn cost 0.140526 seconds if batch True
DEBUG 10-15 15:26:56 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:26:56 lpllm.py:2294] CPU compute cost 0.141590 seconds
DEBUG 10-15 15:26:56 lpllm.py:2312] free cost 0.000096 seconds
DEBUG 10-15 15:26:56 lpllm.py:498] reset update attn
DEBUG 10-15 15:26:56 lpllm.py:1774] update state cost 2.193450927734375e-05 s
DEBUG 10-15 15:26:56 lpllm.py:1743] restore layer func cost 0.0004379749298095703 s
DEBUG 10-15 15:26:56 lpllm.py:511] restore layer cost 0.0006999969482421875 s
DEBUG 10-15 15:26:56 lpllm.py:512] reset_next_layer_need: layer_attn_idx=25, layer_mlp_idx=24, j_loc=49
DEBUG 10-15 15:26:56 lpllm.py:1037] reset layer cost 0.0007746219635009766 s
DEBUG 10-15 15:26:56 lpllm.py:1038] have reset next layer layer_attn 25 layer_mlp 24 
DEBUG 10-15 15:26:56 lpllm.py:1044] j: 49 waiting the layer with layer_idx 25 before wait time 0.44513559341430664 s
INFO 10-15 15:26:56 client.py:117] confirm_model_loaded: Mixtral-8x7B, 7bbd94c1-d79f-4880-a496-9dff3799e2b9
INFO 10-15 15:26:56 client.py:125] Model loaded
DEBUG 10-15 15:26:56 lpllm.py:1048] j: load cost 0.44656944274902344 s waiting cost 0.0014171600341796875 s
DEBUG 10-15 15:26:56 lpllm.py:924] 
DEBUG 10-15 15:26:56 lpllm.py:924] decoder loop j: 50 cur_layer_idx: 25 layer_attn_idx: 25 layer_mlp_idx: 24
DEBUG 10-15 15:26:56 lpllm.py:933] start load next layer cur_layer_idx: 26
DEBUG 10-15 15:26:56 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed6e0>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x004\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x98\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:26:56 client.py:72] load_into_gpu: Mixtral-8x7B, 12462a58-dedd-4c8a-abb4-65794606ca55
INFO 10-15 15:26:56 client.py:113] Model loaded: Mixtral-8x7B, 12462a58-dedd-4c8a-abb4-65794606ca55
DEBUG 10-15 15:26:56 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:26:56 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:26:56 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:26:56 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:26:56 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:26:56 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:56 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:56 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:56 lpmodule.py:506]         ...,
DEBUG 10-15 15:26:56 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:56 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:56 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:26:56 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:26:56 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:26:56 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:26:56 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:26:56 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:56 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:56 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:26:56 lpllm.py:2265] GPU2CPU move cost 0.000647 seconds
DEBUG 10-15 15:26:56 lpmodule.py:340] layer_idx 25 attn layer_idx: 25
DEBUG 10-15 15:26:56 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:26:56 lpmodule.py:364] decoder_attn_batch update batch_dim 0-60
DEBUG 10-15 15:26:56 StaticCacheLen.py:84] static cache update layer_idx: 25, start_batch: 0, end_batch: 60
DEBUG 10-15 15:26:56 lpmodule.py:368] update for kv cache 0-60 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:26:56 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:26:56 lpmodule.py:374] update past key value cost 0.018739 seconds
DEBUG 10-15 15:26:56 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:26:56 lpmodule.py:227] layer idx 24
DEBUG 10-15 15:26:56 lpmodule.py:399] repeat qkv cost 0.029806 seconds
DEBUG 10-15 15:26:56 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:26:56 lpmodule.py:433] dot attn cost 0.032519 seconds
DEBUG 10-15 15:26:56 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:26:56 lpmodule.py:444] time cost move to cuda:1 0.026454687118530273 s
DEBUG 10-15 15:26:56 lpllm.py:2283] CPU attn cost 0.145471 seconds if batch True
DEBUG 10-15 15:26:56 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:26:56 lpllm.py:2294] CPU compute cost 0.146430 seconds
DEBUG 10-15 15:26:56 lpllm.py:2312] free cost 0.000088 seconds
DEBUG 10-15 15:26:56 lpllm.py:503] reset update experts
DEBUG 10-15 15:26:56 lpllm.py:1774] update state cost 2.09808349609375e-05 s
DEBUG 10-15 15:26:56 lpllm.py:1743] restore layer func cost 0.0010013580322265625 s
DEBUG 10-15 15:26:56 lpllm.py:511] restore layer cost 0.001257181167602539 s
DEBUG 10-15 15:26:56 lpllm.py:512] reset_next_layer_need: layer_attn_idx=25, layer_mlp_idx=25, j_loc=50
DEBUG 10-15 15:26:56 lpllm.py:1037] reset layer cost 0.0013492107391357422 s
DEBUG 10-15 15:26:56 lpllm.py:1038] have reset next layer layer_attn 25 layer_mlp 25 
DEBUG 10-15 15:26:56 lpllm.py:924] 
DEBUG 10-15 15:26:56 lpllm.py:924] decoder loop j: 51 cur_layer_idx: 25 layer_attn_idx: 25 layer_mlp_idx: 25
DEBUG 10-15 15:26:56 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:26:56 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:26:56 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:26:56 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:26:56 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:26:56 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:56 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:56 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:56 lpmodule.py:506]         ...,
DEBUG 10-15 15:26:56 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:56 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:56 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:26:56 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:26:56 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:26:56 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:26:56 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:26:56 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:56 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:56 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:26:56 lpllm.py:2265] GPU2CPU move cost 0.000331 seconds
DEBUG 10-15 15:26:56 lpmodule.py:340] layer_idx 25 attn layer_idx: 25
DEBUG 10-15 15:26:56 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:26:56 lpmodule.py:364] decoder_attn_batch update batch_dim 60-120
DEBUG 10-15 15:26:56 StaticCacheLen.py:84] static cache update layer_idx: 25, start_batch: 60, end_batch: 120
DEBUG 10-15 15:26:56 lpmodule.py:368] update for kv cache 60-120 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:26:56 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:26:56 lpmodule.py:374] update past key value cost 0.017072 seconds
DEBUG 10-15 15:26:56 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:26:56 lpmodule.py:227] layer idx 25
DEBUG 10-15 15:26:56 lpmodule.py:399] repeat qkv cost 0.030404 seconds
DEBUG 10-15 15:26:56 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:26:56 lpmodule.py:433] dot attn cost 0.035032 seconds
DEBUG 10-15 15:26:56 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:26:56 lpmodule.py:444] time cost move to cuda:1 0.02649068832397461 s
DEBUG 10-15 15:26:56 lpllm.py:2283] CPU attn cost 0.144466 seconds if batch True
DEBUG 10-15 15:26:56 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:26:56 lpllm.py:2294] CPU compute cost 0.145003 seconds
DEBUG 10-15 15:26:56 lpllm.py:2312] free cost 0.000082 seconds
DEBUG 10-15 15:26:56 lpllm.py:498] reset update attn
DEBUG 10-15 15:26:56 lpllm.py:1774] update state cost 3.7670135498046875e-05 s
DEBUG 10-15 15:26:56 lpllm.py:1743] restore layer func cost 0.00043320655822753906 s
DEBUG 10-15 15:26:56 lpllm.py:511] restore layer cost 0.0007104873657226562 s
DEBUG 10-15 15:26:56 lpllm.py:512] reset_next_layer_need: layer_attn_idx=26, layer_mlp_idx=25, j_loc=51
DEBUG 10-15 15:26:56 lpllm.py:1037] reset layer cost 0.0007867813110351562 s
DEBUG 10-15 15:26:56 lpllm.py:1038] have reset next layer layer_attn 26 layer_mlp 25 
DEBUG 10-15 15:26:56 lpllm.py:1044] j: 51 waiting the layer with layer_idx 26 before wait time 0.44658374786376953 s
INFO 10-15 15:26:56 client.py:117] confirm_model_loaded: Mixtral-8x7B, 12462a58-dedd-4c8a-abb4-65794606ca55
INFO 10-15 15:26:56 client.py:125] Model loaded
DEBUG 10-15 15:26:56 lpllm.py:1048] j: load cost 0.4480929374694824 s waiting cost 0.0014925003051757812 s
DEBUG 10-15 15:26:56 lpllm.py:924] 
DEBUG 10-15 15:26:56 lpllm.py:924] decoder loop j: 52 cur_layer_idx: 26 layer_attn_idx: 26 layer_mlp_idx: 25
DEBUG 10-15 15:26:56 lpllm.py:933] start load next layer cur_layer_idx: 27
DEBUG 10-15 15:26:56 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:26:56 client.py:72] load_into_gpu: Mixtral-8x7B, 56e44771-d35d-4ca6-bb6d-aa39055a8a90
INFO 10-15 15:26:56 client.py:113] Model loaded: Mixtral-8x7B, 56e44771-d35d-4ca6-bb6d-aa39055a8a90
DEBUG 10-15 15:26:56 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:26:56 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:26:56 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:26:56 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:26:56 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:26:56 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:56 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:56 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:56 lpmodule.py:506]         ...,
DEBUG 10-15 15:26:56 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:56 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:56 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:26:56 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:26:56 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:26:56 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:26:56 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:26:56 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:56 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:56 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:26:56 lpllm.py:2265] GPU2CPU move cost 0.000587 seconds
DEBUG 10-15 15:26:56 lpmodule.py:340] layer_idx 26 attn layer_idx: 26
DEBUG 10-15 15:26:56 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:26:56 lpmodule.py:364] decoder_attn_batch update batch_dim 0-60
DEBUG 10-15 15:26:56 StaticCacheLen.py:84] static cache update layer_idx: 26, start_batch: 0, end_batch: 60
DEBUG 10-15 15:26:56 lpmodule.py:368] update for kv cache 0-60 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:26:56 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:26:56 lpmodule.py:374] update past key value cost 0.026013 seconds
DEBUG 10-15 15:26:56 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:26:56 lpmodule.py:227] layer idx 25
DEBUG 10-15 15:26:56 lpmodule.py:399] repeat qkv cost 0.030223 seconds
DEBUG 10-15 15:26:56 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:26:56 lpmodule.py:433] dot attn cost 0.035931 seconds
DEBUG 10-15 15:26:56 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:26:56 lpmodule.py:444] time cost move to cuda:1 0.02600884437561035 s
DEBUG 10-15 15:26:56 lpllm.py:2283] CPU attn cost 0.154539 seconds if batch True
DEBUG 10-15 15:26:56 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:26:56 lpllm.py:2294] CPU compute cost 0.155426 seconds
DEBUG 10-15 15:26:56 lpllm.py:2312] free cost 0.000087 seconds
DEBUG 10-15 15:26:56 lpllm.py:503] reset update experts
DEBUG 10-15 15:26:56 lpllm.py:1774] update state cost 1.9073486328125e-05 s
DEBUG 10-15 15:26:56 lpllm.py:1743] restore layer func cost 0.0009837150573730469 s
DEBUG 10-15 15:26:56 lpllm.py:511] restore layer cost 0.0012345314025878906 s
DEBUG 10-15 15:26:56 lpllm.py:512] reset_next_layer_need: layer_attn_idx=26, layer_mlp_idx=26, j_loc=52
DEBUG 10-15 15:26:56 lpllm.py:1037] reset layer cost 0.0013251304626464844 s
DEBUG 10-15 15:26:56 lpllm.py:1038] have reset next layer layer_attn 26 layer_mlp 26 
DEBUG 10-15 15:26:56 lpllm.py:924] 
DEBUG 10-15 15:26:56 lpllm.py:924] decoder loop j: 53 cur_layer_idx: 26 layer_attn_idx: 26 layer_mlp_idx: 26
DEBUG 10-15 15:26:56 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:26:56 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:26:56 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:26:56 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:26:56 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:26:56 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:56 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:56 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:56 lpmodule.py:506]         ...,
DEBUG 10-15 15:26:56 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:56 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:56 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:26:56 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:26:56 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:26:56 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:26:56 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:26:56 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:56 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:56 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:26:56 lpllm.py:2265] GPU2CPU move cost 0.000526 seconds
DEBUG 10-15 15:26:56 lpmodule.py:340] layer_idx 26 attn layer_idx: 26
DEBUG 10-15 15:26:56 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:26:56 lpmodule.py:364] decoder_attn_batch update batch_dim 60-120
DEBUG 10-15 15:26:56 StaticCacheLen.py:84] static cache update layer_idx: 26, start_batch: 60, end_batch: 120
DEBUG 10-15 15:26:56 lpmodule.py:368] update for kv cache 60-120 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:26:56 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:26:56 lpmodule.py:374] update past key value cost 0.026445 seconds
DEBUG 10-15 15:26:56 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:26:57 lpmodule.py:227] layer idx 26
DEBUG 10-15 15:26:57 lpmodule.py:399] repeat qkv cost 0.029653 seconds
DEBUG 10-15 15:26:57 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:26:57 lpmodule.py:433] dot attn cost 0.042368 seconds
DEBUG 10-15 15:26:57 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:26:57 lpmodule.py:444] time cost move to cuda:1 0.025818824768066406 s
DEBUG 10-15 15:26:57 lpllm.py:2283] CPU attn cost 0.160522 seconds if batch True
DEBUG 10-15 15:26:57 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:26:57 lpllm.py:2294] CPU compute cost 0.161349 seconds
DEBUG 10-15 15:26:57 lpllm.py:2312] free cost 0.000087 seconds
DEBUG 10-15 15:26:57 lpllm.py:498] reset update attn
DEBUG 10-15 15:26:57 lpllm.py:1774] update state cost 2.2411346435546875e-05 s
DEBUG 10-15 15:26:57 lpllm.py:1743] restore layer func cost 0.0004239082336425781 s
DEBUG 10-15 15:26:57 lpllm.py:511] restore layer cost 0.0007021427154541016 s
DEBUG 10-15 15:26:57 lpllm.py:512] reset_next_layer_need: layer_attn_idx=27, layer_mlp_idx=26, j_loc=53
DEBUG 10-15 15:26:57 lpllm.py:1037] reset layer cost 0.0007901191711425781 s
DEBUG 10-15 15:26:57 lpllm.py:1038] have reset next layer layer_attn 27 layer_mlp 26 
DEBUG 10-15 15:26:57 lpllm.py:1044] j: 53 waiting the layer with layer_idx 27 before wait time 0.4565727710723877 s
INFO 10-15 15:26:57 client.py:117] confirm_model_loaded: Mixtral-8x7B, 56e44771-d35d-4ca6-bb6d-aa39055a8a90
INFO 10-15 15:26:57 client.py:125] Model loaded
DEBUG 10-15 15:26:57 lpllm.py:1048] j: load cost 0.45827531814575195 s waiting cost 0.0016863346099853516 s
DEBUG 10-15 15:26:57 lpllm.py:924] 
DEBUG 10-15 15:26:57 lpllm.py:924] decoder loop j: 54 cur_layer_idx: 27 layer_attn_idx: 27 layer_mlp_idx: 26
DEBUG 10-15 15:26:57 lpllm.py:933] start load next layer cur_layer_idx: 28
DEBUG 10-15 15:26:57 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed650>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x003\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x96\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:26:57 client.py:72] load_into_gpu: Mixtral-8x7B, 8167beef-1329-4ef6-8aa8-51578acd0835
INFO 10-15 15:26:57 client.py:113] Model loaded: Mixtral-8x7B, 8167beef-1329-4ef6-8aa8-51578acd0835
DEBUG 10-15 15:26:57 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:26:57 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:26:57 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:26:57 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:26:57 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:26:57 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:57 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:57 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:57 lpmodule.py:506]         ...,
DEBUG 10-15 15:26:57 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:57 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:57 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:26:57 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:26:57 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:26:57 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:26:57 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:26:57 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:57 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:57 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:26:57 lpllm.py:2265] GPU2CPU move cost 0.000617 seconds
DEBUG 10-15 15:26:57 lpmodule.py:340] layer_idx 27 attn layer_idx: 27
DEBUG 10-15 15:26:57 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:26:57 lpmodule.py:364] decoder_attn_batch update batch_dim 0-60
DEBUG 10-15 15:26:57 StaticCacheLen.py:84] static cache update layer_idx: 27, start_batch: 0, end_batch: 60
DEBUG 10-15 15:26:57 lpmodule.py:368] update for kv cache 0-60 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:26:57 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:26:57 lpmodule.py:374] update past key value cost 0.029923 seconds
DEBUG 10-15 15:26:57 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:26:57 lpmodule.py:227] layer idx 26
DEBUG 10-15 15:26:57 lpmodule.py:399] repeat qkv cost 0.030514 seconds
DEBUG 10-15 15:26:57 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:26:57 lpmodule.py:433] dot attn cost 0.041951 seconds
DEBUG 10-15 15:26:57 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:26:57 lpmodule.py:444] time cost move to cuda:1 0.025190114974975586 s
DEBUG 10-15 15:26:57 lpllm.py:2283] CPU attn cost 0.158159 seconds if batch True
DEBUG 10-15 15:26:57 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:26:57 lpllm.py:2294] CPU compute cost 0.159089 seconds
DEBUG 10-15 15:26:57 lpllm.py:2312] free cost 0.000087 seconds
DEBUG 10-15 15:26:57 lpllm.py:503] reset update experts
DEBUG 10-15 15:26:57 lpllm.py:1774] update state cost 3.5762786865234375e-05 s
DEBUG 10-15 15:26:57 lpllm.py:1743] restore layer func cost 0.000988006591796875 s
DEBUG 10-15 15:26:57 lpllm.py:511] restore layer cost 0.0012555122375488281 s
DEBUG 10-15 15:26:57 lpllm.py:512] reset_next_layer_need: layer_attn_idx=27, layer_mlp_idx=27, j_loc=54
DEBUG 10-15 15:26:57 lpllm.py:1037] reset layer cost 0.0013306140899658203 s
DEBUG 10-15 15:26:57 lpllm.py:1038] have reset next layer layer_attn 27 layer_mlp 27 
DEBUG 10-15 15:26:57 lpllm.py:924] 
DEBUG 10-15 15:26:57 lpllm.py:924] decoder loop j: 55 cur_layer_idx: 27 layer_attn_idx: 27 layer_mlp_idx: 27
DEBUG 10-15 15:26:57 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:26:57 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:26:57 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:26:57 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:26:57 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:26:57 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:57 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:57 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:57 lpmodule.py:506]         ...,
DEBUG 10-15 15:26:57 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:57 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:57 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:26:57 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:26:57 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:26:57 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:26:57 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:26:57 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:57 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:57 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:26:57 lpllm.py:2265] GPU2CPU move cost 0.000589 seconds
DEBUG 10-15 15:26:57 lpmodule.py:340] layer_idx 27 attn layer_idx: 27
DEBUG 10-15 15:26:57 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:26:57 lpmodule.py:364] decoder_attn_batch update batch_dim 60-120
DEBUG 10-15 15:26:57 StaticCacheLen.py:84] static cache update layer_idx: 27, start_batch: 60, end_batch: 120
DEBUG 10-15 15:26:57 lpmodule.py:368] update for kv cache 60-120 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:26:57 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:26:57 lpmodule.py:374] update past key value cost 0.027632 seconds
DEBUG 10-15 15:26:57 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:26:57 lpmodule.py:227] layer idx 27
DEBUG 10-15 15:26:57 lpmodule.py:399] repeat qkv cost 0.029547 seconds
DEBUG 10-15 15:26:57 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:26:57 lpmodule.py:433] dot attn cost 0.035469 seconds
DEBUG 10-15 15:26:57 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:26:57 lpmodule.py:444] time cost move to cuda:1 0.02550816535949707 s
DEBUG 10-15 15:26:57 lpllm.py:2283] CPU attn cost 0.148082 seconds if batch True
DEBUG 10-15 15:26:57 lpllm.py:2292] deal attn result cost 0.000003 seconds
DEBUG 10-15 15:26:57 lpllm.py:2294] CPU compute cost 0.148959 seconds
DEBUG 10-15 15:26:57 lpllm.py:2312] free cost 0.000079 seconds
DEBUG 10-15 15:26:57 lpllm.py:498] reset update attn
DEBUG 10-15 15:26:57 lpllm.py:1774] update state cost 3.743171691894531e-05 s
DEBUG 10-15 15:26:57 lpllm.py:1743] restore layer func cost 0.0004334449768066406 s
DEBUG 10-15 15:26:57 lpllm.py:511] restore layer cost 0.0007009506225585938 s
DEBUG 10-15 15:26:57 lpllm.py:512] reset_next_layer_need: layer_attn_idx=28, layer_mlp_idx=27, j_loc=55
DEBUG 10-15 15:26:57 lpllm.py:1037] reset layer cost 0.000774383544921875 s
DEBUG 10-15 15:26:57 lpllm.py:1038] have reset next layer layer_attn 28 layer_mlp 27 
DEBUG 10-15 15:26:57 lpllm.py:1044] j: 55 waiting the layer with layer_idx 28 before wait time 0.44946742057800293 s
INFO 10-15 15:26:57 client.py:117] confirm_model_loaded: Mixtral-8x7B, 8167beef-1329-4ef6-8aa8-51578acd0835
INFO 10-15 15:26:57 client.py:125] Model loaded
DEBUG 10-15 15:26:57 lpllm.py:1048] j: load cost 0.45102739334106445 s waiting cost 0.0015437602996826172 s
DEBUG 10-15 15:26:57 lpllm.py:924] 
DEBUG 10-15 15:26:57 lpllm.py:924] decoder loop j: 56 cur_layer_idx: 28 layer_attn_idx: 28 layer_mlp_idx: 27
DEBUG 10-15 15:26:57 lpllm.py:933] start load next layer cur_layer_idx: 29
DEBUG 10-15 15:26:57 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed6e0>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x004\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x98\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:26:57 client.py:72] load_into_gpu: Mixtral-8x7B, 717355d4-e893-43da-833d-bd29743259f7
INFO 10-15 15:26:57 client.py:113] Model loaded: Mixtral-8x7B, 717355d4-e893-43da-833d-bd29743259f7
DEBUG 10-15 15:26:57 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:26:57 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:26:57 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:26:57 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:26:57 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:26:57 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:57 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:57 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:57 lpmodule.py:506]         ...,
DEBUG 10-15 15:26:57 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:57 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:57 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:26:57 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:26:57 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:26:57 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:26:57 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:26:57 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:57 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:57 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:26:57 lpllm.py:2265] GPU2CPU move cost 0.000584 seconds
DEBUG 10-15 15:26:57 lpmodule.py:340] layer_idx 28 attn layer_idx: 28
DEBUG 10-15 15:26:57 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:26:57 lpmodule.py:364] decoder_attn_batch update batch_dim 0-60
DEBUG 10-15 15:26:57 StaticCacheLen.py:84] static cache update layer_idx: 28, start_batch: 0, end_batch: 60
DEBUG 10-15 15:26:57 lpmodule.py:368] update for kv cache 0-60 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:26:57 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:26:57 lpmodule.py:374] update past key value cost 0.026663 seconds
DEBUG 10-15 15:26:57 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:26:57 lpmodule.py:227] layer idx 27
DEBUG 10-15 15:26:57 lpmodule.py:399] repeat qkv cost 0.029740 seconds
DEBUG 10-15 15:26:57 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:26:57 lpmodule.py:433] dot attn cost 0.035633 seconds
DEBUG 10-15 15:26:57 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:26:57 lpmodule.py:444] time cost move to cuda:1 0.02729630470275879 s
DEBUG 10-15 15:26:57 lpllm.py:2283] CPU attn cost 0.149201 seconds if batch True
DEBUG 10-15 15:26:57 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:26:57 lpllm.py:2294] CPU compute cost 0.150090 seconds
DEBUG 10-15 15:26:57 lpllm.py:2312] free cost 0.000085 seconds
DEBUG 10-15 15:26:57 lpllm.py:503] reset update experts
DEBUG 10-15 15:26:57 lpllm.py:1774] update state cost 3.266334533691406e-05 s
DEBUG 10-15 15:26:57 lpllm.py:1743] restore layer func cost 0.0009860992431640625 s
DEBUG 10-15 15:26:57 lpllm.py:511] restore layer cost 0.0012369155883789062 s
DEBUG 10-15 15:26:57 lpllm.py:512] reset_next_layer_need: layer_attn_idx=28, layer_mlp_idx=28, j_loc=56
DEBUG 10-15 15:26:57 lpllm.py:1037] reset layer cost 0.0013129711151123047 s
DEBUG 10-15 15:26:57 lpllm.py:1038] have reset next layer layer_attn 28 layer_mlp 28 
DEBUG 10-15 15:26:57 lpllm.py:924] 
DEBUG 10-15 15:26:57 lpllm.py:924] decoder loop j: 57 cur_layer_idx: 28 layer_attn_idx: 28 layer_mlp_idx: 28
DEBUG 10-15 15:26:57 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:26:57 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:26:57 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:26:57 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:26:57 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:26:57 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:57 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:57 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:57 lpmodule.py:506]         ...,
DEBUG 10-15 15:26:57 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:57 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:57 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:26:57 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:26:57 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:26:57 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:26:57 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:26:57 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:57 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:57 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:26:57 lpllm.py:2265] GPU2CPU move cost 0.000370 seconds
DEBUG 10-15 15:26:57 lpmodule.py:340] layer_idx 28 attn layer_idx: 28
DEBUG 10-15 15:26:57 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:26:57 lpmodule.py:364] decoder_attn_batch update batch_dim 60-120
DEBUG 10-15 15:26:57 StaticCacheLen.py:84] static cache update layer_idx: 28, start_batch: 60, end_batch: 120
DEBUG 10-15 15:26:57 lpmodule.py:368] update for kv cache 60-120 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:26:57 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:26:57 lpmodule.py:374] update past key value cost 0.026345 seconds
DEBUG 10-15 15:26:57 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:26:57 lpmodule.py:227] layer idx 28
DEBUG 10-15 15:26:57 lpmodule.py:399] repeat qkv cost 0.030376 seconds
DEBUG 10-15 15:26:57 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:26:57 lpmodule.py:433] dot attn cost 0.047258 seconds
DEBUG 10-15 15:26:57 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:26:58 lpmodule.py:444] time cost move to cuda:1 0.025707483291625977 s
DEBUG 10-15 15:26:58 lpllm.py:2283] CPU attn cost 0.159006 seconds if batch True
DEBUG 10-15 15:26:58 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:26:58 lpllm.py:2294] CPU compute cost 0.159609 seconds
DEBUG 10-15 15:26:58 lpllm.py:2312] free cost 0.000092 seconds
DEBUG 10-15 15:26:58 lpllm.py:498] reset update attn
DEBUG 10-15 15:26:58 lpllm.py:1774] update state cost 3.8623809814453125e-05 s
DEBUG 10-15 15:26:58 lpllm.py:1743] restore layer func cost 0.00043010711669921875 s
DEBUG 10-15 15:26:58 lpllm.py:511] restore layer cost 0.0007107257843017578 s
DEBUG 10-15 15:26:58 lpllm.py:512] reset_next_layer_need: layer_attn_idx=29, layer_mlp_idx=28, j_loc=57
DEBUG 10-15 15:26:58 lpllm.py:1037] reset layer cost 0.0007848739624023438 s
DEBUG 10-15 15:26:58 lpllm.py:1038] have reset next layer layer_attn 29 layer_mlp 28 
DEBUG 10-15 15:26:58 lpllm.py:1044] j: 57 waiting the layer with layer_idx 29 before wait time 0.4529857635498047 s
INFO 10-15 15:26:58 client.py:117] confirm_model_loaded: Mixtral-8x7B, 717355d4-e893-43da-833d-bd29743259f7
INFO 10-15 15:26:58 client.py:125] Model loaded
DEBUG 10-15 15:26:58 lpllm.py:1048] j: load cost 0.4546382427215576 s waiting cost 0.001636505126953125 s
DEBUG 10-15 15:26:58 lpllm.py:924] 
DEBUG 10-15 15:26:58 lpllm.py:924] decoder loop j: 58 cur_layer_idx: 29 layer_attn_idx: 29 layer_mlp_idx: 28
DEBUG 10-15 15:26:58 lpllm.py:933] start load next layer cur_layer_idx: 30
DEBUG 10-15 15:26:58 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:26:58 client.py:72] load_into_gpu: Mixtral-8x7B, df0820d1-dcc2-4be5-b1af-6b296cc97778
INFO 10-15 15:26:58 client.py:113] Model loaded: Mixtral-8x7B, df0820d1-dcc2-4be5-b1af-6b296cc97778
DEBUG 10-15 15:26:58 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:26:58 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:26:58 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:26:58 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:26:58 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:26:58 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:58 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:58 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:58 lpmodule.py:506]         ...,
DEBUG 10-15 15:26:58 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:58 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:58 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:26:58 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:26:58 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:26:58 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:26:58 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:26:58 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:58 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:58 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:26:58 lpllm.py:2265] GPU2CPU move cost 0.000593 seconds
DEBUG 10-15 15:26:58 lpmodule.py:340] layer_idx 29 attn layer_idx: 29
DEBUG 10-15 15:26:58 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:26:58 lpmodule.py:364] decoder_attn_batch update batch_dim 0-60
DEBUG 10-15 15:26:58 StaticCacheLen.py:84] static cache update layer_idx: 29, start_batch: 0, end_batch: 60
DEBUG 10-15 15:26:58 lpmodule.py:368] update for kv cache 0-60 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:26:58 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:26:58 lpmodule.py:374] update past key value cost 0.025695 seconds
DEBUG 10-15 15:26:58 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:26:58 lpmodule.py:227] layer idx 28
DEBUG 10-15 15:26:58 lpmodule.py:399] repeat qkv cost 0.030807 seconds
DEBUG 10-15 15:26:58 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:26:58 lpmodule.py:433] dot attn cost 0.035739 seconds
DEBUG 10-15 15:26:58 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:26:58 lpmodule.py:444] time cost move to cuda:1 0.025222063064575195 s
DEBUG 10-15 15:26:58 lpllm.py:2283] CPU attn cost 0.146860 seconds if batch True
DEBUG 10-15 15:26:58 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:26:58 lpllm.py:2294] CPU compute cost 0.147781 seconds
DEBUG 10-15 15:26:58 lpllm.py:2312] free cost 0.000083 seconds
DEBUG 10-15 15:26:58 lpllm.py:503] reset update experts
DEBUG 10-15 15:26:58 lpllm.py:1774] update state cost 3.4809112548828125e-05 s
DEBUG 10-15 15:26:58 lpllm.py:1743] restore layer func cost 0.0009729862213134766 s
DEBUG 10-15 15:26:58 lpllm.py:511] restore layer cost 0.001238107681274414 s
DEBUG 10-15 15:26:58 lpllm.py:512] reset_next_layer_need: layer_attn_idx=29, layer_mlp_idx=29, j_loc=58
DEBUG 10-15 15:26:58 lpllm.py:1037] reset layer cost 0.0013146400451660156 s
DEBUG 10-15 15:26:58 lpllm.py:1038] have reset next layer layer_attn 29 layer_mlp 29 
DEBUG 10-15 15:26:58 lpllm.py:924] 
DEBUG 10-15 15:26:58 lpllm.py:924] decoder loop j: 59 cur_layer_idx: 29 layer_attn_idx: 29 layer_mlp_idx: 29
DEBUG 10-15 15:26:58 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:26:58 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:26:58 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:26:58 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:26:58 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:26:58 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:58 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:58 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:58 lpmodule.py:506]         ...,
DEBUG 10-15 15:26:58 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:58 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:58 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:26:58 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:26:58 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:26:58 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:26:58 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:26:58 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:58 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:58 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:26:58 lpllm.py:2265] GPU2CPU move cost 0.000599 seconds
DEBUG 10-15 15:26:58 lpmodule.py:340] layer_idx 29 attn layer_idx: 29
DEBUG 10-15 15:26:58 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:26:58 lpmodule.py:364] decoder_attn_batch update batch_dim 60-120
DEBUG 10-15 15:26:58 StaticCacheLen.py:84] static cache update layer_idx: 29, start_batch: 60, end_batch: 120
DEBUG 10-15 15:26:58 lpmodule.py:368] update for kv cache 60-120 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:26:58 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:26:58 lpmodule.py:374] update past key value cost 0.027060 seconds
DEBUG 10-15 15:26:58 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:26:58 lpmodule.py:227] layer idx 29
DEBUG 10-15 15:26:58 lpmodule.py:399] repeat qkv cost 0.032678 seconds
DEBUG 10-15 15:26:58 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:26:58 lpmodule.py:433] dot attn cost 0.035946 seconds
DEBUG 10-15 15:26:58 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:26:58 lpmodule.py:444] time cost move to cuda:1 0.026157140731811523 s
DEBUG 10-15 15:26:58 lpllm.py:2283] CPU attn cost 0.152278 seconds if batch True
DEBUG 10-15 15:26:58 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:26:58 lpllm.py:2294] CPU compute cost 0.153174 seconds
DEBUG 10-15 15:26:58 lpllm.py:2312] free cost 0.000083 seconds
DEBUG 10-15 15:26:58 lpllm.py:498] reset update attn
DEBUG 10-15 15:26:58 lpllm.py:1774] update state cost 3.695487976074219e-05 s
DEBUG 10-15 15:26:58 lpllm.py:1743] restore layer func cost 0.00042557716369628906 s
DEBUG 10-15 15:26:58 lpllm.py:511] restore layer cost 0.0007145404815673828 s
DEBUG 10-15 15:26:58 lpllm.py:512] reset_next_layer_need: layer_attn_idx=30, layer_mlp_idx=29, j_loc=59
DEBUG 10-15 15:26:58 lpllm.py:1037] reset layer cost 0.0007891654968261719 s
DEBUG 10-15 15:26:58 lpllm.py:1038] have reset next layer layer_attn 30 layer_mlp 29 
DEBUG 10-15 15:26:58 lpllm.py:1044] j: 59 waiting the layer with layer_idx 30 before wait time 0.44805145263671875 s
INFO 10-15 15:26:58 client.py:117] confirm_model_loaded: Mixtral-8x7B, df0820d1-dcc2-4be5-b1af-6b296cc97778
INFO 10-15 15:26:58 client.py:125] Model loaded
DEBUG 10-15 15:26:58 lpllm.py:1048] j: load cost 0.44985198974609375 s waiting cost 0.001783609390258789 s
DEBUG 10-15 15:26:58 lpllm.py:924] 
DEBUG 10-15 15:26:58 lpllm.py:924] decoder loop j: 60 cur_layer_idx: 30 layer_attn_idx: 30 layer_mlp_idx: 29
DEBUG 10-15 15:26:58 lpllm.py:933] start load next layer cur_layer_idx: 31
DEBUG 10-15 15:26:58 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed650>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x003\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x96\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:26:58 client.py:72] load_into_gpu: Mixtral-8x7B, a285b003-5405-4814-8991-f7088c5cec2d
INFO 10-15 15:26:58 client.py:113] Model loaded: Mixtral-8x7B, a285b003-5405-4814-8991-f7088c5cec2d
DEBUG 10-15 15:26:58 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:26:58 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:26:58 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:26:58 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:26:58 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:26:58 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:58 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:58 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:58 lpmodule.py:506]         ...,
DEBUG 10-15 15:26:58 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:58 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:58 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:26:58 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:26:58 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:26:58 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:26:58 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:26:58 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:58 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:58 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:26:58 lpllm.py:2265] GPU2CPU move cost 0.000588 seconds
DEBUG 10-15 15:26:58 lpmodule.py:340] layer_idx 30 attn layer_idx: 30
DEBUG 10-15 15:26:58 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:26:58 lpmodule.py:364] decoder_attn_batch update batch_dim 0-60
DEBUG 10-15 15:26:58 StaticCacheLen.py:84] static cache update layer_idx: 30, start_batch: 0, end_batch: 60
DEBUG 10-15 15:26:58 lpmodule.py:368] update for kv cache 0-60 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:26:58 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:26:58 lpmodule.py:374] update past key value cost 0.026088 seconds
DEBUG 10-15 15:26:58 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:26:58 lpmodule.py:227] layer idx 29
DEBUG 10-15 15:26:58 lpmodule.py:399] repeat qkv cost 0.032049 seconds
DEBUG 10-15 15:26:58 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:26:58 lpmodule.py:433] dot attn cost 0.036235 seconds
DEBUG 10-15 15:26:58 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:26:58 lpmodule.py:444] time cost move to cuda:1 0.024747848510742188 s
DEBUG 10-15 15:26:58 lpllm.py:2283] CPU attn cost 0.147764 seconds if batch True
DEBUG 10-15 15:26:58 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:26:58 lpllm.py:2294] CPU compute cost 0.148652 seconds
DEBUG 10-15 15:26:58 lpllm.py:2312] free cost 0.000095 seconds
DEBUG 10-15 15:26:58 lpllm.py:503] reset update experts
DEBUG 10-15 15:26:58 lpllm.py:1774] update state cost 1.8596649169921875e-05 s
DEBUG 10-15 15:26:58 lpllm.py:1743] restore layer func cost 0.0010161399841308594 s
DEBUG 10-15 15:26:58 lpllm.py:511] restore layer cost 0.0012621879577636719 s
DEBUG 10-15 15:26:58 lpllm.py:512] reset_next_layer_need: layer_attn_idx=30, layer_mlp_idx=30, j_loc=60
DEBUG 10-15 15:26:58 lpllm.py:1037] reset layer cost 0.0013396739959716797 s
DEBUG 10-15 15:26:58 lpllm.py:1038] have reset next layer layer_attn 30 layer_mlp 30 
DEBUG 10-15 15:26:58 lpllm.py:924] 
DEBUG 10-15 15:26:58 lpllm.py:924] decoder loop j: 61 cur_layer_idx: 30 layer_attn_idx: 30 layer_mlp_idx: 30
DEBUG 10-15 15:26:58 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:26:58 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:26:58 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:26:58 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:26:58 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:26:58 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:58 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:58 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:58 lpmodule.py:506]         ...,
DEBUG 10-15 15:26:58 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:58 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:58 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:26:58 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:26:58 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:26:58 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:26:58 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:26:58 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:58 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:58 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:26:58 lpllm.py:2265] GPU2CPU move cost 0.000639 seconds
DEBUG 10-15 15:26:58 lpmodule.py:340] layer_idx 30 attn layer_idx: 30
DEBUG 10-15 15:26:58 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:26:58 lpmodule.py:364] decoder_attn_batch update batch_dim 60-120
DEBUG 10-15 15:26:58 StaticCacheLen.py:84] static cache update layer_idx: 30, start_batch: 60, end_batch: 120
DEBUG 10-15 15:26:58 lpmodule.py:368] update for kv cache 60-120 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:26:58 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:26:58 lpmodule.py:374] update past key value cost 0.026886 seconds
DEBUG 10-15 15:26:58 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:26:58 lpmodule.py:227] layer idx 30
DEBUG 10-15 15:26:58 lpmodule.py:399] repeat qkv cost 0.031529 seconds
DEBUG 10-15 15:26:58 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:26:58 lpmodule.py:433] dot attn cost 0.036869 seconds
DEBUG 10-15 15:26:58 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:26:58 lpmodule.py:444] time cost move to cuda:1 0.024453401565551758 s
DEBUG 10-15 15:26:58 lpllm.py:2283] CPU attn cost 0.149962 seconds if batch True
DEBUG 10-15 15:26:58 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:26:58 lpllm.py:2294] CPU compute cost 0.150892 seconds
DEBUG 10-15 15:26:58 lpllm.py:2312] free cost 0.000093 seconds
DEBUG 10-15 15:26:58 lpllm.py:498] reset update attn
DEBUG 10-15 15:26:58 lpllm.py:1774] update state cost 3.62396240234375e-05 s
DEBUG 10-15 15:26:58 lpllm.py:1743] restore layer func cost 0.0004215240478515625 s
DEBUG 10-15 15:26:58 lpllm.py:511] restore layer cost 0.0007157325744628906 s
DEBUG 10-15 15:26:58 lpllm.py:512] reset_next_layer_need: layer_attn_idx=31, layer_mlp_idx=30, j_loc=61
DEBUG 10-15 15:26:58 lpllm.py:1037] reset layer cost 0.0007929801940917969 s
DEBUG 10-15 15:26:58 lpllm.py:1038] have reset next layer layer_attn 31 layer_mlp 30 
DEBUG 10-15 15:26:58 lpllm.py:1044] j: 61 waiting the layer with layer_idx 31 before wait time 0.4514737129211426 s
INFO 10-15 15:26:58 client.py:117] confirm_model_loaded: Mixtral-8x7B, a285b003-5405-4814-8991-f7088c5cec2d
INFO 10-15 15:26:58 client.py:125] Model loaded
DEBUG 10-15 15:26:58 lpllm.py:1048] j: load cost 0.453244686126709 s waiting cost 0.0017547607421875 s
DEBUG 10-15 15:26:58 lpllm.py:924] 
DEBUG 10-15 15:26:58 lpllm.py:924] decoder loop j: 62 cur_layer_idx: 31 layer_attn_idx: 31 layer_mlp_idx: 30
DEBUG 10-15 15:26:58 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:26:58 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:26:58 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:26:59 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:26:59 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:26:59 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:59 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:59 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:59 lpmodule.py:506]         ...,
DEBUG 10-15 15:26:59 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:59 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:59 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:26:59 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:26:59 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:26:59 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:26:59 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:26:59 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:59 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:59 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:26:59 lpllm.py:2265] GPU2CPU move cost 0.000604 seconds
DEBUG 10-15 15:26:59 lpmodule.py:340] layer_idx 31 attn layer_idx: 31
DEBUG 10-15 15:26:59 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:26:59 lpmodule.py:364] decoder_attn_batch update batch_dim 0-60
DEBUG 10-15 15:26:59 StaticCacheLen.py:84] static cache update layer_idx: 31, start_batch: 0, end_batch: 60
DEBUG 10-15 15:26:59 lpmodule.py:368] update for kv cache 0-60 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:26:59 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:26:59 lpmodule.py:374] update past key value cost 0.027032 seconds
DEBUG 10-15 15:26:59 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:26:59 lpmodule.py:227] layer idx 30
DEBUG 10-15 15:26:59 lpmodule.py:399] repeat qkv cost 0.032701 seconds
DEBUG 10-15 15:26:59 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:26:59 lpmodule.py:433] dot attn cost 0.041048 seconds
DEBUG 10-15 15:26:59 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:26:59 lpmodule.py:444] time cost move to cuda:1 0.024056196212768555 s
DEBUG 10-15 15:26:59 lpllm.py:2283] CPU attn cost 0.152896 seconds if batch True
DEBUG 10-15 15:26:59 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:26:59 lpllm.py:2294] CPU compute cost 0.153783 seconds
DEBUG 10-15 15:26:59 lpllm.py:2312] free cost 0.000081 seconds
DEBUG 10-15 15:26:59 lpllm.py:503] reset update experts
DEBUG 10-15 15:26:59 lpllm.py:1774] update state cost 1.7881393432617188e-05 s
DEBUG 10-15 15:26:59 lpllm.py:1743] restore layer func cost 0.000985860824584961 s
DEBUG 10-15 15:26:59 lpllm.py:511] restore layer cost 0.0012485980987548828 s
DEBUG 10-15 15:26:59 lpllm.py:512] reset_next_layer_need: layer_attn_idx=31, layer_mlp_idx=31, j_loc=62
DEBUG 10-15 15:26:59 lpllm.py:1037] reset layer cost 0.001325845718383789 s
DEBUG 10-15 15:26:59 lpllm.py:1038] have reset next layer layer_attn 31 layer_mlp 31 
DEBUG 10-15 15:26:59 lpllm.py:924] 
DEBUG 10-15 15:26:59 lpllm.py:924] decoder loop j: 63 cur_layer_idx: 31 layer_attn_idx: 31 layer_mlp_idx: 31
DEBUG 10-15 15:26:59 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:26:59 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:26:59 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:26:59 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:26:59 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:26:59 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:59 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:59 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:59 lpmodule.py:506]         ...,
DEBUG 10-15 15:26:59 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:59 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:59 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:26:59 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:26:59 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:26:59 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:26:59 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:26:59 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:59 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:59 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:26:59 lpllm.py:2265] GPU2CPU move cost 0.000574 seconds
DEBUG 10-15 15:26:59 lpmodule.py:340] layer_idx 31 attn layer_idx: 31
DEBUG 10-15 15:26:59 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:26:59 lpmodule.py:364] decoder_attn_batch update batch_dim 60-120
DEBUG 10-15 15:26:59 StaticCacheLen.py:84] static cache update layer_idx: 31, start_batch: 60, end_batch: 120
DEBUG 10-15 15:26:59 lpmodule.py:368] update for kv cache 60-120 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:26:59 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:26:59 lpmodule.py:374] update past key value cost 0.028065 seconds
DEBUG 10-15 15:26:59 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:26:59 lpmodule.py:227] layer idx 31
DEBUG 10-15 15:26:59 lpmodule.py:399] repeat qkv cost 0.030226 seconds
DEBUG 10-15 15:26:59 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:26:59 lpmodule.py:433] dot attn cost 0.036414 seconds
DEBUG 10-15 15:26:59 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:26:59 lpmodule.py:444] time cost move to cuda:1 0.024580955505371094 s
DEBUG 10-15 15:26:59 lpllm.py:2283] CPU attn cost 0.147305 seconds if batch True
DEBUG 10-15 15:26:59 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:26:59 lpllm.py:2294] CPU compute cost 0.148160 seconds
DEBUG 10-15 15:26:59 lpllm.py:2312] free cost 0.000082 seconds
DEBUG 10-15 15:26:59 lpllm.py:924] 
DEBUG 10-15 15:26:59 lpllm.py:924] decoder loop j: 64 cur_layer_idx: 32 layer_attn_idx: 32 layer_mlp_idx: 31
DEBUG 10-15 15:26:59 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:26:59 lpmodule.py:227] layer idx 31
DEBUG 10-15 15:26:59 lpllm.py:1085] last_mlp_output_chunk shape: torch.Size([60, 512, 4096]), mlp_output_chunk shape: torch.Size([60, 512, 4096])
DEBUG 10-15 15:26:59 lpllm.py:1086] last_mlp_output_chunk len 1, mlp_output_chunk len 1
DEBUG 10-15 15:26:59 lpllm.py:618] decoders batch for 0 cost 16.062800645828247 s
DEBUG 10-15 15:26:59 lpllm.py:848] hidden_states_chunks1 shape: torch.Size([60, 512, 4096]), hidden_states_chunks1 length: 1
DEBUG 10-15 15:26:59 lpllm.py:849] hidden_states_chunks2 shape: torch.Size([60, 512, 4096]), hidden_states_chunks2 length: 1
DEBUG 10-15 15:26:59 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed6e0>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x004\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x98\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:26:59 client.py:72] load_into_gpu: Mixtral-8x7B, b0ed2b9b-6b44-40a2-9f94-3c91dbc4147e
INFO 10-15 15:26:59 client.py:113] Model loaded: Mixtral-8x7B, b0ed2b9b-6b44-40a2-9f94-3c91dbc4147e
DEBUG 10-15 15:26:59 lpllm.py:1743] restore layer func cost 0.001087188720703125 s
INFO 10-15 15:26:59 client.py:117] confirm_model_loaded: Mixtral-8x7B, b0ed2b9b-6b44-40a2-9f94-3c91dbc4147e
INFO 10-15 15:26:59 client.py:125] Model loaded
DEBUG 10-15 15:26:59 lpllm.py:422] prepare layer cost 0.2736635208129883 s
DEBUG 10-15 15:26:59 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:26:59 client.py:72] load_into_gpu: Mixtral-8x7B, 057d6414-7ba7-4423-b172-6a00fd84ecdf
INFO 10-15 15:26:59 client.py:113] Model loaded: Mixtral-8x7B, 057d6414-7ba7-4423-b172-6a00fd84ecdf
DEBUG 10-15 15:26:59 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:26:59 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:26:59 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:26:59 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:26:59 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:26:59 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:59 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:59 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:59 lpmodule.py:506]         ...,
DEBUG 10-15 15:26:59 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:59 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:59 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:26:59 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:26:59 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:26:59 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:26:59 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:26:59 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:59 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:59 lpllm.py:924] 
DEBUG 10-15 15:26:59 lpllm.py:924] decoder loop j: 1 cur_layer_idx: 0 layer_attn_idx: 0 layer_mlp_idx: 0
DEBUG 10-15 15:26:59 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:26:59 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:26:59 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:26:59 lpllm.py:2265] GPU2CPU move cost 0.000577 seconds
DEBUG 10-15 15:26:59 lpmodule.py:340] layer_idx 0 attn layer_idx: 0
DEBUG 10-15 15:26:59 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:26:59 lpmodule.py:364] decoder_attn_batch update batch_dim 120-180
DEBUG 10-15 15:26:59 StaticCacheLen.py:84] static cache update layer_idx: 0, start_batch: 120, end_batch: 180
DEBUG 10-15 15:26:59 lpmodule.py:368] update for kv cache 120-180 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:26:59 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:26:59 lpmodule.py:374] update past key value cost 0.009633 seconds
DEBUG 10-15 15:26:59 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:26:59 lpmodule.py:399] repeat qkv cost 0.036175 seconds
DEBUG 10-15 15:26:59 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:26:59 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:26:59 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:26:59 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:59 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:59 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:59 lpmodule.py:506]         ...,
DEBUG 10-15 15:26:59 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:59 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:26:59 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:26:59 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:26:59 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:26:59 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:26:59 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:26:59 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:59 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:26:59 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:26:59 lpmodule.py:433] dot attn cost 0.036023 seconds
DEBUG 10-15 15:26:59 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:26:59 lpmodule.py:444] time cost move to cuda:1 0.02435469627380371 s
DEBUG 10-15 15:27:00 lpllm.py:2283] CPU attn cost 0.134471 seconds if batch True
DEBUG 10-15 15:27:00 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:00 lpllm.py:2294] CPU compute cost 0.135343 seconds
DEBUG 10-15 15:27:00 lpllm.py:2312] free cost 0.000099 seconds
DEBUG 10-15 15:27:00 lpllm.py:2265] GPU2CPU move cost 0.000255 seconds
DEBUG 10-15 15:27:00 lpmodule.py:340] layer_idx 0 attn layer_idx: 0
DEBUG 10-15 15:27:00 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:00 lpmodule.py:364] decoder_attn_batch update batch_dim 180-240
DEBUG 10-15 15:27:00 lpmodule.py:227] layer idx 0
DEBUG 10-15 15:27:00 StaticCacheLen.py:84] static cache update layer_idx: 0, start_batch: 180, end_batch: 240
DEBUG 10-15 15:27:00 lpmodule.py:368] update for kv cache 180-240 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:00 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:00 lpmodule.py:374] update past key value cost 0.008500 seconds
DEBUG 10-15 15:27:00 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:00 lpmodule.py:399] repeat qkv cost 0.033309 seconds
DEBUG 10-15 15:27:00 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:00 lpmodule.py:433] dot attn cost 0.041032 seconds
DEBUG 10-15 15:27:00 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:00 lpmodule.py:444] time cost move to cuda:1 0.024217605590820312 s
DEBUG 10-15 15:27:00 lpllm.py:2283] CPU attn cost 0.137277 seconds if batch True
DEBUG 10-15 15:27:00 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:00 lpllm.py:2294] CPU compute cost 0.137744 seconds
DEBUG 10-15 15:27:00 lpllm.py:2312] free cost 0.000082 seconds
DEBUG 10-15 15:27:00 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:00 lpllm.py:1774] update state cost 2.288818359375e-05 s
DEBUG 10-15 15:27:00 lpllm.py:1743] restore layer func cost 0.0003731250762939453 s
DEBUG 10-15 15:27:00 lpllm.py:511] restore layer cost 0.000606536865234375 s
DEBUG 10-15 15:27:00 lpllm.py:512] reset_next_layer_need: layer_attn_idx=1, layer_mlp_idx=0, j_loc=1
DEBUG 10-15 15:27:00 lpllm.py:1037] reset layer cost 0.0006775856018066406 s
DEBUG 10-15 15:27:00 lpllm.py:1038] have reset next layer layer_attn 1 layer_mlp 0 
DEBUG 10-15 15:27:00 lpllm.py:1044] j: 1 waiting the layer with layer_idx 1 before wait time 0.3497741222381592 s
INFO 10-15 15:27:00 client.py:117] confirm_model_loaded: Mixtral-8x7B, 057d6414-7ba7-4423-b172-6a00fd84ecdf
INFO 10-15 15:27:00 client.py:125] Model loaded
DEBUG 10-15 15:27:00 lpllm.py:1048] j: load cost 0.35131311416625977 s waiting cost 0.0015234947204589844 s
DEBUG 10-15 15:27:00 lpllm.py:924] 
DEBUG 10-15 15:27:00 lpllm.py:924] decoder loop j: 2 cur_layer_idx: 1 layer_attn_idx: 1 layer_mlp_idx: 0
DEBUG 10-15 15:27:00 lpllm.py:933] start load next layer cur_layer_idx: 2
DEBUG 10-15 15:27:00 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed650>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x003\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x96\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:00 client.py:72] load_into_gpu: Mixtral-8x7B, 3dd27f2b-7300-42ca-9b71-5f6ca16e52b3
INFO 10-15 15:27:00 client.py:113] Model loaded: Mixtral-8x7B, 3dd27f2b-7300-42ca-9b71-5f6ca16e52b3
DEBUG 10-15 15:27:00 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:00 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:00 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:00 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:00 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:00 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:00 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:00 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:00 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:00 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:00 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:00 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:00 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:00 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:00 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:00 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:00 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:00 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:00 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:00 lpllm.py:2265] GPU2CPU move cost 0.000605 seconds
DEBUG 10-15 15:27:00 lpmodule.py:340] layer_idx 1 attn layer_idx: 1
DEBUG 10-15 15:27:00 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:00 lpmodule.py:364] decoder_attn_batch update batch_dim 120-180
DEBUG 10-15 15:27:00 StaticCacheLen.py:84] static cache update layer_idx: 1, start_batch: 120, end_batch: 180
DEBUG 10-15 15:27:00 lpmodule.py:368] update for kv cache 120-180 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:00 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:00 lpmodule.py:374] update past key value cost 0.005519 seconds
DEBUG 10-15 15:27:00 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:00 lpmodule.py:227] layer idx 0
DEBUG 10-15 15:27:00 lpmodule.py:399] repeat qkv cost 0.034883 seconds
DEBUG 10-15 15:27:00 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:00 lpmodule.py:433] dot attn cost 0.042142 seconds
DEBUG 10-15 15:27:00 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:00 lpmodule.py:444] time cost move to cuda:1 0.023865222930908203 s
DEBUG 10-15 15:27:00 lpllm.py:2283] CPU attn cost 0.135126 seconds if batch True
DEBUG 10-15 15:27:00 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:00 lpllm.py:2294] CPU compute cost 0.136072 seconds
DEBUG 10-15 15:27:00 lpllm.py:2312] free cost 0.000093 seconds
DEBUG 10-15 15:27:00 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:00 lpllm.py:1774] update state cost 3.647804260253906e-05 s
DEBUG 10-15 15:27:00 lpllm.py:1743] restore layer func cost 0.0008270740509033203 s
DEBUG 10-15 15:27:00 lpllm.py:511] restore layer cost 0.0010828971862792969 s
DEBUG 10-15 15:27:00 lpllm.py:512] reset_next_layer_need: layer_attn_idx=1, layer_mlp_idx=1, j_loc=2
DEBUG 10-15 15:27:00 lpllm.py:1037] reset layer cost 0.0011534690856933594 s
DEBUG 10-15 15:27:00 lpllm.py:1038] have reset next layer layer_attn 1 layer_mlp 1 
DEBUG 10-15 15:27:00 lpllm.py:924] 
DEBUG 10-15 15:27:00 lpllm.py:924] decoder loop j: 3 cur_layer_idx: 1 layer_attn_idx: 1 layer_mlp_idx: 1
DEBUG 10-15 15:27:00 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:00 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:00 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:00 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:00 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:00 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:00 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:00 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:00 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:00 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:00 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:00 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:00 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:00 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:00 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:00 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:00 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:00 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:00 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:00 lpllm.py:2265] GPU2CPU move cost 0.000712 seconds
DEBUG 10-15 15:27:00 lpmodule.py:340] layer_idx 1 attn layer_idx: 1
DEBUG 10-15 15:27:00 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:00 lpmodule.py:364] decoder_attn_batch update batch_dim 180-240
DEBUG 10-15 15:27:00 StaticCacheLen.py:84] static cache update layer_idx: 1, start_batch: 180, end_batch: 240
DEBUG 10-15 15:27:00 lpmodule.py:368] update for kv cache 180-240 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:00 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:00 lpmodule.py:374] update past key value cost 0.005880 seconds
DEBUG 10-15 15:27:00 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:00 lpmodule.py:227] layer idx 1
DEBUG 10-15 15:27:00 lpmodule.py:399] repeat qkv cost 0.033908 seconds
DEBUG 10-15 15:27:00 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:00 lpmodule.py:433] dot attn cost 0.040642 seconds
DEBUG 10-15 15:27:00 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:00 lpmodule.py:444] time cost move to cuda:1 0.02425241470336914 s
DEBUG 10-15 15:27:00 lpllm.py:2283] CPU attn cost 0.133495 seconds if batch True
DEBUG 10-15 15:27:00 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:00 lpllm.py:2294] CPU compute cost 0.134536 seconds
DEBUG 10-15 15:27:00 lpllm.py:2312] free cost 0.000081 seconds
DEBUG 10-15 15:27:00 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:00 lpllm.py:1774] update state cost 3.719329833984375e-05 s
DEBUG 10-15 15:27:00 lpllm.py:1743] restore layer func cost 0.0003809928894042969 s
DEBUG 10-15 15:27:00 lpllm.py:511] restore layer cost 0.0006489753723144531 s
DEBUG 10-15 15:27:00 lpllm.py:512] reset_next_layer_need: layer_attn_idx=2, layer_mlp_idx=1, j_loc=3
DEBUG 10-15 15:27:00 lpllm.py:1037] reset layer cost 0.0007183551788330078 s
DEBUG 10-15 15:27:00 lpllm.py:1038] have reset next layer layer_attn 2 layer_mlp 1 
DEBUG 10-15 15:27:00 lpllm.py:1044] j: 3 waiting the layer with layer_idx 2 before wait time 0.4461512565612793 s
INFO 10-15 15:27:00 client.py:117] confirm_model_loaded: Mixtral-8x7B, 3dd27f2b-7300-42ca-9b71-5f6ca16e52b3
INFO 10-15 15:27:00 client.py:125] Model loaded
DEBUG 10-15 15:27:00 lpllm.py:1048] j: load cost 0.44772839546203613 s waiting cost 0.0015628337860107422 s
DEBUG 10-15 15:27:00 lpllm.py:924] 
DEBUG 10-15 15:27:00 lpllm.py:924] decoder loop j: 4 cur_layer_idx: 2 layer_attn_idx: 2 layer_mlp_idx: 1
DEBUG 10-15 15:27:00 lpllm.py:933] start load next layer cur_layer_idx: 3
DEBUG 10-15 15:27:00 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed6e0>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x004\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x98\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:00 client.py:72] load_into_gpu: Mixtral-8x7B, 45790270-c735-4fbc-890d-36b44761a95c
INFO 10-15 15:27:00 client.py:113] Model loaded: Mixtral-8x7B, 45790270-c735-4fbc-890d-36b44761a95c
DEBUG 10-15 15:27:00 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:00 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:00 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:00 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:00 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:00 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:00 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:00 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:00 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:00 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:00 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:00 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:00 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:00 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:00 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:00 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:00 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:00 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:00 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:00 lpllm.py:2265] GPU2CPU move cost 0.000578 seconds
DEBUG 10-15 15:27:00 lpmodule.py:340] layer_idx 2 attn layer_idx: 2
DEBUG 10-15 15:27:00 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:00 lpmodule.py:364] decoder_attn_batch update batch_dim 120-180
DEBUG 10-15 15:27:00 StaticCacheLen.py:84] static cache update layer_idx: 2, start_batch: 120, end_batch: 180
DEBUG 10-15 15:27:00 lpmodule.py:368] update for kv cache 120-180 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:00 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:00 lpmodule.py:374] update past key value cost 0.005429 seconds
DEBUG 10-15 15:27:00 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:00 lpmodule.py:227] layer idx 1
DEBUG 10-15 15:27:00 lpmodule.py:399] repeat qkv cost 0.035373 seconds
DEBUG 10-15 15:27:00 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:00 lpmodule.py:433] dot attn cost 0.035199 seconds
DEBUG 10-15 15:27:00 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:00 lpmodule.py:444] time cost move to cuda:1 0.023888826370239258 s
DEBUG 10-15 15:27:00 lpllm.py:2283] CPU attn cost 0.128515 seconds if batch True
DEBUG 10-15 15:27:00 lpllm.py:2292] deal attn result cost 0.000003 seconds
DEBUG 10-15 15:27:00 lpllm.py:2294] CPU compute cost 0.129405 seconds
DEBUG 10-15 15:27:00 lpllm.py:2312] free cost 0.000087 seconds
DEBUG 10-15 15:27:00 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:00 lpllm.py:1774] update state cost 1.9550323486328125e-05 s
DEBUG 10-15 15:27:00 lpllm.py:1743] restore layer func cost 0.0008056163787841797 s
DEBUG 10-15 15:27:00 lpllm.py:511] restore layer cost 0.0010442733764648438 s
DEBUG 10-15 15:27:00 lpllm.py:512] reset_next_layer_need: layer_attn_idx=2, layer_mlp_idx=2, j_loc=4
DEBUG 10-15 15:27:00 lpllm.py:1037] reset layer cost 0.0011143684387207031 s
DEBUG 10-15 15:27:00 lpllm.py:1038] have reset next layer layer_attn 2 layer_mlp 2 
DEBUG 10-15 15:27:00 lpllm.py:924] 
DEBUG 10-15 15:27:00 lpllm.py:924] decoder loop j: 5 cur_layer_idx: 2 layer_attn_idx: 2 layer_mlp_idx: 2
DEBUG 10-15 15:27:00 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:00 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:00 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:00 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:00 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:00 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:00 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:00 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:00 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:00 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:00 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:00 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:00 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:00 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:00 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:00 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:00 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:00 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:00 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:00 lpllm.py:2265] GPU2CPU move cost 0.000581 seconds
DEBUG 10-15 15:27:00 lpmodule.py:340] layer_idx 2 attn layer_idx: 2
DEBUG 10-15 15:27:00 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:00 lpmodule.py:364] decoder_attn_batch update batch_dim 180-240
DEBUG 10-15 15:27:00 StaticCacheLen.py:84] static cache update layer_idx: 2, start_batch: 180, end_batch: 240
DEBUG 10-15 15:27:00 lpmodule.py:368] update for kv cache 180-240 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:00 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:00 lpmodule.py:374] update past key value cost 0.011482 seconds
DEBUG 10-15 15:27:00 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:00 lpmodule.py:227] layer idx 2
DEBUG 10-15 15:27:00 lpmodule.py:399] repeat qkv cost 0.034372 seconds
DEBUG 10-15 15:27:00 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:00 lpmodule.py:433] dot attn cost 0.042185 seconds
DEBUG 10-15 15:27:00 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:01 lpmodule.py:444] time cost move to cuda:1 0.023736238479614258 s
DEBUG 10-15 15:27:01 lpllm.py:2283] CPU attn cost 0.140705 seconds if batch True
DEBUG 10-15 15:27:01 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:27:01 lpllm.py:2294] CPU compute cost 0.141589 seconds
DEBUG 10-15 15:27:01 lpllm.py:2312] free cost 0.000083 seconds
DEBUG 10-15 15:27:01 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:01 lpllm.py:1774] update state cost 3.5762786865234375e-05 s
DEBUG 10-15 15:27:01 lpllm.py:1743] restore layer func cost 0.00037288665771484375 s
DEBUG 10-15 15:27:01 lpllm.py:511] restore layer cost 0.0006544589996337891 s
DEBUG 10-15 15:27:01 lpllm.py:512] reset_next_layer_need: layer_attn_idx=3, layer_mlp_idx=2, j_loc=5
DEBUG 10-15 15:27:01 lpllm.py:1037] reset layer cost 0.0007224082946777344 s
DEBUG 10-15 15:27:01 lpllm.py:1038] have reset next layer layer_attn 3 layer_mlp 2 
DEBUG 10-15 15:27:01 lpllm.py:1044] j: 5 waiting the layer with layer_idx 3 before wait time 0.44916605949401855 s
INFO 10-15 15:27:01 client.py:117] confirm_model_loaded: Mixtral-8x7B, 45790270-c735-4fbc-890d-36b44761a95c
INFO 10-15 15:27:01 client.py:125] Model loaded
DEBUG 10-15 15:27:01 lpllm.py:1048] j: load cost 0.45085763931274414 s waiting cost 0.001676321029663086 s
DEBUG 10-15 15:27:01 lpllm.py:924] 
DEBUG 10-15 15:27:01 lpllm.py:924] decoder loop j: 6 cur_layer_idx: 3 layer_attn_idx: 3 layer_mlp_idx: 2
DEBUG 10-15 15:27:01 lpllm.py:933] start load next layer cur_layer_idx: 4
DEBUG 10-15 15:27:01 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:01 client.py:72] load_into_gpu: Mixtral-8x7B, f28da644-279b-456b-a8e8-06140ccd797e
INFO 10-15 15:27:01 client.py:113] Model loaded: Mixtral-8x7B, f28da644-279b-456b-a8e8-06140ccd797e
DEBUG 10-15 15:27:01 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:01 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:01 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:01 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:01 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:01 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:01 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:01 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:01 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:01 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:01 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:01 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:01 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:01 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:01 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:01 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:01 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:01 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:01 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:01 lpllm.py:2265] GPU2CPU move cost 0.000329 seconds
DEBUG 10-15 15:27:01 lpmodule.py:340] layer_idx 3 attn layer_idx: 3
DEBUG 10-15 15:27:01 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:01 lpmodule.py:364] decoder_attn_batch update batch_dim 120-180
DEBUG 10-15 15:27:01 StaticCacheLen.py:84] static cache update layer_idx: 3, start_batch: 120, end_batch: 180
DEBUG 10-15 15:27:01 lpmodule.py:368] update for kv cache 120-180 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:01 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:01 lpmodule.py:374] update past key value cost 0.004601 seconds
DEBUG 10-15 15:27:01 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:01 lpmodule.py:227] layer idx 2
DEBUG 10-15 15:27:01 lpmodule.py:399] repeat qkv cost 0.033818 seconds
DEBUG 10-15 15:27:01 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:01 lpmodule.py:433] dot attn cost 0.033290 seconds
DEBUG 10-15 15:27:01 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:01 lpmodule.py:444] time cost move to cuda:1 0.024199724197387695 s
DEBUG 10-15 15:27:01 lpllm.py:2283] CPU attn cost 0.125293 seconds if batch True
DEBUG 10-15 15:27:01 lpllm.py:2292] deal attn result cost 0.000003 seconds
DEBUG 10-15 15:27:01 lpllm.py:2294] CPU compute cost 0.125907 seconds
DEBUG 10-15 15:27:01 lpllm.py:2312] free cost 0.000090 seconds
DEBUG 10-15 15:27:01 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:01 lpllm.py:1774] update state cost 1.8358230590820312e-05 s
DEBUG 10-15 15:27:01 lpllm.py:1743] restore layer func cost 0.0008404254913330078 s
DEBUG 10-15 15:27:01 lpllm.py:511] restore layer cost 0.0010676383972167969 s
DEBUG 10-15 15:27:01 lpllm.py:512] reset_next_layer_need: layer_attn_idx=3, layer_mlp_idx=3, j_loc=6
DEBUG 10-15 15:27:01 lpllm.py:1037] reset layer cost 0.001138448715209961 s
DEBUG 10-15 15:27:01 lpllm.py:1038] have reset next layer layer_attn 3 layer_mlp 3 
DEBUG 10-15 15:27:01 lpllm.py:924] 
DEBUG 10-15 15:27:01 lpllm.py:924] decoder loop j: 7 cur_layer_idx: 3 layer_attn_idx: 3 layer_mlp_idx: 3
DEBUG 10-15 15:27:01 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:01 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:01 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:01 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:01 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:01 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:01 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:01 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:01 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:01 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:01 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:01 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:01 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:01 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:01 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:01 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:01 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:01 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:01 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:01 lpllm.py:2265] GPU2CPU move cost 0.000445 seconds
DEBUG 10-15 15:27:01 lpmodule.py:340] layer_idx 3 attn layer_idx: 3
DEBUG 10-15 15:27:01 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:01 lpmodule.py:364] decoder_attn_batch update batch_dim 180-240
DEBUG 10-15 15:27:01 StaticCacheLen.py:84] static cache update layer_idx: 3, start_batch: 180, end_batch: 240
DEBUG 10-15 15:27:01 lpmodule.py:368] update for kv cache 180-240 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:01 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:01 lpmodule.py:374] update past key value cost 0.005634 seconds
DEBUG 10-15 15:27:01 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:01 lpmodule.py:227] layer idx 3
DEBUG 10-15 15:27:01 lpmodule.py:399] repeat qkv cost 0.033750 seconds
DEBUG 10-15 15:27:01 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:01 lpmodule.py:433] dot attn cost 0.039535 seconds
DEBUG 10-15 15:27:01 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:01 lpmodule.py:444] time cost move to cuda:1 0.02454400062561035 s
DEBUG 10-15 15:27:01 lpllm.py:2283] CPU attn cost 0.132398 seconds if batch True
DEBUG 10-15 15:27:01 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:01 lpllm.py:2294] CPU compute cost 0.133151 seconds
DEBUG 10-15 15:27:01 lpllm.py:2312] free cost 0.000083 seconds
DEBUG 10-15 15:27:01 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:01 lpllm.py:1774] update state cost 3.6716461181640625e-05 s
DEBUG 10-15 15:27:01 lpllm.py:1743] restore layer func cost 0.00037407875061035156 s
DEBUG 10-15 15:27:01 lpllm.py:511] restore layer cost 0.0006325244903564453 s
DEBUG 10-15 15:27:01 lpllm.py:512] reset_next_layer_need: layer_attn_idx=4, layer_mlp_idx=3, j_loc=7
DEBUG 10-15 15:27:01 lpllm.py:1037] reset layer cost 0.000701904296875 s
DEBUG 10-15 15:27:01 lpllm.py:1038] have reset next layer layer_attn 4 layer_mlp 3 
DEBUG 10-15 15:27:01 lpllm.py:1044] j: 7 waiting the layer with layer_idx 4 before wait time 0.4417290687561035 s
INFO 10-15 15:27:01 client.py:117] confirm_model_loaded: Mixtral-8x7B, f28da644-279b-456b-a8e8-06140ccd797e
INFO 10-15 15:27:01 client.py:125] Model loaded
DEBUG 10-15 15:27:01 lpllm.py:1048] j: load cost 0.4431190490722656 s waiting cost 0.001374959945678711 s
DEBUG 10-15 15:27:01 lpllm.py:924] 
DEBUG 10-15 15:27:01 lpllm.py:924] decoder loop j: 8 cur_layer_idx: 4 layer_attn_idx: 4 layer_mlp_idx: 3
DEBUG 10-15 15:27:01 lpllm.py:933] start load next layer cur_layer_idx: 5
DEBUG 10-15 15:27:01 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed650>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x003\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x96\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:01 client.py:72] load_into_gpu: Mixtral-8x7B, 253e72b1-83ec-4364-a4a3-c023fe9bcd4d
INFO 10-15 15:27:01 client.py:113] Model loaded: Mixtral-8x7B, 253e72b1-83ec-4364-a4a3-c023fe9bcd4d
DEBUG 10-15 15:27:01 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:01 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:01 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:01 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:01 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:01 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:01 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:01 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:01 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:01 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:01 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:01 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:01 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:01 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:01 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:01 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:01 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:01 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:01 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:01 lpllm.py:2265] GPU2CPU move cost 0.000317 seconds
DEBUG 10-15 15:27:01 lpmodule.py:340] layer_idx 4 attn layer_idx: 4
DEBUG 10-15 15:27:01 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:01 lpmodule.py:364] decoder_attn_batch update batch_dim 120-180
DEBUG 10-15 15:27:01 StaticCacheLen.py:84] static cache update layer_idx: 4, start_batch: 120, end_batch: 180
DEBUG 10-15 15:27:01 lpmodule.py:368] update for kv cache 120-180 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:01 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:01 lpmodule.py:374] update past key value cost 0.004444 seconds
DEBUG 10-15 15:27:01 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:01 lpmodule.py:227] layer idx 3
DEBUG 10-15 15:27:01 lpmodule.py:399] repeat qkv cost 0.032999 seconds
DEBUG 10-15 15:27:01 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:01 lpmodule.py:433] dot attn cost 0.035157 seconds
DEBUG 10-15 15:27:01 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:01 lpmodule.py:444] time cost move to cuda:1 0.024094343185424805 s
DEBUG 10-15 15:27:01 lpllm.py:2283] CPU attn cost 0.125424 seconds if batch True
DEBUG 10-15 15:27:01 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:01 lpllm.py:2294] CPU compute cost 0.125962 seconds
DEBUG 10-15 15:27:01 lpllm.py:2312] free cost 0.000088 seconds
DEBUG 10-15 15:27:01 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:01 lpllm.py:1774] update state cost 1.6927719116210938e-05 s
DEBUG 10-15 15:27:01 lpllm.py:1743] restore layer func cost 0.003026247024536133 s
DEBUG 10-15 15:27:01 lpllm.py:511] restore layer cost 0.0032796859741210938 s
DEBUG 10-15 15:27:01 lpllm.py:512] reset_next_layer_need: layer_attn_idx=4, layer_mlp_idx=4, j_loc=8
DEBUG 10-15 15:27:01 lpllm.py:1037] reset layer cost 0.0033490657806396484 s
DEBUG 10-15 15:27:01 lpllm.py:1038] have reset next layer layer_attn 4 layer_mlp 4 
DEBUG 10-15 15:27:01 lpllm.py:924] 
DEBUG 10-15 15:27:01 lpllm.py:924] decoder loop j: 9 cur_layer_idx: 4 layer_attn_idx: 4 layer_mlp_idx: 4
DEBUG 10-15 15:27:01 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:01 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:01 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:01 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:01 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:01 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:01 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:01 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:01 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:01 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:01 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:01 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:01 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:01 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:01 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:01 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:01 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:01 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:01 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:01 lpllm.py:2265] GPU2CPU move cost 0.000600 seconds
DEBUG 10-15 15:27:01 lpmodule.py:340] layer_idx 4 attn layer_idx: 4
DEBUG 10-15 15:27:01 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:01 lpmodule.py:364] decoder_attn_batch update batch_dim 180-240
DEBUG 10-15 15:27:01 StaticCacheLen.py:84] static cache update layer_idx: 4, start_batch: 180, end_batch: 240
DEBUG 10-15 15:27:01 lpmodule.py:368] update for kv cache 180-240 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:01 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:01 lpmodule.py:374] update past key value cost 0.006297 seconds
DEBUG 10-15 15:27:01 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:01 lpmodule.py:227] layer idx 4
DEBUG 10-15 15:27:01 lpmodule.py:399] repeat qkv cost 0.033383 seconds
DEBUG 10-15 15:27:01 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:01 lpmodule.py:433] dot attn cost 0.046900 seconds
DEBUG 10-15 15:27:01 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:01 lpmodule.py:444] time cost move to cuda:1 0.02381443977355957 s
DEBUG 10-15 15:27:01 lpllm.py:2283] CPU attn cost 0.138378 seconds if batch True
DEBUG 10-15 15:27:01 lpllm.py:2292] deal attn result cost 0.000003 seconds
DEBUG 10-15 15:27:01 lpllm.py:2294] CPU compute cost 0.139262 seconds
DEBUG 10-15 15:27:01 lpllm.py:2312] free cost 0.000080 seconds
DEBUG 10-15 15:27:01 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:01 lpllm.py:1774] update state cost 3.695487976074219e-05 s
DEBUG 10-15 15:27:01 lpllm.py:1743] restore layer func cost 0.0003752708435058594 s
DEBUG 10-15 15:27:01 lpllm.py:511] restore layer cost 0.0006527900695800781 s
DEBUG 10-15 15:27:01 lpllm.py:512] reset_next_layer_need: layer_attn_idx=5, layer_mlp_idx=4, j_loc=9
DEBUG 10-15 15:27:01 lpllm.py:1037] reset layer cost 0.0007359981536865234 s
DEBUG 10-15 15:27:01 lpllm.py:1038] have reset next layer layer_attn 5 layer_mlp 4 
DEBUG 10-15 15:27:01 lpllm.py:1044] j: 9 waiting the layer with layer_idx 5 before wait time 0.444288969039917 s
INFO 10-15 15:27:01 client.py:117] confirm_model_loaded: Mixtral-8x7B, 253e72b1-83ec-4364-a4a3-c023fe9bcd4d
INFO 10-15 15:27:01 client.py:125] Model loaded
DEBUG 10-15 15:27:01 lpllm.py:1048] j: load cost 0.4459559917449951 s waiting cost 0.0016515254974365234 s
DEBUG 10-15 15:27:01 lpllm.py:924] 
DEBUG 10-15 15:27:01 lpllm.py:924] decoder loop j: 10 cur_layer_idx: 5 layer_attn_idx: 5 layer_mlp_idx: 4
DEBUG 10-15 15:27:01 lpllm.py:933] start load next layer cur_layer_idx: 6
DEBUG 10-15 15:27:01 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed6e0>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x004\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x98\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:01 client.py:72] load_into_gpu: Mixtral-8x7B, 03fc98e4-6f46-4528-ba82-8c0022267275
INFO 10-15 15:27:02 client.py:113] Model loaded: Mixtral-8x7B, 03fc98e4-6f46-4528-ba82-8c0022267275
DEBUG 10-15 15:27:02 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:02 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:02 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:02 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:02 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:02 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:02 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:02 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:02 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:02 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:02 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:02 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:02 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:02 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:02 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:02 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:02 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:02 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:02 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:02 lpllm.py:2265] GPU2CPU move cost 0.000583 seconds
DEBUG 10-15 15:27:02 lpmodule.py:340] layer_idx 5 attn layer_idx: 5
DEBUG 10-15 15:27:02 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:02 lpmodule.py:364] decoder_attn_batch update batch_dim 120-180
DEBUG 10-15 15:27:02 StaticCacheLen.py:84] static cache update layer_idx: 5, start_batch: 120, end_batch: 180
DEBUG 10-15 15:27:02 lpmodule.py:368] update for kv cache 120-180 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:02 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:02 lpmodule.py:374] update past key value cost 0.005281 seconds
DEBUG 10-15 15:27:02 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:02 lpmodule.py:227] layer idx 4
DEBUG 10-15 15:27:02 lpmodule.py:399] repeat qkv cost 0.034695 seconds
DEBUG 10-15 15:27:02 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:02 lpmodule.py:433] dot attn cost 0.038856 seconds
DEBUG 10-15 15:27:02 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:02 lpmodule.py:444] time cost move to cuda:1 0.02414250373840332 s
DEBUG 10-15 15:27:02 lpllm.py:2283] CPU attn cost 0.132875 seconds if batch True
DEBUG 10-15 15:27:02 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:02 lpllm.py:2294] CPU compute cost 0.133769 seconds
DEBUG 10-15 15:27:02 lpllm.py:2312] free cost 0.000084 seconds
DEBUG 10-15 15:27:02 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:02 lpllm.py:1774] update state cost 2.0503997802734375e-05 s
DEBUG 10-15 15:27:02 lpllm.py:1743] restore layer func cost 0.0008275508880615234 s
DEBUG 10-15 15:27:02 lpllm.py:511] restore layer cost 0.001077890396118164 s
DEBUG 10-15 15:27:02 lpllm.py:512] reset_next_layer_need: layer_attn_idx=5, layer_mlp_idx=5, j_loc=10
DEBUG 10-15 15:27:02 lpllm.py:1037] reset layer cost 0.0011484622955322266 s
DEBUG 10-15 15:27:02 lpllm.py:1038] have reset next layer layer_attn 5 layer_mlp 5 
DEBUG 10-15 15:27:02 lpllm.py:924] 
DEBUG 10-15 15:27:02 lpllm.py:924] decoder loop j: 11 cur_layer_idx: 5 layer_attn_idx: 5 layer_mlp_idx: 5
DEBUG 10-15 15:27:02 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:02 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:02 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:02 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:02 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:02 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:02 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:02 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:02 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:02 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:02 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:02 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:02 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:02 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:02 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:02 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:02 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:02 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:02 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:02 lpllm.py:2265] GPU2CPU move cost 0.000587 seconds
DEBUG 10-15 15:27:02 lpmodule.py:340] layer_idx 5 attn layer_idx: 5
DEBUG 10-15 15:27:02 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:02 lpmodule.py:364] decoder_attn_batch update batch_dim 180-240
DEBUG 10-15 15:27:02 StaticCacheLen.py:84] static cache update layer_idx: 5, start_batch: 180, end_batch: 240
DEBUG 10-15 15:27:02 lpmodule.py:368] update for kv cache 180-240 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:02 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:02 lpmodule.py:374] update past key value cost 0.006191 seconds
DEBUG 10-15 15:27:02 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:02 lpmodule.py:227] layer idx 5
DEBUG 10-15 15:27:02 lpmodule.py:399] repeat qkv cost 0.034367 seconds
DEBUG 10-15 15:27:02 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:02 lpmodule.py:433] dot attn cost 0.045819 seconds
DEBUG 10-15 15:27:02 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:02 lpmodule.py:444] time cost move to cuda:1 0.025431394577026367 s
DEBUG 10-15 15:27:02 lpllm.py:2283] CPU attn cost 0.140250 seconds if batch True
DEBUG 10-15 15:27:02 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:02 lpllm.py:2294] CPU compute cost 0.141120 seconds
DEBUG 10-15 15:27:02 lpllm.py:2312] free cost 0.000080 seconds
DEBUG 10-15 15:27:02 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:02 lpllm.py:1774] update state cost 2.1219253540039062e-05 s
DEBUG 10-15 15:27:02 lpllm.py:1743] restore layer func cost 0.0003771781921386719 s
DEBUG 10-15 15:27:02 lpllm.py:511] restore layer cost 0.0006403923034667969 s
DEBUG 10-15 15:27:02 lpllm.py:512] reset_next_layer_need: layer_attn_idx=6, layer_mlp_idx=5, j_loc=11
DEBUG 10-15 15:27:02 lpllm.py:1037] reset layer cost 0.0007102489471435547 s
DEBUG 10-15 15:27:02 lpllm.py:1038] have reset next layer layer_attn 6 layer_mlp 5 
DEBUG 10-15 15:27:02 lpllm.py:1044] j: 11 waiting the layer with layer_idx 6 before wait time 0.4525434970855713 s
INFO 10-15 15:27:02 client.py:117] confirm_model_loaded: Mixtral-8x7B, 03fc98e4-6f46-4528-ba82-8c0022267275
INFO 10-15 15:27:02 client.py:125] Model loaded
DEBUG 10-15 15:27:02 lpllm.py:1048] j: load cost 0.45412755012512207 s waiting cost 0.0015685558319091797 s
DEBUG 10-15 15:27:02 lpllm.py:924] 
DEBUG 10-15 15:27:02 lpllm.py:924] decoder loop j: 12 cur_layer_idx: 6 layer_attn_idx: 6 layer_mlp_idx: 5
DEBUG 10-15 15:27:02 lpllm.py:933] start load next layer cur_layer_idx: 7
DEBUG 10-15 15:27:02 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:02 client.py:72] load_into_gpu: Mixtral-8x7B, daf7099d-19c4-492d-821d-2381c6c94813
INFO 10-15 15:27:02 client.py:113] Model loaded: Mixtral-8x7B, daf7099d-19c4-492d-821d-2381c6c94813
DEBUG 10-15 15:27:02 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:02 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:02 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:02 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:02 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:02 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:02 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:02 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:02 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:02 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:02 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:02 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:02 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:02 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:02 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:02 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:02 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:02 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:02 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:02 lpllm.py:2265] GPU2CPU move cost 0.000572 seconds
DEBUG 10-15 15:27:02 lpmodule.py:340] layer_idx 6 attn layer_idx: 6
DEBUG 10-15 15:27:02 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:02 lpmodule.py:364] decoder_attn_batch update batch_dim 120-180
DEBUG 10-15 15:27:02 StaticCacheLen.py:84] static cache update layer_idx: 6, start_batch: 120, end_batch: 180
DEBUG 10-15 15:27:02 lpmodule.py:368] update for kv cache 120-180 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:02 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:02 lpmodule.py:374] update past key value cost 0.005123 seconds
DEBUG 10-15 15:27:02 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:02 lpmodule.py:227] layer idx 5
DEBUG 10-15 15:27:02 lpmodule.py:399] repeat qkv cost 0.034547 seconds
DEBUG 10-15 15:27:02 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:02 lpmodule.py:433] dot attn cost 0.040933 seconds
DEBUG 10-15 15:27:02 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:02 lpmodule.py:444] time cost move to cuda:1 0.02442145347595215 s
DEBUG 10-15 15:27:02 lpllm.py:2283] CPU attn cost 0.134195 seconds if batch True
DEBUG 10-15 15:27:02 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:27:02 lpllm.py:2294] CPU compute cost 0.135068 seconds
DEBUG 10-15 15:27:02 lpllm.py:2312] free cost 0.000082 seconds
DEBUG 10-15 15:27:02 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:02 lpllm.py:1774] update state cost 3.600120544433594e-05 s
DEBUG 10-15 15:27:02 lpllm.py:1743] restore layer func cost 0.00080108642578125 s
DEBUG 10-15 15:27:02 lpllm.py:511] restore layer cost 0.0010647773742675781 s
DEBUG 10-15 15:27:02 lpllm.py:512] reset_next_layer_need: layer_attn_idx=6, layer_mlp_idx=6, j_loc=12
DEBUG 10-15 15:27:02 lpllm.py:1037] reset layer cost 0.0011374950408935547 s
DEBUG 10-15 15:27:02 lpllm.py:1038] have reset next layer layer_attn 6 layer_mlp 6 
DEBUG 10-15 15:27:02 lpllm.py:924] 
DEBUG 10-15 15:27:02 lpllm.py:924] decoder loop j: 13 cur_layer_idx: 6 layer_attn_idx: 6 layer_mlp_idx: 6
DEBUG 10-15 15:27:02 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:02 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:02 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:02 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:02 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:02 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:02 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:02 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:02 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:02 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:02 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:02 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:02 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:02 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:02 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:02 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:02 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:02 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:02 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:02 lpllm.py:2265] GPU2CPU move cost 0.000567 seconds
DEBUG 10-15 15:27:02 lpmodule.py:340] layer_idx 6 attn layer_idx: 6
DEBUG 10-15 15:27:02 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:02 lpmodule.py:364] decoder_attn_batch update batch_dim 180-240
DEBUG 10-15 15:27:02 StaticCacheLen.py:84] static cache update layer_idx: 6, start_batch: 180, end_batch: 240
DEBUG 10-15 15:27:02 lpmodule.py:368] update for kv cache 180-240 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:02 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:02 lpmodule.py:374] update past key value cost 0.006282 seconds
DEBUG 10-15 15:27:02 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:02 lpmodule.py:227] layer idx 6
DEBUG 10-15 15:27:02 lpmodule.py:399] repeat qkv cost 0.032688 seconds
DEBUG 10-15 15:27:02 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:02 lpmodule.py:433] dot attn cost 0.034668 seconds
DEBUG 10-15 15:27:02 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:02 lpmodule.py:444] time cost move to cuda:1 0.024645090103149414 s
DEBUG 10-15 15:27:02 lpllm.py:2283] CPU attn cost 0.127336 seconds if batch True
DEBUG 10-15 15:27:02 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:02 lpllm.py:2294] CPU compute cost 0.128155 seconds
DEBUG 10-15 15:27:02 lpllm.py:2312] free cost 0.000078 seconds
DEBUG 10-15 15:27:02 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:02 lpllm.py:1774] update state cost 3.5762786865234375e-05 s
DEBUG 10-15 15:27:02 lpllm.py:1743] restore layer func cost 0.00038743019104003906 s
DEBUG 10-15 15:27:02 lpllm.py:511] restore layer cost 0.0006539821624755859 s
DEBUG 10-15 15:27:02 lpllm.py:512] reset_next_layer_need: layer_attn_idx=7, layer_mlp_idx=6, j_loc=13
DEBUG 10-15 15:27:02 lpllm.py:1037] reset layer cost 0.00072479248046875 s
DEBUG 10-15 15:27:02 lpllm.py:1038] have reset next layer layer_attn 7 layer_mlp 6 
DEBUG 10-15 15:27:02 lpllm.py:1044] j: 13 waiting the layer with layer_idx 7 before wait time 0.4456632137298584 s
INFO 10-15 15:27:02 client.py:117] confirm_model_loaded: Mixtral-8x7B, daf7099d-19c4-492d-821d-2381c6c94813
INFO 10-15 15:27:02 client.py:125] Model loaded
DEBUG 10-15 15:27:02 lpllm.py:1048] j: load cost 0.4473147392272949 s waiting cost 0.0016362667083740234 s
DEBUG 10-15 15:27:02 lpllm.py:924] 
DEBUG 10-15 15:27:02 lpllm.py:924] decoder loop j: 14 cur_layer_idx: 7 layer_attn_idx: 7 layer_mlp_idx: 6
DEBUG 10-15 15:27:02 lpllm.py:933] start load next layer cur_layer_idx: 8
DEBUG 10-15 15:27:02 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed650>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x003\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x96\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:02 client.py:72] load_into_gpu: Mixtral-8x7B, 45069e87-6005-4710-b17b-dd59d9ab262d
INFO 10-15 15:27:02 client.py:113] Model loaded: Mixtral-8x7B, 45069e87-6005-4710-b17b-dd59d9ab262d
DEBUG 10-15 15:27:02 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:02 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:02 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:02 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:02 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:02 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:02 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:02 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:02 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:02 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:02 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:02 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:02 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:02 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:02 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:02 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:02 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:02 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:02 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:02 lpllm.py:2265] GPU2CPU move cost 0.000571 seconds
DEBUG 10-15 15:27:02 lpmodule.py:340] layer_idx 7 attn layer_idx: 7
DEBUG 10-15 15:27:02 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:02 lpmodule.py:364] decoder_attn_batch update batch_dim 120-180
DEBUG 10-15 15:27:02 StaticCacheLen.py:84] static cache update layer_idx: 7, start_batch: 120, end_batch: 180
DEBUG 10-15 15:27:02 lpmodule.py:368] update for kv cache 120-180 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:02 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:02 lpmodule.py:374] update past key value cost 0.005534 seconds
DEBUG 10-15 15:27:02 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:02 lpmodule.py:227] layer idx 6
DEBUG 10-15 15:27:02 lpmodule.py:399] repeat qkv cost 0.035057 seconds
DEBUG 10-15 15:27:02 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:02 lpmodule.py:433] dot attn cost 0.034667 seconds
DEBUG 10-15 15:27:02 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:03 lpmodule.py:444] time cost move to cuda:1 0.024418354034423828 s
DEBUG 10-15 15:27:03 lpllm.py:2283] CPU attn cost 0.129046 seconds if batch True
DEBUG 10-15 15:27:03 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:03 lpllm.py:2294] CPU compute cost 0.129926 seconds
DEBUG 10-15 15:27:03 lpllm.py:2312] free cost 0.000094 seconds
DEBUG 10-15 15:27:03 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:03 lpllm.py:1774] update state cost 1.8835067749023438e-05 s
DEBUG 10-15 15:27:03 lpllm.py:1743] restore layer func cost 0.0007927417755126953 s
DEBUG 10-15 15:27:03 lpllm.py:511] restore layer cost 0.0010311603546142578 s
DEBUG 10-15 15:27:03 lpllm.py:512] reset_next_layer_need: layer_attn_idx=7, layer_mlp_idx=7, j_loc=14
DEBUG 10-15 15:27:03 lpllm.py:1037] reset layer cost 0.0011029243469238281 s
DEBUG 10-15 15:27:03 lpllm.py:1038] have reset next layer layer_attn 7 layer_mlp 7 
DEBUG 10-15 15:27:03 lpllm.py:924] 
DEBUG 10-15 15:27:03 lpllm.py:924] decoder loop j: 15 cur_layer_idx: 7 layer_attn_idx: 7 layer_mlp_idx: 7
DEBUG 10-15 15:27:03 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:03 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:03 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:03 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:03 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:03 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:03 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:03 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:03 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:03 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:03 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:03 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:03 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:03 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:03 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:03 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:03 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:03 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:03 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:03 lpllm.py:2265] GPU2CPU move cost 0.000544 seconds
DEBUG 10-15 15:27:03 lpmodule.py:340] layer_idx 7 attn layer_idx: 7
DEBUG 10-15 15:27:03 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:03 lpmodule.py:364] decoder_attn_batch update batch_dim 180-240
DEBUG 10-15 15:27:03 StaticCacheLen.py:84] static cache update layer_idx: 7, start_batch: 180, end_batch: 240
DEBUG 10-15 15:27:03 lpmodule.py:368] update for kv cache 180-240 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:03 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:03 lpmodule.py:374] update past key value cost 0.005012 seconds
DEBUG 10-15 15:27:03 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:03 lpmodule.py:227] layer idx 7
DEBUG 10-15 15:27:03 lpmodule.py:399] repeat qkv cost 0.033906 seconds
DEBUG 10-15 15:27:03 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:03 lpmodule.py:433] dot attn cost 0.037708 seconds
DEBUG 10-15 15:27:03 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:03 lpmodule.py:444] time cost move to cuda:1 0.02467966079711914 s
DEBUG 10-15 15:27:03 lpllm.py:2283] CPU attn cost 0.130618 seconds if batch True
DEBUG 10-15 15:27:03 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:03 lpllm.py:2294] CPU compute cost 0.131372 seconds
DEBUG 10-15 15:27:03 lpllm.py:2312] free cost 0.000083 seconds
DEBUG 10-15 15:27:03 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:03 lpllm.py:1774] update state cost 2.0503997802734375e-05 s
DEBUG 10-15 15:27:03 lpllm.py:1743] restore layer func cost 0.0003809928894042969 s
DEBUG 10-15 15:27:03 lpllm.py:511] restore layer cost 0.0006740093231201172 s
DEBUG 10-15 15:27:03 lpllm.py:512] reset_next_layer_need: layer_attn_idx=8, layer_mlp_idx=7, j_loc=15
DEBUG 10-15 15:27:03 lpllm.py:1037] reset layer cost 0.0007460117340087891 s
DEBUG 10-15 15:27:03 lpllm.py:1038] have reset next layer layer_attn 8 layer_mlp 7 
DEBUG 10-15 15:27:03 lpllm.py:1044] j: 15 waiting the layer with layer_idx 8 before wait time 0.4425926208496094 s
INFO 10-15 15:27:03 client.py:117] confirm_model_loaded: Mixtral-8x7B, 45069e87-6005-4710-b17b-dd59d9ab262d
INFO 10-15 15:27:03 client.py:125] Model loaded
DEBUG 10-15 15:27:03 lpllm.py:1048] j: load cost 0.44415974617004395 s waiting cost 0.0015511512756347656 s
DEBUG 10-15 15:27:03 lpllm.py:924] 
DEBUG 10-15 15:27:03 lpllm.py:924] decoder loop j: 16 cur_layer_idx: 8 layer_attn_idx: 8 layer_mlp_idx: 7
DEBUG 10-15 15:27:03 lpllm.py:933] start load next layer cur_layer_idx: 9
DEBUG 10-15 15:27:03 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed6e0>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x004\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x98\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:03 client.py:72] load_into_gpu: Mixtral-8x7B, 4bed5032-561f-4960-b1c8-c895b927aa04
INFO 10-15 15:27:03 client.py:113] Model loaded: Mixtral-8x7B, 4bed5032-561f-4960-b1c8-c895b927aa04
DEBUG 10-15 15:27:03 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:03 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:03 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:03 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:03 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:03 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:03 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:03 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:03 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:03 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:03 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:03 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:03 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:03 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:03 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:03 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:03 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:03 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:03 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:03 lpllm.py:2265] GPU2CPU move cost 0.000582 seconds
DEBUG 10-15 15:27:03 lpmodule.py:340] layer_idx 8 attn layer_idx: 8
DEBUG 10-15 15:27:03 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:03 lpmodule.py:364] decoder_attn_batch update batch_dim 120-180
DEBUG 10-15 15:27:03 StaticCacheLen.py:84] static cache update layer_idx: 8, start_batch: 120, end_batch: 180
DEBUG 10-15 15:27:03 lpmodule.py:368] update for kv cache 120-180 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:03 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:03 lpmodule.py:374] update past key value cost 0.005338 seconds
DEBUG 10-15 15:27:03 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:03 lpmodule.py:227] layer idx 7
DEBUG 10-15 15:27:03 lpmodule.py:399] repeat qkv cost 0.034566 seconds
DEBUG 10-15 15:27:03 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:03 lpmodule.py:433] dot attn cost 0.036995 seconds
DEBUG 10-15 15:27:03 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:03 lpmodule.py:444] time cost move to cuda:1 0.024558544158935547 s
DEBUG 10-15 15:27:03 lpllm.py:2283] CPU attn cost 0.130646 seconds if batch True
DEBUG 10-15 15:27:03 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:03 lpllm.py:2294] CPU compute cost 0.131516 seconds
DEBUG 10-15 15:27:03 lpllm.py:2312] free cost 0.000082 seconds
DEBUG 10-15 15:27:03 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:03 lpllm.py:1774] update state cost 1.7404556274414062e-05 s
DEBUG 10-15 15:27:03 lpllm.py:1743] restore layer func cost 0.00079345703125 s
DEBUG 10-15 15:27:03 lpllm.py:511] restore layer cost 0.0010344982147216797 s
DEBUG 10-15 15:27:03 lpllm.py:512] reset_next_layer_need: layer_attn_idx=8, layer_mlp_idx=8, j_loc=16
DEBUG 10-15 15:27:03 lpllm.py:1037] reset layer cost 0.0011048316955566406 s
DEBUG 10-15 15:27:03 lpllm.py:1038] have reset next layer layer_attn 8 layer_mlp 8 
DEBUG 10-15 15:27:03 lpllm.py:924] 
DEBUG 10-15 15:27:03 lpllm.py:924] decoder loop j: 17 cur_layer_idx: 8 layer_attn_idx: 8 layer_mlp_idx: 8
DEBUG 10-15 15:27:03 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:03 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:03 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:03 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:03 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:03 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:03 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:03 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:03 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:03 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:03 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:03 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:03 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:03 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:03 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:03 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:03 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:03 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:03 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:03 lpllm.py:2265] GPU2CPU move cost 0.000575 seconds
DEBUG 10-15 15:27:03 lpmodule.py:340] layer_idx 8 attn layer_idx: 8
DEBUG 10-15 15:27:03 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:03 lpmodule.py:364] decoder_attn_batch update batch_dim 180-240
DEBUG 10-15 15:27:03 StaticCacheLen.py:84] static cache update layer_idx: 8, start_batch: 180, end_batch: 240
DEBUG 10-15 15:27:03 lpmodule.py:368] update for kv cache 180-240 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:03 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:03 lpmodule.py:374] update past key value cost 0.005882 seconds
DEBUG 10-15 15:27:03 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:03 lpmodule.py:227] layer idx 8
DEBUG 10-15 15:27:03 lpmodule.py:399] repeat qkv cost 0.032528 seconds
DEBUG 10-15 15:27:03 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:03 lpmodule.py:433] dot attn cost 0.048064 seconds
DEBUG 10-15 15:27:03 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:03 lpmodule.py:444] time cost move to cuda:1 0.02464437484741211 s
DEBUG 10-15 15:27:03 lpllm.py:2283] CPU attn cost 0.140010 seconds if batch True
DEBUG 10-15 15:27:03 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:03 lpllm.py:2294] CPU compute cost 0.140864 seconds
DEBUG 10-15 15:27:03 lpllm.py:2312] free cost 0.000079 seconds
DEBUG 10-15 15:27:03 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:03 lpllm.py:1774] update state cost 3.504753112792969e-05 s
DEBUG 10-15 15:27:03 lpllm.py:1743] restore layer func cost 0.0003733634948730469 s
DEBUG 10-15 15:27:03 lpllm.py:511] restore layer cost 0.0006165504455566406 s
DEBUG 10-15 15:27:03 lpllm.py:512] reset_next_layer_need: layer_attn_idx=9, layer_mlp_idx=8, j_loc=17
DEBUG 10-15 15:27:03 lpllm.py:1037] reset layer cost 0.0006854534149169922 s
DEBUG 10-15 15:27:03 lpllm.py:1038] have reset next layer layer_attn 9 layer_mlp 8 
DEBUG 10-15 15:27:03 lpllm.py:1044] j: 17 waiting the layer with layer_idx 9 before wait time 0.4450194835662842 s
INFO 10-15 15:27:03 client.py:117] confirm_model_loaded: Mixtral-8x7B, 4bed5032-561f-4960-b1c8-c895b927aa04
INFO 10-15 15:27:03 client.py:125] Model loaded
DEBUG 10-15 15:27:03 lpllm.py:1048] j: load cost 0.44650721549987793 s waiting cost 0.0014719963073730469 s
DEBUG 10-15 15:27:03 lpllm.py:924] 
DEBUG 10-15 15:27:03 lpllm.py:924] decoder loop j: 18 cur_layer_idx: 9 layer_attn_idx: 9 layer_mlp_idx: 8
DEBUG 10-15 15:27:03 lpllm.py:933] start load next layer cur_layer_idx: 10
DEBUG 10-15 15:27:03 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:03 client.py:72] load_into_gpu: Mixtral-8x7B, af582562-0fe8-4b8f-915a-b82a650cf0e9
INFO 10-15 15:27:03 client.py:113] Model loaded: Mixtral-8x7B, af582562-0fe8-4b8f-915a-b82a650cf0e9
DEBUG 10-15 15:27:03 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:03 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:03 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:03 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:03 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:03 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:03 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:03 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:03 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:03 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:03 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:03 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:03 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:03 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:03 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:03 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:03 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:03 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:03 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:03 lpllm.py:2265] GPU2CPU move cost 0.000572 seconds
DEBUG 10-15 15:27:03 lpmodule.py:340] layer_idx 9 attn layer_idx: 9
DEBUG 10-15 15:27:03 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:03 lpmodule.py:364] decoder_attn_batch update batch_dim 120-180
DEBUG 10-15 15:27:03 StaticCacheLen.py:84] static cache update layer_idx: 9, start_batch: 120, end_batch: 180
DEBUG 10-15 15:27:03 lpmodule.py:368] update for kv cache 120-180 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:03 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:03 lpmodule.py:374] update past key value cost 0.005301 seconds
DEBUG 10-15 15:27:03 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:03 lpmodule.py:227] layer idx 8
DEBUG 10-15 15:27:03 lpmodule.py:399] repeat qkv cost 0.033183 seconds
DEBUG 10-15 15:27:03 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:03 lpmodule.py:433] dot attn cost 0.037353 seconds
DEBUG 10-15 15:27:03 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:03 lpmodule.py:444] time cost move to cuda:1 0.024937868118286133 s
DEBUG 10-15 15:27:03 lpllm.py:2283] CPU attn cost 0.130604 seconds if batch True
DEBUG 10-15 15:27:03 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:03 lpllm.py:2294] CPU compute cost 0.131480 seconds
DEBUG 10-15 15:27:03 lpllm.py:2312] free cost 0.000084 seconds
DEBUG 10-15 15:27:04 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:04 lpllm.py:1774] update state cost 1.811981201171875e-05 s
DEBUG 10-15 15:27:04 lpllm.py:1743] restore layer func cost 0.0008170604705810547 s
DEBUG 10-15 15:27:04 lpllm.py:511] restore layer cost 0.0010571479797363281 s
DEBUG 10-15 15:27:04 lpllm.py:512] reset_next_layer_need: layer_attn_idx=9, layer_mlp_idx=9, j_loc=18
DEBUG 10-15 15:27:04 lpllm.py:1037] reset layer cost 0.001129150390625 s
DEBUG 10-15 15:27:04 lpllm.py:1038] have reset next layer layer_attn 9 layer_mlp 9 
DEBUG 10-15 15:27:04 lpllm.py:924] 
DEBUG 10-15 15:27:04 lpllm.py:924] decoder loop j: 19 cur_layer_idx: 9 layer_attn_idx: 9 layer_mlp_idx: 9
DEBUG 10-15 15:27:04 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:04 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:04 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:04 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:04 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:04 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:04 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:04 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:04 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:04 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:04 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:04 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:04 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:04 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:04 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:04 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:04 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:04 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:04 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:04 lpllm.py:2265] GPU2CPU move cost 0.000580 seconds
DEBUG 10-15 15:27:04 lpmodule.py:340] layer_idx 9 attn layer_idx: 9
DEBUG 10-15 15:27:04 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:04 lpmodule.py:364] decoder_attn_batch update batch_dim 180-240
DEBUG 10-15 15:27:04 StaticCacheLen.py:84] static cache update layer_idx: 9, start_batch: 180, end_batch: 240
DEBUG 10-15 15:27:04 lpmodule.py:368] update for kv cache 180-240 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:04 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:04 lpmodule.py:374] update past key value cost 0.005429 seconds
DEBUG 10-15 15:27:04 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:04 lpmodule.py:227] layer idx 9
DEBUG 10-15 15:27:04 lpmodule.py:399] repeat qkv cost 0.033107 seconds
DEBUG 10-15 15:27:04 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:04 lpmodule.py:433] dot attn cost 0.034181 seconds
DEBUG 10-15 15:27:04 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:04 lpmodule.py:444] time cost move to cuda:1 0.025249242782592773 s
DEBUG 10-15 15:27:04 lpllm.py:2283] CPU attn cost 0.127886 seconds if batch True
DEBUG 10-15 15:27:04 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:04 lpllm.py:2294] CPU compute cost 0.128753 seconds
DEBUG 10-15 15:27:04 lpllm.py:2312] free cost 0.000080 seconds
DEBUG 10-15 15:27:04 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:04 lpllm.py:1774] update state cost 3.743171691894531e-05 s
DEBUG 10-15 15:27:04 lpllm.py:1743] restore layer func cost 0.00037932395935058594 s
DEBUG 10-15 15:27:04 lpllm.py:511] restore layer cost 0.0006377696990966797 s
DEBUG 10-15 15:27:04 lpllm.py:512] reset_next_layer_need: layer_attn_idx=10, layer_mlp_idx=9, j_loc=19
DEBUG 10-15 15:27:04 lpllm.py:1037] reset layer cost 0.0007078647613525391 s
DEBUG 10-15 15:27:04 lpllm.py:1038] have reset next layer layer_attn 10 layer_mlp 9 
DEBUG 10-15 15:27:04 lpllm.py:1044] j: 19 waiting the layer with layer_idx 10 before wait time 0.4412727355957031 s
INFO 10-15 15:27:04 client.py:117] confirm_model_loaded: Mixtral-8x7B, af582562-0fe8-4b8f-915a-b82a650cf0e9
INFO 10-15 15:27:04 client.py:125] Model loaded
DEBUG 10-15 15:27:04 lpllm.py:1048] j: load cost 0.4429342746734619 s waiting cost 0.0016465187072753906 s
DEBUG 10-15 15:27:04 lpllm.py:924] 
DEBUG 10-15 15:27:04 lpllm.py:924] decoder loop j: 20 cur_layer_idx: 10 layer_attn_idx: 10 layer_mlp_idx: 9
DEBUG 10-15 15:27:04 lpllm.py:933] start load next layer cur_layer_idx: 11
DEBUG 10-15 15:27:04 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed650>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x003\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x96\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:04 client.py:72] load_into_gpu: Mixtral-8x7B, 3ad4fe32-7b6d-4dc4-bb72-99070eaa7fd3
INFO 10-15 15:27:04 client.py:113] Model loaded: Mixtral-8x7B, 3ad4fe32-7b6d-4dc4-bb72-99070eaa7fd3
DEBUG 10-15 15:27:04 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:04 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:04 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:04 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:04 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:04 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:04 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:04 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:04 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:04 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:04 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:04 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:04 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:04 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:04 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:04 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:04 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:04 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:04 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:04 lpllm.py:2265] GPU2CPU move cost 0.000581 seconds
DEBUG 10-15 15:27:04 lpmodule.py:340] layer_idx 10 attn layer_idx: 10
DEBUG 10-15 15:27:04 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:04 lpmodule.py:364] decoder_attn_batch update batch_dim 120-180
DEBUG 10-15 15:27:04 StaticCacheLen.py:84] static cache update layer_idx: 10, start_batch: 120, end_batch: 180
DEBUG 10-15 15:27:04 lpmodule.py:368] update for kv cache 120-180 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:04 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:04 lpmodule.py:374] update past key value cost 0.005072 seconds
DEBUG 10-15 15:27:04 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:04 lpmodule.py:227] layer idx 9
DEBUG 10-15 15:27:04 lpmodule.py:399] repeat qkv cost 0.036319 seconds
DEBUG 10-15 15:27:04 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:04 lpmodule.py:433] dot attn cost 0.052192 seconds
DEBUG 10-15 15:27:04 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:04 lpmodule.py:444] time cost move to cuda:1 0.024587631225585938 s
DEBUG 10-15 15:27:04 lpllm.py:2283] CPU attn cost 0.148168 seconds if batch True
DEBUG 10-15 15:27:04 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:04 lpllm.py:2294] CPU compute cost 0.149040 seconds
DEBUG 10-15 15:27:04 lpllm.py:2312] free cost 0.000082 seconds
DEBUG 10-15 15:27:04 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:04 lpllm.py:1774] update state cost 1.9073486328125e-05 s
DEBUG 10-15 15:27:04 lpllm.py:1743] restore layer func cost 0.0008025169372558594 s
DEBUG 10-15 15:27:04 lpllm.py:511] restore layer cost 0.0010535717010498047 s
DEBUG 10-15 15:27:04 lpllm.py:512] reset_next_layer_need: layer_attn_idx=10, layer_mlp_idx=10, j_loc=20
DEBUG 10-15 15:27:04 lpllm.py:1037] reset layer cost 0.0011241436004638672 s
DEBUG 10-15 15:27:04 lpllm.py:1038] have reset next layer layer_attn 10 layer_mlp 10 
DEBUG 10-15 15:27:04 lpllm.py:924] 
DEBUG 10-15 15:27:04 lpllm.py:924] decoder loop j: 21 cur_layer_idx: 10 layer_attn_idx: 10 layer_mlp_idx: 10
DEBUG 10-15 15:27:04 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:04 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:04 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:04 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:04 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:04 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:04 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:04 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:04 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:04 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:04 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:04 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:04 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:04 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:04 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:04 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:04 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:04 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:04 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:04 lpllm.py:2265] GPU2CPU move cost 0.000622 seconds
DEBUG 10-15 15:27:04 lpmodule.py:340] layer_idx 10 attn layer_idx: 10
DEBUG 10-15 15:27:04 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:04 lpmodule.py:364] decoder_attn_batch update batch_dim 180-240
DEBUG 10-15 15:27:04 StaticCacheLen.py:84] static cache update layer_idx: 10, start_batch: 180, end_batch: 240
DEBUG 10-15 15:27:04 lpmodule.py:368] update for kv cache 180-240 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:04 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:04 lpmodule.py:374] update past key value cost 0.005910 seconds
DEBUG 10-15 15:27:04 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:04 lpmodule.py:227] layer idx 10
DEBUG 10-15 15:27:04 lpmodule.py:399] repeat qkv cost 0.034100 seconds
DEBUG 10-15 15:27:04 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:04 lpmodule.py:433] dot attn cost 0.033409 seconds
DEBUG 10-15 15:27:04 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:04 lpmodule.py:444] time cost move to cuda:1 0.026294946670532227 s
DEBUG 10-15 15:27:04 lpllm.py:2283] CPU attn cost 0.129664 seconds if batch True
DEBUG 10-15 15:27:04 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:04 lpllm.py:2294] CPU compute cost 0.130573 seconds
DEBUG 10-15 15:27:04 lpllm.py:2312] free cost 0.000080 seconds
DEBUG 10-15 15:27:04 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:04 lpllm.py:1774] update state cost 3.7670135498046875e-05 s
DEBUG 10-15 15:27:04 lpllm.py:1743] restore layer func cost 0.00037479400634765625 s
DEBUG 10-15 15:27:04 lpllm.py:511] restore layer cost 0.0006363391876220703 s
DEBUG 10-15 15:27:04 lpllm.py:512] reset_next_layer_need: layer_attn_idx=11, layer_mlp_idx=10, j_loc=21
DEBUG 10-15 15:27:04 lpllm.py:1037] reset layer cost 0.0007078647613525391 s
DEBUG 10-15 15:27:04 lpllm.py:1038] have reset next layer layer_attn 11 layer_mlp 10 
DEBUG 10-15 15:27:04 lpllm.py:1044] j: 21 waiting the layer with layer_idx 11 before wait time 0.44086742401123047 s
INFO 10-15 15:27:04 client.py:117] confirm_model_loaded: Mixtral-8x7B, 3ad4fe32-7b6d-4dc4-bb72-99070eaa7fd3
INFO 10-15 15:27:04 client.py:125] Model loaded
DEBUG 10-15 15:27:04 lpllm.py:1048] j: load cost 0.4425022602081299 s waiting cost 0.0016200542449951172 s
DEBUG 10-15 15:27:04 lpllm.py:924] 
DEBUG 10-15 15:27:04 lpllm.py:924] decoder loop j: 22 cur_layer_idx: 11 layer_attn_idx: 11 layer_mlp_idx: 10
DEBUG 10-15 15:27:04 lpllm.py:933] start load next layer cur_layer_idx: 12
DEBUG 10-15 15:27:04 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed6e0>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x004\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x98\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:04 client.py:72] load_into_gpu: Mixtral-8x7B, 1c6912a7-f747-4cf0-86ae-44ffd0b1deb9
INFO 10-15 15:27:04 client.py:113] Model loaded: Mixtral-8x7B, 1c6912a7-f747-4cf0-86ae-44ffd0b1deb9
DEBUG 10-15 15:27:04 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:04 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:04 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:04 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:04 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:04 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:04 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:04 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:04 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:04 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:04 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:04 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:04 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:04 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:04 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:04 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:04 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:04 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:04 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:04 lpllm.py:2265] GPU2CPU move cost 0.000615 seconds
DEBUG 10-15 15:27:04 lpmodule.py:340] layer_idx 11 attn layer_idx: 11
DEBUG 10-15 15:27:04 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:04 lpmodule.py:364] decoder_attn_batch update batch_dim 120-180
DEBUG 10-15 15:27:04 StaticCacheLen.py:84] static cache update layer_idx: 11, start_batch: 120, end_batch: 180
DEBUG 10-15 15:27:04 lpmodule.py:368] update for kv cache 120-180 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:04 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:04 lpmodule.py:374] update past key value cost 0.005217 seconds
DEBUG 10-15 15:27:04 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:04 lpmodule.py:227] layer idx 10
DEBUG 10-15 15:27:04 lpmodule.py:399] repeat qkv cost 0.035116 seconds
DEBUG 10-15 15:27:04 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:04 lpmodule.py:433] dot attn cost 0.035788 seconds
DEBUG 10-15 15:27:04 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:04 lpmodule.py:444] time cost move to cuda:1 0.024425268173217773 s
DEBUG 10-15 15:27:04 lpllm.py:2283] CPU attn cost 0.131059 seconds if batch True
DEBUG 10-15 15:27:04 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:04 lpllm.py:2294] CPU compute cost 0.132018 seconds
DEBUG 10-15 15:27:04 lpllm.py:2312] free cost 0.000085 seconds
DEBUG 10-15 15:27:04 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:04 lpllm.py:1774] update state cost 1.9788742065429688e-05 s
DEBUG 10-15 15:27:04 lpllm.py:1743] restore layer func cost 0.0008177757263183594 s
DEBUG 10-15 15:27:04 lpllm.py:511] restore layer cost 0.0010623931884765625 s
DEBUG 10-15 15:27:04 lpllm.py:512] reset_next_layer_need: layer_attn_idx=11, layer_mlp_idx=11, j_loc=22
DEBUG 10-15 15:27:04 lpllm.py:1037] reset layer cost 0.0011341571807861328 s
DEBUG 10-15 15:27:04 lpllm.py:1038] have reset next layer layer_attn 11 layer_mlp 11 
DEBUG 10-15 15:27:04 lpllm.py:924] 
DEBUG 10-15 15:27:04 lpllm.py:924] decoder loop j: 23 cur_layer_idx: 11 layer_attn_idx: 11 layer_mlp_idx: 11
DEBUG 10-15 15:27:04 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:04 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:04 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:04 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:04 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:04 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:04 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:04 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:04 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:04 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:04 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:04 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:04 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:04 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:04 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:04 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:04 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:04 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:04 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:04 lpllm.py:2265] GPU2CPU move cost 0.000609 seconds
DEBUG 10-15 15:27:04 lpmodule.py:340] layer_idx 11 attn layer_idx: 11
DEBUG 10-15 15:27:04 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:04 lpmodule.py:364] decoder_attn_batch update batch_dim 180-240
DEBUG 10-15 15:27:04 StaticCacheLen.py:84] static cache update layer_idx: 11, start_batch: 180, end_batch: 240
DEBUG 10-15 15:27:04 lpmodule.py:368] update for kv cache 180-240 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:04 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:04 lpmodule.py:374] update past key value cost 0.005750 seconds
DEBUG 10-15 15:27:04 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:04 lpmodule.py:227] layer idx 11
DEBUG 10-15 15:27:04 lpmodule.py:399] repeat qkv cost 0.033450 seconds
DEBUG 10-15 15:27:04 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:04 lpmodule.py:433] dot attn cost 0.042037 seconds
DEBUG 10-15 15:27:04 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:05 lpmodule.py:444] time cost move to cuda:1 0.024707794189453125 s
DEBUG 10-15 15:27:05 lpllm.py:2283] CPU attn cost 0.135882 seconds if batch True
DEBUG 10-15 15:27:05 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:05 lpllm.py:2294] CPU compute cost 0.136787 seconds
DEBUG 10-15 15:27:05 lpllm.py:2312] free cost 0.000080 seconds
DEBUG 10-15 15:27:05 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:05 lpllm.py:1774] update state cost 3.695487976074219e-05 s
DEBUG 10-15 15:27:05 lpllm.py:1743] restore layer func cost 0.0003859996795654297 s
DEBUG 10-15 15:27:05 lpllm.py:511] restore layer cost 0.0006639957427978516 s
DEBUG 10-15 15:27:05 lpllm.py:512] reset_next_layer_need: layer_attn_idx=12, layer_mlp_idx=11, j_loc=23
DEBUG 10-15 15:27:05 lpllm.py:1037] reset layer cost 0.0007350444793701172 s
DEBUG 10-15 15:27:05 lpllm.py:1038] have reset next layer layer_attn 12 layer_mlp 11 
DEBUG 10-15 15:27:05 lpllm.py:1044] j: 23 waiting the layer with layer_idx 12 before wait time 0.44176435470581055 s
INFO 10-15 15:27:05 client.py:117] confirm_model_loaded: Mixtral-8x7B, 1c6912a7-f747-4cf0-86ae-44ffd0b1deb9
INFO 10-15 15:27:05 client.py:125] Model loaded
DEBUG 10-15 15:27:05 lpllm.py:1048] j: load cost 0.4433720111846924 s waiting cost 0.001592397689819336 s
DEBUG 10-15 15:27:05 lpllm.py:924] 
DEBUG 10-15 15:27:05 lpllm.py:924] decoder loop j: 24 cur_layer_idx: 12 layer_attn_idx: 12 layer_mlp_idx: 11
DEBUG 10-15 15:27:05 lpllm.py:933] start load next layer cur_layer_idx: 13
DEBUG 10-15 15:27:05 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:05 client.py:72] load_into_gpu: Mixtral-8x7B, 4e749b11-cf7a-479a-b2f7-c1f75f738712
INFO 10-15 15:27:05 client.py:113] Model loaded: Mixtral-8x7B, 4e749b11-cf7a-479a-b2f7-c1f75f738712
DEBUG 10-15 15:27:05 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:05 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:05 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:05 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:05 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:05 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:05 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:05 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:05 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:05 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:05 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:05 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:05 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:05 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:05 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:05 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:05 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:05 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:05 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:05 lpllm.py:2265] GPU2CPU move cost 0.000593 seconds
DEBUG 10-15 15:27:05 lpmodule.py:340] layer_idx 12 attn layer_idx: 12
DEBUG 10-15 15:27:05 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:05 lpmodule.py:364] decoder_attn_batch update batch_dim 120-180
DEBUG 10-15 15:27:05 StaticCacheLen.py:84] static cache update layer_idx: 12, start_batch: 120, end_batch: 180
DEBUG 10-15 15:27:05 lpmodule.py:368] update for kv cache 120-180 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:05 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:05 lpmodule.py:374] update past key value cost 0.005569 seconds
DEBUG 10-15 15:27:05 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:05 lpmodule.py:227] layer idx 11
DEBUG 10-15 15:27:05 lpmodule.py:399] repeat qkv cost 0.034300 seconds
DEBUG 10-15 15:27:05 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:05 lpmodule.py:433] dot attn cost 0.034876 seconds
DEBUG 10-15 15:27:05 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:05 lpmodule.py:444] time cost move to cuda:1 0.02696061134338379 s
DEBUG 10-15 15:27:05 lpllm.py:2283] CPU attn cost 0.133935 seconds if batch True
DEBUG 10-15 15:27:05 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:05 lpllm.py:2294] CPU compute cost 0.134817 seconds
DEBUG 10-15 15:27:05 lpllm.py:2312] free cost 0.000079 seconds
DEBUG 10-15 15:27:05 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:05 lpllm.py:1774] update state cost 2.002716064453125e-05 s
DEBUG 10-15 15:27:05 lpllm.py:1743] restore layer func cost 0.0008015632629394531 s
DEBUG 10-15 15:27:05 lpllm.py:511] restore layer cost 0.0010440349578857422 s
DEBUG 10-15 15:27:05 lpllm.py:512] reset_next_layer_need: layer_attn_idx=12, layer_mlp_idx=12, j_loc=24
DEBUG 10-15 15:27:05 lpllm.py:1037] reset layer cost 0.0011150836944580078 s
DEBUG 10-15 15:27:05 lpllm.py:1038] have reset next layer layer_attn 12 layer_mlp 12 
DEBUG 10-15 15:27:05 lpllm.py:924] 
DEBUG 10-15 15:27:05 lpllm.py:924] decoder loop j: 25 cur_layer_idx: 12 layer_attn_idx: 12 layer_mlp_idx: 12
DEBUG 10-15 15:27:05 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:05 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:05 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:05 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:05 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:05 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:05 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:05 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:05 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:05 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:05 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:05 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:05 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:05 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:05 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:05 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:05 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:05 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:05 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:05 lpllm.py:2265] GPU2CPU move cost 0.000591 seconds
DEBUG 10-15 15:27:05 lpmodule.py:340] layer_idx 12 attn layer_idx: 12
DEBUG 10-15 15:27:05 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:05 lpmodule.py:364] decoder_attn_batch update batch_dim 180-240
DEBUG 10-15 15:27:05 StaticCacheLen.py:84] static cache update layer_idx: 12, start_batch: 180, end_batch: 240
DEBUG 10-15 15:27:05 lpmodule.py:368] update for kv cache 180-240 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:05 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:05 lpmodule.py:374] update past key value cost 0.010192 seconds
DEBUG 10-15 15:27:05 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:05 lpmodule.py:227] layer idx 12
DEBUG 10-15 15:27:05 lpmodule.py:399] repeat qkv cost 0.035434 seconds
DEBUG 10-15 15:27:05 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:05 lpmodule.py:433] dot attn cost 0.033884 seconds
DEBUG 10-15 15:27:05 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:05 lpmodule.py:444] time cost move to cuda:1 0.024890422821044922 s
DEBUG 10-15 15:27:05 lpllm.py:2283] CPU attn cost 0.134661 seconds if batch True
DEBUG 10-15 15:27:05 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:05 lpllm.py:2294] CPU compute cost 0.135536 seconds
DEBUG 10-15 15:27:05 lpllm.py:2312] free cost 0.000078 seconds
DEBUG 10-15 15:27:05 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:05 lpllm.py:1774] update state cost 2.0503997802734375e-05 s
DEBUG 10-15 15:27:05 lpllm.py:1743] restore layer func cost 0.00038123130798339844 s
DEBUG 10-15 15:27:05 lpllm.py:511] restore layer cost 0.0006365776062011719 s
DEBUG 10-15 15:27:05 lpllm.py:512] reset_next_layer_need: layer_attn_idx=13, layer_mlp_idx=12, j_loc=25
DEBUG 10-15 15:27:05 lpllm.py:1037] reset layer cost 0.0007071495056152344 s
DEBUG 10-15 15:27:05 lpllm.py:1038] have reset next layer layer_attn 13 layer_mlp 12 
DEBUG 10-15 15:27:05 lpllm.py:1044] j: 25 waiting the layer with layer_idx 13 before wait time 0.45256829261779785 s
INFO 10-15 15:27:05 client.py:117] confirm_model_loaded: Mixtral-8x7B, 4e749b11-cf7a-479a-b2f7-c1f75f738712
INFO 10-15 15:27:05 client.py:125] Model loaded
DEBUG 10-15 15:27:05 lpllm.py:1048] j: load cost 0.4541592597961426 s waiting cost 0.0015752315521240234 s
DEBUG 10-15 15:27:05 lpllm.py:924] 
DEBUG 10-15 15:27:05 lpllm.py:924] decoder loop j: 26 cur_layer_idx: 13 layer_attn_idx: 13 layer_mlp_idx: 12
DEBUG 10-15 15:27:05 lpllm.py:933] start load next layer cur_layer_idx: 14
DEBUG 10-15 15:27:05 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed650>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x003\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x96\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:05 client.py:72] load_into_gpu: Mixtral-8x7B, 5b59bc0e-e3e0-43f9-9136-8152d6dded56
INFO 10-15 15:27:05 client.py:113] Model loaded: Mixtral-8x7B, 5b59bc0e-e3e0-43f9-9136-8152d6dded56
DEBUG 10-15 15:27:05 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:05 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:05 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:05 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:05 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:05 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:05 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:05 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:05 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:05 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:05 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:05 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:05 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:05 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:05 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:05 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:05 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:05 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:05 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:05 lpllm.py:2265] GPU2CPU move cost 0.000583 seconds
DEBUG 10-15 15:27:05 lpmodule.py:340] layer_idx 13 attn layer_idx: 13
DEBUG 10-15 15:27:05 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:05 lpmodule.py:364] decoder_attn_batch update batch_dim 120-180
DEBUG 10-15 15:27:05 StaticCacheLen.py:84] static cache update layer_idx: 13, start_batch: 120, end_batch: 180
DEBUG 10-15 15:27:05 lpmodule.py:368] update for kv cache 120-180 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:05 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:05 lpmodule.py:374] update past key value cost 0.005277 seconds
DEBUG 10-15 15:27:05 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:05 lpmodule.py:227] layer idx 12
DEBUG 10-15 15:27:05 lpmodule.py:399] repeat qkv cost 0.034336 seconds
DEBUG 10-15 15:27:05 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:05 lpmodule.py:433] dot attn cost 0.037276 seconds
DEBUG 10-15 15:27:05 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:05 lpmodule.py:444] time cost move to cuda:1 0.026608705520629883 s
DEBUG 10-15 15:27:05 lpllm.py:2283] CPU attn cost 0.134333 seconds if batch True
DEBUG 10-15 15:27:05 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:05 lpllm.py:2294] CPU compute cost 0.135232 seconds
DEBUG 10-15 15:27:05 lpllm.py:2312] free cost 0.000084 seconds
DEBUG 10-15 15:27:05 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:05 lpllm.py:1774] update state cost 1.9073486328125e-05 s
DEBUG 10-15 15:27:05 lpllm.py:1743] restore layer func cost 0.0007960796356201172 s
DEBUG 10-15 15:27:05 lpllm.py:511] restore layer cost 0.0010287761688232422 s
DEBUG 10-15 15:27:05 lpllm.py:512] reset_next_layer_need: layer_attn_idx=13, layer_mlp_idx=13, j_loc=26
DEBUG 10-15 15:27:05 lpllm.py:1037] reset layer cost 0.0011005401611328125 s
DEBUG 10-15 15:27:05 lpllm.py:1038] have reset next layer layer_attn 13 layer_mlp 13 
DEBUG 10-15 15:27:05 lpllm.py:924] 
DEBUG 10-15 15:27:05 lpllm.py:924] decoder loop j: 27 cur_layer_idx: 13 layer_attn_idx: 13 layer_mlp_idx: 13
DEBUG 10-15 15:27:05 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:05 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:05 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:05 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:05 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:05 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:05 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:05 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:05 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:05 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:05 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:05 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:05 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:05 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:05 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:05 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:05 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:05 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:05 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:05 lpllm.py:2265] GPU2CPU move cost 0.000344 seconds
DEBUG 10-15 15:27:05 lpmodule.py:340] layer_idx 13 attn layer_idx: 13
DEBUG 10-15 15:27:05 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:05 lpmodule.py:364] decoder_attn_batch update batch_dim 180-240
DEBUG 10-15 15:27:05 StaticCacheLen.py:84] static cache update layer_idx: 13, start_batch: 180, end_batch: 240
DEBUG 10-15 15:27:05 lpmodule.py:368] update for kv cache 180-240 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:05 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:05 lpmodule.py:374] update past key value cost 0.005603 seconds
DEBUG 10-15 15:27:05 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:05 lpmodule.py:227] layer idx 13
DEBUG 10-15 15:27:05 lpmodule.py:399] repeat qkv cost 0.034900 seconds
DEBUG 10-15 15:27:05 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:05 lpmodule.py:433] dot attn cost 0.032095 seconds
DEBUG 10-15 15:27:05 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:05 lpmodule.py:444] time cost move to cuda:1 0.02460455894470215 s
DEBUG 10-15 15:27:06 lpllm.py:2283] CPU attn cost 0.127357 seconds if batch True
DEBUG 10-15 15:27:06 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:06 lpllm.py:2294] CPU compute cost 0.127909 seconds
DEBUG 10-15 15:27:06 lpllm.py:2312] free cost 0.000083 seconds
DEBUG 10-15 15:27:06 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:06 lpllm.py:1774] update state cost 2.09808349609375e-05 s
DEBUG 10-15 15:27:06 lpllm.py:1743] restore layer func cost 0.0003948211669921875 s
DEBUG 10-15 15:27:06 lpllm.py:511] restore layer cost 0.0006725788116455078 s
DEBUG 10-15 15:27:06 lpllm.py:512] reset_next_layer_need: layer_attn_idx=14, layer_mlp_idx=13, j_loc=27
DEBUG 10-15 15:27:06 lpllm.py:1037] reset layer cost 0.0007433891296386719 s
DEBUG 10-15 15:27:06 lpllm.py:1038] have reset next layer layer_attn 14 layer_mlp 13 
DEBUG 10-15 15:27:06 lpllm.py:1044] j: 27 waiting the layer with layer_idx 14 before wait time 0.5203061103820801 s
INFO 10-15 15:27:06 client.py:117] confirm_model_loaded: Mixtral-8x7B, 5b59bc0e-e3e0-43f9-9136-8152d6dded56
INFO 10-15 15:27:06 client.py:125] Model loaded
DEBUG 10-15 15:27:06 lpllm.py:1048] j: load cost 0.5219199657440186 s waiting cost 0.0015990734100341797 s
DEBUG 10-15 15:27:06 lpllm.py:924] 
DEBUG 10-15 15:27:06 lpllm.py:924] decoder loop j: 28 cur_layer_idx: 14 layer_attn_idx: 14 layer_mlp_idx: 13
DEBUG 10-15 15:27:06 lpllm.py:933] start load next layer cur_layer_idx: 15
DEBUG 10-15 15:27:06 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed6e0>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x004\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x98\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:06 client.py:72] load_into_gpu: Mixtral-8x7B, 393c1a52-a747-4f31-83a1-33db8467bc52
INFO 10-15 15:27:06 client.py:113] Model loaded: Mixtral-8x7B, 393c1a52-a747-4f31-83a1-33db8467bc52
DEBUG 10-15 15:27:06 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:06 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:06 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:06 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:06 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:06 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:06 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:06 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:06 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:06 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:06 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:06 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:06 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:06 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:06 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:06 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:06 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:06 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:06 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:06 lpllm.py:2265] GPU2CPU move cost 0.000579 seconds
DEBUG 10-15 15:27:06 lpmodule.py:340] layer_idx 14 attn layer_idx: 14
DEBUG 10-15 15:27:06 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:06 lpmodule.py:364] decoder_attn_batch update batch_dim 120-180
DEBUG 10-15 15:27:06 StaticCacheLen.py:84] static cache update layer_idx: 14, start_batch: 120, end_batch: 180
DEBUG 10-15 15:27:06 lpmodule.py:368] update for kv cache 120-180 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:06 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:06 lpmodule.py:374] update past key value cost 0.005077 seconds
DEBUG 10-15 15:27:06 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:06 lpmodule.py:227] layer idx 13
DEBUG 10-15 15:27:06 lpmodule.py:399] repeat qkv cost 0.036797 seconds
DEBUG 10-15 15:27:06 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:06 lpmodule.py:433] dot attn cost 0.036048 seconds
DEBUG 10-15 15:27:06 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:06 lpmodule.py:444] time cost move to cuda:1 0.02508068084716797 s
DEBUG 10-15 15:27:06 lpllm.py:2283] CPU attn cost 0.133729 seconds if batch True
DEBUG 10-15 15:27:06 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:06 lpllm.py:2294] CPU compute cost 0.134600 seconds
DEBUG 10-15 15:27:06 lpllm.py:2312] free cost 0.000087 seconds
DEBUG 10-15 15:27:06 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:06 lpllm.py:1774] update state cost 3.409385681152344e-05 s
DEBUG 10-15 15:27:06 lpllm.py:1743] restore layer func cost 0.0008265972137451172 s
DEBUG 10-15 15:27:06 lpllm.py:511] restore layer cost 0.0010933876037597656 s
DEBUG 10-15 15:27:06 lpllm.py:512] reset_next_layer_need: layer_attn_idx=14, layer_mlp_idx=14, j_loc=28
DEBUG 10-15 15:27:06 lpllm.py:1037] reset layer cost 0.0011668205261230469 s
DEBUG 10-15 15:27:06 lpllm.py:1038] have reset next layer layer_attn 14 layer_mlp 14 
DEBUG 10-15 15:27:06 lpllm.py:924] 
DEBUG 10-15 15:27:06 lpllm.py:924] decoder loop j: 29 cur_layer_idx: 14 layer_attn_idx: 14 layer_mlp_idx: 14
DEBUG 10-15 15:27:06 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:06 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:06 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:06 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:06 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:06 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:06 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:06 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:06 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:06 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:06 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:06 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:06 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:06 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:06 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:06 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:06 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:06 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:06 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:06 lpllm.py:2265] GPU2CPU move cost 0.000587 seconds
DEBUG 10-15 15:27:06 lpmodule.py:340] layer_idx 14 attn layer_idx: 14
DEBUG 10-15 15:27:06 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:06 lpmodule.py:364] decoder_attn_batch update batch_dim 180-240
DEBUG 10-15 15:27:06 StaticCacheLen.py:84] static cache update layer_idx: 14, start_batch: 180, end_batch: 240
DEBUG 10-15 15:27:06 lpmodule.py:368] update for kv cache 180-240 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:06 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:06 lpmodule.py:374] update past key value cost 0.005540 seconds
DEBUG 10-15 15:27:06 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:06 lpmodule.py:227] layer idx 14
DEBUG 10-15 15:27:06 lpmodule.py:399] repeat qkv cost 0.036739 seconds
DEBUG 10-15 15:27:06 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:06 lpmodule.py:433] dot attn cost 0.036652 seconds
DEBUG 10-15 15:27:06 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:06 lpmodule.py:444] time cost move to cuda:1 0.024561166763305664 s
DEBUG 10-15 15:27:06 lpllm.py:2283] CPU attn cost 0.136726 seconds if batch True
DEBUG 10-15 15:27:06 lpllm.py:2292] deal attn result cost 0.000003 seconds
DEBUG 10-15 15:27:06 lpllm.py:2294] CPU compute cost 0.137618 seconds
DEBUG 10-15 15:27:06 lpllm.py:2312] free cost 0.000082 seconds
DEBUG 10-15 15:27:06 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:06 lpllm.py:1774] update state cost 3.600120544433594e-05 s
DEBUG 10-15 15:27:06 lpllm.py:1743] restore layer func cost 0.0003802776336669922 s
DEBUG 10-15 15:27:06 lpllm.py:511] restore layer cost 0.0006420612335205078 s
DEBUG 10-15 15:27:06 lpllm.py:512] reset_next_layer_need: layer_attn_idx=15, layer_mlp_idx=14, j_loc=29
DEBUG 10-15 15:27:06 lpllm.py:1037] reset layer cost 0.0007126331329345703 s
DEBUG 10-15 15:27:06 lpllm.py:1038] have reset next layer layer_attn 15 layer_mlp 14 
DEBUG 10-15 15:27:06 lpllm.py:1044] j: 29 waiting the layer with layer_idx 15 before wait time 0.46508097648620605 s
INFO 10-15 15:27:06 client.py:117] confirm_model_loaded: Mixtral-8x7B, 393c1a52-a747-4f31-83a1-33db8467bc52
INFO 10-15 15:27:06 client.py:125] Model loaded
DEBUG 10-15 15:27:06 lpllm.py:1048] j: load cost 0.46694517135620117 s waiting cost 0.0018486976623535156 s
DEBUG 10-15 15:27:06 lpllm.py:924] 
DEBUG 10-15 15:27:06 lpllm.py:924] decoder loop j: 30 cur_layer_idx: 15 layer_attn_idx: 15 layer_mlp_idx: 14
DEBUG 10-15 15:27:06 lpllm.py:933] start load next layer cur_layer_idx: 16
DEBUG 10-15 15:27:06 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:06 client.py:72] load_into_gpu: Mixtral-8x7B, 244da0ab-df80-42d6-9c74-ed765f7867e9
INFO 10-15 15:27:06 client.py:113] Model loaded: Mixtral-8x7B, 244da0ab-df80-42d6-9c74-ed765f7867e9
DEBUG 10-15 15:27:06 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:06 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:06 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:06 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:06 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:06 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:06 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:06 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:06 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:06 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:06 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:06 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:06 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:06 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:06 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:06 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:06 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:06 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:06 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:06 lpllm.py:2265] GPU2CPU move cost 0.000585 seconds
DEBUG 10-15 15:27:06 lpmodule.py:340] layer_idx 15 attn layer_idx: 15
DEBUG 10-15 15:27:06 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:06 lpmodule.py:364] decoder_attn_batch update batch_dim 120-180
DEBUG 10-15 15:27:06 StaticCacheLen.py:84] static cache update layer_idx: 15, start_batch: 120, end_batch: 180
DEBUG 10-15 15:27:06 lpmodule.py:368] update for kv cache 120-180 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:06 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:06 lpmodule.py:374] update past key value cost 0.026938 seconds
DEBUG 10-15 15:27:06 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:06 lpmodule.py:227] layer idx 14
DEBUG 10-15 15:27:06 lpmodule.py:399] repeat qkv cost 0.029374 seconds
DEBUG 10-15 15:27:06 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:06 lpmodule.py:433] dot attn cost 0.040256 seconds
DEBUG 10-15 15:27:06 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:06 lpmodule.py:444] time cost move to cuda:1 0.024321317672729492 s
DEBUG 10-15 15:27:06 lpllm.py:2283] CPU attn cost 0.149216 seconds if batch True
DEBUG 10-15 15:27:06 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:06 lpllm.py:2294] CPU compute cost 0.150086 seconds
DEBUG 10-15 15:27:06 lpllm.py:2312] free cost 0.000082 seconds
DEBUG 10-15 15:27:06 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:06 lpllm.py:1774] update state cost 1.8596649169921875e-05 s
DEBUG 10-15 15:27:06 lpllm.py:1743] restore layer func cost 0.000823974609375 s
DEBUG 10-15 15:27:06 lpllm.py:511] restore layer cost 0.0010640621185302734 s
DEBUG 10-15 15:27:06 lpllm.py:512] reset_next_layer_need: layer_attn_idx=15, layer_mlp_idx=15, j_loc=30
DEBUG 10-15 15:27:06 lpllm.py:1037] reset layer cost 0.001142263412475586 s
DEBUG 10-15 15:27:06 lpllm.py:1038] have reset next layer layer_attn 15 layer_mlp 15 
DEBUG 10-15 15:27:06 lpllm.py:924] 
DEBUG 10-15 15:27:06 lpllm.py:924] decoder loop j: 31 cur_layer_idx: 15 layer_attn_idx: 15 layer_mlp_idx: 15
DEBUG 10-15 15:27:06 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:06 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:06 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:06 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:06 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:06 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:06 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:06 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:06 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:06 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:06 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:06 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:06 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:06 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:06 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:06 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:06 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:06 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:06 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:06 lpllm.py:2265] GPU2CPU move cost 0.000590 seconds
DEBUG 10-15 15:27:06 lpmodule.py:340] layer_idx 15 attn layer_idx: 15
DEBUG 10-15 15:27:06 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:06 lpmodule.py:364] decoder_attn_batch update batch_dim 180-240
DEBUG 10-15 15:27:06 StaticCacheLen.py:84] static cache update layer_idx: 15, start_batch: 180, end_batch: 240
DEBUG 10-15 15:27:06 lpmodule.py:368] update for kv cache 180-240 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:06 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:06 lpmodule.py:374] update past key value cost 0.027406 seconds
DEBUG 10-15 15:27:06 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:06 lpmodule.py:227] layer idx 15
DEBUG 10-15 15:27:06 lpmodule.py:399] repeat qkv cost 0.030341 seconds
DEBUG 10-15 15:27:06 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:06 lpmodule.py:433] dot attn cost 0.035831 seconds
DEBUG 10-15 15:27:06 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:06 lpmodule.py:444] time cost move to cuda:1 0.025449752807617188 s
DEBUG 10-15 15:27:06 lpllm.py:2283] CPU attn cost 0.148585 seconds if batch True
DEBUG 10-15 15:27:06 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:06 lpllm.py:2294] CPU compute cost 0.149446 seconds
DEBUG 10-15 15:27:06 lpllm.py:2312] free cost 0.000079 seconds
DEBUG 10-15 15:27:07 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:07 lpllm.py:1774] update state cost 2.1219253540039062e-05 s
DEBUG 10-15 15:27:07 lpllm.py:1743] restore layer func cost 0.0003886222839355469 s
DEBUG 10-15 15:27:07 lpllm.py:511] restore layer cost 0.0006532669067382812 s
DEBUG 10-15 15:27:07 lpllm.py:512] reset_next_layer_need: layer_attn_idx=16, layer_mlp_idx=15, j_loc=31
DEBUG 10-15 15:27:07 lpllm.py:1037] reset layer cost 0.0007278919219970703 s
DEBUG 10-15 15:27:07 lpllm.py:1038] have reset next layer layer_attn 16 layer_mlp 15 
DEBUG 10-15 15:27:07 lpllm.py:1044] j: 31 waiting the layer with layer_idx 16 before wait time 0.4470856189727783 s
INFO 10-15 15:27:07 client.py:117] confirm_model_loaded: Mixtral-8x7B, 244da0ab-df80-42d6-9c74-ed765f7867e9
INFO 10-15 15:27:07 client.py:125] Model loaded
DEBUG 10-15 15:27:07 lpllm.py:1048] j: load cost 0.448544979095459 s waiting cost 0.001443624496459961 s
DEBUG 10-15 15:27:07 lpllm.py:924] 
DEBUG 10-15 15:27:07 lpllm.py:924] decoder loop j: 32 cur_layer_idx: 16 layer_attn_idx: 16 layer_mlp_idx: 15
DEBUG 10-15 15:27:07 lpllm.py:933] start load next layer cur_layer_idx: 17
DEBUG 10-15 15:27:07 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed650>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x003\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x96\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:07 client.py:72] load_into_gpu: Mixtral-8x7B, 9dc76bdd-2812-4396-9844-9b6d4478569c
INFO 10-15 15:27:07 client.py:113] Model loaded: Mixtral-8x7B, 9dc76bdd-2812-4396-9844-9b6d4478569c
DEBUG 10-15 15:27:07 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:07 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:07 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:07 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:07 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:07 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:07 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:07 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:07 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:07 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:07 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:07 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:07 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:07 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:07 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:07 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:07 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:07 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:07 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:07 lpllm.py:2265] GPU2CPU move cost 0.000583 seconds
DEBUG 10-15 15:27:07 lpmodule.py:340] layer_idx 16 attn layer_idx: 16
DEBUG 10-15 15:27:07 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:07 lpmodule.py:364] decoder_attn_batch update batch_dim 120-180
DEBUG 10-15 15:27:07 StaticCacheLen.py:84] static cache update layer_idx: 16, start_batch: 120, end_batch: 180
DEBUG 10-15 15:27:07 lpmodule.py:368] update for kv cache 120-180 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:07 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:07 lpmodule.py:374] update past key value cost 0.025975 seconds
DEBUG 10-15 15:27:07 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:07 lpmodule.py:227] layer idx 15
DEBUG 10-15 15:27:07 lpmodule.py:399] repeat qkv cost 0.030627 seconds
DEBUG 10-15 15:27:07 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:07 lpmodule.py:433] dot attn cost 0.036700 seconds
DEBUG 10-15 15:27:07 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:07 lpmodule.py:444] time cost move to cuda:1 0.02504897117614746 s
DEBUG 10-15 15:27:07 lpllm.py:2283] CPU attn cost 0.147679 seconds if batch True
DEBUG 10-15 15:27:07 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:07 lpllm.py:2294] CPU compute cost 0.148539 seconds
DEBUG 10-15 15:27:07 lpllm.py:2312] free cost 0.000080 seconds
DEBUG 10-15 15:27:07 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:07 lpllm.py:1774] update state cost 1.8596649169921875e-05 s
DEBUG 10-15 15:27:07 lpllm.py:1743] restore layer func cost 0.0007951259613037109 s
DEBUG 10-15 15:27:07 lpllm.py:511] restore layer cost 0.0010366439819335938 s
DEBUG 10-15 15:27:07 lpllm.py:512] reset_next_layer_need: layer_attn_idx=16, layer_mlp_idx=16, j_loc=32
DEBUG 10-15 15:27:07 lpllm.py:1037] reset layer cost 0.0011088848114013672 s
DEBUG 10-15 15:27:07 lpllm.py:1038] have reset next layer layer_attn 16 layer_mlp 16 
DEBUG 10-15 15:27:07 lpllm.py:924] 
DEBUG 10-15 15:27:07 lpllm.py:924] decoder loop j: 33 cur_layer_idx: 16 layer_attn_idx: 16 layer_mlp_idx: 16
DEBUG 10-15 15:27:07 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:07 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:07 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:07 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:07 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:07 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:07 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:07 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:07 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:07 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:07 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:07 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:07 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:07 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:07 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:07 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:07 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:07 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:07 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:07 lpllm.py:2265] GPU2CPU move cost 0.000585 seconds
DEBUG 10-15 15:27:07 lpmodule.py:340] layer_idx 16 attn layer_idx: 16
DEBUG 10-15 15:27:07 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:07 lpmodule.py:364] decoder_attn_batch update batch_dim 180-240
DEBUG 10-15 15:27:07 StaticCacheLen.py:84] static cache update layer_idx: 16, start_batch: 180, end_batch: 240
DEBUG 10-15 15:27:07 lpmodule.py:368] update for kv cache 180-240 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:07 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:07 lpmodule.py:374] update past key value cost 0.026607 seconds
DEBUG 10-15 15:27:07 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:07 lpmodule.py:227] layer idx 16
DEBUG 10-15 15:27:07 lpmodule.py:399] repeat qkv cost 0.031248 seconds
DEBUG 10-15 15:27:07 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:07 lpmodule.py:433] dot attn cost 0.033609 seconds
DEBUG 10-15 15:27:07 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:07 lpmodule.py:444] time cost move to cuda:1 0.025774717330932617 s
DEBUG 10-15 15:27:07 lpllm.py:2283] CPU attn cost 0.146868 seconds if batch True
DEBUG 10-15 15:27:07 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:07 lpllm.py:2294] CPU compute cost 0.147763 seconds
DEBUG 10-15 15:27:07 lpllm.py:2312] free cost 0.000084 seconds
DEBUG 10-15 15:27:07 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:07 lpllm.py:1774] update state cost 2.1696090698242188e-05 s
DEBUG 10-15 15:27:07 lpllm.py:1743] restore layer func cost 0.0003674030303955078 s
DEBUG 10-15 15:27:07 lpllm.py:511] restore layer cost 0.0006113052368164062 s
DEBUG 10-15 15:27:07 lpllm.py:512] reset_next_layer_need: layer_attn_idx=17, layer_mlp_idx=16, j_loc=33
DEBUG 10-15 15:27:07 lpllm.py:1037] reset layer cost 0.00067901611328125 s
DEBUG 10-15 15:27:07 lpllm.py:1038] have reset next layer layer_attn 17 layer_mlp 16 
DEBUG 10-15 15:27:07 lpllm.py:1044] j: 33 waiting the layer with layer_idx 17 before wait time 0.44590187072753906 s
INFO 10-15 15:27:07 client.py:117] confirm_model_loaded: Mixtral-8x7B, 9dc76bdd-2812-4396-9844-9b6d4478569c
INFO 10-15 15:27:07 client.py:125] Model loaded
DEBUG 10-15 15:27:07 lpllm.py:1048] j: load cost 0.4473273754119873 s waiting cost 0.0014116764068603516 s
DEBUG 10-15 15:27:07 lpllm.py:924] 
DEBUG 10-15 15:27:07 lpllm.py:924] decoder loop j: 34 cur_layer_idx: 17 layer_attn_idx: 17 layer_mlp_idx: 16
DEBUG 10-15 15:27:07 lpllm.py:933] start load next layer cur_layer_idx: 18
DEBUG 10-15 15:27:07 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed6e0>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x004\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x98\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:07 client.py:72] load_into_gpu: Mixtral-8x7B, 88144e23-6adf-43eb-9439-afc70586aa3c
INFO 10-15 15:27:07 client.py:113] Model loaded: Mixtral-8x7B, 88144e23-6adf-43eb-9439-afc70586aa3c
DEBUG 10-15 15:27:07 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:07 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:07 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:07 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:07 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:07 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:07 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:07 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:07 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:07 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:07 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:07 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:07 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:07 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:07 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:07 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:07 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:07 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:07 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:07 lpllm.py:2265] GPU2CPU move cost 0.000585 seconds
DEBUG 10-15 15:27:07 lpmodule.py:340] layer_idx 17 attn layer_idx: 17
DEBUG 10-15 15:27:07 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:07 lpmodule.py:364] decoder_attn_batch update batch_dim 120-180
DEBUG 10-15 15:27:07 StaticCacheLen.py:84] static cache update layer_idx: 17, start_batch: 120, end_batch: 180
DEBUG 10-15 15:27:07 lpmodule.py:368] update for kv cache 120-180 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:07 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:07 lpmodule.py:374] update past key value cost 0.025496 seconds
DEBUG 10-15 15:27:07 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:07 lpmodule.py:227] layer idx 16
DEBUG 10-15 15:27:07 lpmodule.py:399] repeat qkv cost 0.030190 seconds
DEBUG 10-15 15:27:07 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:07 lpmodule.py:433] dot attn cost 0.032414 seconds
DEBUG 10-15 15:27:07 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:07 lpmodule.py:444] time cost move to cuda:1 0.02572774887084961 s
DEBUG 10-15 15:27:07 lpllm.py:2283] CPU attn cost 0.143708 seconds if batch True
DEBUG 10-15 15:27:07 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:07 lpllm.py:2294] CPU compute cost 0.144588 seconds
DEBUG 10-15 15:27:07 lpllm.py:2312] free cost 0.000084 seconds
DEBUG 10-15 15:27:07 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:07 lpllm.py:1774] update state cost 1.7404556274414062e-05 s
DEBUG 10-15 15:27:07 lpllm.py:1743] restore layer func cost 0.0008034706115722656 s
DEBUG 10-15 15:27:07 lpllm.py:511] restore layer cost 0.001039743423461914 s
DEBUG 10-15 15:27:07 lpllm.py:512] reset_next_layer_need: layer_attn_idx=17, layer_mlp_idx=17, j_loc=34
DEBUG 10-15 15:27:07 lpllm.py:1037] reset layer cost 0.0011098384857177734 s
DEBUG 10-15 15:27:07 lpllm.py:1038] have reset next layer layer_attn 17 layer_mlp 17 
DEBUG 10-15 15:27:07 lpllm.py:924] 
DEBUG 10-15 15:27:07 lpllm.py:924] decoder loop j: 35 cur_layer_idx: 17 layer_attn_idx: 17 layer_mlp_idx: 17
DEBUG 10-15 15:27:07 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:07 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:07 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:07 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:07 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:07 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:07 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:07 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:07 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:07 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:07 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:07 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:07 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:07 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:07 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:07 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:07 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:07 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:07 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:07 lpllm.py:2265] GPU2CPU move cost 0.000329 seconds
DEBUG 10-15 15:27:07 lpmodule.py:340] layer_idx 17 attn layer_idx: 17
DEBUG 10-15 15:27:07 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:07 lpmodule.py:364] decoder_attn_batch update batch_dim 180-240
DEBUG 10-15 15:27:07 StaticCacheLen.py:84] static cache update layer_idx: 17, start_batch: 180, end_batch: 240
DEBUG 10-15 15:27:07 lpmodule.py:368] update for kv cache 180-240 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:07 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:07 lpmodule.py:374] update past key value cost 0.024727 seconds
DEBUG 10-15 15:27:07 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:07 lpmodule.py:227] layer idx 17
DEBUG 10-15 15:27:07 lpmodule.py:399] repeat qkv cost 0.030835 seconds
DEBUG 10-15 15:27:07 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:07 lpmodule.py:433] dot attn cost 0.034322 seconds
DEBUG 10-15 15:27:07 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:07 lpmodule.py:444] time cost move to cuda:1 0.02435612678527832 s
DEBUG 10-15 15:27:07 lpllm.py:2283] CPU attn cost 0.140415 seconds if batch True
DEBUG 10-15 15:27:07 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:07 lpllm.py:2294] CPU compute cost 0.140949 seconds
DEBUG 10-15 15:27:07 lpllm.py:2312] free cost 0.000080 seconds
DEBUG 10-15 15:27:07 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:07 lpllm.py:1774] update state cost 2.1696090698242188e-05 s
DEBUG 10-15 15:27:07 lpllm.py:1743] restore layer func cost 0.000377655029296875 s
DEBUG 10-15 15:27:07 lpllm.py:511] restore layer cost 0.0006358623504638672 s
DEBUG 10-15 15:27:07 lpllm.py:512] reset_next_layer_need: layer_attn_idx=18, layer_mlp_idx=17, j_loc=35
DEBUG 10-15 15:27:07 lpllm.py:1037] reset layer cost 0.0007076263427734375 s
DEBUG 10-15 15:27:07 lpllm.py:1038] have reset next layer layer_attn 18 layer_mlp 17 
DEBUG 10-15 15:27:07 lpllm.py:1044] j: 35 waiting the layer with layer_idx 18 before wait time 0.44597816467285156 s
INFO 10-15 15:27:07 client.py:117] confirm_model_loaded: Mixtral-8x7B, 88144e23-6adf-43eb-9439-afc70586aa3c
INFO 10-15 15:27:07 client.py:125] Model loaded
DEBUG 10-15 15:27:07 lpllm.py:1048] j: load cost 0.44742321968078613 s waiting cost 0.0014295578002929688 s
DEBUG 10-15 15:27:07 lpllm.py:924] 
DEBUG 10-15 15:27:07 lpllm.py:924] decoder loop j: 36 cur_layer_idx: 18 layer_attn_idx: 18 layer_mlp_idx: 17
DEBUG 10-15 15:27:07 lpllm.py:933] start load next layer cur_layer_idx: 19
DEBUG 10-15 15:27:07 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:07 client.py:72] load_into_gpu: Mixtral-8x7B, 64ca66e9-55a1-49c5-81bb-16c62dfd64bf
INFO 10-15 15:27:07 client.py:113] Model loaded: Mixtral-8x7B, 64ca66e9-55a1-49c5-81bb-16c62dfd64bf
DEBUG 10-15 15:27:07 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:07 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:07 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:07 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:07 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:07 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:07 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:07 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:07 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:07 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:07 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:07 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:07 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:07 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:07 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:07 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:07 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:07 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:07 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:07 lpllm.py:2265] GPU2CPU move cost 0.000582 seconds
DEBUG 10-15 15:27:07 lpmodule.py:340] layer_idx 18 attn layer_idx: 18
DEBUG 10-15 15:27:07 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:07 lpmodule.py:364] decoder_attn_batch update batch_dim 120-180
DEBUG 10-15 15:27:07 StaticCacheLen.py:84] static cache update layer_idx: 18, start_batch: 120, end_batch: 180
DEBUG 10-15 15:27:07 lpmodule.py:368] update for kv cache 120-180 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:07 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:07 lpmodule.py:374] update past key value cost 0.027554 seconds
DEBUG 10-15 15:27:07 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:07 lpmodule.py:227] layer idx 17
DEBUG 10-15 15:27:07 lpmodule.py:399] repeat qkv cost 0.032900 seconds
DEBUG 10-15 15:27:07 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:08 lpmodule.py:433] dot attn cost 0.051121 seconds
DEBUG 10-15 15:27:08 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:08 lpmodule.py:444] time cost move to cuda:1 0.027440786361694336 s
DEBUG 10-15 15:27:08 lpllm.py:2283] CPU attn cost 0.165255 seconds if batch True
DEBUG 10-15 15:27:08 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:08 lpllm.py:2294] CPU compute cost 0.166140 seconds
DEBUG 10-15 15:27:08 lpllm.py:2312] free cost 0.000083 seconds
DEBUG 10-15 15:27:08 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:08 lpllm.py:1774] update state cost 3.337860107421875e-05 s
DEBUG 10-15 15:27:08 lpllm.py:1743] restore layer func cost 0.0008261203765869141 s
DEBUG 10-15 15:27:08 lpllm.py:511] restore layer cost 0.0010895729064941406 s
DEBUG 10-15 15:27:08 lpllm.py:512] reset_next_layer_need: layer_attn_idx=18, layer_mlp_idx=18, j_loc=36
DEBUG 10-15 15:27:08 lpllm.py:1037] reset layer cost 0.0011649131774902344 s
DEBUG 10-15 15:27:08 lpllm.py:1038] have reset next layer layer_attn 18 layer_mlp 18 
DEBUG 10-15 15:27:08 lpllm.py:924] 
DEBUG 10-15 15:27:08 lpllm.py:924] decoder loop j: 37 cur_layer_idx: 18 layer_attn_idx: 18 layer_mlp_idx: 18
DEBUG 10-15 15:27:08 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:08 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:08 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:08 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:08 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:08 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:08 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:08 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:08 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:08 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:08 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:08 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:08 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:08 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:08 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:08 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:08 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:08 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:08 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:08 lpllm.py:2265] GPU2CPU move cost 0.000580 seconds
DEBUG 10-15 15:27:08 lpmodule.py:340] layer_idx 18 attn layer_idx: 18
DEBUG 10-15 15:27:08 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:08 lpmodule.py:364] decoder_attn_batch update batch_dim 180-240
DEBUG 10-15 15:27:08 StaticCacheLen.py:84] static cache update layer_idx: 18, start_batch: 180, end_batch: 240
DEBUG 10-15 15:27:08 lpmodule.py:368] update for kv cache 180-240 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:08 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:08 lpmodule.py:374] update past key value cost 0.025270 seconds
DEBUG 10-15 15:27:08 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:08 lpmodule.py:227] layer idx 18
DEBUG 10-15 15:27:08 lpmodule.py:399] repeat qkv cost 0.030920 seconds
DEBUG 10-15 15:27:08 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:08 lpmodule.py:433] dot attn cost 0.052154 seconds
DEBUG 10-15 15:27:08 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:08 lpmodule.py:444] time cost move to cuda:1 0.023764848709106445 s
DEBUG 10-15 15:27:08 lpllm.py:2283] CPU attn cost 0.158637 seconds if batch True
DEBUG 10-15 15:27:08 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:08 lpllm.py:2294] CPU compute cost 0.159496 seconds
DEBUG 10-15 15:27:08 lpllm.py:2312] free cost 0.000082 seconds
DEBUG 10-15 15:27:08 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:08 lpllm.py:1774] update state cost 2.0503997802734375e-05 s
DEBUG 10-15 15:27:08 lpllm.py:1743] restore layer func cost 0.0003650188446044922 s
DEBUG 10-15 15:27:08 lpllm.py:511] restore layer cost 0.0006053447723388672 s
DEBUG 10-15 15:27:08 lpllm.py:512] reset_next_layer_need: layer_attn_idx=19, layer_mlp_idx=18, j_loc=37
DEBUG 10-15 15:27:08 lpllm.py:1037] reset layer cost 0.0006740093231201172 s
DEBUG 10-15 15:27:08 lpllm.py:1038] have reset next layer layer_attn 19 layer_mlp 18 
DEBUG 10-15 15:27:08 lpllm.py:1044] j: 37 waiting the layer with layer_idx 19 before wait time 0.4562554359436035 s
INFO 10-15 15:27:08 client.py:117] confirm_model_loaded: Mixtral-8x7B, 64ca66e9-55a1-49c5-81bb-16c62dfd64bf
INFO 10-15 15:27:08 client.py:125] Model loaded
DEBUG 10-15 15:27:08 lpllm.py:1048] j: load cost 0.457653284072876 s waiting cost 0.001383066177368164 s
DEBUG 10-15 15:27:08 lpllm.py:924] 
DEBUG 10-15 15:27:08 lpllm.py:924] decoder loop j: 38 cur_layer_idx: 19 layer_attn_idx: 19 layer_mlp_idx: 18
DEBUG 10-15 15:27:08 lpllm.py:933] start load next layer cur_layer_idx: 20
DEBUG 10-15 15:27:08 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed650>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x003\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x96\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:08 client.py:72] load_into_gpu: Mixtral-8x7B, 97cc5544-1305-4ba9-bee2-65d1df4e9c34
INFO 10-15 15:27:08 client.py:113] Model loaded: Mixtral-8x7B, 97cc5544-1305-4ba9-bee2-65d1df4e9c34
DEBUG 10-15 15:27:08 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:08 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:08 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:08 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:08 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:08 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:08 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:08 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:08 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:08 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:08 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:08 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:08 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:08 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:08 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:08 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:08 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:08 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:08 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:08 lpllm.py:2265] GPU2CPU move cost 0.000580 seconds
DEBUG 10-15 15:27:08 lpmodule.py:340] layer_idx 19 attn layer_idx: 19
DEBUG 10-15 15:27:08 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:08 lpmodule.py:364] decoder_attn_batch update batch_dim 120-180
DEBUG 10-15 15:27:08 StaticCacheLen.py:84] static cache update layer_idx: 19, start_batch: 120, end_batch: 180
DEBUG 10-15 15:27:08 lpmodule.py:368] update for kv cache 120-180 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:08 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:08 lpmodule.py:374] update past key value cost 0.023750 seconds
DEBUG 10-15 15:27:08 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:08 lpmodule.py:227] layer idx 18
DEBUG 10-15 15:27:08 lpmodule.py:399] repeat qkv cost 0.031118 seconds
DEBUG 10-15 15:27:08 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:08 lpmodule.py:433] dot attn cost 0.042854 seconds
DEBUG 10-15 15:27:08 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:08 lpmodule.py:444] time cost move to cuda:1 0.02652883529663086 s
DEBUG 10-15 15:27:08 lpllm.py:2283] CPU attn cost 0.151280 seconds if batch True
DEBUG 10-15 15:27:08 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:08 lpllm.py:2294] CPU compute cost 0.152149 seconds
DEBUG 10-15 15:27:08 lpllm.py:2312] free cost 0.000082 seconds
DEBUG 10-15 15:27:08 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:08 lpllm.py:1774] update state cost 1.9073486328125e-05 s
DEBUG 10-15 15:27:08 lpllm.py:1743] restore layer func cost 0.0008194446563720703 s
DEBUG 10-15 15:27:08 lpllm.py:511] restore layer cost 0.001065969467163086 s
DEBUG 10-15 15:27:08 lpllm.py:512] reset_next_layer_need: layer_attn_idx=19, layer_mlp_idx=19, j_loc=38
DEBUG 10-15 15:27:08 lpllm.py:1037] reset layer cost 0.0011410713195800781 s
DEBUG 10-15 15:27:08 lpllm.py:1038] have reset next layer layer_attn 19 layer_mlp 19 
DEBUG 10-15 15:27:08 lpllm.py:924] 
DEBUG 10-15 15:27:08 lpllm.py:924] decoder loop j: 39 cur_layer_idx: 19 layer_attn_idx: 19 layer_mlp_idx: 19
DEBUG 10-15 15:27:08 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:08 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:08 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:08 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:08 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:08 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:08 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:08 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:08 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:08 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:08 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:08 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:08 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:08 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:08 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:08 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:08 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:08 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:08 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:08 lpllm.py:2265] GPU2CPU move cost 0.000595 seconds
DEBUG 10-15 15:27:08 lpmodule.py:340] layer_idx 19 attn layer_idx: 19
DEBUG 10-15 15:27:08 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:08 lpmodule.py:364] decoder_attn_batch update batch_dim 180-240
DEBUG 10-15 15:27:08 StaticCacheLen.py:84] static cache update layer_idx: 19, start_batch: 180, end_batch: 240
DEBUG 10-15 15:27:08 lpmodule.py:368] update for kv cache 180-240 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:08 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:08 lpmodule.py:374] update past key value cost 0.025389 seconds
DEBUG 10-15 15:27:08 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:08 lpmodule.py:227] layer idx 19
DEBUG 10-15 15:27:08 lpmodule.py:399] repeat qkv cost 0.031902 seconds
DEBUG 10-15 15:27:08 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:08 lpmodule.py:433] dot attn cost 0.041207 seconds
DEBUG 10-15 15:27:08 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:08 lpmodule.py:444] time cost move to cuda:1 0.02382040023803711 s
DEBUG 10-15 15:27:08 lpllm.py:2283] CPU attn cost 0.149400 seconds if batch True
DEBUG 10-15 15:27:08 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:08 lpllm.py:2294] CPU compute cost 0.150272 seconds
DEBUG 10-15 15:27:08 lpllm.py:2312] free cost 0.000081 seconds
DEBUG 10-15 15:27:08 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:08 lpllm.py:1774] update state cost 3.504753112792969e-05 s
DEBUG 10-15 15:27:08 lpllm.py:1743] restore layer func cost 0.00037550926208496094 s
DEBUG 10-15 15:27:08 lpllm.py:511] restore layer cost 0.0006434917449951172 s
DEBUG 10-15 15:27:08 lpllm.py:512] reset_next_layer_need: layer_attn_idx=20, layer_mlp_idx=19, j_loc=39
DEBUG 10-15 15:27:08 lpllm.py:1037] reset layer cost 0.0007112026214599609 s
DEBUG 10-15 15:27:08 lpllm.py:1038] have reset next layer layer_attn 20 layer_mlp 19 
DEBUG 10-15 15:27:08 lpllm.py:1044] j: 39 waiting the layer with layer_idx 20 before wait time 0.4463329315185547 s
INFO 10-15 15:27:08 client.py:117] confirm_model_loaded: Mixtral-8x7B, 97cc5544-1305-4ba9-bee2-65d1df4e9c34
INFO 10-15 15:27:08 client.py:125] Model loaded
DEBUG 10-15 15:27:08 lpllm.py:1048] j: load cost 0.44770026206970215 s waiting cost 0.0013523101806640625 s
DEBUG 10-15 15:27:08 lpllm.py:924] 
DEBUG 10-15 15:27:08 lpllm.py:924] decoder loop j: 40 cur_layer_idx: 20 layer_attn_idx: 20 layer_mlp_idx: 19
DEBUG 10-15 15:27:08 lpllm.py:933] start load next layer cur_layer_idx: 21
DEBUG 10-15 15:27:08 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed6e0>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x004\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x98\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:08 client.py:72] load_into_gpu: Mixtral-8x7B, 989634d1-d180-4e9f-9339-53dea41322e3
INFO 10-15 15:27:08 client.py:113] Model loaded: Mixtral-8x7B, 989634d1-d180-4e9f-9339-53dea41322e3
DEBUG 10-15 15:27:08 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:08 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:08 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:08 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:08 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:08 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:08 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:08 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:08 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:08 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:08 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:08 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:08 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:08 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:08 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:08 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:08 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:08 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:08 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:08 lpllm.py:2265] GPU2CPU move cost 0.000573 seconds
DEBUG 10-15 15:27:08 lpmodule.py:340] layer_idx 20 attn layer_idx: 20
DEBUG 10-15 15:27:08 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:08 lpmodule.py:364] decoder_attn_batch update batch_dim 120-180
DEBUG 10-15 15:27:08 StaticCacheLen.py:84] static cache update layer_idx: 20, start_batch: 120, end_batch: 180
DEBUG 10-15 15:27:08 lpmodule.py:368] update for kv cache 120-180 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:08 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:08 lpmodule.py:374] update past key value cost 0.022600 seconds
DEBUG 10-15 15:27:08 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:08 lpmodule.py:227] layer idx 19
DEBUG 10-15 15:27:08 lpmodule.py:399] repeat qkv cost 0.032003 seconds
DEBUG 10-15 15:27:08 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:08 lpmodule.py:433] dot attn cost 0.035912 seconds
DEBUG 10-15 15:27:08 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:08 lpmodule.py:444] time cost move to cuda:1 0.023792743682861328 s
DEBUG 10-15 15:27:08 lpllm.py:2283] CPU attn cost 0.141368 seconds if batch True
DEBUG 10-15 15:27:08 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:08 lpllm.py:2294] CPU compute cost 0.142222 seconds
DEBUG 10-15 15:27:08 lpllm.py:2312] free cost 0.000083 seconds
DEBUG 10-15 15:27:09 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:09 lpllm.py:1774] update state cost 3.9577484130859375e-05 s
DEBUG 10-15 15:27:09 lpllm.py:1743] restore layer func cost 0.0011649131774902344 s
DEBUG 10-15 15:27:09 lpllm.py:511] restore layer cost 0.0014824867248535156 s
DEBUG 10-15 15:27:09 lpllm.py:512] reset_next_layer_need: layer_attn_idx=20, layer_mlp_idx=20, j_loc=40
DEBUG 10-15 15:27:09 lpllm.py:1037] reset layer cost 0.0015728473663330078 s
DEBUG 10-15 15:27:09 lpllm.py:1038] have reset next layer layer_attn 20 layer_mlp 20 
DEBUG 10-15 15:27:09 lpllm.py:924] 
DEBUG 10-15 15:27:09 lpllm.py:924] decoder loop j: 41 cur_layer_idx: 20 layer_attn_idx: 20 layer_mlp_idx: 20
DEBUG 10-15 15:27:09 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:09 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:09 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:09 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:09 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:09 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:09 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:09 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:09 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:09 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:09 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:09 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:09 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:09 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:09 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:09 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:09 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:09 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:09 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:09 lpllm.py:2265] GPU2CPU move cost 0.000574 seconds
DEBUG 10-15 15:27:09 lpmodule.py:340] layer_idx 20 attn layer_idx: 20
DEBUG 10-15 15:27:09 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:09 lpmodule.py:364] decoder_attn_batch update batch_dim 180-240
DEBUG 10-15 15:27:09 StaticCacheLen.py:84] static cache update layer_idx: 20, start_batch: 180, end_batch: 240
DEBUG 10-15 15:27:09 lpmodule.py:368] update for kv cache 180-240 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:09 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:09 lpmodule.py:374] update past key value cost 0.025363 seconds
DEBUG 10-15 15:27:09 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:09 lpmodule.py:227] layer idx 20
DEBUG 10-15 15:27:09 lpmodule.py:399] repeat qkv cost 0.030268 seconds
DEBUG 10-15 15:27:09 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:09 lpmodule.py:433] dot attn cost 0.036963 seconds
DEBUG 10-15 15:27:09 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:09 lpmodule.py:444] time cost move to cuda:1 0.024321317672729492 s
DEBUG 10-15 15:27:09 lpllm.py:2283] CPU attn cost 0.144145 seconds if batch True
DEBUG 10-15 15:27:09 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:09 lpllm.py:2294] CPU compute cost 0.145000 seconds
DEBUG 10-15 15:27:09 lpllm.py:2312] free cost 0.000082 seconds
DEBUG 10-15 15:27:09 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:09 lpllm.py:1774] update state cost 2.1457672119140625e-05 s
DEBUG 10-15 15:27:09 lpllm.py:1743] restore layer func cost 0.00036525726318359375 s
DEBUG 10-15 15:27:09 lpllm.py:511] restore layer cost 0.0006177425384521484 s
DEBUG 10-15 15:27:09 lpllm.py:512] reset_next_layer_need: layer_attn_idx=21, layer_mlp_idx=20, j_loc=41
DEBUG 10-15 15:27:09 lpllm.py:1037] reset layer cost 0.0006840229034423828 s
DEBUG 10-15 15:27:09 lpllm.py:1038] have reset next layer layer_attn 21 layer_mlp 20 
DEBUG 10-15 15:27:09 lpllm.py:1044] j: 41 waiting the layer with layer_idx 21 before wait time 0.43892955780029297 s
INFO 10-15 15:27:09 client.py:117] confirm_model_loaded: Mixtral-8x7B, 989634d1-d180-4e9f-9339-53dea41322e3
INFO 10-15 15:27:09 client.py:125] Model loaded
DEBUG 10-15 15:27:09 lpllm.py:1048] j: load cost 0.44040966033935547 s waiting cost 0.0014662742614746094 s
DEBUG 10-15 15:27:09 lpllm.py:924] 
DEBUG 10-15 15:27:09 lpllm.py:924] decoder loop j: 42 cur_layer_idx: 21 layer_attn_idx: 21 layer_mlp_idx: 20
DEBUG 10-15 15:27:09 lpllm.py:933] start load next layer cur_layer_idx: 22
DEBUG 10-15 15:27:09 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:09 client.py:72] load_into_gpu: Mixtral-8x7B, 6385fbe1-4527-46da-972f-34225fb4e078
INFO 10-15 15:27:09 client.py:113] Model loaded: Mixtral-8x7B, 6385fbe1-4527-46da-972f-34225fb4e078
DEBUG 10-15 15:27:09 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:09 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:09 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:09 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:09 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:09 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:09 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:09 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:09 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:09 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:09 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:09 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:09 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:09 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:09 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:09 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:09 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:09 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:09 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:09 lpllm.py:2265] GPU2CPU move cost 0.000576 seconds
DEBUG 10-15 15:27:09 lpmodule.py:340] layer_idx 21 attn layer_idx: 21
DEBUG 10-15 15:27:09 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:09 lpmodule.py:364] decoder_attn_batch update batch_dim 120-180
DEBUG 10-15 15:27:09 StaticCacheLen.py:84] static cache update layer_idx: 21, start_batch: 120, end_batch: 180
DEBUG 10-15 15:27:09 lpmodule.py:368] update for kv cache 120-180 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:09 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:09 lpmodule.py:374] update past key value cost 0.030046 seconds
DEBUG 10-15 15:27:09 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:09 lpmodule.py:227] layer idx 20
DEBUG 10-15 15:27:09 lpmodule.py:399] repeat qkv cost 0.030723 seconds
DEBUG 10-15 15:27:09 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:09 lpmodule.py:433] dot attn cost 0.034976 seconds
DEBUG 10-15 15:27:09 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:09 lpmodule.py:444] time cost move to cuda:1 0.027181148529052734 s
DEBUG 10-15 15:27:09 lpllm.py:2283] CPU attn cost 0.151049 seconds if batch True
DEBUG 10-15 15:27:09 lpllm.py:2292] deal attn result cost 0.000003 seconds
DEBUG 10-15 15:27:09 lpllm.py:2294] CPU compute cost 0.151848 seconds
DEBUG 10-15 15:27:09 lpllm.py:2312] free cost 0.000083 seconds
DEBUG 10-15 15:27:09 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:09 lpllm.py:1774] update state cost 1.9073486328125e-05 s
DEBUG 10-15 15:27:09 lpllm.py:1743] restore layer func cost 0.0007910728454589844 s
DEBUG 10-15 15:27:09 lpllm.py:511] restore layer cost 0.0010371208190917969 s
DEBUG 10-15 15:27:09 lpllm.py:512] reset_next_layer_need: layer_attn_idx=21, layer_mlp_idx=21, j_loc=42
DEBUG 10-15 15:27:09 lpllm.py:1037] reset layer cost 0.0011093616485595703 s
DEBUG 10-15 15:27:09 lpllm.py:1038] have reset next layer layer_attn 21 layer_mlp 21 
DEBUG 10-15 15:27:09 lpllm.py:924] 
DEBUG 10-15 15:27:09 lpllm.py:924] decoder loop j: 43 cur_layer_idx: 21 layer_attn_idx: 21 layer_mlp_idx: 21
DEBUG 10-15 15:27:09 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:09 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:09 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:09 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:09 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:09 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:09 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:09 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:09 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:09 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:09 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:09 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:09 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:09 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:09 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:09 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:09 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:09 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:09 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:09 lpllm.py:2265] GPU2CPU move cost 0.000576 seconds
DEBUG 10-15 15:27:09 lpmodule.py:340] layer_idx 21 attn layer_idx: 21
DEBUG 10-15 15:27:09 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:09 lpmodule.py:364] decoder_attn_batch update batch_dim 180-240
DEBUG 10-15 15:27:09 StaticCacheLen.py:84] static cache update layer_idx: 21, start_batch: 180, end_batch: 240
DEBUG 10-15 15:27:09 lpmodule.py:368] update for kv cache 180-240 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:09 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:09 lpmodule.py:374] update past key value cost 0.026293 seconds
DEBUG 10-15 15:27:09 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:09 lpmodule.py:227] layer idx 21
DEBUG 10-15 15:27:09 lpmodule.py:399] repeat qkv cost 0.030249 seconds
DEBUG 10-15 15:27:09 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:09 lpmodule.py:433] dot attn cost 0.034585 seconds
DEBUG 10-15 15:27:09 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:09 lpmodule.py:444] time cost move to cuda:1 0.02357959747314453 s
DEBUG 10-15 15:27:09 lpllm.py:2283] CPU attn cost 0.142933 seconds if batch True
DEBUG 10-15 15:27:09 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:09 lpllm.py:2294] CPU compute cost 0.143790 seconds
DEBUG 10-15 15:27:09 lpllm.py:2312] free cost 0.000080 seconds
DEBUG 10-15 15:27:09 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:09 lpllm.py:1774] update state cost 2.1219253540039062e-05 s
DEBUG 10-15 15:27:09 lpllm.py:1743] restore layer func cost 0.00038313865661621094 s
DEBUG 10-15 15:27:09 lpllm.py:511] restore layer cost 0.0006606578826904297 s
DEBUG 10-15 15:27:09 lpllm.py:512] reset_next_layer_need: layer_attn_idx=22, layer_mlp_idx=21, j_loc=43
DEBUG 10-15 15:27:09 lpllm.py:1037] reset layer cost 0.0007295608520507812 s
DEBUG 10-15 15:27:09 lpllm.py:1038] have reset next layer layer_attn 22 layer_mlp 21 
DEBUG 10-15 15:27:09 lpllm.py:1044] j: 43 waiting the layer with layer_idx 22 before wait time 0.4532489776611328 s
INFO 10-15 15:27:09 client.py:117] confirm_model_loaded: Mixtral-8x7B, 6385fbe1-4527-46da-972f-34225fb4e078
INFO 10-15 15:27:09 client.py:125] Model loaded
DEBUG 10-15 15:27:09 lpllm.py:1048] j: load cost 0.454681396484375 s waiting cost 0.0014176368713378906 s
DEBUG 10-15 15:27:09 lpllm.py:924] 
DEBUG 10-15 15:27:09 lpllm.py:924] decoder loop j: 44 cur_layer_idx: 22 layer_attn_idx: 22 layer_mlp_idx: 21
DEBUG 10-15 15:27:09 lpllm.py:933] start load next layer cur_layer_idx: 23
DEBUG 10-15 15:27:09 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed650>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x003\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x96\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:09 client.py:72] load_into_gpu: Mixtral-8x7B, fd21c3b1-c7c2-4769-93f4-23434ba1a974
INFO 10-15 15:27:09 client.py:113] Model loaded: Mixtral-8x7B, fd21c3b1-c7c2-4769-93f4-23434ba1a974
DEBUG 10-15 15:27:09 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:09 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:09 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:09 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:09 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:09 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:09 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:09 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:09 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:09 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:09 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:09 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:09 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:09 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:09 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:09 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:09 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:09 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:09 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:09 lpllm.py:2265] GPU2CPU move cost 0.000582 seconds
DEBUG 10-15 15:27:09 lpmodule.py:340] layer_idx 22 attn layer_idx: 22
DEBUG 10-15 15:27:09 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:09 lpmodule.py:364] decoder_attn_batch update batch_dim 120-180
DEBUG 10-15 15:27:09 StaticCacheLen.py:84] static cache update layer_idx: 22, start_batch: 120, end_batch: 180
DEBUG 10-15 15:27:09 lpmodule.py:368] update for kv cache 120-180 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:09 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:09 lpmodule.py:374] update past key value cost 0.024503 seconds
DEBUG 10-15 15:27:09 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:09 lpmodule.py:227] layer idx 21
DEBUG 10-15 15:27:09 lpmodule.py:399] repeat qkv cost 0.030485 seconds
DEBUG 10-15 15:27:09 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:09 lpmodule.py:433] dot attn cost 0.033751 seconds
DEBUG 10-15 15:27:09 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:09 lpmodule.py:444] time cost move to cuda:1 0.02597951889038086 s
DEBUG 10-15 15:27:09 lpllm.py:2283] CPU attn cost 0.140007 seconds if batch True
DEBUG 10-15 15:27:09 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:09 lpllm.py:2294] CPU compute cost 0.140868 seconds
DEBUG 10-15 15:27:09 lpllm.py:2312] free cost 0.000084 seconds
DEBUG 10-15 15:27:09 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:09 lpllm.py:1774] update state cost 1.811981201171875e-05 s
DEBUG 10-15 15:27:09 lpllm.py:1743] restore layer func cost 0.003912448883056641 s
DEBUG 10-15 15:27:09 lpllm.py:511] restore layer cost 0.004170656204223633 s
DEBUG 10-15 15:27:09 lpllm.py:512] reset_next_layer_need: layer_attn_idx=22, layer_mlp_idx=22, j_loc=44
DEBUG 10-15 15:27:09 lpllm.py:1037] reset layer cost 0.004242897033691406 s
DEBUG 10-15 15:27:09 lpllm.py:1038] have reset next layer layer_attn 22 layer_mlp 22 
DEBUG 10-15 15:27:09 lpllm.py:924] 
DEBUG 10-15 15:27:09 lpllm.py:924] decoder loop j: 45 cur_layer_idx: 22 layer_attn_idx: 22 layer_mlp_idx: 22
DEBUG 10-15 15:27:09 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:09 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:09 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:09 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:09 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:09 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:09 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:09 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:09 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:09 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:09 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:09 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:09 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:09 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:09 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:09 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:09 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:09 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:09 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:09 lpllm.py:2265] GPU2CPU move cost 0.000582 seconds
DEBUG 10-15 15:27:09 lpmodule.py:340] layer_idx 22 attn layer_idx: 22
DEBUG 10-15 15:27:09 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:09 lpmodule.py:364] decoder_attn_batch update batch_dim 180-240
DEBUG 10-15 15:27:09 StaticCacheLen.py:84] static cache update layer_idx: 22, start_batch: 180, end_batch: 240
DEBUG 10-15 15:27:09 lpmodule.py:368] update for kv cache 180-240 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:09 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:09 lpmodule.py:374] update past key value cost 0.024334 seconds
DEBUG 10-15 15:27:09 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:09 lpmodule.py:227] layer idx 22
DEBUG 10-15 15:27:10 lpmodule.py:399] repeat qkv cost 0.033258 seconds
DEBUG 10-15 15:27:10 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:10 lpmodule.py:433] dot attn cost 0.038364 seconds
DEBUG 10-15 15:27:10 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:10 lpmodule.py:444] time cost move to cuda:1 0.024212360382080078 s
DEBUG 10-15 15:27:10 lpllm.py:2283] CPU attn cost 0.146570 seconds if batch True
DEBUG 10-15 15:27:10 lpllm.py:2292] deal attn result cost 0.000003 seconds
DEBUG 10-15 15:27:10 lpllm.py:2294] CPU compute cost 0.147468 seconds
DEBUG 10-15 15:27:10 lpllm.py:2312] free cost 0.000085 seconds
DEBUG 10-15 15:27:10 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:10 lpllm.py:1774] update state cost 3.600120544433594e-05 s
DEBUG 10-15 15:27:10 lpllm.py:1743] restore layer func cost 0.0003883838653564453 s
DEBUG 10-15 15:27:10 lpllm.py:511] restore layer cost 0.0006611347198486328 s
DEBUG 10-15 15:27:10 lpllm.py:512] reset_next_layer_need: layer_attn_idx=23, layer_mlp_idx=22, j_loc=45
DEBUG 10-15 15:27:10 lpllm.py:1037] reset layer cost 0.00074005126953125 s
DEBUG 10-15 15:27:10 lpllm.py:1038] have reset next layer layer_attn 23 layer_mlp 22 
DEBUG 10-15 15:27:10 lpllm.py:1044] j: 45 waiting the layer with layer_idx 23 before wait time 0.45488429069519043 s
INFO 10-15 15:27:10 client.py:117] confirm_model_loaded: Mixtral-8x7B, fd21c3b1-c7c2-4769-93f4-23434ba1a974
INFO 10-15 15:27:10 client.py:125] Model loaded
DEBUG 10-15 15:27:10 lpllm.py:1048] j: load cost 0.4564015865325928 s waiting cost 0.001501321792602539 s
DEBUG 10-15 15:27:10 lpllm.py:924] 
DEBUG 10-15 15:27:10 lpllm.py:924] decoder loop j: 46 cur_layer_idx: 23 layer_attn_idx: 23 layer_mlp_idx: 22
DEBUG 10-15 15:27:10 lpllm.py:933] start load next layer cur_layer_idx: 24
DEBUG 10-15 15:27:10 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed6e0>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x004\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x98\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:10 client.py:72] load_into_gpu: Mixtral-8x7B, 7f71fc52-a7f0-4c90-b574-4dd38243c3d2
INFO 10-15 15:27:10 client.py:113] Model loaded: Mixtral-8x7B, 7f71fc52-a7f0-4c90-b574-4dd38243c3d2
DEBUG 10-15 15:27:10 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:10 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:10 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:10 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:10 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:10 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:10 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:10 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:10 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:10 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:10 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:10 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:10 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:10 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:10 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:10 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:10 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:10 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:10 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:10 lpllm.py:2265] GPU2CPU move cost 0.000728 seconds
DEBUG 10-15 15:27:10 lpmodule.py:340] layer_idx 23 attn layer_idx: 23
DEBUG 10-15 15:27:10 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:10 lpmodule.py:364] decoder_attn_batch update batch_dim 120-180
DEBUG 10-15 15:27:10 StaticCacheLen.py:84] static cache update layer_idx: 23, start_batch: 120, end_batch: 180
DEBUG 10-15 15:27:10 lpmodule.py:368] update for kv cache 120-180 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:10 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:10 lpmodule.py:374] update past key value cost 0.025079 seconds
DEBUG 10-15 15:27:10 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:10 lpmodule.py:227] layer idx 22
DEBUG 10-15 15:27:10 lpmodule.py:399] repeat qkv cost 0.031495 seconds
DEBUG 10-15 15:27:10 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:10 lpmodule.py:433] dot attn cost 0.034554 seconds
DEBUG 10-15 15:27:10 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:10 lpmodule.py:444] time cost move to cuda:1 0.023993492126464844 s
DEBUG 10-15 15:27:10 lpllm.py:2283] CPU attn cost 0.140866 seconds if batch True
DEBUG 10-15 15:27:10 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:10 lpllm.py:2294] CPU compute cost 0.141917 seconds
DEBUG 10-15 15:27:10 lpllm.py:2312] free cost 0.000084 seconds
DEBUG 10-15 15:27:10 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:10 lpllm.py:1774] update state cost 1.811981201171875e-05 s
DEBUG 10-15 15:27:10 lpllm.py:1743] restore layer func cost 0.0008037090301513672 s
DEBUG 10-15 15:27:10 lpllm.py:511] restore layer cost 0.0010519027709960938 s
DEBUG 10-15 15:27:10 lpllm.py:512] reset_next_layer_need: layer_attn_idx=23, layer_mlp_idx=23, j_loc=46
DEBUG 10-15 15:27:10 lpllm.py:1037] reset layer cost 0.0011229515075683594 s
DEBUG 10-15 15:27:10 lpllm.py:1038] have reset next layer layer_attn 23 layer_mlp 23 
DEBUG 10-15 15:27:10 lpllm.py:924] 
DEBUG 10-15 15:27:10 lpllm.py:924] decoder loop j: 47 cur_layer_idx: 23 layer_attn_idx: 23 layer_mlp_idx: 23
DEBUG 10-15 15:27:10 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:10 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:10 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:10 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:10 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:10 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:10 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:10 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:10 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:10 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:10 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:10 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:10 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:10 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:10 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:10 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:10 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:10 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:10 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:10 lpllm.py:2265] GPU2CPU move cost 0.000573 seconds
DEBUG 10-15 15:27:10 lpmodule.py:340] layer_idx 23 attn layer_idx: 23
DEBUG 10-15 15:27:10 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:10 lpmodule.py:364] decoder_attn_batch update batch_dim 180-240
DEBUG 10-15 15:27:10 StaticCacheLen.py:84] static cache update layer_idx: 23, start_batch: 180, end_batch: 240
DEBUG 10-15 15:27:10 lpmodule.py:368] update for kv cache 180-240 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:10 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:10 lpmodule.py:374] update past key value cost 0.023730 seconds
DEBUG 10-15 15:27:10 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:10 lpmodule.py:227] layer idx 23
DEBUG 10-15 15:27:10 lpmodule.py:399] repeat qkv cost 0.032314 seconds
DEBUG 10-15 15:27:10 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:10 lpmodule.py:433] dot attn cost 0.044272 seconds
DEBUG 10-15 15:27:10 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:10 lpmodule.py:444] time cost move to cuda:1 0.02463507652282715 s
DEBUG 10-15 15:27:10 lpllm.py:2283] CPU attn cost 0.151451 seconds if batch True
DEBUG 10-15 15:27:10 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:10 lpllm.py:2294] CPU compute cost 0.152325 seconds
DEBUG 10-15 15:27:10 lpllm.py:2312] free cost 0.000082 seconds
DEBUG 10-15 15:27:10 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:10 lpllm.py:1774] update state cost 2.0265579223632812e-05 s
DEBUG 10-15 15:27:10 lpllm.py:1743] restore layer func cost 0.0003819465637207031 s
DEBUG 10-15 15:27:10 lpllm.py:511] restore layer cost 0.0006196498870849609 s
DEBUG 10-15 15:27:10 lpllm.py:512] reset_next_layer_need: layer_attn_idx=24, layer_mlp_idx=23, j_loc=47
DEBUG 10-15 15:27:10 lpllm.py:1037] reset layer cost 0.000698089599609375 s
DEBUG 10-15 15:27:10 lpllm.py:1038] have reset next layer layer_attn 24 layer_mlp 23 
DEBUG 10-15 15:27:10 lpllm.py:1044] j: 47 waiting the layer with layer_idx 24 before wait time 0.5910091400146484 s
INFO 10-15 15:27:10 client.py:117] confirm_model_loaded: Mixtral-8x7B, 7f71fc52-a7f0-4c90-b574-4dd38243c3d2
INFO 10-15 15:27:10 client.py:125] Model loaded
DEBUG 10-15 15:27:10 lpllm.py:1048] j: load cost 0.5924491882324219 s waiting cost 0.001424551010131836 s
DEBUG 10-15 15:27:10 lpllm.py:924] 
DEBUG 10-15 15:27:10 lpllm.py:924] decoder loop j: 48 cur_layer_idx: 24 layer_attn_idx: 24 layer_mlp_idx: 23
DEBUG 10-15 15:27:10 lpllm.py:933] start load next layer cur_layer_idx: 25
DEBUG 10-15 15:27:10 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:10 client.py:72] load_into_gpu: Mixtral-8x7B, 6b9295ae-ac71-4c5b-a9e9-7f0bdbf50c08
INFO 10-15 15:27:10 client.py:113] Model loaded: Mixtral-8x7B, 6b9295ae-ac71-4c5b-a9e9-7f0bdbf50c08
DEBUG 10-15 15:27:10 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:10 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:10 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:10 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:10 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:10 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:10 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:10 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:10 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:10 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:10 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:10 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:10 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:10 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:10 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:10 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:10 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:10 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:10 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:10 lpllm.py:2265] GPU2CPU move cost 0.000584 seconds
DEBUG 10-15 15:27:10 lpmodule.py:340] layer_idx 24 attn layer_idx: 24
DEBUG 10-15 15:27:10 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:10 lpmodule.py:364] decoder_attn_batch update batch_dim 120-180
DEBUG 10-15 15:27:10 StaticCacheLen.py:84] static cache update layer_idx: 24, start_batch: 120, end_batch: 180
DEBUG 10-15 15:27:10 lpmodule.py:368] update for kv cache 120-180 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:10 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:10 lpmodule.py:374] update past key value cost 0.024923 seconds
DEBUG 10-15 15:27:10 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:10 lpmodule.py:227] layer idx 23
DEBUG 10-15 15:27:10 lpmodule.py:399] repeat qkv cost 0.032168 seconds
DEBUG 10-15 15:27:10 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:10 lpmodule.py:433] dot attn cost 0.039522 seconds
DEBUG 10-15 15:27:10 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:10 lpmodule.py:444] time cost move to cuda:1 0.024960994720458984 s
DEBUG 10-15 15:27:10 lpllm.py:2283] CPU attn cost 0.147179 seconds if batch True
DEBUG 10-15 15:27:10 lpllm.py:2292] deal attn result cost 0.000003 seconds
DEBUG 10-15 15:27:10 lpllm.py:2294] CPU compute cost 0.148050 seconds
DEBUG 10-15 15:27:10 lpllm.py:2312] free cost 0.000092 seconds
DEBUG 10-15 15:27:10 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:10 lpllm.py:1774] update state cost 3.409385681152344e-05 s
DEBUG 10-15 15:27:10 lpllm.py:1743] restore layer func cost 0.0008046627044677734 s
DEBUG 10-15 15:27:10 lpllm.py:511] restore layer cost 0.0010609626770019531 s
DEBUG 10-15 15:27:10 lpllm.py:512] reset_next_layer_need: layer_attn_idx=24, layer_mlp_idx=24, j_loc=48
DEBUG 10-15 15:27:10 lpllm.py:1037] reset layer cost 0.0011322498321533203 s
DEBUG 10-15 15:27:10 lpllm.py:1038] have reset next layer layer_attn 24 layer_mlp 24 
DEBUG 10-15 15:27:10 lpllm.py:924] 
DEBUG 10-15 15:27:10 lpllm.py:924] decoder loop j: 49 cur_layer_idx: 24 layer_attn_idx: 24 layer_mlp_idx: 24
DEBUG 10-15 15:27:10 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:10 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:10 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:10 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:10 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:10 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:10 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:10 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:10 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:10 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:10 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:10 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:10 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:10 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:10 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:10 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:10 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:10 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:10 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:10 lpllm.py:2265] GPU2CPU move cost 0.000322 seconds
DEBUG 10-15 15:27:10 lpmodule.py:340] layer_idx 24 attn layer_idx: 24
DEBUG 10-15 15:27:10 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:10 lpmodule.py:364] decoder_attn_batch update batch_dim 180-240
DEBUG 10-15 15:27:11 StaticCacheLen.py:84] static cache update layer_idx: 24, start_batch: 180, end_batch: 240
DEBUG 10-15 15:27:11 lpmodule.py:368] update for kv cache 180-240 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:11 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:11 lpmodule.py:374] update past key value cost 0.025133 seconds
DEBUG 10-15 15:27:11 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:11 lpmodule.py:227] layer idx 24
DEBUG 10-15 15:27:11 lpmodule.py:399] repeat qkv cost 0.030763 seconds
DEBUG 10-15 15:27:11 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:11 lpmodule.py:433] dot attn cost 0.041753 seconds
DEBUG 10-15 15:27:11 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:11 lpmodule.py:444] time cost move to cuda:1 0.028140783309936523 s
DEBUG 10-15 15:27:11 lpllm.py:2283] CPU attn cost 0.152150 seconds if batch True
DEBUG 10-15 15:27:11 lpllm.py:2292] deal attn result cost 0.000003 seconds
DEBUG 10-15 15:27:11 lpllm.py:2294] CPU compute cost 0.152724 seconds
DEBUG 10-15 15:27:11 lpllm.py:2312] free cost 0.000089 seconds
DEBUG 10-15 15:27:11 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:11 lpllm.py:1774] update state cost 3.6716461181640625e-05 s
DEBUG 10-15 15:27:11 lpllm.py:1743] restore layer func cost 0.00038123130798339844 s
DEBUG 10-15 15:27:11 lpllm.py:511] restore layer cost 0.0006635189056396484 s
DEBUG 10-15 15:27:11 lpllm.py:512] reset_next_layer_need: layer_attn_idx=25, layer_mlp_idx=24, j_loc=49
DEBUG 10-15 15:27:11 lpllm.py:1037] reset layer cost 0.0007328987121582031 s
DEBUG 10-15 15:27:11 lpllm.py:1038] have reset next layer layer_attn 25 layer_mlp 24 
DEBUG 10-15 15:27:11 lpllm.py:1044] j: 49 waiting the layer with layer_idx 25 before wait time 0.44571828842163086 s
INFO 10-15 15:27:11 client.py:117] confirm_model_loaded: Mixtral-8x7B, 6b9295ae-ac71-4c5b-a9e9-7f0bdbf50c08
INFO 10-15 15:27:11 client.py:125] Model loaded
DEBUG 10-15 15:27:11 lpllm.py:1048] j: load cost 0.44703197479248047 s waiting cost 0.0012989044189453125 s
DEBUG 10-15 15:27:11 lpllm.py:924] 
DEBUG 10-15 15:27:11 lpllm.py:924] decoder loop j: 50 cur_layer_idx: 25 layer_attn_idx: 25 layer_mlp_idx: 24
DEBUG 10-15 15:27:11 lpllm.py:933] start load next layer cur_layer_idx: 26
DEBUG 10-15 15:27:11 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed650>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x003\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x96\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:11 client.py:72] load_into_gpu: Mixtral-8x7B, d27a63e1-cde4-4445-a514-b6c0172b47ee
INFO 10-15 15:27:11 client.py:113] Model loaded: Mixtral-8x7B, d27a63e1-cde4-4445-a514-b6c0172b47ee
DEBUG 10-15 15:27:11 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:11 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:11 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:11 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:11 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:11 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:11 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:11 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:11 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:11 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:11 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:11 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:11 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:11 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:11 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:11 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:11 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:11 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:11 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:11 lpllm.py:2265] GPU2CPU move cost 0.000482 seconds
DEBUG 10-15 15:27:11 lpmodule.py:340] layer_idx 25 attn layer_idx: 25
DEBUG 10-15 15:27:11 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:11 lpmodule.py:364] decoder_attn_batch update batch_dim 120-180
DEBUG 10-15 15:27:11 StaticCacheLen.py:84] static cache update layer_idx: 25, start_batch: 120, end_batch: 180
DEBUG 10-15 15:27:11 lpmodule.py:368] update for kv cache 120-180 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:11 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:11 lpmodule.py:374] update past key value cost 0.023261 seconds
DEBUG 10-15 15:27:11 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:11 lpmodule.py:227] layer idx 24
DEBUG 10-15 15:27:11 lpmodule.py:399] repeat qkv cost 0.030760 seconds
DEBUG 10-15 15:27:11 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:11 lpmodule.py:433] dot attn cost 0.036594 seconds
DEBUG 10-15 15:27:11 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:11 lpmodule.py:444] time cost move to cuda:1 0.024392366409301758 s
DEBUG 10-15 15:27:11 lpllm.py:2283] CPU attn cost 0.140643 seconds if batch True
DEBUG 10-15 15:27:11 lpllm.py:2292] deal attn result cost 0.000003 seconds
DEBUG 10-15 15:27:11 lpllm.py:2294] CPU compute cost 0.141398 seconds
DEBUG 10-15 15:27:11 lpllm.py:2312] free cost 0.000083 seconds
DEBUG 10-15 15:27:11 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:11 lpllm.py:1774] update state cost 2.47955322265625e-05 s
DEBUG 10-15 15:27:11 lpllm.py:1743] restore layer func cost 0.0011796951293945312 s
DEBUG 10-15 15:27:11 lpllm.py:511] restore layer cost 0.0014772415161132812 s
DEBUG 10-15 15:27:11 lpllm.py:512] reset_next_layer_need: layer_attn_idx=25, layer_mlp_idx=25, j_loc=50
DEBUG 10-15 15:27:11 lpllm.py:1037] reset layer cost 0.0016019344329833984 s
DEBUG 10-15 15:27:11 lpllm.py:1038] have reset next layer layer_attn 25 layer_mlp 25 
DEBUG 10-15 15:27:11 lpllm.py:924] 
DEBUG 10-15 15:27:11 lpllm.py:924] decoder loop j: 51 cur_layer_idx: 25 layer_attn_idx: 25 layer_mlp_idx: 25
DEBUG 10-15 15:27:11 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:11 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:11 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:11 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:11 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:11 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:11 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:11 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:11 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:11 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:11 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:11 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:11 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:11 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:11 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:11 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:11 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:11 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:11 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:11 lpllm.py:2265] GPU2CPU move cost 0.000549 seconds
DEBUG 10-15 15:27:11 lpmodule.py:340] layer_idx 25 attn layer_idx: 25
DEBUG 10-15 15:27:11 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:11 lpmodule.py:364] decoder_attn_batch update batch_dim 180-240
DEBUG 10-15 15:27:11 StaticCacheLen.py:84] static cache update layer_idx: 25, start_batch: 180, end_batch: 240
DEBUG 10-15 15:27:11 lpmodule.py:368] update for kv cache 180-240 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:11 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:11 lpmodule.py:374] update past key value cost 0.025053 seconds
DEBUG 10-15 15:27:11 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:11 lpmodule.py:227] layer idx 25
DEBUG 10-15 15:27:11 lpmodule.py:399] repeat qkv cost 0.030681 seconds
DEBUG 10-15 15:27:11 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:11 lpmodule.py:433] dot attn cost 0.036465 seconds
DEBUG 10-15 15:27:11 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:11 lpmodule.py:444] time cost move to cuda:1 0.02400827407836914 s
DEBUG 10-15 15:27:11 lpllm.py:2283] CPU attn cost 0.142445 seconds if batch True
DEBUG 10-15 15:27:11 lpllm.py:2292] deal attn result cost 0.000003 seconds
DEBUG 10-15 15:27:11 lpllm.py:2294] CPU compute cost 0.143216 seconds
DEBUG 10-15 15:27:11 lpllm.py:2312] free cost 0.000083 seconds
DEBUG 10-15 15:27:11 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:11 lpllm.py:1774] update state cost 1.9788742065429688e-05 s
DEBUG 10-15 15:27:11 lpllm.py:1743] restore layer func cost 0.00038433074951171875 s
DEBUG 10-15 15:27:11 lpllm.py:511] restore layer cost 0.0006611347198486328 s
DEBUG 10-15 15:27:11 lpllm.py:512] reset_next_layer_need: layer_attn_idx=26, layer_mlp_idx=25, j_loc=51
DEBUG 10-15 15:27:11 lpllm.py:1037] reset layer cost 0.0007295608520507812 s
DEBUG 10-15 15:27:11 lpllm.py:1038] have reset next layer layer_attn 26 layer_mlp 25 
DEBUG 10-15 15:27:11 lpllm.py:1044] j: 51 waiting the layer with layer_idx 26 before wait time 0.449796199798584 s
INFO 10-15 15:27:11 client.py:117] confirm_model_loaded: Mixtral-8x7B, d27a63e1-cde4-4445-a514-b6c0172b47ee
INFO 10-15 15:27:11 client.py:125] Model loaded
DEBUG 10-15 15:27:11 lpllm.py:1048] j: load cost 0.4511988162994385 s waiting cost 0.0013875961303710938 s
DEBUG 10-15 15:27:11 lpllm.py:924] 
DEBUG 10-15 15:27:11 lpllm.py:924] decoder loop j: 52 cur_layer_idx: 26 layer_attn_idx: 26 layer_mlp_idx: 25
DEBUG 10-15 15:27:11 lpllm.py:933] start load next layer cur_layer_idx: 27
DEBUG 10-15 15:27:11 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed6e0>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x004\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x98\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:11 client.py:72] load_into_gpu: Mixtral-8x7B, a23a0825-a4c6-40b0-87ee-4619db59da6d
INFO 10-15 15:27:11 client.py:113] Model loaded: Mixtral-8x7B, a23a0825-a4c6-40b0-87ee-4619db59da6d
DEBUG 10-15 15:27:11 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:11 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:11 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:11 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:11 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:11 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:11 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:11 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:11 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:11 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:11 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:11 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:11 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:11 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:11 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:11 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:11 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:11 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:11 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:11 lpllm.py:2265] GPU2CPU move cost 0.000576 seconds
DEBUG 10-15 15:27:11 lpmodule.py:340] layer_idx 26 attn layer_idx: 26
DEBUG 10-15 15:27:11 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:11 lpmodule.py:364] decoder_attn_batch update batch_dim 120-180
DEBUG 10-15 15:27:11 StaticCacheLen.py:84] static cache update layer_idx: 26, start_batch: 120, end_batch: 180
DEBUG 10-15 15:27:11 lpmodule.py:368] update for kv cache 120-180 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:11 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:11 lpmodule.py:374] update past key value cost 0.023502 seconds
DEBUG 10-15 15:27:11 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:11 lpmodule.py:227] layer idx 25
DEBUG 10-15 15:27:11 lpmodule.py:399] repeat qkv cost 0.030837 seconds
DEBUG 10-15 15:27:11 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:11 lpmodule.py:433] dot attn cost 0.034700 seconds
DEBUG 10-15 15:27:11 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:11 lpmodule.py:444] time cost move to cuda:1 0.0241091251373291 s
DEBUG 10-15 15:27:11 lpllm.py:2283] CPU attn cost 0.140169 seconds if batch True
DEBUG 10-15 15:27:11 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:11 lpllm.py:2294] CPU compute cost 0.141021 seconds
DEBUG 10-15 15:27:11 lpllm.py:2312] free cost 0.000082 seconds
DEBUG 10-15 15:27:11 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:11 lpllm.py:1774] update state cost 1.8358230590820312e-05 s
DEBUG 10-15 15:27:11 lpllm.py:1743] restore layer func cost 0.0008068084716796875 s
DEBUG 10-15 15:27:11 lpllm.py:511] restore layer cost 0.0010471343994140625 s
DEBUG 10-15 15:27:11 lpllm.py:512] reset_next_layer_need: layer_attn_idx=26, layer_mlp_idx=26, j_loc=52
DEBUG 10-15 15:27:11 lpllm.py:1037] reset layer cost 0.0011181831359863281 s
DEBUG 10-15 15:27:11 lpllm.py:1038] have reset next layer layer_attn 26 layer_mlp 26 
DEBUG 10-15 15:27:11 lpllm.py:924] 
DEBUG 10-15 15:27:11 lpllm.py:924] decoder loop j: 53 cur_layer_idx: 26 layer_attn_idx: 26 layer_mlp_idx: 26
DEBUG 10-15 15:27:11 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:11 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:11 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:11 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:11 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:11 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:11 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:11 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:11 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:11 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:11 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:11 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:11 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:11 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:11 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:11 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:11 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:11 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:11 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:11 lpllm.py:2265] GPU2CPU move cost 0.000614 seconds
DEBUG 10-15 15:27:11 lpmodule.py:340] layer_idx 26 attn layer_idx: 26
DEBUG 10-15 15:27:11 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:11 lpmodule.py:364] decoder_attn_batch update batch_dim 180-240
DEBUG 10-15 15:27:11 StaticCacheLen.py:84] static cache update layer_idx: 26, start_batch: 180, end_batch: 240
DEBUG 10-15 15:27:11 lpmodule.py:368] update for kv cache 180-240 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:11 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:11 lpmodule.py:374] update past key value cost 0.022836 seconds
DEBUG 10-15 15:27:11 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:11 lpmodule.py:227] layer idx 26
DEBUG 10-15 15:27:11 lpmodule.py:399] repeat qkv cost 0.030852 seconds
DEBUG 10-15 15:27:11 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:11 lpmodule.py:433] dot attn cost 0.033884 seconds
DEBUG 10-15 15:27:11 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:12 lpmodule.py:444] time cost move to cuda:1 0.024213075637817383 s
DEBUG 10-15 15:27:12 lpllm.py:2283] CPU attn cost 0.139288 seconds if batch True
DEBUG 10-15 15:27:12 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:12 lpllm.py:2294] CPU compute cost 0.140176 seconds
DEBUG 10-15 15:27:12 lpllm.py:2312] free cost 0.000083 seconds
DEBUG 10-15 15:27:12 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:12 lpllm.py:1774] update state cost 3.6716461181640625e-05 s
DEBUG 10-15 15:27:12 lpllm.py:1743] restore layer func cost 0.00037789344787597656 s
DEBUG 10-15 15:27:12 lpllm.py:511] restore layer cost 0.0006282329559326172 s
DEBUG 10-15 15:27:12 lpllm.py:512] reset_next_layer_need: layer_attn_idx=27, layer_mlp_idx=26, j_loc=53
DEBUG 10-15 15:27:12 lpllm.py:1037] reset layer cost 0.0006990432739257812 s
DEBUG 10-15 15:27:12 lpllm.py:1038] have reset next layer layer_attn 27 layer_mlp 26 
DEBUG 10-15 15:27:12 lpllm.py:1044] j: 53 waiting the layer with layer_idx 27 before wait time 0.4482111930847168 s
INFO 10-15 15:27:12 client.py:117] confirm_model_loaded: Mixtral-8x7B, a23a0825-a4c6-40b0-87ee-4619db59da6d
INFO 10-15 15:27:12 client.py:125] Model loaded
DEBUG 10-15 15:27:12 lpllm.py:1048] j: load cost 0.449596643447876 s waiting cost 0.0013706684112548828 s
DEBUG 10-15 15:27:12 lpllm.py:924] 
DEBUG 10-15 15:27:12 lpllm.py:924] decoder loop j: 54 cur_layer_idx: 27 layer_attn_idx: 27 layer_mlp_idx: 26
DEBUG 10-15 15:27:12 lpllm.py:933] start load next layer cur_layer_idx: 28
DEBUG 10-15 15:27:12 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:12 client.py:72] load_into_gpu: Mixtral-8x7B, e484d552-74eb-41a6-a753-92f816a67bf6
INFO 10-15 15:27:12 client.py:113] Model loaded: Mixtral-8x7B, e484d552-74eb-41a6-a753-92f816a67bf6
DEBUG 10-15 15:27:12 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:12 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:12 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:12 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:12 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:12 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:12 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:12 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:12 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:12 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:12 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:12 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:12 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:12 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:12 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:12 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:12 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:12 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:12 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:12 lpllm.py:2265] GPU2CPU move cost 0.000618 seconds
DEBUG 10-15 15:27:12 lpmodule.py:340] layer_idx 27 attn layer_idx: 27
DEBUG 10-15 15:27:12 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:12 lpmodule.py:364] decoder_attn_batch update batch_dim 120-180
DEBUG 10-15 15:27:12 StaticCacheLen.py:84] static cache update layer_idx: 27, start_batch: 120, end_batch: 180
DEBUG 10-15 15:27:12 lpmodule.py:368] update for kv cache 120-180 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:12 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:12 lpmodule.py:374] update past key value cost 0.024328 seconds
DEBUG 10-15 15:27:12 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:12 lpmodule.py:227] layer idx 26
DEBUG 10-15 15:27:12 lpmodule.py:399] repeat qkv cost 0.032333 seconds
DEBUG 10-15 15:27:12 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:12 lpmodule.py:433] dot attn cost 0.037178 seconds
DEBUG 10-15 15:27:12 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:12 lpmodule.py:444] time cost move to cuda:1 0.024007081985473633 s
DEBUG 10-15 15:27:12 lpllm.py:2283] CPU attn cost 0.147723 seconds if batch True
DEBUG 10-15 15:27:12 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:12 lpllm.py:2294] CPU compute cost 0.148681 seconds
DEBUG 10-15 15:27:12 lpllm.py:2312] free cost 0.000092 seconds
DEBUG 10-15 15:27:12 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:12 lpllm.py:1774] update state cost 1.8358230590820312e-05 s
DEBUG 10-15 15:27:12 lpllm.py:1743] restore layer func cost 0.0008039474487304688 s
DEBUG 10-15 15:27:12 lpllm.py:511] restore layer cost 0.001039743423461914 s
DEBUG 10-15 15:27:12 lpllm.py:512] reset_next_layer_need: layer_attn_idx=27, layer_mlp_idx=27, j_loc=54
DEBUG 10-15 15:27:12 lpllm.py:1037] reset layer cost 0.0011112689971923828 s
DEBUG 10-15 15:27:12 lpllm.py:1038] have reset next layer layer_attn 27 layer_mlp 27 
DEBUG 10-15 15:27:12 lpllm.py:924] 
DEBUG 10-15 15:27:12 lpllm.py:924] decoder loop j: 55 cur_layer_idx: 27 layer_attn_idx: 27 layer_mlp_idx: 27
DEBUG 10-15 15:27:12 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:12 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:12 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:12 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:12 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:12 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:12 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:12 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:12 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:12 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:12 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:12 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:12 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:12 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:12 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:12 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:12 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:12 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:12 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:12 lpllm.py:2265] GPU2CPU move cost 0.000418 seconds
DEBUG 10-15 15:27:12 lpmodule.py:340] layer_idx 27 attn layer_idx: 27
DEBUG 10-15 15:27:12 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:12 lpmodule.py:364] decoder_attn_batch update batch_dim 180-240
DEBUG 10-15 15:27:12 StaticCacheLen.py:84] static cache update layer_idx: 27, start_batch: 180, end_batch: 240
DEBUG 10-15 15:27:12 lpmodule.py:368] update for kv cache 180-240 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:12 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:12 lpmodule.py:374] update past key value cost 0.023018 seconds
DEBUG 10-15 15:27:12 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:12 lpmodule.py:227] layer idx 27
DEBUG 10-15 15:27:12 lpmodule.py:399] repeat qkv cost 0.030210 seconds
DEBUG 10-15 15:27:12 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:12 lpmodule.py:433] dot attn cost 0.035254 seconds
DEBUG 10-15 15:27:12 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:12 lpmodule.py:444] time cost move to cuda:1 0.02436065673828125 s
DEBUG 10-15 15:27:12 lpllm.py:2283] CPU attn cost 0.137884 seconds if batch True
DEBUG 10-15 15:27:12 lpllm.py:2292] deal attn result cost 0.000003 seconds
DEBUG 10-15 15:27:12 lpllm.py:2294] CPU compute cost 0.138579 seconds
DEBUG 10-15 15:27:12 lpllm.py:2312] free cost 0.000084 seconds
DEBUG 10-15 15:27:12 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:12 lpllm.py:1774] update state cost 2.0503997802734375e-05 s
DEBUG 10-15 15:27:12 lpllm.py:1743] restore layer func cost 0.0003829002380371094 s
DEBUG 10-15 15:27:12 lpllm.py:511] restore layer cost 0.0006694793701171875 s
DEBUG 10-15 15:27:12 lpllm.py:512] reset_next_layer_need: layer_attn_idx=28, layer_mlp_idx=27, j_loc=55
DEBUG 10-15 15:27:12 lpllm.py:1037] reset layer cost 0.0007407665252685547 s
DEBUG 10-15 15:27:12 lpllm.py:1038] have reset next layer layer_attn 28 layer_mlp 27 
DEBUG 10-15 15:27:12 lpllm.py:1044] j: 55 waiting the layer with layer_idx 28 before wait time 0.4424769878387451 s
INFO 10-15 15:27:12 client.py:117] confirm_model_loaded: Mixtral-8x7B, e484d552-74eb-41a6-a753-92f816a67bf6
INFO 10-15 15:27:12 client.py:125] Model loaded
DEBUG 10-15 15:27:12 lpllm.py:1048] j: load cost 0.4437429904937744 s waiting cost 0.0012507438659667969 s
DEBUG 10-15 15:27:12 lpllm.py:924] 
DEBUG 10-15 15:27:12 lpllm.py:924] decoder loop j: 56 cur_layer_idx: 28 layer_attn_idx: 28 layer_mlp_idx: 27
DEBUG 10-15 15:27:12 lpllm.py:933] start load next layer cur_layer_idx: 29
DEBUG 10-15 15:27:12 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed650>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x003\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x96\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:12 client.py:72] load_into_gpu: Mixtral-8x7B, 3333c6cf-d3d7-46ca-a06a-81de595009ce
INFO 10-15 15:27:12 client.py:113] Model loaded: Mixtral-8x7B, 3333c6cf-d3d7-46ca-a06a-81de595009ce
DEBUG 10-15 15:27:12 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:12 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:12 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:12 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:12 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:12 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:12 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:12 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:12 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:12 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:12 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:12 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:12 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:12 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:12 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:12 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:12 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:12 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:12 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:12 lpllm.py:2265] GPU2CPU move cost 0.000574 seconds
DEBUG 10-15 15:27:12 lpmodule.py:340] layer_idx 28 attn layer_idx: 28
DEBUG 10-15 15:27:12 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:12 lpmodule.py:364] decoder_attn_batch update batch_dim 120-180
DEBUG 10-15 15:27:12 StaticCacheLen.py:84] static cache update layer_idx: 28, start_batch: 120, end_batch: 180
DEBUG 10-15 15:27:12 lpmodule.py:368] update for kv cache 120-180 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:12 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:12 lpmodule.py:374] update past key value cost 0.023142 seconds
DEBUG 10-15 15:27:12 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:12 lpmodule.py:227] layer idx 27
DEBUG 10-15 15:27:12 lpmodule.py:399] repeat qkv cost 0.031297 seconds
DEBUG 10-15 15:27:12 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:12 lpmodule.py:433] dot attn cost 0.037923 seconds
DEBUG 10-15 15:27:12 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:12 lpmodule.py:444] time cost move to cuda:1 0.031037092208862305 s
DEBUG 10-15 15:27:12 lpllm.py:2283] CPU attn cost 0.163654 seconds if batch True
DEBUG 10-15 15:27:12 lpllm.py:2292] deal attn result cost 0.000003 seconds
DEBUG 10-15 15:27:12 lpllm.py:2294] CPU compute cost 0.164604 seconds
DEBUG 10-15 15:27:12 lpllm.py:2312] free cost 0.000111 seconds
DEBUG 10-15 15:27:12 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:12 lpllm.py:1774] update state cost 3.8623809814453125e-05 s
DEBUG 10-15 15:27:12 lpllm.py:1743] restore layer func cost 0.0011763572692871094 s
DEBUG 10-15 15:27:12 lpllm.py:511] restore layer cost 0.0014691352844238281 s
DEBUG 10-15 15:27:12 lpllm.py:512] reset_next_layer_need: layer_attn_idx=28, layer_mlp_idx=28, j_loc=56
DEBUG 10-15 15:27:12 lpllm.py:1037] reset layer cost 0.0015740394592285156 s
DEBUG 10-15 15:27:12 lpllm.py:1038] have reset next layer layer_attn 28 layer_mlp 28 
DEBUG 10-15 15:27:12 lpllm.py:924] 
DEBUG 10-15 15:27:12 lpllm.py:924] decoder loop j: 57 cur_layer_idx: 28 layer_attn_idx: 28 layer_mlp_idx: 28
DEBUG 10-15 15:27:12 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:12 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:12 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:12 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:12 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:12 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:12 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:12 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:12 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:12 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:12 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:12 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:12 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:12 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:12 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:12 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:12 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:12 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:12 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:12 lpllm.py:2265] GPU2CPU move cost 0.000639 seconds
DEBUG 10-15 15:27:12 lpmodule.py:340] layer_idx 28 attn layer_idx: 28
DEBUG 10-15 15:27:12 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:12 lpmodule.py:364] decoder_attn_batch update batch_dim 180-240
DEBUG 10-15 15:27:12 StaticCacheLen.py:84] static cache update layer_idx: 28, start_batch: 180, end_batch: 240
DEBUG 10-15 15:27:12 lpmodule.py:368] update for kv cache 180-240 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:12 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:12 lpmodule.py:374] update past key value cost 0.024018 seconds
DEBUG 10-15 15:27:12 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:12 lpmodule.py:227] layer idx 28
DEBUG 10-15 15:27:12 lpmodule.py:399] repeat qkv cost 0.030270 seconds
DEBUG 10-15 15:27:12 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:12 lpmodule.py:433] dot attn cost 0.035575 seconds
DEBUG 10-15 15:27:12 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:12 lpmodule.py:444] time cost move to cuda:1 0.02468395233154297 s
DEBUG 10-15 15:27:12 lpllm.py:2283] CPU attn cost 0.141418 seconds if batch True
DEBUG 10-15 15:27:12 lpllm.py:2292] deal attn result cost 0.000003 seconds
DEBUG 10-15 15:27:12 lpllm.py:2294] CPU compute cost 0.142355 seconds
DEBUG 10-15 15:27:12 lpllm.py:2312] free cost 0.000082 seconds
DEBUG 10-15 15:27:12 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:12 lpllm.py:1774] update state cost 3.62396240234375e-05 s
DEBUG 10-15 15:27:12 lpllm.py:1743] restore layer func cost 0.00038170814514160156 s
DEBUG 10-15 15:27:12 lpllm.py:511] restore layer cost 0.0006680488586425781 s
DEBUG 10-15 15:27:12 lpllm.py:512] reset_next_layer_need: layer_attn_idx=29, layer_mlp_idx=28, j_loc=57
DEBUG 10-15 15:27:12 lpllm.py:1037] reset layer cost 0.0007367134094238281 s
DEBUG 10-15 15:27:12 lpllm.py:1038] have reset next layer layer_attn 29 layer_mlp 28 
DEBUG 10-15 15:27:12 lpllm.py:1044] j: 57 waiting the layer with layer_idx 29 before wait time 0.44580864906311035 s
INFO 10-15 15:27:12 client.py:117] confirm_model_loaded: Mixtral-8x7B, 3333c6cf-d3d7-46ca-a06a-81de595009ce
INFO 10-15 15:27:12 client.py:125] Model loaded
DEBUG 10-15 15:27:12 lpllm.py:1048] j: load cost 0.44721150398254395 s waiting cost 0.0013875961303710938 s
DEBUG 10-15 15:27:12 lpllm.py:924] 
DEBUG 10-15 15:27:12 lpllm.py:924] decoder loop j: 58 cur_layer_idx: 29 layer_attn_idx: 29 layer_mlp_idx: 28
DEBUG 10-15 15:27:12 lpllm.py:933] start load next layer cur_layer_idx: 30
DEBUG 10-15 15:27:12 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed6e0>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x004\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x98\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:12 client.py:72] load_into_gpu: Mixtral-8x7B, 1ccef912-282f-408c-a8fa-c70dbbef9606
INFO 10-15 15:27:12 client.py:113] Model loaded: Mixtral-8x7B, 1ccef912-282f-408c-a8fa-c70dbbef9606
DEBUG 10-15 15:27:12 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:12 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:12 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:13 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:13 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:13 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:13 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:13 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:13 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:13 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:13 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:13 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:13 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:13 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:13 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:13 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:13 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:13 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:13 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:13 lpllm.py:2265] GPU2CPU move cost 0.000573 seconds
DEBUG 10-15 15:27:13 lpmodule.py:340] layer_idx 29 attn layer_idx: 29
DEBUG 10-15 15:27:13 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:13 lpmodule.py:364] decoder_attn_batch update batch_dim 120-180
DEBUG 10-15 15:27:13 StaticCacheLen.py:84] static cache update layer_idx: 29, start_batch: 120, end_batch: 180
DEBUG 10-15 15:27:13 lpmodule.py:368] update for kv cache 120-180 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:13 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:13 lpmodule.py:374] update past key value cost 0.025707 seconds
DEBUG 10-15 15:27:13 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:13 lpmodule.py:227] layer idx 28
DEBUG 10-15 15:27:13 lpmodule.py:399] repeat qkv cost 0.030349 seconds
DEBUG 10-15 15:27:13 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:13 lpmodule.py:433] dot attn cost 0.034559 seconds
DEBUG 10-15 15:27:13 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:13 lpmodule.py:444] time cost move to cuda:1 0.025976896286010742 s
DEBUG 10-15 15:27:13 lpllm.py:2283] CPU attn cost 0.144655 seconds if batch True
DEBUG 10-15 15:27:13 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:13 lpllm.py:2294] CPU compute cost 0.145539 seconds
DEBUG 10-15 15:27:13 lpllm.py:2312] free cost 0.000085 seconds
DEBUG 10-15 15:27:13 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:13 lpllm.py:1774] update state cost 2.002716064453125e-05 s
DEBUG 10-15 15:27:13 lpllm.py:1743] restore layer func cost 0.0007991790771484375 s
DEBUG 10-15 15:27:13 lpllm.py:511] restore layer cost 0.001043081283569336 s
DEBUG 10-15 15:27:13 lpllm.py:512] reset_next_layer_need: layer_attn_idx=29, layer_mlp_idx=29, j_loc=58
DEBUG 10-15 15:27:13 lpllm.py:1037] reset layer cost 0.001116037368774414 s
DEBUG 10-15 15:27:13 lpllm.py:1038] have reset next layer layer_attn 29 layer_mlp 29 
DEBUG 10-15 15:27:13 lpllm.py:924] 
DEBUG 10-15 15:27:13 lpllm.py:924] decoder loop j: 59 cur_layer_idx: 29 layer_attn_idx: 29 layer_mlp_idx: 29
DEBUG 10-15 15:27:13 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:13 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:13 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:13 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:13 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:13 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:13 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:13 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:13 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:13 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:13 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:13 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:13 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:13 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:13 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:13 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:13 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:13 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:13 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:13 lpllm.py:2265] GPU2CPU move cost 0.000653 seconds
DEBUG 10-15 15:27:13 lpmodule.py:340] layer_idx 29 attn layer_idx: 29
DEBUG 10-15 15:27:13 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:13 lpmodule.py:364] decoder_attn_batch update batch_dim 180-240
DEBUG 10-15 15:27:13 StaticCacheLen.py:84] static cache update layer_idx: 29, start_batch: 180, end_batch: 240
DEBUG 10-15 15:27:13 lpmodule.py:368] update for kv cache 180-240 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:13 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:13 lpmodule.py:374] update past key value cost 0.028991 seconds
DEBUG 10-15 15:27:13 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:13 lpmodule.py:227] layer idx 29
DEBUG 10-15 15:27:13 lpmodule.py:399] repeat qkv cost 0.031257 seconds
DEBUG 10-15 15:27:13 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:13 lpmodule.py:433] dot attn cost 0.036026 seconds
DEBUG 10-15 15:27:13 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:13 lpmodule.py:444] time cost move to cuda:1 0.0245816707611084 s
DEBUG 10-15 15:27:13 lpllm.py:2283] CPU attn cost 0.146542 seconds if batch True
DEBUG 10-15 15:27:13 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:13 lpllm.py:2294] CPU compute cost 0.147513 seconds
DEBUG 10-15 15:27:13 lpllm.py:2312] free cost 0.000082 seconds
DEBUG 10-15 15:27:13 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:13 lpllm.py:1774] update state cost 3.719329833984375e-05 s
DEBUG 10-15 15:27:13 lpllm.py:1743] restore layer func cost 0.0003750324249267578 s
DEBUG 10-15 15:27:13 lpllm.py:511] restore layer cost 0.0006527900695800781 s
DEBUG 10-15 15:27:13 lpllm.py:512] reset_next_layer_need: layer_attn_idx=30, layer_mlp_idx=29, j_loc=59
DEBUG 10-15 15:27:13 lpllm.py:1037] reset layer cost 0.0007221698760986328 s
DEBUG 10-15 15:27:13 lpllm.py:1038] have reset next layer layer_attn 30 layer_mlp 29 
DEBUG 10-15 15:27:13 lpllm.py:1044] j: 59 waiting the layer with layer_idx 30 before wait time 0.4437141418457031 s
INFO 10-15 15:27:13 client.py:117] confirm_model_loaded: Mixtral-8x7B, 1ccef912-282f-408c-a8fa-c70dbbef9606
INFO 10-15 15:27:13 client.py:125] Model loaded
DEBUG 10-15 15:27:13 lpllm.py:1048] j: load cost 0.44525790214538574 s waiting cost 0.0015289783477783203 s
DEBUG 10-15 15:27:13 lpllm.py:924] 
DEBUG 10-15 15:27:13 lpllm.py:924] decoder loop j: 60 cur_layer_idx: 30 layer_attn_idx: 30 layer_mlp_idx: 29
DEBUG 10-15 15:27:13 lpllm.py:933] start load next layer cur_layer_idx: 31
DEBUG 10-15 15:27:13 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:13 client.py:72] load_into_gpu: Mixtral-8x7B, da50a100-213b-48f2-9db4-b48a1c8596c3
INFO 10-15 15:27:13 client.py:113] Model loaded: Mixtral-8x7B, da50a100-213b-48f2-9db4-b48a1c8596c3
DEBUG 10-15 15:27:13 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:13 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:13 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:13 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:13 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:13 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:13 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:13 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:13 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:13 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:13 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:13 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:13 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:13 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:13 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:13 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:13 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:13 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:13 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:13 lpllm.py:2265] GPU2CPU move cost 0.000617 seconds
DEBUG 10-15 15:27:13 lpmodule.py:340] layer_idx 30 attn layer_idx: 30
DEBUG 10-15 15:27:13 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:13 lpmodule.py:364] decoder_attn_batch update batch_dim 120-180
DEBUG 10-15 15:27:13 StaticCacheLen.py:84] static cache update layer_idx: 30, start_batch: 120, end_batch: 180
DEBUG 10-15 15:27:13 lpmodule.py:368] update for kv cache 120-180 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:13 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:13 lpmodule.py:374] update past key value cost 0.024800 seconds
DEBUG 10-15 15:27:13 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:13 lpmodule.py:227] layer idx 29
DEBUG 10-15 15:27:13 lpmodule.py:399] repeat qkv cost 0.030481 seconds
DEBUG 10-15 15:27:13 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:13 lpmodule.py:433] dot attn cost 0.040996 seconds
DEBUG 10-15 15:27:13 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:13 lpmodule.py:444] time cost move to cuda:1 0.032060861587524414 s
DEBUG 10-15 15:27:13 lpllm.py:2283] CPU attn cost 0.154824 seconds if batch True
DEBUG 10-15 15:27:13 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:13 lpllm.py:2294] CPU compute cost 0.155729 seconds
DEBUG 10-15 15:27:13 lpllm.py:2312] free cost 0.000082 seconds
DEBUG 10-15 15:27:13 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:13 lpllm.py:1774] update state cost 3.504753112792969e-05 s
DEBUG 10-15 15:27:13 lpllm.py:1743] restore layer func cost 0.0008027553558349609 s
DEBUG 10-15 15:27:13 lpllm.py:511] restore layer cost 0.0010569095611572266 s
DEBUG 10-15 15:27:13 lpllm.py:512] reset_next_layer_need: layer_attn_idx=30, layer_mlp_idx=30, j_loc=60
DEBUG 10-15 15:27:13 lpllm.py:1037] reset layer cost 0.0011281967163085938 s
DEBUG 10-15 15:27:13 lpllm.py:1038] have reset next layer layer_attn 30 layer_mlp 30 
DEBUG 10-15 15:27:13 lpllm.py:924] 
DEBUG 10-15 15:27:13 lpllm.py:924] decoder loop j: 61 cur_layer_idx: 30 layer_attn_idx: 30 layer_mlp_idx: 30
DEBUG 10-15 15:27:13 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:13 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:13 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:13 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:13 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:13 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:13 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:13 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:13 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:13 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:13 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:13 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:13 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:13 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:13 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:13 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:13 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:13 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:13 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:13 lpllm.py:2265] GPU2CPU move cost 0.000578 seconds
DEBUG 10-15 15:27:13 lpmodule.py:340] layer_idx 30 attn layer_idx: 30
DEBUG 10-15 15:27:13 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:13 lpmodule.py:364] decoder_attn_batch update batch_dim 180-240
DEBUG 10-15 15:27:13 StaticCacheLen.py:84] static cache update layer_idx: 30, start_batch: 180, end_batch: 240
DEBUG 10-15 15:27:13 lpmodule.py:368] update for kv cache 180-240 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:13 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:13 lpmodule.py:374] update past key value cost 0.025663 seconds
DEBUG 10-15 15:27:13 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:13 lpmodule.py:227] layer idx 30
DEBUG 10-15 15:27:13 lpmodule.py:399] repeat qkv cost 0.030835 seconds
DEBUG 10-15 15:27:13 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:13 lpmodule.py:433] dot attn cost 0.036596 seconds
DEBUG 10-15 15:27:13 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:13 lpmodule.py:444] time cost move to cuda:1 0.024552583694458008 s
DEBUG 10-15 15:27:13 lpllm.py:2283] CPU attn cost 0.143662 seconds if batch True
DEBUG 10-15 15:27:13 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:13 lpllm.py:2294] CPU compute cost 0.144542 seconds
DEBUG 10-15 15:27:13 lpllm.py:2312] free cost 0.000082 seconds
DEBUG 10-15 15:27:13 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:13 lpllm.py:1774] update state cost 1.9550323486328125e-05 s
DEBUG 10-15 15:27:13 lpllm.py:1743] restore layer func cost 0.0003821849822998047 s
DEBUG 10-15 15:27:13 lpllm.py:511] restore layer cost 0.0006711483001708984 s
DEBUG 10-15 15:27:13 lpllm.py:512] reset_next_layer_need: layer_attn_idx=31, layer_mlp_idx=30, j_loc=61
DEBUG 10-15 15:27:13 lpllm.py:1037] reset layer cost 0.0007421970367431641 s
DEBUG 10-15 15:27:13 lpllm.py:1038] have reset next layer layer_attn 31 layer_mlp 30 
DEBUG 10-15 15:27:13 lpllm.py:1044] j: 61 waiting the layer with layer_idx 31 before wait time 0.45404791831970215 s
INFO 10-15 15:27:13 client.py:117] confirm_model_loaded: Mixtral-8x7B, da50a100-213b-48f2-9db4-b48a1c8596c3
INFO 10-15 15:27:13 client.py:125] Model loaded
DEBUG 10-15 15:27:13 lpllm.py:1048] j: load cost 0.4554438591003418 s waiting cost 0.0013811588287353516 s
DEBUG 10-15 15:27:13 lpllm.py:924] 
DEBUG 10-15 15:27:13 lpllm.py:924] decoder loop j: 62 cur_layer_idx: 31 layer_attn_idx: 31 layer_mlp_idx: 30
DEBUG 10-15 15:27:13 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:13 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:13 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:13 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:13 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:13 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:13 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:13 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:13 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:13 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:13 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:13 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:13 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:13 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:13 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:13 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:13 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:13 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:13 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:13 lpllm.py:2265] GPU2CPU move cost 0.000578 seconds
DEBUG 10-15 15:27:13 lpmodule.py:340] layer_idx 31 attn layer_idx: 31
DEBUG 10-15 15:27:13 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:13 lpmodule.py:364] decoder_attn_batch update batch_dim 120-180
DEBUG 10-15 15:27:13 StaticCacheLen.py:84] static cache update layer_idx: 31, start_batch: 120, end_batch: 180
DEBUG 10-15 15:27:13 lpmodule.py:368] update for kv cache 120-180 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:13 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:13 lpmodule.py:374] update past key value cost 0.024162 seconds
DEBUG 10-15 15:27:13 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:13 lpmodule.py:227] layer idx 30
DEBUG 10-15 15:27:13 lpmodule.py:399] repeat qkv cost 0.030329 seconds
DEBUG 10-15 15:27:13 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:14 lpmodule.py:433] dot attn cost 0.035356 seconds
DEBUG 10-15 15:27:14 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:14 lpmodule.py:444] time cost move to cuda:1 0.02508687973022461 s
DEBUG 10-15 15:27:14 lpllm.py:2283] CPU attn cost 0.144717 seconds if batch True
DEBUG 10-15 15:27:14 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:14 lpllm.py:2294] CPU compute cost 0.145582 seconds
DEBUG 10-15 15:27:14 lpllm.py:2312] free cost 0.000083 seconds
DEBUG 10-15 15:27:14 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:14 lpllm.py:1774] update state cost 3.266334533691406e-05 s
DEBUG 10-15 15:27:14 lpllm.py:1743] restore layer func cost 0.0008013248443603516 s
DEBUG 10-15 15:27:14 lpllm.py:511] restore layer cost 0.0010700225830078125 s
DEBUG 10-15 15:27:14 lpllm.py:512] reset_next_layer_need: layer_attn_idx=31, layer_mlp_idx=31, j_loc=62
DEBUG 10-15 15:27:14 lpllm.py:1037] reset layer cost 0.0011475086212158203 s
DEBUG 10-15 15:27:14 lpllm.py:1038] have reset next layer layer_attn 31 layer_mlp 31 
DEBUG 10-15 15:27:14 lpllm.py:924] 
DEBUG 10-15 15:27:14 lpllm.py:924] decoder loop j: 63 cur_layer_idx: 31 layer_attn_idx: 31 layer_mlp_idx: 31
DEBUG 10-15 15:27:14 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:14 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:14 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:14 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:14 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:14 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:14 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:14 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:14 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:14 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:14 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:14 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:14 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:14 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:14 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:14 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:14 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:14 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:14 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:14 lpllm.py:2265] GPU2CPU move cost 0.000586 seconds
DEBUG 10-15 15:27:14 lpmodule.py:340] layer_idx 31 attn layer_idx: 31
DEBUG 10-15 15:27:14 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:14 lpmodule.py:364] decoder_attn_batch update batch_dim 180-240
DEBUG 10-15 15:27:14 StaticCacheLen.py:84] static cache update layer_idx: 31, start_batch: 180, end_batch: 240
DEBUG 10-15 15:27:14 lpmodule.py:368] update for kv cache 180-240 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:14 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:14 lpmodule.py:374] update past key value cost 0.023822 seconds
DEBUG 10-15 15:27:14 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:14 lpmodule.py:227] layer idx 31
DEBUG 10-15 15:27:14 lpmodule.py:399] repeat qkv cost 0.030764 seconds
DEBUG 10-15 15:27:14 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:14 lpmodule.py:433] dot attn cost 0.035002 seconds
DEBUG 10-15 15:27:14 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:14 lpmodule.py:444] time cost move to cuda:1 0.024479389190673828 s
DEBUG 10-15 15:27:14 lpllm.py:2283] CPU attn cost 0.141492 seconds if batch True
DEBUG 10-15 15:27:14 lpllm.py:2292] deal attn result cost 0.000003 seconds
DEBUG 10-15 15:27:14 lpllm.py:2294] CPU compute cost 0.142401 seconds
DEBUG 10-15 15:27:14 lpllm.py:2312] free cost 0.000086 seconds
DEBUG 10-15 15:27:14 lpllm.py:924] 
DEBUG 10-15 15:27:14 lpllm.py:924] decoder loop j: 64 cur_layer_idx: 32 layer_attn_idx: 32 layer_mlp_idx: 31
DEBUG 10-15 15:27:14 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:14 lpmodule.py:227] layer idx 31
DEBUG 10-15 15:27:14 lpllm.py:1085] last_mlp_output_chunk shape: torch.Size([60, 512, 4096]), mlp_output_chunk shape: torch.Size([60, 512, 4096])
DEBUG 10-15 15:27:14 lpllm.py:1086] last_mlp_output_chunk len 1, mlp_output_chunk len 1
DEBUG 10-15 15:27:14 lpllm.py:618] decoders batch for 1 cost 14.908291578292847 s
DEBUG 10-15 15:27:14 lpllm.py:848] hidden_states_chunks1 shape: torch.Size([60, 512, 4096]), hidden_states_chunks1 length: 1
DEBUG 10-15 15:27:14 lpllm.py:849] hidden_states_chunks2 shape: torch.Size([60, 512, 4096]), hidden_states_chunks2 length: 1
DEBUG 10-15 15:27:14 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed650>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x003\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x96\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:14 client.py:72] load_into_gpu: Mixtral-8x7B, 3e57250f-fdff-4df4-aacb-2e25c04d719e
INFO 10-15 15:27:14 client.py:113] Model loaded: Mixtral-8x7B, 3e57250f-fdff-4df4-aacb-2e25c04d719e
DEBUG 10-15 15:27:14 lpllm.py:1743] restore layer func cost 0.0009293556213378906 s
INFO 10-15 15:27:14 client.py:117] confirm_model_loaded: Mixtral-8x7B, 3e57250f-fdff-4df4-aacb-2e25c04d719e
INFO 10-15 15:27:14 client.py:125] Model loaded
DEBUG 10-15 15:27:14 lpllm.py:422] prepare layer cost 0.2698683738708496 s
DEBUG 10-15 15:27:14 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed6e0>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x004\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x98\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:14 client.py:72] load_into_gpu: Mixtral-8x7B, fa9032c8-609e-4ce7-9c08-95e19b4d6455
INFO 10-15 15:27:14 client.py:113] Model loaded: Mixtral-8x7B, fa9032c8-609e-4ce7-9c08-95e19b4d6455
DEBUG 10-15 15:27:14 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:14 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:14 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:14 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:14 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:14 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:14 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:14 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:14 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:14 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:14 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:14 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:14 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:14 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:14 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:14 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:14 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:14 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:14 lpllm.py:924] 
DEBUG 10-15 15:27:14 lpllm.py:924] decoder loop j: 1 cur_layer_idx: 0 layer_attn_idx: 0 layer_mlp_idx: 0
DEBUG 10-15 15:27:14 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:14 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:14 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:14 lpllm.py:2265] GPU2CPU move cost 0.000655 seconds
DEBUG 10-15 15:27:14 lpmodule.py:340] layer_idx 0 attn layer_idx: 0
DEBUG 10-15 15:27:14 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:14 lpmodule.py:364] decoder_attn_batch update batch_dim 240-300
DEBUG 10-15 15:27:14 StaticCacheLen.py:84] static cache update layer_idx: 0, start_batch: 240, end_batch: 300
DEBUG 10-15 15:27:14 lpmodule.py:368] update for kv cache 240-300 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:14 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:14 lpmodule.py:374] update past key value cost 0.005280 seconds
DEBUG 10-15 15:27:14 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:14 lpmodule.py:399] repeat qkv cost 0.036386 seconds
DEBUG 10-15 15:27:14 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:14 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:14 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:14 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:14 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:14 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:14 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:14 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:14 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:14 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:14 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:14 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:14 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:14 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:14 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:14 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:14 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:14 lpmodule.py:433] dot attn cost 0.036067 seconds
DEBUG 10-15 15:27:14 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:14 lpmodule.py:444] time cost move to cuda:1 0.026431798934936523 s
DEBUG 10-15 15:27:14 lpllm.py:2283] CPU attn cost 0.131839 seconds if batch True
DEBUG 10-15 15:27:14 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:27:14 lpllm.py:2294] CPU compute cost 0.132791 seconds
DEBUG 10-15 15:27:14 lpllm.py:2312] free cost 0.000108 seconds
DEBUG 10-15 15:27:14 lpllm.py:2265] GPU2CPU move cost 0.000272 seconds
DEBUG 10-15 15:27:14 lpmodule.py:340] layer_idx 0 attn layer_idx: 0
DEBUG 10-15 15:27:14 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:14 lpmodule.py:364] decoder_attn_batch update batch_dim 300-360
DEBUG 10-15 15:27:14 StaticCacheLen.py:84] static cache update layer_idx: 0, start_batch: 300, end_batch: 360
DEBUG 10-15 15:27:14 lpmodule.py:368] update for kv cache 300-360 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:14 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:14 lpmodule.py:374] update past key value cost 0.004458 seconds
DEBUG 10-15 15:27:14 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:14 lpmodule.py:227] layer idx 0
DEBUG 10-15 15:27:14 lpmodule.py:399] repeat qkv cost 0.032317 seconds
DEBUG 10-15 15:27:14 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:15 lpmodule.py:433] dot attn cost 0.038042 seconds
DEBUG 10-15 15:27:15 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:15 lpmodule.py:444] time cost move to cuda:1 0.024561166763305664 s
DEBUG 10-15 15:27:15 lpllm.py:2283] CPU attn cost 0.126026 seconds if batch True
DEBUG 10-15 15:27:15 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:15 lpllm.py:2294] CPU compute cost 0.126485 seconds
DEBUG 10-15 15:27:15 lpllm.py:2312] free cost 0.000101 seconds
DEBUG 10-15 15:27:15 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:15 lpllm.py:1774] update state cost 3.647804260253906e-05 s
DEBUG 10-15 15:27:15 lpllm.py:1743] restore layer func cost 0.0003924369812011719 s
DEBUG 10-15 15:27:15 lpllm.py:511] restore layer cost 0.0006573200225830078 s
DEBUG 10-15 15:27:15 lpllm.py:512] reset_next_layer_need: layer_attn_idx=1, layer_mlp_idx=0, j_loc=1
DEBUG 10-15 15:27:15 lpllm.py:1037] reset layer cost 0.0007297992706298828 s
DEBUG 10-15 15:27:15 lpllm.py:1038] have reset next layer layer_attn 1 layer_mlp 0 
DEBUG 10-15 15:27:15 lpllm.py:1044] j: 1 waiting the layer with layer_idx 1 before wait time 0.3553740978240967 s
INFO 10-15 15:27:15 client.py:117] confirm_model_loaded: Mixtral-8x7B, fa9032c8-609e-4ce7-9c08-95e19b4d6455
INFO 10-15 15:27:15 client.py:125] Model loaded
DEBUG 10-15 15:27:15 lpllm.py:1048] j: load cost 0.35670924186706543 s waiting cost 0.0013196468353271484 s
DEBUG 10-15 15:27:15 lpllm.py:924] 
DEBUG 10-15 15:27:15 lpllm.py:924] decoder loop j: 2 cur_layer_idx: 1 layer_attn_idx: 1 layer_mlp_idx: 0
DEBUG 10-15 15:27:15 lpllm.py:933] start load next layer cur_layer_idx: 2
DEBUG 10-15 15:27:15 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:15 client.py:72] load_into_gpu: Mixtral-8x7B, bf2ab80b-91dc-4bc6-9bfb-7e36e5065d55
INFO 10-15 15:27:15 client.py:113] Model loaded: Mixtral-8x7B, bf2ab80b-91dc-4bc6-9bfb-7e36e5065d55
DEBUG 10-15 15:27:15 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:15 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:15 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:15 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:15 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:15 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:15 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:15 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:15 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:15 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:15 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:15 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:15 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:15 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:15 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:15 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:15 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:15 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:15 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:15 lpllm.py:2265] GPU2CPU move cost 0.000593 seconds
DEBUG 10-15 15:27:15 lpmodule.py:340] layer_idx 1 attn layer_idx: 1
DEBUG 10-15 15:27:15 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:15 lpmodule.py:364] decoder_attn_batch update batch_dim 240-300
DEBUG 10-15 15:27:15 StaticCacheLen.py:84] static cache update layer_idx: 1, start_batch: 240, end_batch: 300
DEBUG 10-15 15:27:15 lpmodule.py:368] update for kv cache 240-300 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:15 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:15 lpmodule.py:374] update past key value cost 0.005682 seconds
DEBUG 10-15 15:27:15 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:15 lpmodule.py:227] layer idx 0
DEBUG 10-15 15:27:15 lpmodule.py:399] repeat qkv cost 0.033823 seconds
DEBUG 10-15 15:27:15 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:15 lpmodule.py:433] dot attn cost 0.038851 seconds
DEBUG 10-15 15:27:15 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:15 lpmodule.py:444] time cost move to cuda:1 0.024074316024780273 s
DEBUG 10-15 15:27:15 lpllm.py:2283] CPU attn cost 0.129862 seconds if batch True
DEBUG 10-15 15:27:15 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:15 lpllm.py:2294] CPU compute cost 0.130739 seconds
DEBUG 10-15 15:27:15 lpllm.py:2312] free cost 0.000082 seconds
DEBUG 10-15 15:27:15 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:15 lpllm.py:1774] update state cost 3.4332275390625e-05 s
DEBUG 10-15 15:27:15 lpllm.py:1743] restore layer func cost 0.0008246898651123047 s
DEBUG 10-15 15:27:15 lpllm.py:511] restore layer cost 0.0010862350463867188 s
DEBUG 10-15 15:27:15 lpllm.py:512] reset_next_layer_need: layer_attn_idx=1, layer_mlp_idx=1, j_loc=2
DEBUG 10-15 15:27:15 lpllm.py:1037] reset layer cost 0.0011577606201171875 s
DEBUG 10-15 15:27:15 lpllm.py:1038] have reset next layer layer_attn 1 layer_mlp 1 
DEBUG 10-15 15:27:15 lpllm.py:924] 
DEBUG 10-15 15:27:15 lpllm.py:924] decoder loop j: 3 cur_layer_idx: 1 layer_attn_idx: 1 layer_mlp_idx: 1
DEBUG 10-15 15:27:15 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:15 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:15 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:15 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:15 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:15 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:15 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:15 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:15 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:15 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:15 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:15 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:15 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:15 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:15 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:15 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:15 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:15 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:15 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:15 lpllm.py:2265] GPU2CPU move cost 0.000606 seconds
DEBUG 10-15 15:27:15 lpmodule.py:340] layer_idx 1 attn layer_idx: 1
DEBUG 10-15 15:27:15 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:15 lpmodule.py:364] decoder_attn_batch update batch_dim 300-360
DEBUG 10-15 15:27:15 StaticCacheLen.py:84] static cache update layer_idx: 1, start_batch: 300, end_batch: 360
DEBUG 10-15 15:27:15 lpmodule.py:368] update for kv cache 300-360 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:15 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:15 lpmodule.py:374] update past key value cost 0.004736 seconds
DEBUG 10-15 15:27:15 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:15 lpmodule.py:227] layer idx 1
DEBUG 10-15 15:27:15 lpmodule.py:399] repeat qkv cost 0.036555 seconds
DEBUG 10-15 15:27:15 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:15 lpmodule.py:433] dot attn cost 0.043260 seconds
DEBUG 10-15 15:27:15 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:15 lpmodule.py:444] time cost move to cuda:1 0.024396896362304688 s
DEBUG 10-15 15:27:15 lpllm.py:2283] CPU attn cost 0.136228 seconds if batch True
DEBUG 10-15 15:27:15 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:15 lpllm.py:2294] CPU compute cost 0.137139 seconds
DEBUG 10-15 15:27:15 lpllm.py:2312] free cost 0.000082 seconds
DEBUG 10-15 15:27:15 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:15 lpllm.py:1774] update state cost 3.5762786865234375e-05 s
DEBUG 10-15 15:27:15 lpllm.py:1743] restore layer func cost 0.00038313865661621094 s
DEBUG 10-15 15:27:15 lpllm.py:511] restore layer cost 0.0006363391876220703 s
DEBUG 10-15 15:27:15 lpllm.py:512] reset_next_layer_need: layer_attn_idx=2, layer_mlp_idx=1, j_loc=3
DEBUG 10-15 15:27:15 lpllm.py:1037] reset layer cost 0.0007061958312988281 s
DEBUG 10-15 15:27:15 lpllm.py:1038] have reset next layer layer_attn 2 layer_mlp 1 
DEBUG 10-15 15:27:15 lpllm.py:1044] j: 3 waiting the layer with layer_idx 2 before wait time 0.4523591995239258 s
INFO 10-15 15:27:15 client.py:117] confirm_model_loaded: Mixtral-8x7B, bf2ab80b-91dc-4bc6-9bfb-7e36e5065d55
INFO 10-15 15:27:15 client.py:125] Model loaded
DEBUG 10-15 15:27:15 lpllm.py:1048] j: load cost 0.4536893367767334 s waiting cost 0.0013151168823242188 s
DEBUG 10-15 15:27:15 lpllm.py:924] 
DEBUG 10-15 15:27:15 lpllm.py:924] decoder loop j: 4 cur_layer_idx: 2 layer_attn_idx: 2 layer_mlp_idx: 1
DEBUG 10-15 15:27:15 lpllm.py:933] start load next layer cur_layer_idx: 3
DEBUG 10-15 15:27:15 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed650>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x003\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x96\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:15 client.py:72] load_into_gpu: Mixtral-8x7B, eaa316da-ceed-4b5f-8779-ebd9761fccb4
INFO 10-15 15:27:15 client.py:113] Model loaded: Mixtral-8x7B, eaa316da-ceed-4b5f-8779-ebd9761fccb4
DEBUG 10-15 15:27:15 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:15 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:15 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:15 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:15 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:15 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:15 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:15 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:15 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:15 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:15 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:15 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:15 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:15 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:15 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:15 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:15 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:15 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:15 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:15 lpllm.py:2265] GPU2CPU move cost 0.000591 seconds
DEBUG 10-15 15:27:15 lpmodule.py:340] layer_idx 2 attn layer_idx: 2
DEBUG 10-15 15:27:15 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:15 lpmodule.py:364] decoder_attn_batch update batch_dim 240-300
DEBUG 10-15 15:27:15 StaticCacheLen.py:84] static cache update layer_idx: 2, start_batch: 240, end_batch: 300
DEBUG 10-15 15:27:15 lpmodule.py:368] update for kv cache 240-300 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:15 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:15 lpmodule.py:374] update past key value cost 0.005355 seconds
DEBUG 10-15 15:27:15 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:15 lpmodule.py:227] layer idx 1
DEBUG 10-15 15:27:15 lpmodule.py:399] repeat qkv cost 0.033466 seconds
DEBUG 10-15 15:27:15 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:15 lpmodule.py:433] dot attn cost 0.043865 seconds
DEBUG 10-15 15:27:15 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:15 lpmodule.py:444] time cost move to cuda:1 0.024001359939575195 s
DEBUG 10-15 15:27:15 lpllm.py:2283] CPU attn cost 0.140188 seconds if batch True
DEBUG 10-15 15:27:15 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:15 lpllm.py:2294] CPU compute cost 0.141058 seconds
DEBUG 10-15 15:27:15 lpllm.py:2312] free cost 0.000099 seconds
DEBUG 10-15 15:27:15 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:15 lpllm.py:1774] update state cost 1.7881393432617188e-05 s
DEBUG 10-15 15:27:15 lpllm.py:1743] restore layer func cost 0.0008065700531005859 s
DEBUG 10-15 15:27:15 lpllm.py:511] restore layer cost 0.0010557174682617188 s
DEBUG 10-15 15:27:15 lpllm.py:512] reset_next_layer_need: layer_attn_idx=2, layer_mlp_idx=2, j_loc=4
DEBUG 10-15 15:27:15 lpllm.py:1037] reset layer cost 0.0011277198791503906 s
DEBUG 10-15 15:27:15 lpllm.py:1038] have reset next layer layer_attn 2 layer_mlp 2 
DEBUG 10-15 15:27:15 lpllm.py:924] 
DEBUG 10-15 15:27:15 lpllm.py:924] decoder loop j: 5 cur_layer_idx: 2 layer_attn_idx: 2 layer_mlp_idx: 2
DEBUG 10-15 15:27:15 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:15 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:15 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:15 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:15 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:15 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:15 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:15 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:15 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:15 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:15 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:15 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:15 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:15 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:15 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:15 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:15 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:15 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:15 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:15 lpllm.py:2265] GPU2CPU move cost 0.000603 seconds
DEBUG 10-15 15:27:15 lpmodule.py:340] layer_idx 2 attn layer_idx: 2
DEBUG 10-15 15:27:15 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:15 lpmodule.py:364] decoder_attn_batch update batch_dim 300-360
DEBUG 10-15 15:27:15 StaticCacheLen.py:84] static cache update layer_idx: 2, start_batch: 300, end_batch: 360
DEBUG 10-15 15:27:15 lpmodule.py:368] update for kv cache 300-360 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:15 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:15 lpmodule.py:374] update past key value cost 0.011571 seconds
DEBUG 10-15 15:27:15 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:15 lpmodule.py:227] layer idx 2
DEBUG 10-15 15:27:15 lpmodule.py:399] repeat qkv cost 0.036047 seconds
DEBUG 10-15 15:27:15 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:15 lpmodule.py:433] dot attn cost 0.033381 seconds
DEBUG 10-15 15:27:15 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:15 lpmodule.py:444] time cost move to cuda:1 0.02428412437438965 s
DEBUG 10-15 15:27:15 lpllm.py:2283] CPU attn cost 0.132317 seconds if batch True
DEBUG 10-15 15:27:15 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:15 lpllm.py:2294] CPU compute cost 0.133198 seconds
DEBUG 10-15 15:27:15 lpllm.py:2312] free cost 0.000081 seconds
DEBUG 10-15 15:27:16 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:16 lpllm.py:1774] update state cost 2.09808349609375e-05 s
DEBUG 10-15 15:27:16 lpllm.py:1743] restore layer func cost 0.00037741661071777344 s
DEBUG 10-15 15:27:16 lpllm.py:511] restore layer cost 0.0006189346313476562 s
DEBUG 10-15 15:27:16 lpllm.py:512] reset_next_layer_need: layer_attn_idx=3, layer_mlp_idx=2, j_loc=5
DEBUG 10-15 15:27:16 lpllm.py:1037] reset layer cost 0.0006909370422363281 s
DEBUG 10-15 15:27:16 lpllm.py:1038] have reset next layer layer_attn 3 layer_mlp 2 
DEBUG 10-15 15:27:16 lpllm.py:1044] j: 5 waiting the layer with layer_idx 3 before wait time 0.4548027515411377 s
INFO 10-15 15:27:16 client.py:117] confirm_model_loaded: Mixtral-8x7B, eaa316da-ceed-4b5f-8779-ebd9761fccb4
INFO 10-15 15:27:16 client.py:125] Model loaded
DEBUG 10-15 15:27:16 lpllm.py:1048] j: load cost 0.45615291595458984 s waiting cost 0.00133514404296875 s
DEBUG 10-15 15:27:16 lpllm.py:924] 
DEBUG 10-15 15:27:16 lpllm.py:924] decoder loop j: 6 cur_layer_idx: 3 layer_attn_idx: 3 layer_mlp_idx: 2
DEBUG 10-15 15:27:16 lpllm.py:933] start load next layer cur_layer_idx: 4
DEBUG 10-15 15:27:16 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed6e0>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x004\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x98\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:16 client.py:72] load_into_gpu: Mixtral-8x7B, f7af99c1-d02f-412c-adf9-9a4ea7345f77
INFO 10-15 15:27:16 client.py:113] Model loaded: Mixtral-8x7B, f7af99c1-d02f-412c-adf9-9a4ea7345f77
DEBUG 10-15 15:27:16 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:16 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:16 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:16 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:16 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:16 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:16 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:16 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:16 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:16 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:16 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:16 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:16 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:16 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:16 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:16 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:16 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:16 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:16 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:16 lpllm.py:2265] GPU2CPU move cost 0.000641 seconds
DEBUG 10-15 15:27:16 lpmodule.py:340] layer_idx 3 attn layer_idx: 3
DEBUG 10-15 15:27:16 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:16 lpmodule.py:364] decoder_attn_batch update batch_dim 240-300
DEBUG 10-15 15:27:16 StaticCacheLen.py:84] static cache update layer_idx: 3, start_batch: 240, end_batch: 300
DEBUG 10-15 15:27:16 lpmodule.py:368] update for kv cache 240-300 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:16 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:16 lpmodule.py:374] update past key value cost 0.005881 seconds
DEBUG 10-15 15:27:16 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:16 lpmodule.py:227] layer idx 2
DEBUG 10-15 15:27:16 lpmodule.py:399] repeat qkv cost 0.034270 seconds
DEBUG 10-15 15:27:16 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:16 lpmodule.py:433] dot attn cost 0.044596 seconds
DEBUG 10-15 15:27:16 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:16 lpmodule.py:444] time cost move to cuda:1 0.02419424057006836 s
DEBUG 10-15 15:27:16 lpllm.py:2283] CPU attn cost 0.138415 seconds if batch True
DEBUG 10-15 15:27:16 lpllm.py:2292] deal attn result cost 0.000003 seconds
DEBUG 10-15 15:27:16 lpllm.py:2294] CPU compute cost 0.139383 seconds
DEBUG 10-15 15:27:16 lpllm.py:2312] free cost 0.000098 seconds
DEBUG 10-15 15:27:16 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:16 lpllm.py:1774] update state cost 1.9311904907226562e-05 s
DEBUG 10-15 15:27:16 lpllm.py:1743] restore layer func cost 0.0007956027984619141 s
DEBUG 10-15 15:27:16 lpllm.py:511] restore layer cost 0.0010323524475097656 s
DEBUG 10-15 15:27:16 lpllm.py:512] reset_next_layer_need: layer_attn_idx=3, layer_mlp_idx=3, j_loc=6
DEBUG 10-15 15:27:16 lpllm.py:1037] reset layer cost 0.0011038780212402344 s
DEBUG 10-15 15:27:16 lpllm.py:1038] have reset next layer layer_attn 3 layer_mlp 3 
DEBUG 10-15 15:27:16 lpllm.py:924] 
DEBUG 10-15 15:27:16 lpllm.py:924] decoder loop j: 7 cur_layer_idx: 3 layer_attn_idx: 3 layer_mlp_idx: 3
DEBUG 10-15 15:27:16 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:16 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:16 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:16 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:16 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:16 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:16 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:16 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:16 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:16 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:16 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:16 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:16 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:16 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:16 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:16 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:16 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:16 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:16 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:16 lpllm.py:2265] GPU2CPU move cost 0.000650 seconds
DEBUG 10-15 15:27:16 lpmodule.py:340] layer_idx 3 attn layer_idx: 3
DEBUG 10-15 15:27:16 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:16 lpmodule.py:364] decoder_attn_batch update batch_dim 300-360
DEBUG 10-15 15:27:16 StaticCacheLen.py:84] static cache update layer_idx: 3, start_batch: 300, end_batch: 360
DEBUG 10-15 15:27:16 lpmodule.py:368] update for kv cache 300-360 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:16 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:16 lpmodule.py:374] update past key value cost 0.005950 seconds
DEBUG 10-15 15:27:16 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:16 lpmodule.py:227] layer idx 3
DEBUG 10-15 15:27:16 lpmodule.py:399] repeat qkv cost 0.034425 seconds
DEBUG 10-15 15:27:16 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:16 lpmodule.py:433] dot attn cost 0.033803 seconds
DEBUG 10-15 15:27:16 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:16 lpmodule.py:444] time cost move to cuda:1 0.024961233139038086 s
DEBUG 10-15 15:27:16 lpllm.py:2283] CPU attn cost 0.129364 seconds if batch True
DEBUG 10-15 15:27:16 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:16 lpllm.py:2294] CPU compute cost 0.130455 seconds
DEBUG 10-15 15:27:16 lpllm.py:2312] free cost 0.000094 seconds
DEBUG 10-15 15:27:16 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:16 lpllm.py:1774] update state cost 1.9788742065429688e-05 s
DEBUG 10-15 15:27:16 lpllm.py:1743] restore layer func cost 0.0003819465637207031 s
DEBUG 10-15 15:27:16 lpllm.py:511] restore layer cost 0.0006203651428222656 s
DEBUG 10-15 15:27:16 lpllm.py:512] reset_next_layer_need: layer_attn_idx=4, layer_mlp_idx=3, j_loc=7
DEBUG 10-15 15:27:16 lpllm.py:1037] reset layer cost 0.0006911754608154297 s
DEBUG 10-15 15:27:16 lpllm.py:1038] have reset next layer layer_attn 4 layer_mlp 3 
DEBUG 10-15 15:27:16 lpllm.py:1044] j: 7 waiting the layer with layer_idx 4 before wait time 0.44146060943603516 s
INFO 10-15 15:27:16 client.py:117] confirm_model_loaded: Mixtral-8x7B, f7af99c1-d02f-412c-adf9-9a4ea7345f77
INFO 10-15 15:27:16 client.py:125] Model loaded
DEBUG 10-15 15:27:16 lpllm.py:1048] j: load cost 0.443192720413208 s waiting cost 0.0017163753509521484 s
DEBUG 10-15 15:27:16 lpllm.py:924] 
DEBUG 10-15 15:27:16 lpllm.py:924] decoder loop j: 8 cur_layer_idx: 4 layer_attn_idx: 4 layer_mlp_idx: 3
DEBUG 10-15 15:27:16 lpllm.py:933] start load next layer cur_layer_idx: 5
DEBUG 10-15 15:27:16 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:16 client.py:72] load_into_gpu: Mixtral-8x7B, 17529cf0-dcf5-4a38-bf21-7ed2fa20d424
INFO 10-15 15:27:16 client.py:113] Model loaded: Mixtral-8x7B, 17529cf0-dcf5-4a38-bf21-7ed2fa20d424
DEBUG 10-15 15:27:16 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:16 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:16 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:16 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:16 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:16 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:16 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:16 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:16 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:16 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:16 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:16 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:16 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:16 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:16 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:16 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:16 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:16 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:16 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:16 lpllm.py:2265] GPU2CPU move cost 0.000666 seconds
DEBUG 10-15 15:27:16 lpmodule.py:340] layer_idx 4 attn layer_idx: 4
DEBUG 10-15 15:27:16 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:16 lpmodule.py:364] decoder_attn_batch update batch_dim 240-300
DEBUG 10-15 15:27:16 StaticCacheLen.py:84] static cache update layer_idx: 4, start_batch: 240, end_batch: 300
DEBUG 10-15 15:27:16 lpmodule.py:368] update for kv cache 240-300 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:16 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:16 lpmodule.py:374] update past key value cost 0.025881 seconds
DEBUG 10-15 15:27:16 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:16 lpmodule.py:227] layer idx 3
DEBUG 10-15 15:27:16 lpmodule.py:399] repeat qkv cost 0.032747 seconds
DEBUG 10-15 15:27:16 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:16 lpmodule.py:433] dot attn cost 0.040026 seconds
DEBUG 10-15 15:27:16 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:16 lpmodule.py:444] time cost move to cuda:1 0.02483654022216797 s
DEBUG 10-15 15:27:16 lpllm.py:2283] CPU attn cost 0.151215 seconds if batch True
DEBUG 10-15 15:27:16 lpllm.py:2292] deal attn result cost 0.000003 seconds
DEBUG 10-15 15:27:16 lpllm.py:2294] CPU compute cost 0.152258 seconds
DEBUG 10-15 15:27:16 lpllm.py:2312] free cost 0.000089 seconds
DEBUG 10-15 15:27:16 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:16 lpllm.py:1774] update state cost 3.2901763916015625e-05 s
DEBUG 10-15 15:27:16 lpllm.py:1743] restore layer func cost 0.0007989406585693359 s
DEBUG 10-15 15:27:16 lpllm.py:511] restore layer cost 0.001043081283569336 s
DEBUG 10-15 15:27:16 lpllm.py:512] reset_next_layer_need: layer_attn_idx=4, layer_mlp_idx=4, j_loc=8
DEBUG 10-15 15:27:16 lpllm.py:1037] reset layer cost 0.0011129379272460938 s
DEBUG 10-15 15:27:16 lpllm.py:1038] have reset next layer layer_attn 4 layer_mlp 4 
DEBUG 10-15 15:27:16 lpllm.py:924] 
DEBUG 10-15 15:27:16 lpllm.py:924] decoder loop j: 9 cur_layer_idx: 4 layer_attn_idx: 4 layer_mlp_idx: 4
DEBUG 10-15 15:27:16 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:16 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:16 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:16 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:16 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:16 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:16 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:16 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:16 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:16 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:16 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:16 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:16 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:16 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:16 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:16 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:16 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:16 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:16 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:16 lpllm.py:2265] GPU2CPU move cost 0.000682 seconds
DEBUG 10-15 15:27:16 lpmodule.py:340] layer_idx 4 attn layer_idx: 4
DEBUG 10-15 15:27:16 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:16 lpmodule.py:364] decoder_attn_batch update batch_dim 300-360
DEBUG 10-15 15:27:16 StaticCacheLen.py:84] static cache update layer_idx: 4, start_batch: 300, end_batch: 360
DEBUG 10-15 15:27:16 lpmodule.py:368] update for kv cache 300-360 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:16 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:16 lpmodule.py:374] update past key value cost 0.024318 seconds
DEBUG 10-15 15:27:16 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:16 lpmodule.py:227] layer idx 4
DEBUG 10-15 15:27:16 lpmodule.py:399] repeat qkv cost 0.033020 seconds
DEBUG 10-15 15:27:16 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:16 lpmodule.py:433] dot attn cost 0.033613 seconds
DEBUG 10-15 15:27:16 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:16 lpmodule.py:444] time cost move to cuda:1 0.023170948028564453 s
DEBUG 10-15 15:27:16 lpllm.py:2283] CPU attn cost 0.141006 seconds if batch True
DEBUG 10-15 15:27:16 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:16 lpllm.py:2294] CPU compute cost 0.142100 seconds
DEBUG 10-15 15:27:16 lpllm.py:2312] free cost 0.000090 seconds
DEBUG 10-15 15:27:16 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:16 lpllm.py:1774] update state cost 3.62396240234375e-05 s
DEBUG 10-15 15:27:16 lpllm.py:1743] restore layer func cost 0.00038170814514160156 s
DEBUG 10-15 15:27:16 lpllm.py:511] restore layer cost 0.0006420612335205078 s
DEBUG 10-15 15:27:16 lpllm.py:512] reset_next_layer_need: layer_attn_idx=5, layer_mlp_idx=4, j_loc=9
DEBUG 10-15 15:27:16 lpllm.py:1037] reset layer cost 0.0007131099700927734 s
DEBUG 10-15 15:27:16 lpllm.py:1038] have reset next layer layer_attn 5 layer_mlp 4 
DEBUG 10-15 15:27:16 lpllm.py:1044] j: 9 waiting the layer with layer_idx 5 before wait time 0.4488046169281006 s
INFO 10-15 15:27:16 client.py:117] confirm_model_loaded: Mixtral-8x7B, 17529cf0-dcf5-4a38-bf21-7ed2fa20d424
INFO 10-15 15:27:16 client.py:125] Model loaded
DEBUG 10-15 15:27:16 lpllm.py:1048] j: load cost 0.4506103992462158 s waiting cost 0.0017898082733154297 s
DEBUG 10-15 15:27:16 lpllm.py:924] 
DEBUG 10-15 15:27:16 lpllm.py:924] decoder loop j: 10 cur_layer_idx: 5 layer_attn_idx: 5 layer_mlp_idx: 4
DEBUG 10-15 15:27:16 lpllm.py:933] start load next layer cur_layer_idx: 6
DEBUG 10-15 15:27:16 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed650>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x003\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x96\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:16 client.py:72] load_into_gpu: Mixtral-8x7B, 035f2204-7148-4b42-82af-d4dbd86d0a4a
INFO 10-15 15:27:16 client.py:113] Model loaded: Mixtral-8x7B, 035f2204-7148-4b42-82af-d4dbd86d0a4a
DEBUG 10-15 15:27:16 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:16 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:16 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:16 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:16 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:16 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:16 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:16 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:16 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:16 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:16 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:16 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:16 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:16 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:16 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:16 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:16 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:16 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:16 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:16 lpllm.py:2265] GPU2CPU move cost 0.000712 seconds
DEBUG 10-15 15:27:16 lpmodule.py:340] layer_idx 5 attn layer_idx: 5
DEBUG 10-15 15:27:16 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:16 lpmodule.py:364] decoder_attn_batch update batch_dim 240-300
DEBUG 10-15 15:27:16 StaticCacheLen.py:84] static cache update layer_idx: 5, start_batch: 240, end_batch: 300
DEBUG 10-15 15:27:16 lpmodule.py:368] update for kv cache 240-300 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:16 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:16 lpmodule.py:374] update past key value cost 0.023866 seconds
DEBUG 10-15 15:27:16 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:16 lpmodule.py:227] layer idx 4
DEBUG 10-15 15:27:17 lpmodule.py:399] repeat qkv cost 0.030133 seconds
DEBUG 10-15 15:27:17 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:17 lpmodule.py:433] dot attn cost 0.038235 seconds
DEBUG 10-15 15:27:17 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:17 lpmodule.py:444] time cost move to cuda:1 0.02469182014465332 s
DEBUG 10-15 15:27:17 lpllm.py:2283] CPU attn cost 0.143559 seconds if batch True
DEBUG 10-15 15:27:17 lpllm.py:2292] deal attn result cost 0.000003 seconds
DEBUG 10-15 15:27:17 lpllm.py:2294] CPU compute cost 0.144618 seconds
DEBUG 10-15 15:27:17 lpllm.py:2312] free cost 0.000083 seconds
DEBUG 10-15 15:27:17 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:17 lpllm.py:1774] update state cost 1.8596649169921875e-05 s
DEBUG 10-15 15:27:17 lpllm.py:1743] restore layer func cost 0.000812530517578125 s
DEBUG 10-15 15:27:17 lpllm.py:511] restore layer cost 0.0010530948638916016 s
DEBUG 10-15 15:27:17 lpllm.py:512] reset_next_layer_need: layer_attn_idx=5, layer_mlp_idx=5, j_loc=10
DEBUG 10-15 15:27:17 lpllm.py:1037] reset layer cost 0.0011267662048339844 s
DEBUG 10-15 15:27:17 lpllm.py:1038] have reset next layer layer_attn 5 layer_mlp 5 
DEBUG 10-15 15:27:17 lpllm.py:924] 
DEBUG 10-15 15:27:17 lpllm.py:924] decoder loop j: 11 cur_layer_idx: 5 layer_attn_idx: 5 layer_mlp_idx: 5
DEBUG 10-15 15:27:17 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:17 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:17 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:17 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:17 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:17 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:17 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:17 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:17 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:17 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:17 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:17 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:17 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:17 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:17 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:17 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:17 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:17 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:17 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:17 lpllm.py:2265] GPU2CPU move cost 0.000593 seconds
DEBUG 10-15 15:27:17 lpmodule.py:340] layer_idx 5 attn layer_idx: 5
DEBUG 10-15 15:27:17 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:17 lpmodule.py:364] decoder_attn_batch update batch_dim 300-360
DEBUG 10-15 15:27:17 StaticCacheLen.py:84] static cache update layer_idx: 5, start_batch: 300, end_batch: 360
DEBUG 10-15 15:27:17 lpmodule.py:368] update for kv cache 300-360 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:17 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:17 lpmodule.py:374] update past key value cost 0.025040 seconds
DEBUG 10-15 15:27:17 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:17 lpmodule.py:227] layer idx 5
DEBUG 10-15 15:27:17 lpmodule.py:399] repeat qkv cost 0.030240 seconds
DEBUG 10-15 15:27:17 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:17 lpmodule.py:433] dot attn cost 0.040499 seconds
DEBUG 10-15 15:27:17 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:17 lpmodule.py:444] time cost move to cuda:1 0.023172378540039062 s
DEBUG 10-15 15:27:17 lpllm.py:2283] CPU attn cost 0.145335 seconds if batch True
DEBUG 10-15 15:27:17 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:17 lpllm.py:2294] CPU compute cost 0.146255 seconds
DEBUG 10-15 15:27:17 lpllm.py:2312] free cost 0.000084 seconds
DEBUG 10-15 15:27:17 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:17 lpllm.py:1774] update state cost 2.1219253540039062e-05 s
DEBUG 10-15 15:27:17 lpllm.py:1743] restore layer func cost 0.0003790855407714844 s
DEBUG 10-15 15:27:17 lpllm.py:511] restore layer cost 0.0006229877471923828 s
DEBUG 10-15 15:27:17 lpllm.py:512] reset_next_layer_need: layer_attn_idx=6, layer_mlp_idx=5, j_loc=11
DEBUG 10-15 15:27:17 lpllm.py:1037] reset layer cost 0.0006930828094482422 s
DEBUG 10-15 15:27:17 lpllm.py:1038] have reset next layer layer_attn 6 layer_mlp 5 
DEBUG 10-15 15:27:17 lpllm.py:1044] j: 11 waiting the layer with layer_idx 6 before wait time 0.4509899616241455 s
INFO 10-15 15:27:17 client.py:117] confirm_model_loaded: Mixtral-8x7B, 035f2204-7148-4b42-82af-d4dbd86d0a4a
INFO 10-15 15:27:17 client.py:125] Model loaded
DEBUG 10-15 15:27:17 lpllm.py:1048] j: load cost 0.45270395278930664 s waiting cost 0.0016982555389404297 s
DEBUG 10-15 15:27:17 lpllm.py:924] 
DEBUG 10-15 15:27:17 lpllm.py:924] decoder loop j: 12 cur_layer_idx: 6 layer_attn_idx: 6 layer_mlp_idx: 5
DEBUG 10-15 15:27:17 lpllm.py:933] start load next layer cur_layer_idx: 7
DEBUG 10-15 15:27:17 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed6e0>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x004\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x98\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:17 client.py:72] load_into_gpu: Mixtral-8x7B, 06b0befd-33e4-4982-99ba-011358039b82
INFO 10-15 15:27:17 client.py:113] Model loaded: Mixtral-8x7B, 06b0befd-33e4-4982-99ba-011358039b82
DEBUG 10-15 15:27:17 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:17 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:17 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:17 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:17 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:17 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:17 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:17 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:17 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:17 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:17 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:17 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:17 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:17 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:17 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:17 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:17 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:17 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:17 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:17 lpllm.py:2265] GPU2CPU move cost 0.000459 seconds
DEBUG 10-15 15:27:17 lpmodule.py:340] layer_idx 6 attn layer_idx: 6
DEBUG 10-15 15:27:17 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:17 lpmodule.py:364] decoder_attn_batch update batch_dim 240-300
DEBUG 10-15 15:27:17 StaticCacheLen.py:84] static cache update layer_idx: 6, start_batch: 240, end_batch: 300
DEBUG 10-15 15:27:17 lpmodule.py:368] update for kv cache 240-300 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:17 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:17 lpmodule.py:374] update past key value cost 0.025130 seconds
DEBUG 10-15 15:27:17 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:17 lpmodule.py:227] layer idx 5
DEBUG 10-15 15:27:17 lpmodule.py:399] repeat qkv cost 0.030551 seconds
DEBUG 10-15 15:27:17 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:17 lpmodule.py:433] dot attn cost 0.039186 seconds
DEBUG 10-15 15:27:17 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:17 lpmodule.py:444] time cost move to cuda:1 0.024051427841186523 s
DEBUG 10-15 15:27:17 lpllm.py:2283] CPU attn cost 0.146502 seconds if batch True
DEBUG 10-15 15:27:17 lpllm.py:2292] deal attn result cost 0.000003 seconds
DEBUG 10-15 15:27:17 lpllm.py:2294] CPU compute cost 0.147224 seconds
DEBUG 10-15 15:27:17 lpllm.py:2312] free cost 0.000084 seconds
DEBUG 10-15 15:27:17 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:17 lpllm.py:1774] update state cost 3.409385681152344e-05 s
DEBUG 10-15 15:27:17 lpllm.py:1743] restore layer func cost 0.0008101463317871094 s
DEBUG 10-15 15:27:17 lpllm.py:511] restore layer cost 0.0010559558868408203 s
DEBUG 10-15 15:27:17 lpllm.py:512] reset_next_layer_need: layer_attn_idx=6, layer_mlp_idx=6, j_loc=12
DEBUG 10-15 15:27:17 lpllm.py:1037] reset layer cost 0.0011258125305175781 s
DEBUG 10-15 15:27:17 lpllm.py:1038] have reset next layer layer_attn 6 layer_mlp 6 
DEBUG 10-15 15:27:17 lpllm.py:924] 
DEBUG 10-15 15:27:17 lpllm.py:924] decoder loop j: 13 cur_layer_idx: 6 layer_attn_idx: 6 layer_mlp_idx: 6
DEBUG 10-15 15:27:17 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:17 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:17 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:17 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:17 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:17 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:17 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:17 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:17 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:17 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:17 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:17 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:17 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:17 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:17 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:17 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:17 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:17 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:17 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:17 lpllm.py:2265] GPU2CPU move cost 0.000497 seconds
DEBUG 10-15 15:27:17 lpmodule.py:340] layer_idx 6 attn layer_idx: 6
DEBUG 10-15 15:27:17 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:17 lpmodule.py:364] decoder_attn_batch update batch_dim 300-360
DEBUG 10-15 15:27:17 StaticCacheLen.py:84] static cache update layer_idx: 6, start_batch: 300, end_batch: 360
DEBUG 10-15 15:27:17 lpmodule.py:368] update for kv cache 300-360 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:17 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:17 lpmodule.py:374] update past key value cost 0.023685 seconds
DEBUG 10-15 15:27:17 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:17 lpmodule.py:227] layer idx 6
DEBUG 10-15 15:27:17 lpmodule.py:399] repeat qkv cost 0.032723 seconds
DEBUG 10-15 15:27:17 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:17 lpmodule.py:433] dot attn cost 0.034633 seconds
DEBUG 10-15 15:27:17 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:17 lpmodule.py:444] time cost move to cuda:1 0.02586984634399414 s
DEBUG 10-15 15:27:17 lpllm.py:2283] CPU attn cost 0.143143 seconds if batch True
DEBUG 10-15 15:27:17 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:17 lpllm.py:2294] CPU compute cost 0.143924 seconds
DEBUG 10-15 15:27:17 lpllm.py:2312] free cost 0.000081 seconds
DEBUG 10-15 15:27:17 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:17 lpllm.py:1774] update state cost 3.504753112792969e-05 s
DEBUG 10-15 15:27:17 lpllm.py:1743] restore layer func cost 0.0003802776336669922 s
DEBUG 10-15 15:27:17 lpllm.py:511] restore layer cost 0.0006566047668457031 s
DEBUG 10-15 15:27:17 lpllm.py:512] reset_next_layer_need: layer_attn_idx=7, layer_mlp_idx=6, j_loc=13
DEBUG 10-15 15:27:17 lpllm.py:1037] reset layer cost 0.0007255077362060547 s
DEBUG 10-15 15:27:17 lpllm.py:1038] have reset next layer layer_attn 7 layer_mlp 6 
DEBUG 10-15 15:27:17 lpllm.py:1044] j: 13 waiting the layer with layer_idx 7 before wait time 0.45459413528442383 s
INFO 10-15 15:27:17 client.py:117] confirm_model_loaded: Mixtral-8x7B, 06b0befd-33e4-4982-99ba-011358039b82
INFO 10-15 15:27:17 client.py:125] Model loaded
DEBUG 10-15 15:27:17 lpllm.py:1048] j: load cost 0.45630621910095215 s waiting cost 0.001697540283203125 s
DEBUG 10-15 15:27:17 lpllm.py:924] 
DEBUG 10-15 15:27:17 lpllm.py:924] decoder loop j: 14 cur_layer_idx: 7 layer_attn_idx: 7 layer_mlp_idx: 6
DEBUG 10-15 15:27:17 lpllm.py:933] start load next layer cur_layer_idx: 8
DEBUG 10-15 15:27:17 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:17 client.py:72] load_into_gpu: Mixtral-8x7B, 628d4a54-b924-4645-8ff3-fde529a15c21
INFO 10-15 15:27:17 client.py:113] Model loaded: Mixtral-8x7B, 628d4a54-b924-4645-8ff3-fde529a15c21
DEBUG 10-15 15:27:17 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:17 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:17 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:17 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:17 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:17 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:17 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:17 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:17 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:17 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:17 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:17 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:17 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:17 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:17 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:17 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:17 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:17 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:17 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:17 lpllm.py:2265] GPU2CPU move cost 0.000571 seconds
DEBUG 10-15 15:27:17 lpmodule.py:340] layer_idx 7 attn layer_idx: 7
DEBUG 10-15 15:27:17 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:17 lpmodule.py:364] decoder_attn_batch update batch_dim 240-300
DEBUG 10-15 15:27:17 StaticCacheLen.py:84] static cache update layer_idx: 7, start_batch: 240, end_batch: 300
DEBUG 10-15 15:27:17 lpmodule.py:368] update for kv cache 240-300 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:17 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:17 lpmodule.py:374] update past key value cost 0.025612 seconds
DEBUG 10-15 15:27:17 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:17 lpmodule.py:227] layer idx 6
DEBUG 10-15 15:27:17 lpmodule.py:399] repeat qkv cost 0.030806 seconds
DEBUG 10-15 15:27:17 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:17 lpmodule.py:433] dot attn cost 0.045225 seconds
DEBUG 10-15 15:27:17 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:17 lpmodule.py:444] time cost move to cuda:1 0.027997970581054688 s
DEBUG 10-15 15:27:18 lpllm.py:2283] CPU attn cost 0.155610 seconds if batch True
DEBUG 10-15 15:27:18 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:18 lpllm.py:2294] CPU compute cost 0.156467 seconds
DEBUG 10-15 15:27:18 lpllm.py:2312] free cost 0.000083 seconds
DEBUG 10-15 15:27:18 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:18 lpllm.py:1774] update state cost 3.4809112548828125e-05 s
DEBUG 10-15 15:27:18 lpllm.py:1743] restore layer func cost 0.0008127689361572266 s
DEBUG 10-15 15:27:18 lpllm.py:511] restore layer cost 0.0010781288146972656 s
DEBUG 10-15 15:27:18 lpllm.py:512] reset_next_layer_need: layer_attn_idx=7, layer_mlp_idx=7, j_loc=14
DEBUG 10-15 15:27:18 lpllm.py:1037] reset layer cost 0.0011479854583740234 s
DEBUG 10-15 15:27:18 lpllm.py:1038] have reset next layer layer_attn 7 layer_mlp 7 
DEBUG 10-15 15:27:18 lpllm.py:924] 
DEBUG 10-15 15:27:18 lpllm.py:924] decoder loop j: 15 cur_layer_idx: 7 layer_attn_idx: 7 layer_mlp_idx: 7
DEBUG 10-15 15:27:18 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:18 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:18 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:18 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:18 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:18 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:18 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:18 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:18 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:18 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:18 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:18 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:18 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:18 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:18 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:18 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:18 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:18 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:18 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:18 lpllm.py:2265] GPU2CPU move cost 0.000580 seconds
DEBUG 10-15 15:27:18 lpmodule.py:340] layer_idx 7 attn layer_idx: 7
DEBUG 10-15 15:27:18 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:18 lpmodule.py:364] decoder_attn_batch update batch_dim 300-360
DEBUG 10-15 15:27:18 StaticCacheLen.py:84] static cache update layer_idx: 7, start_batch: 300, end_batch: 360
DEBUG 10-15 15:27:18 lpmodule.py:368] update for kv cache 300-360 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:18 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:18 lpmodule.py:374] update past key value cost 0.026410 seconds
DEBUG 10-15 15:27:18 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:18 lpmodule.py:227] layer idx 7
DEBUG 10-15 15:27:18 lpmodule.py:399] repeat qkv cost 0.030779 seconds
DEBUG 10-15 15:27:18 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:18 lpmodule.py:433] dot attn cost 0.034574 seconds
DEBUG 10-15 15:27:18 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:18 lpmodule.py:444] time cost move to cuda:1 0.033438920974731445 s
DEBUG 10-15 15:27:18 lpllm.py:2283] CPU attn cost 0.151478 seconds if batch True
DEBUG 10-15 15:27:18 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:18 lpllm.py:2294] CPU compute cost 0.152358 seconds
DEBUG 10-15 15:27:18 lpllm.py:2312] free cost 0.000081 seconds
DEBUG 10-15 15:27:18 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:18 lpllm.py:1774] update state cost 3.647804260253906e-05 s
DEBUG 10-15 15:27:18 lpllm.py:1743] restore layer func cost 0.0003826618194580078 s
DEBUG 10-15 15:27:18 lpllm.py:511] restore layer cost 0.0006475448608398438 s
DEBUG 10-15 15:27:18 lpllm.py:512] reset_next_layer_need: layer_attn_idx=8, layer_mlp_idx=7, j_loc=15
DEBUG 10-15 15:27:18 lpllm.py:1037] reset layer cost 0.0007169246673583984 s
DEBUG 10-15 15:27:18 lpllm.py:1038] have reset next layer layer_attn 8 layer_mlp 7 
DEBUG 10-15 15:27:18 lpllm.py:1044] j: 15 waiting the layer with layer_idx 8 before wait time 0.4473259449005127 s
INFO 10-15 15:27:18 client.py:117] confirm_model_loaded: Mixtral-8x7B, 628d4a54-b924-4645-8ff3-fde529a15c21
INFO 10-15 15:27:18 client.py:125] Model loaded
DEBUG 10-15 15:27:18 lpllm.py:1048] j: load cost 0.4490649700164795 s waiting cost 0.0017235279083251953 s
DEBUG 10-15 15:27:18 lpllm.py:924] 
DEBUG 10-15 15:27:18 lpllm.py:924] decoder loop j: 16 cur_layer_idx: 8 layer_attn_idx: 8 layer_mlp_idx: 7
DEBUG 10-15 15:27:18 lpllm.py:933] start load next layer cur_layer_idx: 9
DEBUG 10-15 15:27:18 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed650>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x003\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x96\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:18 client.py:72] load_into_gpu: Mixtral-8x7B, ea0a08a9-6cbb-42d2-b002-deab813744c1
INFO 10-15 15:27:18 client.py:113] Model loaded: Mixtral-8x7B, ea0a08a9-6cbb-42d2-b002-deab813744c1
DEBUG 10-15 15:27:18 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:18 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:18 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:18 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:18 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:18 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:18 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:18 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:18 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:18 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:18 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:18 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:18 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:18 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:18 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:18 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:18 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:18 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:18 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:18 lpllm.py:2265] GPU2CPU move cost 0.000576 seconds
DEBUG 10-15 15:27:18 lpmodule.py:340] layer_idx 8 attn layer_idx: 8
DEBUG 10-15 15:27:18 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:18 lpmodule.py:364] decoder_attn_batch update batch_dim 240-300
DEBUG 10-15 15:27:18 StaticCacheLen.py:84] static cache update layer_idx: 8, start_batch: 240, end_batch: 300
DEBUG 10-15 15:27:18 lpmodule.py:368] update for kv cache 240-300 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:18 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:18 lpmodule.py:374] update past key value cost 0.027199 seconds
DEBUG 10-15 15:27:18 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:18 lpmodule.py:227] layer idx 7
DEBUG 10-15 15:27:18 lpmodule.py:399] repeat qkv cost 0.031080 seconds
DEBUG 10-15 15:27:18 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:18 lpmodule.py:433] dot attn cost 0.035978 seconds
DEBUG 10-15 15:27:18 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:18 lpmodule.py:444] time cost move to cuda:1 0.031032562255859375 s
DEBUG 10-15 15:27:18 lpllm.py:2283] CPU attn cost 0.151406 seconds if batch True
DEBUG 10-15 15:27:18 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:18 lpllm.py:2294] CPU compute cost 0.152265 seconds
DEBUG 10-15 15:27:18 lpllm.py:2312] free cost 0.000082 seconds
DEBUG 10-15 15:27:18 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:18 lpllm.py:1774] update state cost 3.361701965332031e-05 s
DEBUG 10-15 15:27:18 lpllm.py:1743] restore layer func cost 0.0008020401000976562 s
DEBUG 10-15 15:27:18 lpllm.py:511] restore layer cost 0.0010678768157958984 s
DEBUG 10-15 15:27:18 lpllm.py:512] reset_next_layer_need: layer_attn_idx=8, layer_mlp_idx=8, j_loc=16
DEBUG 10-15 15:27:18 lpllm.py:1037] reset layer cost 0.0011377334594726562 s
DEBUG 10-15 15:27:18 lpllm.py:1038] have reset next layer layer_attn 8 layer_mlp 8 
DEBUG 10-15 15:27:18 lpllm.py:924] 
DEBUG 10-15 15:27:18 lpllm.py:924] decoder loop j: 17 cur_layer_idx: 8 layer_attn_idx: 8 layer_mlp_idx: 8
DEBUG 10-15 15:27:18 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:18 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:18 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:18 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:18 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:18 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:18 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:18 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:18 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:18 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:18 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:18 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:18 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:18 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:18 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:18 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:18 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:18 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:18 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:18 lpllm.py:2265] GPU2CPU move cost 0.000584 seconds
DEBUG 10-15 15:27:18 lpmodule.py:340] layer_idx 8 attn layer_idx: 8
DEBUG 10-15 15:27:18 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:18 lpmodule.py:364] decoder_attn_batch update batch_dim 300-360
DEBUG 10-15 15:27:18 StaticCacheLen.py:84] static cache update layer_idx: 8, start_batch: 300, end_batch: 360
DEBUG 10-15 15:27:18 lpmodule.py:368] update for kv cache 300-360 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:18 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:18 lpmodule.py:374] update past key value cost 0.026115 seconds
DEBUG 10-15 15:27:18 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:18 lpmodule.py:227] layer idx 8
DEBUG 10-15 15:27:18 lpmodule.py:399] repeat qkv cost 0.032242 seconds
DEBUG 10-15 15:27:18 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:18 lpmodule.py:433] dot attn cost 0.035156 seconds
DEBUG 10-15 15:27:18 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:18 lpmodule.py:444] time cost move to cuda:1 0.035544395446777344 s
DEBUG 10-15 15:27:18 lpllm.py:2283] CPU attn cost 0.154599 seconds if batch True
DEBUG 10-15 15:27:18 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:18 lpllm.py:2294] CPU compute cost 0.155457 seconds
DEBUG 10-15 15:27:18 lpllm.py:2312] free cost 0.000079 seconds
DEBUG 10-15 15:27:18 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:18 lpllm.py:1774] update state cost 2.0742416381835938e-05 s
DEBUG 10-15 15:27:18 lpllm.py:1743] restore layer func cost 0.00037980079650878906 s
DEBUG 10-15 15:27:18 lpllm.py:511] restore layer cost 0.0006496906280517578 s
DEBUG 10-15 15:27:18 lpllm.py:512] reset_next_layer_need: layer_attn_idx=9, layer_mlp_idx=8, j_loc=17
DEBUG 10-15 15:27:18 lpllm.py:1037] reset layer cost 0.0007183551788330078 s
DEBUG 10-15 15:27:18 lpllm.py:1038] have reset next layer layer_attn 9 layer_mlp 8 
DEBUG 10-15 15:27:18 lpllm.py:1044] j: 17 waiting the layer with layer_idx 9 before wait time 0.448087215423584 s
INFO 10-15 15:27:18 client.py:117] confirm_model_loaded: Mixtral-8x7B, ea0a08a9-6cbb-42d2-b002-deab813744c1
INFO 10-15 15:27:18 client.py:125] Model loaded
DEBUG 10-15 15:27:18 lpllm.py:1048] j: load cost 0.44980907440185547 s waiting cost 0.0017070770263671875 s
DEBUG 10-15 15:27:18 lpllm.py:924] 
DEBUG 10-15 15:27:18 lpllm.py:924] decoder loop j: 18 cur_layer_idx: 9 layer_attn_idx: 9 layer_mlp_idx: 8
DEBUG 10-15 15:27:18 lpllm.py:933] start load next layer cur_layer_idx: 10
DEBUG 10-15 15:27:18 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed6e0>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x004\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x98\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:18 client.py:72] load_into_gpu: Mixtral-8x7B, 3a58a9e1-87bd-467b-93fe-1dd4ca924991
INFO 10-15 15:27:18 client.py:113] Model loaded: Mixtral-8x7B, 3a58a9e1-87bd-467b-93fe-1dd4ca924991
DEBUG 10-15 15:27:18 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:18 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:18 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:18 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:18 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:18 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:18 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:18 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:18 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:18 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:18 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:18 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:18 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:18 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:18 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:18 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:18 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:18 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:18 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:18 lpllm.py:2265] GPU2CPU move cost 0.000583 seconds
DEBUG 10-15 15:27:18 lpmodule.py:340] layer_idx 9 attn layer_idx: 9
DEBUG 10-15 15:27:18 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:18 lpmodule.py:364] decoder_attn_batch update batch_dim 240-300
DEBUG 10-15 15:27:18 StaticCacheLen.py:84] static cache update layer_idx: 9, start_batch: 240, end_batch: 300
DEBUG 10-15 15:27:18 lpmodule.py:368] update for kv cache 240-300 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:18 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:18 lpmodule.py:374] update past key value cost 0.026001 seconds
DEBUG 10-15 15:27:18 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:18 lpmodule.py:227] layer idx 8
DEBUG 10-15 15:27:18 lpmodule.py:399] repeat qkv cost 0.030079 seconds
DEBUG 10-15 15:27:18 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:18 lpmodule.py:433] dot attn cost 0.037449 seconds
DEBUG 10-15 15:27:18 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:18 lpmodule.py:444] time cost move to cuda:1 0.02943563461303711 s
DEBUG 10-15 15:27:18 lpllm.py:2283] CPU attn cost 0.151931 seconds if batch True
DEBUG 10-15 15:27:18 lpllm.py:2292] deal attn result cost 0.000003 seconds
DEBUG 10-15 15:27:18 lpllm.py:2294] CPU compute cost 0.152812 seconds
DEBUG 10-15 15:27:18 lpllm.py:2312] free cost 0.000086 seconds
DEBUG 10-15 15:27:18 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:18 lpllm.py:1774] update state cost 3.314018249511719e-05 s
DEBUG 10-15 15:27:18 lpllm.py:1743] restore layer func cost 0.0008111000061035156 s
DEBUG 10-15 15:27:18 lpllm.py:511] restore layer cost 0.001065969467163086 s
DEBUG 10-15 15:27:18 lpllm.py:512] reset_next_layer_need: layer_attn_idx=9, layer_mlp_idx=9, j_loc=18
DEBUG 10-15 15:27:18 lpllm.py:1037] reset layer cost 0.0011353492736816406 s
DEBUG 10-15 15:27:18 lpllm.py:1038] have reset next layer layer_attn 9 layer_mlp 9 
DEBUG 10-15 15:27:18 lpllm.py:924] 
DEBUG 10-15 15:27:18 lpllm.py:924] decoder loop j: 19 cur_layer_idx: 9 layer_attn_idx: 9 layer_mlp_idx: 9
DEBUG 10-15 15:27:18 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:18 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:18 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:18 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:18 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:18 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:18 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:18 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:18 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:18 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:18 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:18 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:18 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:18 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:18 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:18 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:18 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:18 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:18 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:18 lpllm.py:2265] GPU2CPU move cost 0.000440 seconds
DEBUG 10-15 15:27:18 lpmodule.py:340] layer_idx 9 attn layer_idx: 9
DEBUG 10-15 15:27:18 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:18 lpmodule.py:364] decoder_attn_batch update batch_dim 300-360
DEBUG 10-15 15:27:19 StaticCacheLen.py:84] static cache update layer_idx: 9, start_batch: 300, end_batch: 360
DEBUG 10-15 15:27:19 lpmodule.py:368] update for kv cache 300-360 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:19 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:19 lpmodule.py:374] update past key value cost 0.025323 seconds
DEBUG 10-15 15:27:19 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:19 lpmodule.py:227] layer idx 9
DEBUG 10-15 15:27:19 lpmodule.py:399] repeat qkv cost 0.030875 seconds
DEBUG 10-15 15:27:19 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:19 lpmodule.py:433] dot attn cost 0.033337 seconds
DEBUG 10-15 15:27:19 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:19 lpmodule.py:444] time cost move to cuda:1 0.030676603317260742 s
DEBUG 10-15 15:27:19 lpllm.py:2283] CPU attn cost 0.147690 seconds if batch True
DEBUG 10-15 15:27:19 lpllm.py:2292] deal attn result cost 0.000003 seconds
DEBUG 10-15 15:27:19 lpllm.py:2294] CPU compute cost 0.148373 seconds
DEBUG 10-15 15:27:19 lpllm.py:2312] free cost 0.000081 seconds
DEBUG 10-15 15:27:19 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:19 lpllm.py:1774] update state cost 3.6716461181640625e-05 s
DEBUG 10-15 15:27:19 lpllm.py:1743] restore layer func cost 0.00038743019104003906 s
DEBUG 10-15 15:27:19 lpllm.py:511] restore layer cost 0.0006625652313232422 s
DEBUG 10-15 15:27:19 lpllm.py:512] reset_next_layer_need: layer_attn_idx=10, layer_mlp_idx=9, j_loc=19
DEBUG 10-15 15:27:19 lpllm.py:1037] reset layer cost 0.0007314682006835938 s
DEBUG 10-15 15:27:19 lpllm.py:1038] have reset next layer layer_attn 10 layer_mlp 9 
DEBUG 10-15 15:27:19 lpllm.py:1044] j: 19 waiting the layer with layer_idx 10 before wait time 0.44579005241394043 s
INFO 10-15 15:27:19 client.py:117] confirm_model_loaded: Mixtral-8x7B, 3a58a9e1-87bd-467b-93fe-1dd4ca924991
INFO 10-15 15:27:19 client.py:125] Model loaded
DEBUG 10-15 15:27:19 lpllm.py:1048] j: load cost 0.447495698928833 s waiting cost 0.0016906261444091797 s
DEBUG 10-15 15:27:19 lpllm.py:924] 
DEBUG 10-15 15:27:19 lpllm.py:924] decoder loop j: 20 cur_layer_idx: 10 layer_attn_idx: 10 layer_mlp_idx: 9
DEBUG 10-15 15:27:19 lpllm.py:933] start load next layer cur_layer_idx: 11
DEBUG 10-15 15:27:19 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:19 client.py:72] load_into_gpu: Mixtral-8x7B, 9ce2f0d0-1fb9-41de-9dff-f71d112cf0d6
INFO 10-15 15:27:19 client.py:113] Model loaded: Mixtral-8x7B, 9ce2f0d0-1fb9-41de-9dff-f71d112cf0d6
DEBUG 10-15 15:27:19 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:19 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:19 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:19 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:19 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:19 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:19 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:19 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:19 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:19 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:19 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:19 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:19 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:19 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:19 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:19 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:19 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:19 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:19 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:19 lpllm.py:2265] GPU2CPU move cost 0.000569 seconds
DEBUG 10-15 15:27:19 lpmodule.py:340] layer_idx 10 attn layer_idx: 10
DEBUG 10-15 15:27:19 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:19 lpmodule.py:364] decoder_attn_batch update batch_dim 240-300
DEBUG 10-15 15:27:19 StaticCacheLen.py:84] static cache update layer_idx: 10, start_batch: 240, end_batch: 300
DEBUG 10-15 15:27:19 lpmodule.py:368] update for kv cache 240-300 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:19 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:19 lpmodule.py:374] update past key value cost 0.027982 seconds
DEBUG 10-15 15:27:19 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:19 lpmodule.py:227] layer idx 9
DEBUG 10-15 15:27:19 lpmodule.py:399] repeat qkv cost 0.031993 seconds
DEBUG 10-15 15:27:19 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:19 lpmodule.py:433] dot attn cost 0.037124 seconds
DEBUG 10-15 15:27:19 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:19 lpmodule.py:444] time cost move to cuda:1 0.02765369415283203 s
DEBUG 10-15 15:27:19 lpllm.py:2283] CPU attn cost 0.150807 seconds if batch True
DEBUG 10-15 15:27:19 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:19 lpllm.py:2294] CPU compute cost 0.151683 seconds
DEBUG 10-15 15:27:19 lpllm.py:2312] free cost 0.000084 seconds
DEBUG 10-15 15:27:19 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:19 lpllm.py:1774] update state cost 3.3855438232421875e-05 s
DEBUG 10-15 15:27:19 lpllm.py:1743] restore layer func cost 0.000812530517578125 s
DEBUG 10-15 15:27:19 lpllm.py:511] restore layer cost 0.0010726451873779297 s
DEBUG 10-15 15:27:19 lpllm.py:512] reset_next_layer_need: layer_attn_idx=10, layer_mlp_idx=10, j_loc=20
DEBUG 10-15 15:27:19 lpllm.py:1037] reset layer cost 0.0011432170867919922 s
DEBUG 10-15 15:27:19 lpllm.py:1038] have reset next layer layer_attn 10 layer_mlp 10 
DEBUG 10-15 15:27:19 lpllm.py:924] 
DEBUG 10-15 15:27:19 lpllm.py:924] decoder loop j: 21 cur_layer_idx: 10 layer_attn_idx: 10 layer_mlp_idx: 10
DEBUG 10-15 15:27:19 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:19 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:19 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:19 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:19 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:19 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:19 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:19 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:19 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:19 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:19 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:19 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:19 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:19 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:19 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:19 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:19 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:19 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:19 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:19 lpllm.py:2265] GPU2CPU move cost 0.000540 seconds
DEBUG 10-15 15:27:19 lpmodule.py:340] layer_idx 10 attn layer_idx: 10
DEBUG 10-15 15:27:19 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:19 lpmodule.py:364] decoder_attn_batch update batch_dim 300-360
DEBUG 10-15 15:27:19 StaticCacheLen.py:84] static cache update layer_idx: 10, start_batch: 300, end_batch: 360
DEBUG 10-15 15:27:19 lpmodule.py:368] update for kv cache 300-360 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:19 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:19 lpmodule.py:374] update past key value cost 0.024110 seconds
DEBUG 10-15 15:27:19 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:19 lpmodule.py:227] layer idx 10
DEBUG 10-15 15:27:19 lpmodule.py:399] repeat qkv cost 0.031351 seconds
DEBUG 10-15 15:27:19 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:19 lpmodule.py:433] dot attn cost 0.039359 seconds
DEBUG 10-15 15:27:19 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:19 lpmodule.py:444] time cost move to cuda:1 0.022922039031982422 s
DEBUG 10-15 15:27:19 lpllm.py:2283] CPU attn cost 0.143839 seconds if batch True
DEBUG 10-15 15:27:19 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:19 lpllm.py:2294] CPU compute cost 0.144596 seconds
DEBUG 10-15 15:27:19 lpllm.py:2312] free cost 0.000084 seconds
DEBUG 10-15 15:27:19 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:19 lpllm.py:1774] update state cost 2.6226043701171875e-05 s
DEBUG 10-15 15:27:19 lpllm.py:1743] restore layer func cost 0.00038695335388183594 s
DEBUG 10-15 15:27:19 lpllm.py:511] restore layer cost 0.0006737709045410156 s
DEBUG 10-15 15:27:19 lpllm.py:512] reset_next_layer_need: layer_attn_idx=11, layer_mlp_idx=10, j_loc=21
DEBUG 10-15 15:27:19 lpllm.py:1037] reset layer cost 0.0007612705230712891 s
DEBUG 10-15 15:27:19 lpllm.py:1038] have reset next layer layer_attn 11 layer_mlp 10 
DEBUG 10-15 15:27:19 lpllm.py:1044] j: 21 waiting the layer with layer_idx 11 before wait time 0.44415879249572754 s
INFO 10-15 15:27:19 client.py:117] confirm_model_loaded: Mixtral-8x7B, 9ce2f0d0-1fb9-41de-9dff-f71d112cf0d6
INFO 10-15 15:27:19 client.py:125] Model loaded
DEBUG 10-15 15:27:19 lpllm.py:1048] j: load cost 0.44594287872314453 s waiting cost 0.0017664432525634766 s
DEBUG 10-15 15:27:19 lpllm.py:924] 
DEBUG 10-15 15:27:19 lpllm.py:924] decoder loop j: 22 cur_layer_idx: 11 layer_attn_idx: 11 layer_mlp_idx: 10
DEBUG 10-15 15:27:19 lpllm.py:933] start load next layer cur_layer_idx: 12
DEBUG 10-15 15:27:19 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed650>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x003\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x96\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:19 client.py:72] load_into_gpu: Mixtral-8x7B, 5e11858c-7883-48f8-be3d-92be5fcc432c
INFO 10-15 15:27:19 client.py:113] Model loaded: Mixtral-8x7B, 5e11858c-7883-48f8-be3d-92be5fcc432c
DEBUG 10-15 15:27:19 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:19 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:19 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:19 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:19 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:19 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:19 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:19 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:19 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:19 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:19 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:19 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:19 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:19 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:19 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:19 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:19 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:19 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:19 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:19 lpllm.py:2265] GPU2CPU move cost 0.000581 seconds
DEBUG 10-15 15:27:19 lpmodule.py:340] layer_idx 11 attn layer_idx: 11
DEBUG 10-15 15:27:19 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:19 lpmodule.py:364] decoder_attn_batch update batch_dim 240-300
DEBUG 10-15 15:27:19 StaticCacheLen.py:84] static cache update layer_idx: 11, start_batch: 240, end_batch: 300
DEBUG 10-15 15:27:19 lpmodule.py:368] update for kv cache 240-300 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:19 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:19 lpmodule.py:374] update past key value cost 0.024542 seconds
DEBUG 10-15 15:27:19 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:19 lpmodule.py:227] layer idx 10
DEBUG 10-15 15:27:19 lpmodule.py:399] repeat qkv cost 0.030537 seconds
DEBUG 10-15 15:27:19 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:19 lpmodule.py:433] dot attn cost 0.043380 seconds
DEBUG 10-15 15:27:19 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:19 lpmodule.py:444] time cost move to cuda:1 0.027917146682739258 s
DEBUG 10-15 15:27:19 lpllm.py:2283] CPU attn cost 0.152960 seconds if batch True
DEBUG 10-15 15:27:19 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:19 lpllm.py:2294] CPU compute cost 0.153860 seconds
DEBUG 10-15 15:27:19 lpllm.py:2312] free cost 0.000088 seconds
DEBUG 10-15 15:27:19 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:19 lpllm.py:1774] update state cost 3.24249267578125e-05 s
DEBUG 10-15 15:27:19 lpllm.py:1743] restore layer func cost 0.0008065700531005859 s
DEBUG 10-15 15:27:19 lpllm.py:511] restore layer cost 0.0010554790496826172 s
DEBUG 10-15 15:27:19 lpllm.py:512] reset_next_layer_need: layer_attn_idx=11, layer_mlp_idx=11, j_loc=22
DEBUG 10-15 15:27:19 lpllm.py:1037] reset layer cost 0.0011243820190429688 s
DEBUG 10-15 15:27:19 lpllm.py:1038] have reset next layer layer_attn 11 layer_mlp 11 
DEBUG 10-15 15:27:19 lpllm.py:924] 
DEBUG 10-15 15:27:19 lpllm.py:924] decoder loop j: 23 cur_layer_idx: 11 layer_attn_idx: 11 layer_mlp_idx: 11
DEBUG 10-15 15:27:19 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:19 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:19 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:19 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:19 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:19 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:19 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:19 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:19 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:19 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:19 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:19 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:19 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:19 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:19 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:19 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:19 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:19 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:19 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:19 lpllm.py:2265] GPU2CPU move cost 0.000487 seconds
DEBUG 10-15 15:27:19 lpmodule.py:340] layer_idx 11 attn layer_idx: 11
DEBUG 10-15 15:27:19 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:19 lpmodule.py:364] decoder_attn_batch update batch_dim 300-360
DEBUG 10-15 15:27:19 StaticCacheLen.py:84] static cache update layer_idx: 11, start_batch: 300, end_batch: 360
DEBUG 10-15 15:27:19 lpmodule.py:368] update for kv cache 300-360 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:19 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:19 lpmodule.py:374] update past key value cost 0.025410 seconds
DEBUG 10-15 15:27:19 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:19 lpmodule.py:227] layer idx 11
DEBUG 10-15 15:27:19 lpmodule.py:399] repeat qkv cost 0.030433 seconds
DEBUG 10-15 15:27:19 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:19 lpmodule.py:433] dot attn cost 0.036010 seconds
DEBUG 10-15 15:27:19 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:19 lpmodule.py:444] time cost move to cuda:1 0.03474092483520508 s
DEBUG 10-15 15:27:20 lpllm.py:2283] CPU attn cost 0.152597 seconds if batch True
DEBUG 10-15 15:27:20 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:20 lpllm.py:2294] CPU compute cost 0.153350 seconds
DEBUG 10-15 15:27:20 lpllm.py:2312] free cost 0.000082 seconds
DEBUG 10-15 15:27:20 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:20 lpllm.py:1774] update state cost 3.647804260253906e-05 s
DEBUG 10-15 15:27:20 lpllm.py:1743] restore layer func cost 0.0003814697265625 s
DEBUG 10-15 15:27:20 lpllm.py:511] restore layer cost 0.0006616115570068359 s
DEBUG 10-15 15:27:20 lpllm.py:512] reset_next_layer_need: layer_attn_idx=12, layer_mlp_idx=11, j_loc=23
DEBUG 10-15 15:27:20 lpllm.py:1037] reset layer cost 0.0007336139678955078 s
DEBUG 10-15 15:27:20 lpllm.py:1038] have reset next layer layer_attn 12 layer_mlp 11 
DEBUG 10-15 15:27:20 lpllm.py:1044] j: 23 waiting the layer with layer_idx 12 before wait time 0.44617509841918945 s
INFO 10-15 15:27:20 client.py:117] confirm_model_loaded: Mixtral-8x7B, 5e11858c-7883-48f8-be3d-92be5fcc432c
INFO 10-15 15:27:20 client.py:125] Model loaded
DEBUG 10-15 15:27:20 lpllm.py:1048] j: load cost 0.4478418827056885 s waiting cost 0.0016515254974365234 s
DEBUG 10-15 15:27:20 lpllm.py:924] 
DEBUG 10-15 15:27:20 lpllm.py:924] decoder loop j: 24 cur_layer_idx: 12 layer_attn_idx: 12 layer_mlp_idx: 11
DEBUG 10-15 15:27:20 lpllm.py:933] start load next layer cur_layer_idx: 13
DEBUG 10-15 15:27:20 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed6e0>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x004\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x98\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:20 client.py:72] load_into_gpu: Mixtral-8x7B, 5c670ab1-f504-4479-8e69-c8cce9de5433
INFO 10-15 15:27:20 client.py:113] Model loaded: Mixtral-8x7B, 5c670ab1-f504-4479-8e69-c8cce9de5433
DEBUG 10-15 15:27:20 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:20 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:20 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:20 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:20 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:20 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:20 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:20 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:20 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:20 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:20 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:20 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:20 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:20 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:20 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:20 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:20 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:20 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:20 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:20 lpllm.py:2265] GPU2CPU move cost 0.000580 seconds
DEBUG 10-15 15:27:20 lpmodule.py:340] layer_idx 12 attn layer_idx: 12
DEBUG 10-15 15:27:20 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:20 lpmodule.py:364] decoder_attn_batch update batch_dim 240-300
DEBUG 10-15 15:27:20 StaticCacheLen.py:84] static cache update layer_idx: 12, start_batch: 240, end_batch: 300
DEBUG 10-15 15:27:20 lpmodule.py:368] update for kv cache 240-300 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:20 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:20 lpmodule.py:374] update past key value cost 0.025090 seconds
DEBUG 10-15 15:27:20 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:20 lpmodule.py:227] layer idx 11
DEBUG 10-15 15:27:20 lpmodule.py:399] repeat qkv cost 0.030020 seconds
DEBUG 10-15 15:27:20 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:20 lpmodule.py:433] dot attn cost 0.044211 seconds
DEBUG 10-15 15:27:20 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:20 lpmodule.py:444] time cost move to cuda:1 0.02564859390258789 s
DEBUG 10-15 15:27:20 lpllm.py:2283] CPU attn cost 0.151517 seconds if batch True
DEBUG 10-15 15:27:20 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:20 lpllm.py:2294] CPU compute cost 0.152384 seconds
DEBUG 10-15 15:27:20 lpllm.py:2312] free cost 0.000085 seconds
DEBUG 10-15 15:27:20 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:20 lpllm.py:1774] update state cost 3.361701965332031e-05 s
DEBUG 10-15 15:27:20 lpllm.py:1743] restore layer func cost 0.0008068084716796875 s
DEBUG 10-15 15:27:20 lpllm.py:511] restore layer cost 0.0010662078857421875 s
DEBUG 10-15 15:27:20 lpllm.py:512] reset_next_layer_need: layer_attn_idx=12, layer_mlp_idx=12, j_loc=24
DEBUG 10-15 15:27:20 lpllm.py:1037] reset layer cost 0.0011348724365234375 s
DEBUG 10-15 15:27:20 lpllm.py:1038] have reset next layer layer_attn 12 layer_mlp 12 
DEBUG 10-15 15:27:20 lpllm.py:924] 
DEBUG 10-15 15:27:20 lpllm.py:924] decoder loop j: 25 cur_layer_idx: 12 layer_attn_idx: 12 layer_mlp_idx: 12
DEBUG 10-15 15:27:20 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:20 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:20 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:20 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:20 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:20 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:20 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:20 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:20 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:20 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:20 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:20 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:20 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:20 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:20 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:20 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:20 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:20 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:20 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:20 lpllm.py:2265] GPU2CPU move cost 0.000377 seconds
DEBUG 10-15 15:27:20 lpmodule.py:340] layer_idx 12 attn layer_idx: 12
DEBUG 10-15 15:27:20 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:20 lpmodule.py:364] decoder_attn_batch update batch_dim 300-360
DEBUG 10-15 15:27:20 StaticCacheLen.py:84] static cache update layer_idx: 12, start_batch: 300, end_batch: 360
DEBUG 10-15 15:27:20 lpmodule.py:368] update for kv cache 300-360 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:20 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:20 lpmodule.py:374] update past key value cost 0.022625 seconds
DEBUG 10-15 15:27:20 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:20 lpmodule.py:227] layer idx 12
DEBUG 10-15 15:27:20 lpmodule.py:399] repeat qkv cost 0.029533 seconds
DEBUG 10-15 15:27:20 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:20 lpmodule.py:433] dot attn cost 0.040482 seconds
DEBUG 10-15 15:27:20 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:20 lpmodule.py:444] time cost move to cuda:1 0.027035951614379883 s
DEBUG 10-15 15:27:20 lpllm.py:2283] CPU attn cost 0.153415 seconds if batch True
DEBUG 10-15 15:27:20 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:27:20 lpllm.py:2294] CPU compute cost 0.154009 seconds
DEBUG 10-15 15:27:20 lpllm.py:2312] free cost 0.000080 seconds
DEBUG 10-15 15:27:20 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:20 lpllm.py:1774] update state cost 2.7418136596679688e-05 s
DEBUG 10-15 15:27:20 lpllm.py:1743] restore layer func cost 0.0003857612609863281 s
DEBUG 10-15 15:27:20 lpllm.py:511] restore layer cost 0.0006918907165527344 s
DEBUG 10-15 15:27:20 lpllm.py:512] reset_next_layer_need: layer_attn_idx=13, layer_mlp_idx=12, j_loc=25
DEBUG 10-15 15:27:20 lpllm.py:1037] reset layer cost 0.0007736682891845703 s
DEBUG 10-15 15:27:20 lpllm.py:1038] have reset next layer layer_attn 13 layer_mlp 12 
DEBUG 10-15 15:27:20 lpllm.py:1044] j: 25 waiting the layer with layer_idx 13 before wait time 0.4470360279083252 s
INFO 10-15 15:27:20 client.py:117] confirm_model_loaded: Mixtral-8x7B, 5c670ab1-f504-4479-8e69-c8cce9de5433
INFO 10-15 15:27:20 client.py:125] Model loaded
DEBUG 10-15 15:27:20 lpllm.py:1048] j: load cost 0.4489552974700928 s waiting cost 0.0019023418426513672 s
DEBUG 10-15 15:27:20 lpllm.py:924] 
DEBUG 10-15 15:27:20 lpllm.py:924] decoder loop j: 26 cur_layer_idx: 13 layer_attn_idx: 13 layer_mlp_idx: 12
DEBUG 10-15 15:27:20 lpllm.py:933] start load next layer cur_layer_idx: 14
DEBUG 10-15 15:27:20 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:20 client.py:72] load_into_gpu: Mixtral-8x7B, c43c4ce0-841b-44e0-be19-7c4022b9c108
INFO 10-15 15:27:20 client.py:113] Model loaded: Mixtral-8x7B, c43c4ce0-841b-44e0-be19-7c4022b9c108
DEBUG 10-15 15:27:20 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:20 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:20 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:20 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:20 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:20 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:20 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:20 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:20 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:20 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:20 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:20 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:20 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:20 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:20 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:20 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:20 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:20 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:20 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:20 lpllm.py:2265] GPU2CPU move cost 0.000579 seconds
DEBUG 10-15 15:27:20 lpmodule.py:340] layer_idx 13 attn layer_idx: 13
DEBUG 10-15 15:27:20 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:20 lpmodule.py:364] decoder_attn_batch update batch_dim 240-300
DEBUG 10-15 15:27:20 StaticCacheLen.py:84] static cache update layer_idx: 13, start_batch: 240, end_batch: 300
DEBUG 10-15 15:27:20 lpmodule.py:368] update for kv cache 240-300 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:20 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:20 lpmodule.py:374] update past key value cost 0.024400 seconds
DEBUG 10-15 15:27:20 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:20 lpmodule.py:227] layer idx 12
DEBUG 10-15 15:27:20 lpmodule.py:399] repeat qkv cost 0.030841 seconds
DEBUG 10-15 15:27:20 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:20 lpmodule.py:433] dot attn cost 0.036009 seconds
DEBUG 10-15 15:27:20 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:20 lpmodule.py:444] time cost move to cuda:1 0.024888038635253906 s
DEBUG 10-15 15:27:20 lpllm.py:2283] CPU attn cost 0.142930 seconds if batch True
DEBUG 10-15 15:27:20 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:20 lpllm.py:2294] CPU compute cost 0.143781 seconds
DEBUG 10-15 15:27:20 lpllm.py:2312] free cost 0.000081 seconds
DEBUG 10-15 15:27:20 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:20 lpllm.py:1774] update state cost 3.361701965332031e-05 s
DEBUG 10-15 15:27:20 lpllm.py:1743] restore layer func cost 0.0008099079132080078 s
DEBUG 10-15 15:27:20 lpllm.py:511] restore layer cost 0.0010671615600585938 s
DEBUG 10-15 15:27:20 lpllm.py:512] reset_next_layer_need: layer_attn_idx=13, layer_mlp_idx=13, j_loc=26
DEBUG 10-15 15:27:20 lpllm.py:1037] reset layer cost 0.0011360645294189453 s
DEBUG 10-15 15:27:20 lpllm.py:1038] have reset next layer layer_attn 13 layer_mlp 13 
DEBUG 10-15 15:27:20 lpllm.py:924] 
DEBUG 10-15 15:27:20 lpllm.py:924] decoder loop j: 27 cur_layer_idx: 13 layer_attn_idx: 13 layer_mlp_idx: 13
DEBUG 10-15 15:27:20 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:20 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:20 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:20 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:20 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:20 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:20 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:20 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:20 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:20 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:20 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:20 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:20 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:20 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:20 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:20 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:20 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:20 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:20 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:20 lpllm.py:2265] GPU2CPU move cost 0.000590 seconds
DEBUG 10-15 15:27:20 lpmodule.py:340] layer_idx 13 attn layer_idx: 13
DEBUG 10-15 15:27:20 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:20 lpmodule.py:364] decoder_attn_batch update batch_dim 300-360
DEBUG 10-15 15:27:20 StaticCacheLen.py:84] static cache update layer_idx: 13, start_batch: 300, end_batch: 360
DEBUG 10-15 15:27:20 lpmodule.py:368] update for kv cache 300-360 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:20 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:20 lpmodule.py:374] update past key value cost 0.024722 seconds
DEBUG 10-15 15:27:20 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:20 lpmodule.py:227] layer idx 13
DEBUG 10-15 15:27:20 lpmodule.py:399] repeat qkv cost 0.031391 seconds
DEBUG 10-15 15:27:20 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:20 lpmodule.py:433] dot attn cost 0.034135 seconds
DEBUG 10-15 15:27:20 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:20 lpmodule.py:444] time cost move to cuda:1 0.02278590202331543 s
DEBUG 10-15 15:27:20 lpllm.py:2283] CPU attn cost 0.139940 seconds if batch True
DEBUG 10-15 15:27:20 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:20 lpllm.py:2294] CPU compute cost 0.140862 seconds
DEBUG 10-15 15:27:20 lpllm.py:2312] free cost 0.000088 seconds
DEBUG 10-15 15:27:20 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:20 lpllm.py:1774] update state cost 2.47955322265625e-05 s
DEBUG 10-15 15:27:20 lpllm.py:1743] restore layer func cost 0.00036597251892089844 s
DEBUG 10-15 15:27:20 lpllm.py:511] restore layer cost 0.0006489753723144531 s
DEBUG 10-15 15:27:20 lpllm.py:512] reset_next_layer_need: layer_attn_idx=14, layer_mlp_idx=13, j_loc=27
DEBUG 10-15 15:27:20 lpllm.py:1037] reset layer cost 0.000728607177734375 s
DEBUG 10-15 15:27:20 lpllm.py:1038] have reset next layer layer_attn 14 layer_mlp 13 
DEBUG 10-15 15:27:20 lpllm.py:1044] j: 27 waiting the layer with layer_idx 14 before wait time 0.44654130935668945 s
INFO 10-15 15:27:20 client.py:117] confirm_model_loaded: Mixtral-8x7B, c43c4ce0-841b-44e0-be19-7c4022b9c108
INFO 10-15 15:27:20 client.py:125] Model loaded
DEBUG 10-15 15:27:20 lpllm.py:1048] j: load cost 0.44832324981689453 s waiting cost 0.0017652511596679688 s
DEBUG 10-15 15:27:20 lpllm.py:924] 
DEBUG 10-15 15:27:20 lpllm.py:924] decoder loop j: 28 cur_layer_idx: 14 layer_attn_idx: 14 layer_mlp_idx: 13
DEBUG 10-15 15:27:20 lpllm.py:933] start load next layer cur_layer_idx: 15
DEBUG 10-15 15:27:20 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed650>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x003\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x96\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:20 client.py:72] load_into_gpu: Mixtral-8x7B, 75f18747-4474-49eb-b7ea-4afdf1983174
INFO 10-15 15:27:20 client.py:113] Model loaded: Mixtral-8x7B, 75f18747-4474-49eb-b7ea-4afdf1983174
DEBUG 10-15 15:27:20 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:20 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:20 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:20 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:20 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:20 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:20 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:20 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:20 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:20 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:20 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:20 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:20 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:20 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:20 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:20 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:20 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:20 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:20 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:20 lpllm.py:2265] GPU2CPU move cost 0.000483 seconds
DEBUG 10-15 15:27:20 lpmodule.py:340] layer_idx 14 attn layer_idx: 14
DEBUG 10-15 15:27:20 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:20 lpmodule.py:364] decoder_attn_batch update batch_dim 240-300
DEBUG 10-15 15:27:21 StaticCacheLen.py:84] static cache update layer_idx: 14, start_batch: 240, end_batch: 300
DEBUG 10-15 15:27:21 lpmodule.py:368] update for kv cache 240-300 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:21 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:21 lpmodule.py:374] update past key value cost 0.025582 seconds
DEBUG 10-15 15:27:21 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:21 lpmodule.py:227] layer idx 13
DEBUG 10-15 15:27:21 lpmodule.py:399] repeat qkv cost 0.030475 seconds
DEBUG 10-15 15:27:21 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:21 lpmodule.py:433] dot attn cost 0.037773 seconds
DEBUG 10-15 15:27:21 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:21 lpmodule.py:444] time cost move to cuda:1 0.022936105728149414 s
DEBUG 10-15 15:27:21 lpllm.py:2283] CPU attn cost 0.143325 seconds if batch True
DEBUG 10-15 15:27:21 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:21 lpllm.py:2294] CPU compute cost 0.144063 seconds
DEBUG 10-15 15:27:21 lpllm.py:2312] free cost 0.000085 seconds
DEBUG 10-15 15:27:21 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:21 lpllm.py:1774] update state cost 3.552436828613281e-05 s
DEBUG 10-15 15:27:21 lpllm.py:1743] restore layer func cost 0.0008082389831542969 s
DEBUG 10-15 15:27:21 lpllm.py:511] restore layer cost 0.0010781288146972656 s
DEBUG 10-15 15:27:21 lpllm.py:512] reset_next_layer_need: layer_attn_idx=14, layer_mlp_idx=14, j_loc=28
DEBUG 10-15 15:27:21 lpllm.py:1037] reset layer cost 0.0011489391326904297 s
DEBUG 10-15 15:27:21 lpllm.py:1038] have reset next layer layer_attn 14 layer_mlp 14 
DEBUG 10-15 15:27:21 lpllm.py:924] 
DEBUG 10-15 15:27:21 lpllm.py:924] decoder loop j: 29 cur_layer_idx: 14 layer_attn_idx: 14 layer_mlp_idx: 14
DEBUG 10-15 15:27:21 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:21 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:21 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:21 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:21 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:21 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:21 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:21 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:21 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:21 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:21 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:21 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:21 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:21 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:21 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:21 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:21 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:21 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:21 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:21 lpllm.py:2265] GPU2CPU move cost 0.000567 seconds
DEBUG 10-15 15:27:21 lpmodule.py:340] layer_idx 14 attn layer_idx: 14
DEBUG 10-15 15:27:21 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:21 lpmodule.py:364] decoder_attn_batch update batch_dim 300-360
DEBUG 10-15 15:27:21 StaticCacheLen.py:84] static cache update layer_idx: 14, start_batch: 300, end_batch: 360
DEBUG 10-15 15:27:21 lpmodule.py:368] update for kv cache 300-360 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:21 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:21 lpmodule.py:374] update past key value cost 0.022699 seconds
DEBUG 10-15 15:27:21 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:21 lpmodule.py:227] layer idx 14
DEBUG 10-15 15:27:21 lpmodule.py:399] repeat qkv cost 0.034137 seconds
DEBUG 10-15 15:27:21 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:21 lpmodule.py:433] dot attn cost 0.035991 seconds
DEBUG 10-15 15:27:21 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:21 lpmodule.py:444] time cost move to cuda:1 0.02480340003967285 s
DEBUG 10-15 15:27:21 lpllm.py:2283] CPU attn cost 0.147220 seconds if batch True
DEBUG 10-15 15:27:21 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:21 lpllm.py:2294] CPU compute cost 0.148009 seconds
DEBUG 10-15 15:27:21 lpllm.py:2312] free cost 0.000082 seconds
DEBUG 10-15 15:27:21 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:21 lpllm.py:1774] update state cost 2.288818359375e-05 s
DEBUG 10-15 15:27:21 lpllm.py:1743] restore layer func cost 0.00036787986755371094 s
DEBUG 10-15 15:27:21 lpllm.py:511] restore layer cost 0.0006477832794189453 s
DEBUG 10-15 15:27:21 lpllm.py:512] reset_next_layer_need: layer_attn_idx=15, layer_mlp_idx=14, j_loc=29
DEBUG 10-15 15:27:21 lpllm.py:1037] reset layer cost 0.0007205009460449219 s
DEBUG 10-15 15:27:21 lpllm.py:1038] have reset next layer layer_attn 15 layer_mlp 14 
DEBUG 10-15 15:27:21 lpllm.py:1044] j: 29 waiting the layer with layer_idx 15 before wait time 0.44802165031433105 s
INFO 10-15 15:27:21 client.py:117] confirm_model_loaded: Mixtral-8x7B, 75f18747-4474-49eb-b7ea-4afdf1983174
INFO 10-15 15:27:21 client.py:125] Model loaded
DEBUG 10-15 15:27:21 lpllm.py:1048] j: load cost 0.4497487545013428 s waiting cost 0.0017120838165283203 s
DEBUG 10-15 15:27:21 lpllm.py:924] 
DEBUG 10-15 15:27:21 lpllm.py:924] decoder loop j: 30 cur_layer_idx: 15 layer_attn_idx: 15 layer_mlp_idx: 14
DEBUG 10-15 15:27:21 lpllm.py:933] start load next layer cur_layer_idx: 16
DEBUG 10-15 15:27:21 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed6e0>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x004\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x98\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:21 client.py:72] load_into_gpu: Mixtral-8x7B, 399a5ca6-26d8-4f9a-b1a6-63bb9c3d2ece
INFO 10-15 15:27:21 client.py:113] Model loaded: Mixtral-8x7B, 399a5ca6-26d8-4f9a-b1a6-63bb9c3d2ece
DEBUG 10-15 15:27:21 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:21 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:21 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:21 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:21 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:21 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:21 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:21 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:21 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:21 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:21 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:21 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:21 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:21 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:21 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:21 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:21 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:21 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:21 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:21 lpllm.py:2265] GPU2CPU move cost 0.000613 seconds
DEBUG 10-15 15:27:21 lpmodule.py:340] layer_idx 15 attn layer_idx: 15
DEBUG 10-15 15:27:21 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:21 lpmodule.py:364] decoder_attn_batch update batch_dim 240-300
DEBUG 10-15 15:27:21 StaticCacheLen.py:84] static cache update layer_idx: 15, start_batch: 240, end_batch: 300
DEBUG 10-15 15:27:21 lpmodule.py:368] update for kv cache 240-300 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:21 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:21 lpmodule.py:374] update past key value cost 0.028860 seconds
DEBUG 10-15 15:27:21 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:21 lpmodule.py:227] layer idx 14
DEBUG 10-15 15:27:21 lpmodule.py:399] repeat qkv cost 0.031136 seconds
DEBUG 10-15 15:27:21 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:21 lpmodule.py:433] dot attn cost 0.043889 seconds
DEBUG 10-15 15:27:21 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:21 lpmodule.py:444] time cost move to cuda:1 0.023436784744262695 s
DEBUG 10-15 15:27:21 lpllm.py:2283] CPU attn cost 0.153541 seconds if batch True
DEBUG 10-15 15:27:21 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:21 lpllm.py:2294] CPU compute cost 0.154406 seconds
DEBUG 10-15 15:27:21 lpllm.py:2312] free cost 0.000080 seconds
DEBUG 10-15 15:27:21 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:21 lpllm.py:1774] update state cost 3.4809112548828125e-05 s
DEBUG 10-15 15:27:21 lpllm.py:1743] restore layer func cost 0.0008013248443603516 s
DEBUG 10-15 15:27:21 lpllm.py:511] restore layer cost 0.0010609626770019531 s
DEBUG 10-15 15:27:21 lpllm.py:512] reset_next_layer_need: layer_attn_idx=15, layer_mlp_idx=15, j_loc=30
DEBUG 10-15 15:27:21 lpllm.py:1037] reset layer cost 0.0011379718780517578 s
DEBUG 10-15 15:27:21 lpllm.py:1038] have reset next layer layer_attn 15 layer_mlp 15 
DEBUG 10-15 15:27:21 lpllm.py:924] 
DEBUG 10-15 15:27:21 lpllm.py:924] decoder loop j: 31 cur_layer_idx: 15 layer_attn_idx: 15 layer_mlp_idx: 15
DEBUG 10-15 15:27:21 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:21 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:21 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:21 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:21 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:21 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:21 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:21 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:21 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:21 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:21 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:21 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:21 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:21 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:21 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:21 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:21 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:21 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:21 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:21 lpllm.py:2265] GPU2CPU move cost 0.000646 seconds
DEBUG 10-15 15:27:21 lpmodule.py:340] layer_idx 15 attn layer_idx: 15
DEBUG 10-15 15:27:21 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:21 lpmodule.py:364] decoder_attn_batch update batch_dim 300-360
DEBUG 10-15 15:27:21 StaticCacheLen.py:84] static cache update layer_idx: 15, start_batch: 300, end_batch: 360
DEBUG 10-15 15:27:21 lpmodule.py:368] update for kv cache 300-360 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:21 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:21 lpmodule.py:374] update past key value cost 0.024396 seconds
DEBUG 10-15 15:27:21 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:21 lpmodule.py:227] layer idx 15
DEBUG 10-15 15:27:21 lpmodule.py:399] repeat qkv cost 0.031293 seconds
DEBUG 10-15 15:27:21 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:21 lpmodule.py:433] dot attn cost 0.038483 seconds
DEBUG 10-15 15:27:21 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:21 lpmodule.py:444] time cost move to cuda:1 0.0304110050201416 s
DEBUG 10-15 15:27:21 lpllm.py:2283] CPU attn cost 0.151940 seconds if batch True
DEBUG 10-15 15:27:21 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:21 lpllm.py:2294] CPU compute cost 0.152854 seconds
DEBUG 10-15 15:27:21 lpllm.py:2312] free cost 0.000077 seconds
DEBUG 10-15 15:27:21 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:21 lpllm.py:1774] update state cost 4.1961669921875e-05 s
DEBUG 10-15 15:27:21 lpllm.py:1743] restore layer func cost 0.00037479400634765625 s
DEBUG 10-15 15:27:21 lpllm.py:511] restore layer cost 0.0006496906280517578 s
DEBUG 10-15 15:27:21 lpllm.py:512] reset_next_layer_need: layer_attn_idx=16, layer_mlp_idx=15, j_loc=31
DEBUG 10-15 15:27:21 lpllm.py:1037] reset layer cost 0.0007212162017822266 s
DEBUG 10-15 15:27:21 lpllm.py:1038] have reset next layer layer_attn 16 layer_mlp 15 
DEBUG 10-15 15:27:21 lpllm.py:1044] j: 31 waiting the layer with layer_idx 16 before wait time 0.44518256187438965 s
INFO 10-15 15:27:21 client.py:117] confirm_model_loaded: Mixtral-8x7B, 399a5ca6-26d8-4f9a-b1a6-63bb9c3d2ece
INFO 10-15 15:27:21 client.py:125] Model loaded
DEBUG 10-15 15:27:21 lpllm.py:1048] j: load cost 0.44699740409851074 s waiting cost 0.0018000602722167969 s
DEBUG 10-15 15:27:21 lpllm.py:924] 
DEBUG 10-15 15:27:21 lpllm.py:924] decoder loop j: 32 cur_layer_idx: 16 layer_attn_idx: 16 layer_mlp_idx: 15
DEBUG 10-15 15:27:21 lpllm.py:933] start load next layer cur_layer_idx: 17
DEBUG 10-15 15:27:21 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:21 client.py:72] load_into_gpu: Mixtral-8x7B, 553301ea-e7a1-4b0c-a85f-6babde6e2b70
INFO 10-15 15:27:21 client.py:113] Model loaded: Mixtral-8x7B, 553301ea-e7a1-4b0c-a85f-6babde6e2b70
DEBUG 10-15 15:27:21 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:21 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:21 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:21 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:21 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:21 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:21 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:21 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:21 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:21 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:21 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:21 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:21 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:21 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:21 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:21 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:21 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:21 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:21 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:21 lpllm.py:2265] GPU2CPU move cost 0.000579 seconds
DEBUG 10-15 15:27:21 lpmodule.py:340] layer_idx 16 attn layer_idx: 16
DEBUG 10-15 15:27:21 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:21 lpmodule.py:364] decoder_attn_batch update batch_dim 240-300
DEBUG 10-15 15:27:21 StaticCacheLen.py:84] static cache update layer_idx: 16, start_batch: 240, end_batch: 300
DEBUG 10-15 15:27:21 lpmodule.py:368] update for kv cache 240-300 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:21 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:21 lpmodule.py:374] update past key value cost 0.026294 seconds
DEBUG 10-15 15:27:21 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:21 lpmodule.py:227] layer idx 15
DEBUG 10-15 15:27:21 lpmodule.py:399] repeat qkv cost 0.030811 seconds
DEBUG 10-15 15:27:21 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:21 lpmodule.py:433] dot attn cost 0.033757 seconds
DEBUG 10-15 15:27:21 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:22 lpmodule.py:444] time cost move to cuda:1 0.024558067321777344 s
DEBUG 10-15 15:27:22 lpllm.py:2283] CPU attn cost 0.144684 seconds if batch True
DEBUG 10-15 15:27:22 lpllm.py:2292] deal attn result cost 0.000006 seconds
DEBUG 10-15 15:27:22 lpllm.py:2294] CPU compute cost 0.146204 seconds
DEBUG 10-15 15:27:22 lpllm.py:2312] free cost 0.000222 seconds
DEBUG 10-15 15:27:22 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:22 lpllm.py:1774] update state cost 2.7179718017578125e-05 s
DEBUG 10-15 15:27:22 lpllm.py:1743] restore layer func cost 0.0009069442749023438 s
DEBUG 10-15 15:27:22 lpllm.py:511] restore layer cost 0.0011854171752929688 s
DEBUG 10-15 15:27:22 lpllm.py:512] reset_next_layer_need: layer_attn_idx=16, layer_mlp_idx=16, j_loc=32
DEBUG 10-15 15:27:22 lpllm.py:1037] reset layer cost 0.0012996196746826172 s
DEBUG 10-15 15:27:22 lpllm.py:1038] have reset next layer layer_attn 16 layer_mlp 16 
DEBUG 10-15 15:27:22 lpllm.py:924] 
DEBUG 10-15 15:27:22 lpllm.py:924] decoder loop j: 33 cur_layer_idx: 16 layer_attn_idx: 16 layer_mlp_idx: 16
DEBUG 10-15 15:27:22 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:22 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:22 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:22 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:22 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:22 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:22 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:22 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:22 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:22 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:22 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:22 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:22 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:22 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:22 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:22 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:22 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:22 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:22 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:22 lpllm.py:2265] GPU2CPU move cost 0.000948 seconds
DEBUG 10-15 15:27:22 lpmodule.py:340] layer_idx 16 attn layer_idx: 16
DEBUG 10-15 15:27:22 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:22 lpmodule.py:364] decoder_attn_batch update batch_dim 300-360
DEBUG 10-15 15:27:22 StaticCacheLen.py:84] static cache update layer_idx: 16, start_batch: 300, end_batch: 360
DEBUG 10-15 15:27:22 lpmodule.py:368] update for kv cache 300-360 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:22 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:22 lpmodule.py:374] update past key value cost 0.023265 seconds
DEBUG 10-15 15:27:22 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:22 lpmodule.py:227] layer idx 16
DEBUG 10-15 15:27:22 lpmodule.py:399] repeat qkv cost 0.029890 seconds
DEBUG 10-15 15:27:22 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:22 lpmodule.py:433] dot attn cost 0.030406 seconds
DEBUG 10-15 15:27:22 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:22 lpmodule.py:444] time cost move to cuda:1 0.024486541748046875 s
DEBUG 10-15 15:27:22 lpllm.py:2283] CPU attn cost 0.135768 seconds if batch True
DEBUG 10-15 15:27:22 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:22 lpllm.py:2294] CPU compute cost 0.137071 seconds
DEBUG 10-15 15:27:22 lpllm.py:2312] free cost 0.000098 seconds
DEBUG 10-15 15:27:22 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:22 lpllm.py:1774] update state cost 2.7418136596679688e-05 s
DEBUG 10-15 15:27:22 lpllm.py:1743] restore layer func cost 0.0003941059112548828 s
DEBUG 10-15 15:27:22 lpllm.py:511] restore layer cost 0.0006582736968994141 s
DEBUG 10-15 15:27:22 lpllm.py:512] reset_next_layer_need: layer_attn_idx=17, layer_mlp_idx=16, j_loc=33
DEBUG 10-15 15:27:22 lpllm.py:1037] reset layer cost 0.0007462501525878906 s
DEBUG 10-15 15:27:22 lpllm.py:1038] have reset next layer layer_attn 17 layer_mlp 16 
DEBUG 10-15 15:27:22 lpllm.py:1044] j: 33 waiting the layer with layer_idx 17 before wait time 0.4488966464996338 s
INFO 10-15 15:27:22 client.py:117] confirm_model_loaded: Mixtral-8x7B, 553301ea-e7a1-4b0c-a85f-6babde6e2b70
INFO 10-15 15:27:22 client.py:125] Model loaded
DEBUG 10-15 15:27:22 lpllm.py:1048] j: load cost 0.4510056972503662 s waiting cost 0.002093076705932617 s
DEBUG 10-15 15:27:22 lpllm.py:924] 
DEBUG 10-15 15:27:22 lpllm.py:924] decoder loop j: 34 cur_layer_idx: 17 layer_attn_idx: 17 layer_mlp_idx: 16
DEBUG 10-15 15:27:22 lpllm.py:933] start load next layer cur_layer_idx: 18
DEBUG 10-15 15:27:22 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed650>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x003\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x96\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:22 client.py:72] load_into_gpu: Mixtral-8x7B, 3ec9b4a7-6205-4272-ac0a-c659adfa10ef
INFO 10-15 15:27:22 client.py:113] Model loaded: Mixtral-8x7B, 3ec9b4a7-6205-4272-ac0a-c659adfa10ef
DEBUG 10-15 15:27:22 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:22 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:22 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:22 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:22 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:22 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:22 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:22 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:22 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:22 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:22 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:22 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:22 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:22 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:22 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:22 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:22 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:22 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:22 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:22 lpllm.py:2265] GPU2CPU move cost 0.000743 seconds
DEBUG 10-15 15:27:22 lpmodule.py:340] layer_idx 17 attn layer_idx: 17
DEBUG 10-15 15:27:22 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:22 lpmodule.py:364] decoder_attn_batch update batch_dim 240-300
DEBUG 10-15 15:27:22 StaticCacheLen.py:84] static cache update layer_idx: 17, start_batch: 240, end_batch: 300
DEBUG 10-15 15:27:22 lpmodule.py:368] update for kv cache 240-300 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:22 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:22 lpmodule.py:374] update past key value cost 0.024648 seconds
DEBUG 10-15 15:27:22 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:22 lpmodule.py:227] layer idx 16
DEBUG 10-15 15:27:22 lpmodule.py:399] repeat qkv cost 0.031746 seconds
DEBUG 10-15 15:27:22 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:22 lpmodule.py:433] dot attn cost 0.033381 seconds
DEBUG 10-15 15:27:22 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:22 lpmodule.py:444] time cost move to cuda:1 0.024977445602416992 s
DEBUG 10-15 15:27:22 lpllm.py:2283] CPU attn cost 0.142976 seconds if batch True
DEBUG 10-15 15:27:22 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:22 lpllm.py:2294] CPU compute cost 0.144052 seconds
DEBUG 10-15 15:27:22 lpllm.py:2312] free cost 0.000123 seconds
DEBUG 10-15 15:27:22 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:22 lpllm.py:1774] update state cost 2.2649765014648438e-05 s
DEBUG 10-15 15:27:22 lpllm.py:1743] restore layer func cost 0.0008602142333984375 s
DEBUG 10-15 15:27:22 lpllm.py:511] restore layer cost 0.0010979175567626953 s
DEBUG 10-15 15:27:22 lpllm.py:512] reset_next_layer_need: layer_attn_idx=17, layer_mlp_idx=17, j_loc=34
DEBUG 10-15 15:27:22 lpllm.py:1037] reset layer cost 0.0012218952178955078 s
DEBUG 10-15 15:27:22 lpllm.py:1038] have reset next layer layer_attn 17 layer_mlp 17 
DEBUG 10-15 15:27:22 lpllm.py:924] 
DEBUG 10-15 15:27:22 lpllm.py:924] decoder loop j: 35 cur_layer_idx: 17 layer_attn_idx: 17 layer_mlp_idx: 17
DEBUG 10-15 15:27:22 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:22 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:22 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:22 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:22 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:22 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:22 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:22 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:22 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:22 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:22 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:22 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:22 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:22 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:22 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:22 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:22 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:22 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:22 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:22 lpllm.py:2265] GPU2CPU move cost 0.000893 seconds
DEBUG 10-15 15:27:22 lpmodule.py:340] layer_idx 17 attn layer_idx: 17
DEBUG 10-15 15:27:22 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:22 lpmodule.py:364] decoder_attn_batch update batch_dim 300-360
DEBUG 10-15 15:27:22 StaticCacheLen.py:84] static cache update layer_idx: 17, start_batch: 300, end_batch: 360
DEBUG 10-15 15:27:22 lpmodule.py:368] update for kv cache 300-360 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:22 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:22 lpmodule.py:374] update past key value cost 0.024020 seconds
DEBUG 10-15 15:27:22 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:22 lpmodule.py:227] layer idx 17
DEBUG 10-15 15:27:22 lpmodule.py:399] repeat qkv cost 0.031939 seconds
DEBUG 10-15 15:27:22 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:22 lpmodule.py:433] dot attn cost 0.034402 seconds
DEBUG 10-15 15:27:22 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:22 lpmodule.py:444] time cost move to cuda:1 0.024224042892456055 s
DEBUG 10-15 15:27:22 lpllm.py:2283] CPU attn cost 0.141073 seconds if batch True
DEBUG 10-15 15:27:22 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:22 lpllm.py:2294] CPU compute cost 0.142317 seconds
DEBUG 10-15 15:27:22 lpllm.py:2312] free cost 0.000093 seconds
DEBUG 10-15 15:27:22 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:22 lpllm.py:1774] update state cost 2.8133392333984375e-05 s
DEBUG 10-15 15:27:22 lpllm.py:1743] restore layer func cost 0.0006756782531738281 s
DEBUG 10-15 15:27:22 lpllm.py:511] restore layer cost 0.0009469985961914062 s
DEBUG 10-15 15:27:22 lpllm.py:512] reset_next_layer_need: layer_attn_idx=18, layer_mlp_idx=17, j_loc=35
DEBUG 10-15 15:27:22 lpllm.py:1037] reset layer cost 0.0010335445404052734 s
DEBUG 10-15 15:27:22 lpllm.py:1038] have reset next layer layer_attn 18 layer_mlp 17 
DEBUG 10-15 15:27:22 lpllm.py:1044] j: 35 waiting the layer with layer_idx 18 before wait time 0.45069265365600586 s
INFO 10-15 15:27:22 client.py:117] confirm_model_loaded: Mixtral-8x7B, 3ec9b4a7-6205-4272-ac0a-c659adfa10ef
INFO 10-15 15:27:22 client.py:125] Model loaded
DEBUG 10-15 15:27:22 lpllm.py:1048] j: load cost 0.45247960090637207 s waiting cost 0.0017704963684082031 s
DEBUG 10-15 15:27:22 lpllm.py:924] 
DEBUG 10-15 15:27:22 lpllm.py:924] decoder loop j: 36 cur_layer_idx: 18 layer_attn_idx: 18 layer_mlp_idx: 17
DEBUG 10-15 15:27:22 lpllm.py:933] start load next layer cur_layer_idx: 19
DEBUG 10-15 15:27:22 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed6e0>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x004\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x98\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:22 client.py:72] load_into_gpu: Mixtral-8x7B, 9020e777-92ea-490b-a1f5-0666877d0b68
INFO 10-15 15:27:22 client.py:113] Model loaded: Mixtral-8x7B, 9020e777-92ea-490b-a1f5-0666877d0b68
DEBUG 10-15 15:27:22 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:22 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:22 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:22 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:22 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:22 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:22 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:22 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:22 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:22 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:22 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:22 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:22 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:22 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:22 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:22 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:22 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:22 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:22 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:22 lpllm.py:2265] GPU2CPU move cost 0.000733 seconds
DEBUG 10-15 15:27:22 lpmodule.py:340] layer_idx 18 attn layer_idx: 18
DEBUG 10-15 15:27:22 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:22 lpmodule.py:364] decoder_attn_batch update batch_dim 240-300
DEBUG 10-15 15:27:22 StaticCacheLen.py:84] static cache update layer_idx: 18, start_batch: 240, end_batch: 300
DEBUG 10-15 15:27:22 lpmodule.py:368] update for kv cache 240-300 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:22 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:22 lpmodule.py:374] update past key value cost 0.024346 seconds
DEBUG 10-15 15:27:22 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:22 lpmodule.py:227] layer idx 17
DEBUG 10-15 15:27:22 lpmodule.py:399] repeat qkv cost 0.031181 seconds
DEBUG 10-15 15:27:22 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:22 lpmodule.py:433] dot attn cost 0.033518 seconds
DEBUG 10-15 15:27:22 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:22 lpmodule.py:444] time cost move to cuda:1 0.023423194885253906 s
DEBUG 10-15 15:27:22 lpllm.py:2283] CPU attn cost 0.139064 seconds if batch True
DEBUG 10-15 15:27:22 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:27:22 lpllm.py:2294] CPU compute cost 0.140141 seconds
DEBUG 10-15 15:27:22 lpllm.py:2312] free cost 0.000099 seconds
DEBUG 10-15 15:27:22 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:22 lpllm.py:1774] update state cost 2.5510787963867188e-05 s
DEBUG 10-15 15:27:22 lpllm.py:1743] restore layer func cost 0.0008511543273925781 s
DEBUG 10-15 15:27:22 lpllm.py:511] restore layer cost 0.0011146068572998047 s
DEBUG 10-15 15:27:22 lpllm.py:512] reset_next_layer_need: layer_attn_idx=18, layer_mlp_idx=18, j_loc=36
DEBUG 10-15 15:27:22 lpllm.py:1037] reset layer cost 0.0011992454528808594 s
DEBUG 10-15 15:27:22 lpllm.py:1038] have reset next layer layer_attn 18 layer_mlp 18 
DEBUG 10-15 15:27:22 lpllm.py:924] 
DEBUG 10-15 15:27:22 lpllm.py:924] decoder loop j: 37 cur_layer_idx: 18 layer_attn_idx: 18 layer_mlp_idx: 18
DEBUG 10-15 15:27:22 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:22 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:22 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:23 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:23 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:23 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:23 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:23 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:23 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:23 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:23 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:23 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:23 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:23 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:23 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:23 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:23 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:23 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:23 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:23 lpllm.py:2265] GPU2CPU move cost 0.000744 seconds
DEBUG 10-15 15:27:23 lpmodule.py:340] layer_idx 18 attn layer_idx: 18
DEBUG 10-15 15:27:23 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:23 lpmodule.py:364] decoder_attn_batch update batch_dim 300-360
DEBUG 10-15 15:27:23 StaticCacheLen.py:84] static cache update layer_idx: 18, start_batch: 300, end_batch: 360
DEBUG 10-15 15:27:23 lpmodule.py:368] update for kv cache 300-360 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:23 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:23 lpmodule.py:374] update past key value cost 0.024364 seconds
DEBUG 10-15 15:27:23 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:23 lpmodule.py:227] layer idx 18
DEBUG 10-15 15:27:23 lpmodule.py:399] repeat qkv cost 0.031487 seconds
DEBUG 10-15 15:27:23 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:23 lpmodule.py:433] dot attn cost 0.044158 seconds
DEBUG 10-15 15:27:23 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:23 lpmodule.py:444] time cost move to cuda:1 0.026573896408081055 s
DEBUG 10-15 15:27:23 lpllm.py:2283] CPU attn cost 0.152632 seconds if batch True
DEBUG 10-15 15:27:23 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:23 lpllm.py:2294] CPU compute cost 0.153677 seconds
DEBUG 10-15 15:27:23 lpllm.py:2312] free cost 0.000078 seconds
DEBUG 10-15 15:27:23 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:23 lpllm.py:1774] update state cost 2.9802322387695312e-05 s
DEBUG 10-15 15:27:23 lpllm.py:1743] restore layer func cost 0.0004086494445800781 s
DEBUG 10-15 15:27:23 lpllm.py:511] restore layer cost 0.0006766319274902344 s
DEBUG 10-15 15:27:23 lpllm.py:512] reset_next_layer_need: layer_attn_idx=19, layer_mlp_idx=18, j_loc=37
DEBUG 10-15 15:27:23 lpllm.py:1037] reset layer cost 0.0007634162902832031 s
DEBUG 10-15 15:27:23 lpllm.py:1038] have reset next layer layer_attn 19 layer_mlp 18 
DEBUG 10-15 15:27:23 lpllm.py:1044] j: 37 waiting the layer with layer_idx 19 before wait time 0.4478631019592285 s
INFO 10-15 15:27:23 client.py:117] confirm_model_loaded: Mixtral-8x7B, 9020e777-92ea-490b-a1f5-0666877d0b68
INFO 10-15 15:27:23 client.py:125] Model loaded
DEBUG 10-15 15:27:23 lpllm.py:1048] j: load cost 0.44948649406433105 s waiting cost 0.0016052722930908203 s
DEBUG 10-15 15:27:23 lpllm.py:924] 
DEBUG 10-15 15:27:23 lpllm.py:924] decoder loop j: 38 cur_layer_idx: 19 layer_attn_idx: 19 layer_mlp_idx: 18
DEBUG 10-15 15:27:23 lpllm.py:933] start load next layer cur_layer_idx: 20
DEBUG 10-15 15:27:23 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:23 client.py:72] load_into_gpu: Mixtral-8x7B, 41605843-f256-4e49-83df-304cb98e8f93
INFO 10-15 15:27:23 client.py:113] Model loaded: Mixtral-8x7B, 41605843-f256-4e49-83df-304cb98e8f93
DEBUG 10-15 15:27:23 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:23 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:23 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:23 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:23 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:23 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:23 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:23 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:23 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:23 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:23 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:23 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:23 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:23 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:23 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:23 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:23 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:23 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:23 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:23 lpllm.py:2265] GPU2CPU move cost 0.000653 seconds
DEBUG 10-15 15:27:23 lpmodule.py:340] layer_idx 19 attn layer_idx: 19
DEBUG 10-15 15:27:23 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:23 lpmodule.py:364] decoder_attn_batch update batch_dim 240-300
DEBUG 10-15 15:27:23 StaticCacheLen.py:84] static cache update layer_idx: 19, start_batch: 240, end_batch: 300
DEBUG 10-15 15:27:23 lpmodule.py:368] update for kv cache 240-300 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:23 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:23 lpmodule.py:374] update past key value cost 0.023254 seconds
DEBUG 10-15 15:27:23 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:23 lpmodule.py:227] layer idx 18
DEBUG 10-15 15:27:23 lpmodule.py:399] repeat qkv cost 0.030743 seconds
DEBUG 10-15 15:27:23 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:23 lpmodule.py:433] dot attn cost 0.033839 seconds
DEBUG 10-15 15:27:23 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:23 lpmodule.py:444] time cost move to cuda:1 0.023876428604125977 s
DEBUG 10-15 15:27:23 lpllm.py:2283] CPU attn cost 0.138324 seconds if batch True
DEBUG 10-15 15:27:23 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:27:23 lpllm.py:2294] CPU compute cost 0.139264 seconds
DEBUG 10-15 15:27:23 lpllm.py:2312] free cost 0.000085 seconds
DEBUG 10-15 15:27:23 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:23 lpllm.py:1774] update state cost 3.457069396972656e-05 s
DEBUG 10-15 15:27:23 lpllm.py:1743] restore layer func cost 0.0008661746978759766 s
DEBUG 10-15 15:27:23 lpllm.py:511] restore layer cost 0.0011081695556640625 s
DEBUG 10-15 15:27:23 lpllm.py:512] reset_next_layer_need: layer_attn_idx=19, layer_mlp_idx=19, j_loc=38
DEBUG 10-15 15:27:23 lpllm.py:1037] reset layer cost 0.0011818408966064453 s
DEBUG 10-15 15:27:23 lpllm.py:1038] have reset next layer layer_attn 19 layer_mlp 19 
DEBUG 10-15 15:27:23 lpllm.py:924] 
DEBUG 10-15 15:27:23 lpllm.py:924] decoder loop j: 39 cur_layer_idx: 19 layer_attn_idx: 19 layer_mlp_idx: 19
DEBUG 10-15 15:27:23 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:23 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:23 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:23 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:23 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:23 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:23 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:23 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:23 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:23 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:23 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:23 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:23 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:23 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:23 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:23 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:23 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:23 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:23 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:23 lpllm.py:2265] GPU2CPU move cost 0.000636 seconds
DEBUG 10-15 15:27:23 lpmodule.py:340] layer_idx 19 attn layer_idx: 19
DEBUG 10-15 15:27:23 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:23 lpmodule.py:364] decoder_attn_batch update batch_dim 300-360
DEBUG 10-15 15:27:23 StaticCacheLen.py:84] static cache update layer_idx: 19, start_batch: 300, end_batch: 360
DEBUG 10-15 15:27:23 lpmodule.py:368] update for kv cache 300-360 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:23 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:23 lpmodule.py:374] update past key value cost 0.024376 seconds
DEBUG 10-15 15:27:23 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:23 lpmodule.py:227] layer idx 19
DEBUG 10-15 15:27:23 lpmodule.py:399] repeat qkv cost 0.030186 seconds
DEBUG 10-15 15:27:23 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:23 lpmodule.py:433] dot attn cost 0.033314 seconds
DEBUG 10-15 15:27:23 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:23 lpmodule.py:444] time cost move to cuda:1 0.02349567413330078 s
DEBUG 10-15 15:27:23 lpllm.py:2283] CPU attn cost 0.136853 seconds if batch True
DEBUG 10-15 15:27:23 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:23 lpllm.py:2294] CPU compute cost 0.137894 seconds
DEBUG 10-15 15:27:23 lpllm.py:2312] free cost 0.000092 seconds
DEBUG 10-15 15:27:23 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:23 lpllm.py:1774] update state cost 3.719329833984375e-05 s
DEBUG 10-15 15:27:23 lpllm.py:1743] restore layer func cost 0.00040721893310546875 s
DEBUG 10-15 15:27:23 lpllm.py:511] restore layer cost 0.0006623268127441406 s
DEBUG 10-15 15:27:23 lpllm.py:512] reset_next_layer_need: layer_attn_idx=20, layer_mlp_idx=19, j_loc=39
DEBUG 10-15 15:27:23 lpllm.py:1037] reset layer cost 0.0007340908050537109 s
DEBUG 10-15 15:27:23 lpllm.py:1038] have reset next layer layer_attn 20 layer_mlp 19 
DEBUG 10-15 15:27:23 lpllm.py:1044] j: 39 waiting the layer with layer_idx 20 before wait time 0.4531242847442627 s
INFO 10-15 15:27:23 client.py:117] confirm_model_loaded: Mixtral-8x7B, 41605843-f256-4e49-83df-304cb98e8f93
INFO 10-15 15:27:23 client.py:125] Model loaded
DEBUG 10-15 15:27:23 lpllm.py:1048] j: load cost 0.4549674987792969 s waiting cost 0.0018274784088134766 s
DEBUG 10-15 15:27:23 lpllm.py:924] 
DEBUG 10-15 15:27:23 lpllm.py:924] decoder loop j: 40 cur_layer_idx: 20 layer_attn_idx: 20 layer_mlp_idx: 19
DEBUG 10-15 15:27:23 lpllm.py:933] start load next layer cur_layer_idx: 21
DEBUG 10-15 15:27:23 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed650>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x003\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x96\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:23 client.py:72] load_into_gpu: Mixtral-8x7B, f51eccc7-aa10-426e-8673-7b75309a724d
INFO 10-15 15:27:23 client.py:113] Model loaded: Mixtral-8x7B, f51eccc7-aa10-426e-8673-7b75309a724d
DEBUG 10-15 15:27:23 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:23 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:23 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:23 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:23 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:23 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:23 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:23 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:23 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:23 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:23 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:23 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:23 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:23 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:23 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:23 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:23 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:23 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:23 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:23 lpllm.py:2265] GPU2CPU move cost 0.000709 seconds
DEBUG 10-15 15:27:23 lpmodule.py:340] layer_idx 20 attn layer_idx: 20
DEBUG 10-15 15:27:23 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:23 lpmodule.py:364] decoder_attn_batch update batch_dim 240-300
DEBUG 10-15 15:27:23 StaticCacheLen.py:84] static cache update layer_idx: 20, start_batch: 240, end_batch: 300
DEBUG 10-15 15:27:23 lpmodule.py:368] update for kv cache 240-300 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:23 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:23 lpmodule.py:374] update past key value cost 0.023245 seconds
DEBUG 10-15 15:27:23 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:23 lpmodule.py:227] layer idx 19
DEBUG 10-15 15:27:23 lpmodule.py:399] repeat qkv cost 0.030749 seconds
DEBUG 10-15 15:27:23 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:23 lpmodule.py:433] dot attn cost 0.033109 seconds
DEBUG 10-15 15:27:23 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:23 lpmodule.py:444] time cost move to cuda:1 0.02328658103942871 s
DEBUG 10-15 15:27:23 lpllm.py:2283] CPU attn cost 0.136527 seconds if batch True
DEBUG 10-15 15:27:23 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:23 lpllm.py:2294] CPU compute cost 0.137521 seconds
DEBUG 10-15 15:27:23 lpllm.py:2312] free cost 0.000078 seconds
DEBUG 10-15 15:27:23 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:23 lpllm.py:1774] update state cost 2.1457672119140625e-05 s
DEBUG 10-15 15:27:23 lpllm.py:1743] restore layer func cost 0.0008523464202880859 s
DEBUG 10-15 15:27:23 lpllm.py:511] restore layer cost 0.001115560531616211 s
DEBUG 10-15 15:27:23 lpllm.py:512] reset_next_layer_need: layer_attn_idx=20, layer_mlp_idx=20, j_loc=40
DEBUG 10-15 15:27:23 lpllm.py:1037] reset layer cost 0.0011899471282958984 s
DEBUG 10-15 15:27:23 lpllm.py:1038] have reset next layer layer_attn 20 layer_mlp 20 
DEBUG 10-15 15:27:23 lpllm.py:924] 
DEBUG 10-15 15:27:23 lpllm.py:924] decoder loop j: 41 cur_layer_idx: 20 layer_attn_idx: 20 layer_mlp_idx: 20
DEBUG 10-15 15:27:23 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:23 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:23 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:23 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:23 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:23 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:23 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:23 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:23 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:23 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:23 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:23 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:23 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:23 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:23 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:23 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:23 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:23 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:23 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:23 lpllm.py:2265] GPU2CPU move cost 0.000584 seconds
DEBUG 10-15 15:27:23 lpmodule.py:340] layer_idx 20 attn layer_idx: 20
DEBUG 10-15 15:27:23 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:23 lpmodule.py:364] decoder_attn_batch update batch_dim 300-360
DEBUG 10-15 15:27:23 StaticCacheLen.py:84] static cache update layer_idx: 20, start_batch: 300, end_batch: 360
DEBUG 10-15 15:27:23 lpmodule.py:368] update for kv cache 300-360 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:23 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:23 lpmodule.py:374] update past key value cost 0.022533 seconds
DEBUG 10-15 15:27:23 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:23 lpmodule.py:227] layer idx 20
DEBUG 10-15 15:27:23 lpmodule.py:399] repeat qkv cost 0.031864 seconds
DEBUG 10-15 15:27:23 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:24 lpmodule.py:433] dot attn cost 0.035728 seconds
DEBUG 10-15 15:27:24 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:24 lpmodule.py:444] time cost move to cuda:1 0.023107290267944336 s
DEBUG 10-15 15:27:24 lpllm.py:2283] CPU attn cost 0.139006 seconds if batch True
DEBUG 10-15 15:27:24 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:27:24 lpllm.py:2294] CPU compute cost 0.139903 seconds
DEBUG 10-15 15:27:24 lpllm.py:2312] free cost 0.000077 seconds
DEBUG 10-15 15:27:24 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:24 lpllm.py:1774] update state cost 2.2411346435546875e-05 s
DEBUG 10-15 15:27:24 lpllm.py:1743] restore layer func cost 0.00040411949157714844 s
DEBUG 10-15 15:27:24 lpllm.py:511] restore layer cost 0.0006365776062011719 s
DEBUG 10-15 15:27:24 lpllm.py:512] reset_next_layer_need: layer_attn_idx=21, layer_mlp_idx=20, j_loc=41
DEBUG 10-15 15:27:24 lpllm.py:1037] reset layer cost 0.0007293224334716797 s
DEBUG 10-15 15:27:24 lpllm.py:1038] have reset next layer layer_attn 21 layer_mlp 20 
DEBUG 10-15 15:27:24 lpllm.py:1044] j: 41 waiting the layer with layer_idx 21 before wait time 0.4447760581970215 s
INFO 10-15 15:27:24 client.py:117] confirm_model_loaded: Mixtral-8x7B, f51eccc7-aa10-426e-8673-7b75309a724d
INFO 10-15 15:27:24 client.py:125] Model loaded
DEBUG 10-15 15:27:24 lpllm.py:1048] j: load cost 0.4465487003326416 s waiting cost 0.0017571449279785156 s
DEBUG 10-15 15:27:24 lpllm.py:924] 
DEBUG 10-15 15:27:24 lpllm.py:924] decoder loop j: 42 cur_layer_idx: 21 layer_attn_idx: 21 layer_mlp_idx: 20
DEBUG 10-15 15:27:24 lpllm.py:933] start load next layer cur_layer_idx: 22
DEBUG 10-15 15:27:24 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed6e0>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x004\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x98\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:24 client.py:72] load_into_gpu: Mixtral-8x7B, fc30303e-c535-4f3c-b8dc-8da96b94eb14
INFO 10-15 15:27:24 client.py:113] Model loaded: Mixtral-8x7B, fc30303e-c535-4f3c-b8dc-8da96b94eb14
DEBUG 10-15 15:27:24 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:24 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:24 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:24 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:24 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:24 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:24 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:24 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:24 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:24 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:24 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:24 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:24 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:24 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:24 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:24 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:24 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:24 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:24 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:24 lpllm.py:2265] GPU2CPU move cost 0.000581 seconds
DEBUG 10-15 15:27:24 lpmodule.py:340] layer_idx 21 attn layer_idx: 21
DEBUG 10-15 15:27:24 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:24 lpmodule.py:364] decoder_attn_batch update batch_dim 240-300
DEBUG 10-15 15:27:24 StaticCacheLen.py:84] static cache update layer_idx: 21, start_batch: 240, end_batch: 300
DEBUG 10-15 15:27:24 lpmodule.py:368] update for kv cache 240-300 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:24 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:24 lpmodule.py:374] update past key value cost 0.022433 seconds
DEBUG 10-15 15:27:24 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:24 lpmodule.py:227] layer idx 20
DEBUG 10-15 15:27:24 lpmodule.py:399] repeat qkv cost 0.030088 seconds
DEBUG 10-15 15:27:24 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:24 lpmodule.py:433] dot attn cost 0.033549 seconds
DEBUG 10-15 15:27:24 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:24 lpmodule.py:444] time cost move to cuda:1 0.02580404281616211 s
DEBUG 10-15 15:27:24 lpllm.py:2283] CPU attn cost 0.137688 seconds if batch True
DEBUG 10-15 15:27:24 lpllm.py:2292] deal attn result cost 0.000003 seconds
DEBUG 10-15 15:27:24 lpllm.py:2294] CPU compute cost 0.138552 seconds
DEBUG 10-15 15:27:24 lpllm.py:2312] free cost 0.000079 seconds
DEBUG 10-15 15:27:24 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:24 lpllm.py:1774] update state cost 1.9311904907226562e-05 s
DEBUG 10-15 15:27:24 lpllm.py:1743] restore layer func cost 0.0008299350738525391 s
DEBUG 10-15 15:27:24 lpllm.py:511] restore layer cost 0.0010671615600585938 s
DEBUG 10-15 15:27:24 lpllm.py:512] reset_next_layer_need: layer_attn_idx=21, layer_mlp_idx=21, j_loc=42
DEBUG 10-15 15:27:24 lpllm.py:1037] reset layer cost 0.001138925552368164 s
DEBUG 10-15 15:27:24 lpllm.py:1038] have reset next layer layer_attn 21 layer_mlp 21 
DEBUG 10-15 15:27:24 lpllm.py:924] 
DEBUG 10-15 15:27:24 lpllm.py:924] decoder loop j: 43 cur_layer_idx: 21 layer_attn_idx: 21 layer_mlp_idx: 21
DEBUG 10-15 15:27:24 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:24 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:24 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:24 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:24 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:24 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:24 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:24 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:24 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:24 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:24 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:24 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:24 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:24 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:24 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:24 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:24 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:24 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:24 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:24 lpllm.py:2265] GPU2CPU move cost 0.000603 seconds
DEBUG 10-15 15:27:24 lpmodule.py:340] layer_idx 21 attn layer_idx: 21
DEBUG 10-15 15:27:24 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:24 lpmodule.py:364] decoder_attn_batch update batch_dim 300-360
DEBUG 10-15 15:27:24 StaticCacheLen.py:84] static cache update layer_idx: 21, start_batch: 300, end_batch: 360
DEBUG 10-15 15:27:24 lpmodule.py:368] update for kv cache 300-360 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:24 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:24 lpmodule.py:374] update past key value cost 0.023061 seconds
DEBUG 10-15 15:27:24 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:24 lpmodule.py:227] layer idx 21
DEBUG 10-15 15:27:24 lpmodule.py:399] repeat qkv cost 0.030999 seconds
DEBUG 10-15 15:27:24 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:24 lpmodule.py:433] dot attn cost 0.034203 seconds
DEBUG 10-15 15:27:24 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:24 lpmodule.py:444] time cost move to cuda:1 0.02268671989440918 s
DEBUG 10-15 15:27:24 lpllm.py:2283] CPU attn cost 0.136589 seconds if batch True
DEBUG 10-15 15:27:24 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:24 lpllm.py:2294] CPU compute cost 0.137494 seconds
DEBUG 10-15 15:27:24 lpllm.py:2312] free cost 0.000079 seconds
DEBUG 10-15 15:27:24 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:24 lpllm.py:1774] update state cost 2.1696090698242188e-05 s
DEBUG 10-15 15:27:24 lpllm.py:1743] restore layer func cost 0.0004024505615234375 s
DEBUG 10-15 15:27:24 lpllm.py:511] restore layer cost 0.0006499290466308594 s
DEBUG 10-15 15:27:24 lpllm.py:512] reset_next_layer_need: layer_attn_idx=22, layer_mlp_idx=21, j_loc=43
DEBUG 10-15 15:27:24 lpllm.py:1037] reset layer cost 0.0007197856903076172 s
DEBUG 10-15 15:27:24 lpllm.py:1038] have reset next layer layer_attn 22 layer_mlp 21 
DEBUG 10-15 15:27:24 lpllm.py:1044] j: 43 waiting the layer with layer_idx 22 before wait time 0.4436795711517334 s
INFO 10-15 15:27:24 client.py:117] confirm_model_loaded: Mixtral-8x7B, fc30303e-c535-4f3c-b8dc-8da96b94eb14
INFO 10-15 15:27:24 client.py:125] Model loaded
DEBUG 10-15 15:27:24 lpllm.py:1048] j: load cost 0.445509672164917 s waiting cost 0.0018160343170166016 s
DEBUG 10-15 15:27:24 lpllm.py:924] 
DEBUG 10-15 15:27:24 lpllm.py:924] decoder loop j: 44 cur_layer_idx: 22 layer_attn_idx: 22 layer_mlp_idx: 21
DEBUG 10-15 15:27:24 lpllm.py:933] start load next layer cur_layer_idx: 23
DEBUG 10-15 15:27:24 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:24 client.py:72] load_into_gpu: Mixtral-8x7B, 61299a2b-0e95-4be8-9f63-c776a6bb4594
INFO 10-15 15:27:24 client.py:113] Model loaded: Mixtral-8x7B, 61299a2b-0e95-4be8-9f63-c776a6bb4594
DEBUG 10-15 15:27:24 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:24 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:24 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:24 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:24 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:24 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:24 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:24 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:24 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:24 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:24 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:24 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:24 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:24 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:24 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:24 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:24 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:24 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:24 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:24 lpllm.py:2265] GPU2CPU move cost 0.000604 seconds
DEBUG 10-15 15:27:24 lpmodule.py:340] layer_idx 22 attn layer_idx: 22
DEBUG 10-15 15:27:24 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:24 lpmodule.py:364] decoder_attn_batch update batch_dim 240-300
DEBUG 10-15 15:27:24 StaticCacheLen.py:84] static cache update layer_idx: 22, start_batch: 240, end_batch: 300
DEBUG 10-15 15:27:24 lpmodule.py:368] update for kv cache 240-300 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:24 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:24 lpmodule.py:374] update past key value cost 0.022053 seconds
DEBUG 10-15 15:27:24 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:24 lpmodule.py:227] layer idx 21
DEBUG 10-15 15:27:24 lpmodule.py:399] repeat qkv cost 0.031478 seconds
DEBUG 10-15 15:27:24 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:24 lpmodule.py:433] dot attn cost 0.043609 seconds
DEBUG 10-15 15:27:24 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:24 lpmodule.py:444] time cost move to cuda:1 0.02319502830505371 s
DEBUG 10-15 15:27:24 lpllm.py:2283] CPU attn cost 0.145285 seconds if batch True
DEBUG 10-15 15:27:24 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:27:24 lpllm.py:2294] CPU compute cost 0.146175 seconds
DEBUG 10-15 15:27:24 lpllm.py:2312] free cost 0.000077 seconds
DEBUG 10-15 15:27:24 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:24 lpllm.py:1774] update state cost 2.1457672119140625e-05 s
DEBUG 10-15 15:27:24 lpllm.py:1743] restore layer func cost 0.0008072853088378906 s
DEBUG 10-15 15:27:24 lpllm.py:511] restore layer cost 0.0010418891906738281 s
DEBUG 10-15 15:27:24 lpllm.py:512] reset_next_layer_need: layer_attn_idx=22, layer_mlp_idx=22, j_loc=44
DEBUG 10-15 15:27:24 lpllm.py:1037] reset layer cost 0.0011136531829833984 s
DEBUG 10-15 15:27:24 lpllm.py:1038] have reset next layer layer_attn 22 layer_mlp 22 
DEBUG 10-15 15:27:24 lpllm.py:924] 
DEBUG 10-15 15:27:24 lpllm.py:924] decoder loop j: 45 cur_layer_idx: 22 layer_attn_idx: 22 layer_mlp_idx: 22
DEBUG 10-15 15:27:24 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:24 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:24 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:24 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:24 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:24 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:24 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:24 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:24 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:24 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:24 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:24 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:24 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:24 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:24 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:24 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:24 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:24 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:24 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:24 lpllm.py:2265] GPU2CPU move cost 0.000613 seconds
DEBUG 10-15 15:27:24 lpmodule.py:340] layer_idx 22 attn layer_idx: 22
DEBUG 10-15 15:27:24 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:24 lpmodule.py:364] decoder_attn_batch update batch_dim 300-360
DEBUG 10-15 15:27:24 StaticCacheLen.py:84] static cache update layer_idx: 22, start_batch: 300, end_batch: 360
DEBUG 10-15 15:27:24 lpmodule.py:368] update for kv cache 300-360 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:24 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:24 lpmodule.py:374] update past key value cost 0.023186 seconds
DEBUG 10-15 15:27:24 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:24 lpmodule.py:227] layer idx 22
DEBUG 10-15 15:27:24 lpmodule.py:399] repeat qkv cost 0.031645 seconds
DEBUG 10-15 15:27:24 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:24 lpmodule.py:433] dot attn cost 0.033610 seconds
DEBUG 10-15 15:27:24 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:24 lpmodule.py:444] time cost move to cuda:1 0.023444414138793945 s
DEBUG 10-15 15:27:24 lpllm.py:2283] CPU attn cost 0.137358 seconds if batch True
DEBUG 10-15 15:27:24 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:27:24 lpllm.py:2294] CPU compute cost 0.138259 seconds
DEBUG 10-15 15:27:24 lpllm.py:2312] free cost 0.000079 seconds
DEBUG 10-15 15:27:25 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:25 lpllm.py:1774] update state cost 2.3126602172851562e-05 s
DEBUG 10-15 15:27:25 lpllm.py:1743] restore layer func cost 0.0004100799560546875 s
DEBUG 10-15 15:27:25 lpllm.py:511] restore layer cost 0.0006697177886962891 s
DEBUG 10-15 15:27:25 lpllm.py:512] reset_next_layer_need: layer_attn_idx=23, layer_mlp_idx=22, j_loc=45
DEBUG 10-15 15:27:25 lpllm.py:1037] reset layer cost 0.0007569789886474609 s
DEBUG 10-15 15:27:25 lpllm.py:1038] have reset next layer layer_attn 23 layer_mlp 22 
DEBUG 10-15 15:27:25 lpllm.py:1044] j: 45 waiting the layer with layer_idx 23 before wait time 0.4574439525604248 s
INFO 10-15 15:27:25 client.py:117] confirm_model_loaded: Mixtral-8x7B, 61299a2b-0e95-4be8-9f63-c776a6bb4594
INFO 10-15 15:27:25 client.py:125] Model loaded
DEBUG 10-15 15:27:25 lpllm.py:1048] j: load cost 0.45921778678894043 s waiting cost 0.001758575439453125 s
DEBUG 10-15 15:27:25 lpllm.py:924] 
DEBUG 10-15 15:27:25 lpllm.py:924] decoder loop j: 46 cur_layer_idx: 23 layer_attn_idx: 23 layer_mlp_idx: 22
DEBUG 10-15 15:27:25 lpllm.py:933] start load next layer cur_layer_idx: 24
DEBUG 10-15 15:27:25 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed650>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x003\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x96\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:25 client.py:72] load_into_gpu: Mixtral-8x7B, 99c76364-bae2-4d14-8eef-06c8bd658a01
INFO 10-15 15:27:25 client.py:113] Model loaded: Mixtral-8x7B, 99c76364-bae2-4d14-8eef-06c8bd658a01
DEBUG 10-15 15:27:25 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:25 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:25 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:25 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:25 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:25 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:25 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:25 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:25 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:25 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:25 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:25 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:25 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:25 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:25 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:25 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:25 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:25 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:25 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:25 lpllm.py:2265] GPU2CPU move cost 0.000582 seconds
DEBUG 10-15 15:27:25 lpmodule.py:340] layer_idx 23 attn layer_idx: 23
DEBUG 10-15 15:27:25 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:25 lpmodule.py:364] decoder_attn_batch update batch_dim 240-300
DEBUG 10-15 15:27:25 StaticCacheLen.py:84] static cache update layer_idx: 23, start_batch: 240, end_batch: 300
DEBUG 10-15 15:27:25 lpmodule.py:368] update for kv cache 240-300 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:25 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:25 lpmodule.py:374] update past key value cost 0.022726 seconds
DEBUG 10-15 15:27:25 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:25 lpmodule.py:227] layer idx 22
DEBUG 10-15 15:27:25 lpmodule.py:399] repeat qkv cost 0.031908 seconds
DEBUG 10-15 15:27:25 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:25 lpmodule.py:433] dot attn cost 0.043844 seconds
DEBUG 10-15 15:27:25 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:25 lpmodule.py:444] time cost move to cuda:1 0.02289438247680664 s
DEBUG 10-15 15:27:25 lpllm.py:2283] CPU attn cost 0.146696 seconds if batch True
DEBUG 10-15 15:27:25 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:27:25 lpllm.py:2294] CPU compute cost 0.147565 seconds
DEBUG 10-15 15:27:25 lpllm.py:2312] free cost 0.000078 seconds
DEBUG 10-15 15:27:25 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:25 lpllm.py:1774] update state cost 1.9073486328125e-05 s
DEBUG 10-15 15:27:25 lpllm.py:1743] restore layer func cost 0.0008730888366699219 s
DEBUG 10-15 15:27:25 lpllm.py:511] restore layer cost 0.0011146068572998047 s
DEBUG 10-15 15:27:25 lpllm.py:512] reset_next_layer_need: layer_attn_idx=23, layer_mlp_idx=23, j_loc=46
DEBUG 10-15 15:27:25 lpllm.py:1037] reset layer cost 0.0011920928955078125 s
DEBUG 10-15 15:27:25 lpllm.py:1038] have reset next layer layer_attn 23 layer_mlp 23 
DEBUG 10-15 15:27:25 lpllm.py:924] 
DEBUG 10-15 15:27:25 lpllm.py:924] decoder loop j: 47 cur_layer_idx: 23 layer_attn_idx: 23 layer_mlp_idx: 23
DEBUG 10-15 15:27:25 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:25 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:25 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:25 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:25 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:25 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:25 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:25 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:25 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:25 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:25 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:25 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:25 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:25 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:25 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:25 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:25 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:25 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:25 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:25 lpllm.py:2265] GPU2CPU move cost 0.000578 seconds
DEBUG 10-15 15:27:25 lpmodule.py:340] layer_idx 23 attn layer_idx: 23
DEBUG 10-15 15:27:25 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:25 lpmodule.py:364] decoder_attn_batch update batch_dim 300-360
DEBUG 10-15 15:27:25 StaticCacheLen.py:84] static cache update layer_idx: 23, start_batch: 300, end_batch: 360
DEBUG 10-15 15:27:25 lpmodule.py:368] update for kv cache 300-360 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:25 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:25 lpmodule.py:374] update past key value cost 0.023238 seconds
DEBUG 10-15 15:27:25 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:25 lpmodule.py:227] layer idx 23
DEBUG 10-15 15:27:25 lpmodule.py:399] repeat qkv cost 0.031124 seconds
DEBUG 10-15 15:27:25 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:25 lpmodule.py:433] dot attn cost 0.033190 seconds
DEBUG 10-15 15:27:25 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:25 lpmodule.py:444] time cost move to cuda:1 0.02329564094543457 s
DEBUG 10-15 15:27:25 lpllm.py:2283] CPU attn cost 0.136006 seconds if batch True
DEBUG 10-15 15:27:25 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:25 lpllm.py:2294] CPU compute cost 0.136872 seconds
DEBUG 10-15 15:27:25 lpllm.py:2312] free cost 0.000092 seconds
DEBUG 10-15 15:27:25 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:25 lpllm.py:1774] update state cost 2.3126602172851562e-05 s
DEBUG 10-15 15:27:25 lpllm.py:1743] restore layer func cost 0.00042557716369628906 s
DEBUG 10-15 15:27:25 lpllm.py:511] restore layer cost 0.0006694793701171875 s
DEBUG 10-15 15:27:25 lpllm.py:512] reset_next_layer_need: layer_attn_idx=24, layer_mlp_idx=23, j_loc=47
DEBUG 10-15 15:27:25 lpllm.py:1037] reset layer cost 0.0007429122924804688 s
DEBUG 10-15 15:27:25 lpllm.py:1038] have reset next layer layer_attn 24 layer_mlp 23 
DEBUG 10-15 15:27:25 lpllm.py:1044] j: 47 waiting the layer with layer_idx 24 before wait time 0.44722628593444824 s
INFO 10-15 15:27:25 client.py:117] confirm_model_loaded: Mixtral-8x7B, 99c76364-bae2-4d14-8eef-06c8bd658a01
INFO 10-15 15:27:25 client.py:125] Model loaded
DEBUG 10-15 15:27:25 lpllm.py:1048] j: load cost 0.449812650680542 s waiting cost 0.0025718212127685547 s
DEBUG 10-15 15:27:25 lpllm.py:924] 
DEBUG 10-15 15:27:25 lpllm.py:924] decoder loop j: 48 cur_layer_idx: 24 layer_attn_idx: 24 layer_mlp_idx: 23
DEBUG 10-15 15:27:25 lpllm.py:933] start load next layer cur_layer_idx: 25
DEBUG 10-15 15:27:25 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed6e0>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x004\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x98\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:25 client.py:72] load_into_gpu: Mixtral-8x7B, 2f346d47-8d26-4cfb-b03c-a74e9cf8f39d
INFO 10-15 15:27:25 client.py:113] Model loaded: Mixtral-8x7B, 2f346d47-8d26-4cfb-b03c-a74e9cf8f39d
DEBUG 10-15 15:27:25 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:25 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:25 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:25 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:25 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:25 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:25 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:25 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:25 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:25 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:25 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:25 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:25 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:25 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:25 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:25 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:25 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:25 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:25 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:25 lpllm.py:2265] GPU2CPU move cost 0.000630 seconds
DEBUG 10-15 15:27:25 lpmodule.py:340] layer_idx 24 attn layer_idx: 24
DEBUG 10-15 15:27:25 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:25 lpmodule.py:364] decoder_attn_batch update batch_dim 240-300
DEBUG 10-15 15:27:25 StaticCacheLen.py:84] static cache update layer_idx: 24, start_batch: 240, end_batch: 300
DEBUG 10-15 15:27:25 lpmodule.py:368] update for kv cache 240-300 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:25 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:25 lpmodule.py:374] update past key value cost 0.023155 seconds
DEBUG 10-15 15:27:25 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:25 lpmodule.py:227] layer idx 23
DEBUG 10-15 15:27:25 lpmodule.py:399] repeat qkv cost 0.030729 seconds
DEBUG 10-15 15:27:25 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:25 lpmodule.py:433] dot attn cost 0.034040 seconds
DEBUG 10-15 15:27:25 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:25 lpmodule.py:444] time cost move to cuda:1 0.023415327072143555 s
DEBUG 10-15 15:27:25 lpllm.py:2283] CPU attn cost 0.136277 seconds if batch True
DEBUG 10-15 15:27:25 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:27:25 lpllm.py:2294] CPU compute cost 0.137202 seconds
DEBUG 10-15 15:27:25 lpllm.py:2312] free cost 0.000094 seconds
DEBUG 10-15 15:27:25 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:25 lpllm.py:1774] update state cost 2.5033950805664062e-05 s
DEBUG 10-15 15:27:25 lpllm.py:1743] restore layer func cost 0.0007951259613037109 s
DEBUG 10-15 15:27:25 lpllm.py:511] restore layer cost 0.0010421276092529297 s
DEBUG 10-15 15:27:25 lpllm.py:512] reset_next_layer_need: layer_attn_idx=24, layer_mlp_idx=24, j_loc=48
DEBUG 10-15 15:27:25 lpllm.py:1037] reset layer cost 0.001119375228881836 s
DEBUG 10-15 15:27:25 lpllm.py:1038] have reset next layer layer_attn 24 layer_mlp 24 
DEBUG 10-15 15:27:25 lpllm.py:924] 
DEBUG 10-15 15:27:25 lpllm.py:924] decoder loop j: 49 cur_layer_idx: 24 layer_attn_idx: 24 layer_mlp_idx: 24
DEBUG 10-15 15:27:25 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:25 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:25 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:25 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:25 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:25 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:25 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:25 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:25 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:25 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:25 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:25 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:25 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:25 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:25 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:25 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:25 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:25 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:25 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:25 lpllm.py:2265] GPU2CPU move cost 0.000724 seconds
DEBUG 10-15 15:27:25 lpmodule.py:340] layer_idx 24 attn layer_idx: 24
DEBUG 10-15 15:27:25 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:25 lpmodule.py:364] decoder_attn_batch update batch_dim 300-360
DEBUG 10-15 15:27:25 StaticCacheLen.py:84] static cache update layer_idx: 24, start_batch: 300, end_batch: 360
DEBUG 10-15 15:27:25 lpmodule.py:368] update for kv cache 300-360 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:25 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:25 lpmodule.py:374] update past key value cost 0.021713 seconds
DEBUG 10-15 15:27:25 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:25 lpmodule.py:227] layer idx 24
DEBUG 10-15 15:27:25 lpmodule.py:399] repeat qkv cost 0.031682 seconds
DEBUG 10-15 15:27:25 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:25 lpmodule.py:433] dot attn cost 0.032644 seconds
DEBUG 10-15 15:27:25 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:25 lpmodule.py:444] time cost move to cuda:1 0.023696422576904297 s
DEBUG 10-15 15:27:25 lpllm.py:2283] CPU attn cost 0.135358 seconds if batch True
DEBUG 10-15 15:27:25 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:25 lpllm.py:2294] CPU compute cost 0.136384 seconds
DEBUG 10-15 15:27:25 lpllm.py:2312] free cost 0.000093 seconds
DEBUG 10-15 15:27:25 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:25 lpllm.py:1774] update state cost 2.2411346435546875e-05 s
DEBUG 10-15 15:27:25 lpllm.py:1743] restore layer func cost 0.0003895759582519531 s
DEBUG 10-15 15:27:25 lpllm.py:511] restore layer cost 0.0006215572357177734 s
DEBUG 10-15 15:27:25 lpllm.py:512] reset_next_layer_need: layer_attn_idx=25, layer_mlp_idx=24, j_loc=49
DEBUG 10-15 15:27:25 lpllm.py:1037] reset layer cost 0.0006909370422363281 s
DEBUG 10-15 15:27:25 lpllm.py:1038] have reset next layer layer_attn 25 layer_mlp 24 
DEBUG 10-15 15:27:25 lpllm.py:1044] j: 49 waiting the layer with layer_idx 25 before wait time 0.4503746032714844 s
INFO 10-15 15:27:25 client.py:117] confirm_model_loaded: Mixtral-8x7B, 2f346d47-8d26-4cfb-b03c-a74e9cf8f39d
INFO 10-15 15:27:25 client.py:125] Model loaded
DEBUG 10-15 15:27:25 lpllm.py:1048] j: load cost 0.45230555534362793 s waiting cost 0.0019173622131347656 s
DEBUG 10-15 15:27:25 lpllm.py:924] 
DEBUG 10-15 15:27:25 lpllm.py:924] decoder loop j: 50 cur_layer_idx: 25 layer_attn_idx: 25 layer_mlp_idx: 24
DEBUG 10-15 15:27:25 lpllm.py:933] start load next layer cur_layer_idx: 26
DEBUG 10-15 15:27:25 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:25 client.py:72] load_into_gpu: Mixtral-8x7B, cbce0085-f4ad-47ca-8d70-b3bb1498a407
INFO 10-15 15:27:25 client.py:113] Model loaded: Mixtral-8x7B, cbce0085-f4ad-47ca-8d70-b3bb1498a407
DEBUG 10-15 15:27:25 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:25 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:25 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:25 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:25 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:25 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:25 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:25 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:25 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:25 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:25 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:25 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:25 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:25 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:25 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:25 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:25 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:25 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:25 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:25 lpllm.py:2265] GPU2CPU move cost 0.000726 seconds
DEBUG 10-15 15:27:25 lpmodule.py:340] layer_idx 25 attn layer_idx: 25
DEBUG 10-15 15:27:25 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:25 lpmodule.py:364] decoder_attn_batch update batch_dim 240-300
DEBUG 10-15 15:27:25 StaticCacheLen.py:84] static cache update layer_idx: 25, start_batch: 240, end_batch: 300
DEBUG 10-15 15:27:25 lpmodule.py:368] update for kv cache 240-300 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:25 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:25 lpmodule.py:374] update past key value cost 0.025140 seconds
DEBUG 10-15 15:27:25 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:25 lpmodule.py:227] layer idx 24
DEBUG 10-15 15:27:26 lpmodule.py:399] repeat qkv cost 0.031860 seconds
DEBUG 10-15 15:27:26 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:26 lpmodule.py:433] dot attn cost 0.032114 seconds
DEBUG 10-15 15:27:26 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:26 lpmodule.py:444] time cost move to cuda:1 0.02343010902404785 s
DEBUG 10-15 15:27:26 lpllm.py:2283] CPU attn cost 0.138232 seconds if batch True
DEBUG 10-15 15:27:26 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:26 lpllm.py:2294] CPU compute cost 0.139264 seconds
DEBUG 10-15 15:27:26 lpllm.py:2312] free cost 0.000095 seconds
DEBUG 10-15 15:27:26 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:26 lpllm.py:1774] update state cost 1.9550323486328125e-05 s
DEBUG 10-15 15:27:26 lpllm.py:1743] restore layer func cost 0.0007987022399902344 s
DEBUG 10-15 15:27:26 lpllm.py:511] restore layer cost 0.001039743423461914 s
DEBUG 10-15 15:27:26 lpllm.py:512] reset_next_layer_need: layer_attn_idx=25, layer_mlp_idx=25, j_loc=50
DEBUG 10-15 15:27:26 lpllm.py:1037] reset layer cost 0.00112152099609375 s
DEBUG 10-15 15:27:26 lpllm.py:1038] have reset next layer layer_attn 25 layer_mlp 25 
DEBUG 10-15 15:27:26 lpllm.py:924] 
DEBUG 10-15 15:27:26 lpllm.py:924] decoder loop j: 51 cur_layer_idx: 25 layer_attn_idx: 25 layer_mlp_idx: 25
DEBUG 10-15 15:27:26 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:26 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:26 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:26 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:26 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:26 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:26 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:26 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:26 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:26 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:26 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:26 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:26 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:26 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:26 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:26 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:26 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:26 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:26 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:26 lpllm.py:2265] GPU2CPU move cost 0.000796 seconds
DEBUG 10-15 15:27:26 lpmodule.py:340] layer_idx 25 attn layer_idx: 25
DEBUG 10-15 15:27:26 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:26 lpmodule.py:364] decoder_attn_batch update batch_dim 300-360
DEBUG 10-15 15:27:26 StaticCacheLen.py:84] static cache update layer_idx: 25, start_batch: 300, end_batch: 360
DEBUG 10-15 15:27:26 lpmodule.py:368] update for kv cache 300-360 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:26 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:26 lpmodule.py:374] update past key value cost 0.023556 seconds
DEBUG 10-15 15:27:26 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:26 lpmodule.py:227] layer idx 25
DEBUG 10-15 15:27:26 lpmodule.py:399] repeat qkv cost 0.030009 seconds
DEBUG 10-15 15:27:26 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:26 lpmodule.py:433] dot attn cost 0.033504 seconds
DEBUG 10-15 15:27:26 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:26 lpmodule.py:444] time cost move to cuda:1 0.026165485382080078 s
DEBUG 10-15 15:27:26 lpllm.py:2283] CPU attn cost 0.138032 seconds if batch True
DEBUG 10-15 15:27:26 lpllm.py:2292] deal attn result cost 0.000003 seconds
DEBUG 10-15 15:27:26 lpllm.py:2294] CPU compute cost 0.139198 seconds
DEBUG 10-15 15:27:26 lpllm.py:2312] free cost 0.000098 seconds
DEBUG 10-15 15:27:26 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:26 lpllm.py:1774] update state cost 2.4557113647460938e-05 s
DEBUG 10-15 15:27:26 lpllm.py:1743] restore layer func cost 0.0004258155822753906 s
DEBUG 10-15 15:27:26 lpllm.py:511] restore layer cost 0.0006594657897949219 s
DEBUG 10-15 15:27:26 lpllm.py:512] reset_next_layer_need: layer_attn_idx=26, layer_mlp_idx=25, j_loc=51
DEBUG 10-15 15:27:26 lpllm.py:1037] reset layer cost 0.0007371902465820312 s
DEBUG 10-15 15:27:26 lpllm.py:1038] have reset next layer layer_attn 26 layer_mlp 25 
DEBUG 10-15 15:27:26 lpllm.py:1044] j: 51 waiting the layer with layer_idx 26 before wait time 0.4547445774078369 s
INFO 10-15 15:27:26 client.py:117] confirm_model_loaded: Mixtral-8x7B, cbce0085-f4ad-47ca-8d70-b3bb1498a407
INFO 10-15 15:27:26 client.py:125] Model loaded
DEBUG 10-15 15:27:26 lpllm.py:1048] j: load cost 0.45658063888549805 s waiting cost 0.0018205642700195312 s
DEBUG 10-15 15:27:26 lpllm.py:924] 
DEBUG 10-15 15:27:26 lpllm.py:924] decoder loop j: 52 cur_layer_idx: 26 layer_attn_idx: 26 layer_mlp_idx: 25
DEBUG 10-15 15:27:26 lpllm.py:933] start load next layer cur_layer_idx: 27
DEBUG 10-15 15:27:26 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed650>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x003\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x96\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:26 client.py:72] load_into_gpu: Mixtral-8x7B, 61c15f3c-d338-4cbb-8672-4aa1e4f2c930
INFO 10-15 15:27:26 client.py:113] Model loaded: Mixtral-8x7B, 61c15f3c-d338-4cbb-8672-4aa1e4f2c930
DEBUG 10-15 15:27:26 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:26 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:26 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:26 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:26 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:26 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:26 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:26 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:26 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:26 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:26 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:26 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:26 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:26 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:26 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:26 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:26 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:26 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:26 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:26 lpllm.py:2265] GPU2CPU move cost 0.000754 seconds
DEBUG 10-15 15:27:26 lpmodule.py:340] layer_idx 26 attn layer_idx: 26
DEBUG 10-15 15:27:26 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:26 lpmodule.py:364] decoder_attn_batch update batch_dim 240-300
DEBUG 10-15 15:27:26 StaticCacheLen.py:84] static cache update layer_idx: 26, start_batch: 240, end_batch: 300
DEBUG 10-15 15:27:26 lpmodule.py:368] update for kv cache 240-300 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:26 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:26 lpmodule.py:374] update past key value cost 0.025056 seconds
DEBUG 10-15 15:27:26 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:26 lpmodule.py:227] layer idx 25
DEBUG 10-15 15:27:26 lpmodule.py:399] repeat qkv cost 0.031219 seconds
DEBUG 10-15 15:27:26 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:26 lpmodule.py:433] dot attn cost 0.033073 seconds
DEBUG 10-15 15:27:26 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:26 lpmodule.py:444] time cost move to cuda:1 0.022992849349975586 s
DEBUG 10-15 15:27:26 lpllm.py:2283] CPU attn cost 0.136663 seconds if batch True
DEBUG 10-15 15:27:26 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:26 lpllm.py:2294] CPU compute cost 0.137724 seconds
DEBUG 10-15 15:27:26 lpllm.py:2312] free cost 0.000095 seconds
DEBUG 10-15 15:27:26 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:26 lpllm.py:1774] update state cost 3.719329833984375e-05 s
DEBUG 10-15 15:27:26 lpllm.py:1743] restore layer func cost 0.0008413791656494141 s
DEBUG 10-15 15:27:26 lpllm.py:511] restore layer cost 0.001094818115234375 s
DEBUG 10-15 15:27:26 lpllm.py:512] reset_next_layer_need: layer_attn_idx=26, layer_mlp_idx=26, j_loc=52
DEBUG 10-15 15:27:26 lpllm.py:1037] reset layer cost 0.0011761188507080078 s
DEBUG 10-15 15:27:26 lpllm.py:1038] have reset next layer layer_attn 26 layer_mlp 26 
DEBUG 10-15 15:27:26 lpllm.py:924] 
DEBUG 10-15 15:27:26 lpllm.py:924] decoder loop j: 53 cur_layer_idx: 26 layer_attn_idx: 26 layer_mlp_idx: 26
DEBUG 10-15 15:27:26 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:26 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:26 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:26 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:26 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:26 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:26 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:26 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:26 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:26 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:26 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:26 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:26 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:26 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:26 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:26 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:26 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:26 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:26 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:26 lpllm.py:2265] GPU2CPU move cost 0.000778 seconds
DEBUG 10-15 15:27:26 lpmodule.py:340] layer_idx 26 attn layer_idx: 26
DEBUG 10-15 15:27:26 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:26 lpmodule.py:364] decoder_attn_batch update batch_dim 300-360
DEBUG 10-15 15:27:26 StaticCacheLen.py:84] static cache update layer_idx: 26, start_batch: 300, end_batch: 360
DEBUG 10-15 15:27:26 lpmodule.py:368] update for kv cache 300-360 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:26 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:26 lpmodule.py:374] update past key value cost 0.025207 seconds
DEBUG 10-15 15:27:26 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:26 lpmodule.py:227] layer idx 26
DEBUG 10-15 15:27:26 lpmodule.py:399] repeat qkv cost 0.031008 seconds
DEBUG 10-15 15:27:26 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:26 lpmodule.py:433] dot attn cost 0.044335 seconds
DEBUG 10-15 15:27:26 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:26 lpmodule.py:444] time cost move to cuda:1 0.024491071701049805 s
DEBUG 10-15 15:27:26 lpllm.py:2283] CPU attn cost 0.149500 seconds if batch True
DEBUG 10-15 15:27:26 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:27:26 lpllm.py:2294] CPU compute cost 0.150578 seconds
DEBUG 10-15 15:27:26 lpllm.py:2312] free cost 0.000076 seconds
DEBUG 10-15 15:27:26 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:26 lpllm.py:1774] update state cost 4.267692565917969e-05 s
DEBUG 10-15 15:27:26 lpllm.py:1743] restore layer func cost 0.0004410743713378906 s
DEBUG 10-15 15:27:26 lpllm.py:511] restore layer cost 0.0007076263427734375 s
DEBUG 10-15 15:27:26 lpllm.py:512] reset_next_layer_need: layer_attn_idx=27, layer_mlp_idx=26, j_loc=53
DEBUG 10-15 15:27:26 lpllm.py:1037] reset layer cost 0.0007915496826171875 s
DEBUG 10-15 15:27:26 lpllm.py:1038] have reset next layer layer_attn 27 layer_mlp 26 
DEBUG 10-15 15:27:26 lpllm.py:1044] j: 53 waiting the layer with layer_idx 27 before wait time 0.4491710662841797 s
INFO 10-15 15:27:26 client.py:117] confirm_model_loaded: Mixtral-8x7B, 61c15f3c-d338-4cbb-8672-4aa1e4f2c930
INFO 10-15 15:27:26 client.py:125] Model loaded
DEBUG 10-15 15:27:26 lpllm.py:1048] j: load cost 0.4515511989593506 s waiting cost 0.0023648738861083984 s
DEBUG 10-15 15:27:26 lpllm.py:924] 
DEBUG 10-15 15:27:26 lpllm.py:924] decoder loop j: 54 cur_layer_idx: 27 layer_attn_idx: 27 layer_mlp_idx: 26
DEBUG 10-15 15:27:26 lpllm.py:933] start load next layer cur_layer_idx: 28
DEBUG 10-15 15:27:26 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed6e0>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x004\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x98\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:26 client.py:72] load_into_gpu: Mixtral-8x7B, fae9b8fa-c944-4f8a-ab3f-88c998c6a3c4
INFO 10-15 15:27:26 client.py:113] Model loaded: Mixtral-8x7B, fae9b8fa-c944-4f8a-ab3f-88c998c6a3c4
DEBUG 10-15 15:27:26 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:26 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:26 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:26 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:26 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:26 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:26 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:26 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:26 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:26 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:26 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:26 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:26 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:26 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:26 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:26 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:26 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:26 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:26 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:26 lpllm.py:2265] GPU2CPU move cost 0.000679 seconds
DEBUG 10-15 15:27:26 lpmodule.py:340] layer_idx 27 attn layer_idx: 27
DEBUG 10-15 15:27:26 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:26 lpmodule.py:364] decoder_attn_batch update batch_dim 240-300
DEBUG 10-15 15:27:26 StaticCacheLen.py:84] static cache update layer_idx: 27, start_batch: 240, end_batch: 300
DEBUG 10-15 15:27:26 lpmodule.py:368] update for kv cache 240-300 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:26 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:26 lpmodule.py:374] update past key value cost 0.026786 seconds
DEBUG 10-15 15:27:26 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:26 lpmodule.py:227] layer idx 26
DEBUG 10-15 15:27:26 lpmodule.py:399] repeat qkv cost 0.031509 seconds
DEBUG 10-15 15:27:26 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:26 lpmodule.py:433] dot attn cost 0.033931 seconds
DEBUG 10-15 15:27:26 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:26 lpmodule.py:444] time cost move to cuda:1 0.027413368225097656 s
DEBUG 10-15 15:27:27 lpllm.py:2283] CPU attn cost 0.150105 seconds if batch True
DEBUG 10-15 15:27:27 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:27 lpllm.py:2294] CPU compute cost 0.151166 seconds
DEBUG 10-15 15:27:27 lpllm.py:2312] free cost 0.000094 seconds
DEBUG 10-15 15:27:27 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:27 lpllm.py:1774] update state cost 3.790855407714844e-05 s
DEBUG 10-15 15:27:27 lpllm.py:1743] restore layer func cost 0.003900289535522461 s
DEBUG 10-15 15:27:27 lpllm.py:511] restore layer cost 0.004215240478515625 s
DEBUG 10-15 15:27:27 lpllm.py:512] reset_next_layer_need: layer_attn_idx=27, layer_mlp_idx=27, j_loc=54
DEBUG 10-15 15:27:27 lpllm.py:1037] reset layer cost 0.004307985305786133 s
DEBUG 10-15 15:27:27 lpllm.py:1038] have reset next layer layer_attn 27 layer_mlp 27 
DEBUG 10-15 15:27:27 lpllm.py:924] 
DEBUG 10-15 15:27:27 lpllm.py:924] decoder loop j: 55 cur_layer_idx: 27 layer_attn_idx: 27 layer_mlp_idx: 27
DEBUG 10-15 15:27:27 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:27 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:27 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:27 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:27 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:27 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:27 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:27 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:27 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:27 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:27 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:27 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:27 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:27 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:27 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:27 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:27 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:27 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:27 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:27 lpllm.py:2265] GPU2CPU move cost 0.000657 seconds
DEBUG 10-15 15:27:27 lpmodule.py:340] layer_idx 27 attn layer_idx: 27
DEBUG 10-15 15:27:27 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:27 lpmodule.py:364] decoder_attn_batch update batch_dim 300-360
DEBUG 10-15 15:27:27 StaticCacheLen.py:84] static cache update layer_idx: 27, start_batch: 300, end_batch: 360
DEBUG 10-15 15:27:27 lpmodule.py:368] update for kv cache 300-360 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:27 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:27 lpmodule.py:374] update past key value cost 0.023021 seconds
DEBUG 10-15 15:27:27 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:27 lpmodule.py:227] layer idx 27
DEBUG 10-15 15:27:27 lpmodule.py:399] repeat qkv cost 0.032497 seconds
DEBUG 10-15 15:27:27 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:27 lpmodule.py:433] dot attn cost 0.044263 seconds
DEBUG 10-15 15:27:27 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:27 lpmodule.py:444] time cost move to cuda:1 0.025257587432861328 s
DEBUG 10-15 15:27:27 lpllm.py:2283] CPU attn cost 0.149028 seconds if batch True
DEBUG 10-15 15:27:27 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:27 lpllm.py:2294] CPU compute cost 0.149971 seconds
DEBUG 10-15 15:27:27 lpllm.py:2312] free cost 0.000076 seconds
DEBUG 10-15 15:27:27 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:27 lpllm.py:1774] update state cost 3.9577484130859375e-05 s
DEBUG 10-15 15:27:27 lpllm.py:1743] restore layer func cost 0.00040340423583984375 s
DEBUG 10-15 15:27:27 lpllm.py:511] restore layer cost 0.0006704330444335938 s
DEBUG 10-15 15:27:27 lpllm.py:512] reset_next_layer_need: layer_attn_idx=28, layer_mlp_idx=27, j_loc=55
DEBUG 10-15 15:27:27 lpllm.py:1037] reset layer cost 0.0007469654083251953 s
DEBUG 10-15 15:27:27 lpllm.py:1038] have reset next layer layer_attn 28 layer_mlp 27 
DEBUG 10-15 15:27:27 lpllm.py:1044] j: 55 waiting the layer with layer_idx 28 before wait time 0.46231508255004883 s
INFO 10-15 15:27:27 client.py:117] confirm_model_loaded: Mixtral-8x7B, fae9b8fa-c944-4f8a-ab3f-88c998c6a3c4
INFO 10-15 15:27:27 client.py:125] Model loaded
DEBUG 10-15 15:27:27 lpllm.py:1048] j: load cost 0.46413731575012207 s waiting cost 0.0018055438995361328 s
DEBUG 10-15 15:27:27 lpllm.py:924] 
DEBUG 10-15 15:27:27 lpllm.py:924] decoder loop j: 56 cur_layer_idx: 28 layer_attn_idx: 28 layer_mlp_idx: 27
DEBUG 10-15 15:27:27 lpllm.py:933] start load next layer cur_layer_idx: 29
DEBUG 10-15 15:27:27 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:27 client.py:72] load_into_gpu: Mixtral-8x7B, e99ce1f2-495e-45b3-b7b2-726e9ef52687
INFO 10-15 15:27:27 client.py:113] Model loaded: Mixtral-8x7B, e99ce1f2-495e-45b3-b7b2-726e9ef52687
DEBUG 10-15 15:27:27 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:27 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:27 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:27 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:27 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:27 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:27 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:27 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:27 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:27 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:27 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:27 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:27 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:27 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:27 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:27 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:27 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:27 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:27 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:27 lpllm.py:2265] GPU2CPU move cost 0.000612 seconds
DEBUG 10-15 15:27:27 lpmodule.py:340] layer_idx 28 attn layer_idx: 28
DEBUG 10-15 15:27:27 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:27 lpmodule.py:364] decoder_attn_batch update batch_dim 240-300
DEBUG 10-15 15:27:27 StaticCacheLen.py:84] static cache update layer_idx: 28, start_batch: 240, end_batch: 300
DEBUG 10-15 15:27:27 lpmodule.py:368] update for kv cache 240-300 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:27 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:27 lpmodule.py:374] update past key value cost 0.026286 seconds
DEBUG 10-15 15:27:27 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:27 lpmodule.py:227] layer idx 27
DEBUG 10-15 15:27:27 lpmodule.py:399] repeat qkv cost 0.033078 seconds
DEBUG 10-15 15:27:27 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:27 lpmodule.py:433] dot attn cost 0.039090 seconds
DEBUG 10-15 15:27:27 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:27 lpmodule.py:444] time cost move to cuda:1 0.023287057876586914 s
DEBUG 10-15 15:27:27 lpllm.py:2283] CPU attn cost 0.146000 seconds if batch True
DEBUG 10-15 15:27:27 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:27:27 lpllm.py:2294] CPU compute cost 0.146900 seconds
DEBUG 10-15 15:27:27 lpllm.py:2312] free cost 0.000077 seconds
DEBUG 10-15 15:27:27 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:27 lpllm.py:1774] update state cost 3.695487976074219e-05 s
DEBUG 10-15 15:27:27 lpllm.py:1743] restore layer func cost 0.000835418701171875 s
DEBUG 10-15 15:27:27 lpllm.py:511] restore layer cost 0.0010843276977539062 s
DEBUG 10-15 15:27:27 lpllm.py:512] reset_next_layer_need: layer_attn_idx=28, layer_mlp_idx=28, j_loc=56
DEBUG 10-15 15:27:27 lpllm.py:1037] reset layer cost 0.0011637210845947266 s
DEBUG 10-15 15:27:27 lpllm.py:1038] have reset next layer layer_attn 28 layer_mlp 28 
DEBUG 10-15 15:27:27 lpllm.py:924] 
DEBUG 10-15 15:27:27 lpllm.py:924] decoder loop j: 57 cur_layer_idx: 28 layer_attn_idx: 28 layer_mlp_idx: 28
DEBUG 10-15 15:27:27 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:27 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:27 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:27 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:27 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:27 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:27 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:27 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:27 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:27 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:27 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:27 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:27 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:27 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:27 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:27 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:27 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:27 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:27 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:27 lpllm.py:2265] GPU2CPU move cost 0.000576 seconds
DEBUG 10-15 15:27:27 lpmodule.py:340] layer_idx 28 attn layer_idx: 28
DEBUG 10-15 15:27:27 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:27 lpmodule.py:364] decoder_attn_batch update batch_dim 300-360
DEBUG 10-15 15:27:27 StaticCacheLen.py:84] static cache update layer_idx: 28, start_batch: 300, end_batch: 360
DEBUG 10-15 15:27:27 lpmodule.py:368] update for kv cache 300-360 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:27 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:27 lpmodule.py:374] update past key value cost 0.024126 seconds
DEBUG 10-15 15:27:27 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:27 lpmodule.py:227] layer idx 28
DEBUG 10-15 15:27:27 lpmodule.py:399] repeat qkv cost 0.034835 seconds
DEBUG 10-15 15:27:27 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:27 lpmodule.py:433] dot attn cost 0.040058 seconds
DEBUG 10-15 15:27:27 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:27 lpmodule.py:444] time cost move to cuda:1 0.022455930709838867 s
DEBUG 10-15 15:27:27 lpllm.py:2283] CPU attn cost 0.145549 seconds if batch True
DEBUG 10-15 15:27:27 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:27 lpllm.py:2294] CPU compute cost 0.146410 seconds
DEBUG 10-15 15:27:27 lpllm.py:2312] free cost 0.000077 seconds
DEBUG 10-15 15:27:27 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:27 lpllm.py:1774] update state cost 4.029273986816406e-05 s
DEBUG 10-15 15:27:27 lpllm.py:1743] restore layer func cost 0.0003948211669921875 s
DEBUG 10-15 15:27:27 lpllm.py:511] restore layer cost 0.0006413459777832031 s
DEBUG 10-15 15:27:27 lpllm.py:512] reset_next_layer_need: layer_attn_idx=29, layer_mlp_idx=28, j_loc=57
DEBUG 10-15 15:27:27 lpllm.py:1037] reset layer cost 0.0007123947143554688 s
DEBUG 10-15 15:27:27 lpllm.py:1038] have reset next layer layer_attn 29 layer_mlp 28 
DEBUG 10-15 15:27:27 lpllm.py:1044] j: 57 waiting the layer with layer_idx 29 before wait time 0.453723669052124 s
INFO 10-15 15:27:27 client.py:117] confirm_model_loaded: Mixtral-8x7B, e99ce1f2-495e-45b3-b7b2-726e9ef52687
INFO 10-15 15:27:27 client.py:125] Model loaded
DEBUG 10-15 15:27:27 lpllm.py:1048] j: load cost 0.45525622367858887 s waiting cost 0.0015184879302978516 s
DEBUG 10-15 15:27:27 lpllm.py:924] 
DEBUG 10-15 15:27:27 lpllm.py:924] decoder loop j: 58 cur_layer_idx: 29 layer_attn_idx: 29 layer_mlp_idx: 28
DEBUG 10-15 15:27:27 lpllm.py:933] start load next layer cur_layer_idx: 30
DEBUG 10-15 15:27:27 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed650>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x003\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x96\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:27 client.py:72] load_into_gpu: Mixtral-8x7B, 34813db4-8ce2-4317-afd2-c649c94aaa33
INFO 10-15 15:27:27 client.py:113] Model loaded: Mixtral-8x7B, 34813db4-8ce2-4317-afd2-c649c94aaa33
DEBUG 10-15 15:27:27 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:27 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:27 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:27 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:27 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:27 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:27 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:27 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:27 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:27 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:27 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:27 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:27 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:27 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:27 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:27 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:27 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:27 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:27 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:27 lpllm.py:2265] GPU2CPU move cost 0.000613 seconds
DEBUG 10-15 15:27:27 lpmodule.py:340] layer_idx 29 attn layer_idx: 29
DEBUG 10-15 15:27:27 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:27 lpmodule.py:364] decoder_attn_batch update batch_dim 240-300
DEBUG 10-15 15:27:27 StaticCacheLen.py:84] static cache update layer_idx: 29, start_batch: 240, end_batch: 300
DEBUG 10-15 15:27:27 lpmodule.py:368] update for kv cache 240-300 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:27 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:27 lpmodule.py:374] update past key value cost 0.023853 seconds
DEBUG 10-15 15:27:27 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:27 lpmodule.py:227] layer idx 28
DEBUG 10-15 15:27:27 lpmodule.py:399] repeat qkv cost 0.031492 seconds
DEBUG 10-15 15:27:27 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:27 lpmodule.py:433] dot attn cost 0.032572 seconds
DEBUG 10-15 15:27:27 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:27 lpmodule.py:444] time cost move to cuda:1 0.0232694149017334 s
DEBUG 10-15 15:27:27 lpllm.py:2283] CPU attn cost 0.135637 seconds if batch True
DEBUG 10-15 15:27:27 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:27 lpllm.py:2294] CPU compute cost 0.136528 seconds
DEBUG 10-15 15:27:27 lpllm.py:2312] free cost 0.000078 seconds
DEBUG 10-15 15:27:28 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:28 lpllm.py:1774] update state cost 3.743171691894531e-05 s
DEBUG 10-15 15:27:28 lpllm.py:1743] restore layer func cost 0.000858306884765625 s
DEBUG 10-15 15:27:28 lpllm.py:511] restore layer cost 0.001117706298828125 s
DEBUG 10-15 15:27:28 lpllm.py:512] reset_next_layer_need: layer_attn_idx=29, layer_mlp_idx=29, j_loc=58
DEBUG 10-15 15:27:28 lpllm.py:1037] reset layer cost 0.0011935234069824219 s
DEBUG 10-15 15:27:28 lpllm.py:1038] have reset next layer layer_attn 29 layer_mlp 29 
DEBUG 10-15 15:27:28 lpllm.py:924] 
DEBUG 10-15 15:27:28 lpllm.py:924] decoder loop j: 59 cur_layer_idx: 29 layer_attn_idx: 29 layer_mlp_idx: 29
DEBUG 10-15 15:27:28 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:28 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:28 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:28 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:28 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:28 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:28 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:28 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:28 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:28 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:28 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:28 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:28 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:28 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:28 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:28 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:28 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:28 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:28 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:28 lpllm.py:2265] GPU2CPU move cost 0.000579 seconds
DEBUG 10-15 15:27:28 lpmodule.py:340] layer_idx 29 attn layer_idx: 29
DEBUG 10-15 15:27:28 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:28 lpmodule.py:364] decoder_attn_batch update batch_dim 300-360
DEBUG 10-15 15:27:28 StaticCacheLen.py:84] static cache update layer_idx: 29, start_batch: 300, end_batch: 360
DEBUG 10-15 15:27:28 lpmodule.py:368] update for kv cache 300-360 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:28 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:28 lpmodule.py:374] update past key value cost 0.021946 seconds
DEBUG 10-15 15:27:28 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:28 lpmodule.py:227] layer idx 29
DEBUG 10-15 15:27:28 lpmodule.py:399] repeat qkv cost 0.032621 seconds
DEBUG 10-15 15:27:28 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:28 lpmodule.py:433] dot attn cost 0.042847 seconds
DEBUG 10-15 15:27:28 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:28 lpmodule.py:444] time cost move to cuda:1 0.0228424072265625 s
DEBUG 10-15 15:27:28 lpllm.py:2283] CPU attn cost 0.144762 seconds if batch True
DEBUG 10-15 15:27:28 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:27:28 lpllm.py:2294] CPU compute cost 0.145629 seconds
DEBUG 10-15 15:27:28 lpllm.py:2312] free cost 0.000079 seconds
DEBUG 10-15 15:27:28 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:28 lpllm.py:1774] update state cost 2.1696090698242188e-05 s
DEBUG 10-15 15:27:28 lpllm.py:1743] restore layer func cost 0.0003838539123535156 s
DEBUG 10-15 15:27:28 lpllm.py:511] restore layer cost 0.0006120204925537109 s
DEBUG 10-15 15:27:28 lpllm.py:512] reset_next_layer_need: layer_attn_idx=30, layer_mlp_idx=29, j_loc=59
DEBUG 10-15 15:27:28 lpllm.py:1037] reset layer cost 0.0006833076477050781 s
DEBUG 10-15 15:27:28 lpllm.py:1038] have reset next layer layer_attn 30 layer_mlp 29 
DEBUG 10-15 15:27:28 lpllm.py:1044] j: 59 waiting the layer with layer_idx 30 before wait time 0.52752685546875 s
INFO 10-15 15:27:28 client.py:117] confirm_model_loaded: Mixtral-8x7B, 34813db4-8ce2-4317-afd2-c649c94aaa33
INFO 10-15 15:27:28 client.py:125] Model loaded
DEBUG 10-15 15:27:28 lpllm.py:1048] j: load cost 0.5293042659759521 s waiting cost 0.0017631053924560547 s
DEBUG 10-15 15:27:28 lpllm.py:924] 
DEBUG 10-15 15:27:28 lpllm.py:924] decoder loop j: 60 cur_layer_idx: 30 layer_attn_idx: 30 layer_mlp_idx: 29
DEBUG 10-15 15:27:28 lpllm.py:933] start load next layer cur_layer_idx: 31
DEBUG 10-15 15:27:28 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed6e0>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x004\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x98\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:28 client.py:72] load_into_gpu: Mixtral-8x7B, 5e02d095-06e6-4b32-9749-9b337729b97c
INFO 10-15 15:27:28 client.py:113] Model loaded: Mixtral-8x7B, 5e02d095-06e6-4b32-9749-9b337729b97c
DEBUG 10-15 15:27:28 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:28 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:28 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:28 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:28 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:28 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:28 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:28 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:28 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:28 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:28 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:28 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:28 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:28 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:28 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:28 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:28 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:28 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:28 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:28 lpllm.py:2265] GPU2CPU move cost 0.000575 seconds
DEBUG 10-15 15:27:28 lpmodule.py:340] layer_idx 30 attn layer_idx: 30
DEBUG 10-15 15:27:28 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:28 lpmodule.py:364] decoder_attn_batch update batch_dim 240-300
DEBUG 10-15 15:27:28 StaticCacheLen.py:84] static cache update layer_idx: 30, start_batch: 240, end_batch: 300
DEBUG 10-15 15:27:28 lpmodule.py:368] update for kv cache 240-300 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:28 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:28 lpmodule.py:374] update past key value cost 0.026610 seconds
DEBUG 10-15 15:27:28 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:28 lpmodule.py:227] layer idx 29
DEBUG 10-15 15:27:28 lpmodule.py:399] repeat qkv cost 0.030372 seconds
DEBUG 10-15 15:27:28 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:28 lpmodule.py:433] dot attn cost 0.033112 seconds
DEBUG 10-15 15:27:28 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:28 lpmodule.py:444] time cost move to cuda:1 0.023766517639160156 s
DEBUG 10-15 15:27:28 lpllm.py:2283] CPU attn cost 0.140334 seconds if batch True
DEBUG 10-15 15:27:28 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:28 lpllm.py:2294] CPU compute cost 0.141203 seconds
DEBUG 10-15 15:27:28 lpllm.py:2312] free cost 0.000077 seconds
DEBUG 10-15 15:27:28 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:28 lpllm.py:1774] update state cost 3.814697265625e-05 s
DEBUG 10-15 15:27:28 lpllm.py:1743] restore layer func cost 0.0008418560028076172 s
DEBUG 10-15 15:27:28 lpllm.py:511] restore layer cost 0.0010974407196044922 s
DEBUG 10-15 15:27:28 lpllm.py:512] reset_next_layer_need: layer_attn_idx=30, layer_mlp_idx=30, j_loc=60
DEBUG 10-15 15:27:28 lpllm.py:1037] reset layer cost 0.0011763572692871094 s
DEBUG 10-15 15:27:28 lpllm.py:1038] have reset next layer layer_attn 30 layer_mlp 30 
DEBUG 10-15 15:27:28 lpllm.py:924] 
DEBUG 10-15 15:27:28 lpllm.py:924] decoder loop j: 61 cur_layer_idx: 30 layer_attn_idx: 30 layer_mlp_idx: 30
DEBUG 10-15 15:27:28 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:28 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:28 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:28 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:28 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:28 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:28 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:28 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:28 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:28 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:28 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:28 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:28 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:28 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:28 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:28 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:28 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:28 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:28 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:28 lpllm.py:2265] GPU2CPU move cost 0.000622 seconds
DEBUG 10-15 15:27:28 lpmodule.py:340] layer_idx 30 attn layer_idx: 30
DEBUG 10-15 15:27:28 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:28 lpmodule.py:364] decoder_attn_batch update batch_dim 300-360
DEBUG 10-15 15:27:28 StaticCacheLen.py:84] static cache update layer_idx: 30, start_batch: 300, end_batch: 360
DEBUG 10-15 15:27:28 lpmodule.py:368] update for kv cache 300-360 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:28 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:28 lpmodule.py:374] update past key value cost 0.024333 seconds
DEBUG 10-15 15:27:28 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:28 lpmodule.py:227] layer idx 30
DEBUG 10-15 15:27:28 lpmodule.py:399] repeat qkv cost 0.030228 seconds
DEBUG 10-15 15:27:28 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:28 lpmodule.py:433] dot attn cost 0.033489 seconds
DEBUG 10-15 15:27:28 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:28 lpmodule.py:444] time cost move to cuda:1 0.02332472801208496 s
DEBUG 10-15 15:27:28 lpllm.py:2283] CPU attn cost 0.138303 seconds if batch True
DEBUG 10-15 15:27:28 lpllm.py:2292] deal attn result cost 0.000003 seconds
DEBUG 10-15 15:27:28 lpllm.py:2294] CPU compute cost 0.139218 seconds
DEBUG 10-15 15:27:28 lpllm.py:2312] free cost 0.000079 seconds
DEBUG 10-15 15:27:28 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:28 lpllm.py:1774] update state cost 3.838539123535156e-05 s
DEBUG 10-15 15:27:28 lpllm.py:1743] restore layer func cost 0.0004017353057861328 s
DEBUG 10-15 15:27:28 lpllm.py:511] restore layer cost 0.0006463527679443359 s
DEBUG 10-15 15:27:28 lpllm.py:512] reset_next_layer_need: layer_attn_idx=31, layer_mlp_idx=30, j_loc=61
DEBUG 10-15 15:27:28 lpllm.py:1037] reset layer cost 0.0007212162017822266 s
DEBUG 10-15 15:27:28 lpllm.py:1038] have reset next layer layer_attn 31 layer_mlp 30 
DEBUG 10-15 15:27:28 lpllm.py:1044] j: 61 waiting the layer with layer_idx 31 before wait time 0.44945549964904785 s
INFO 10-15 15:27:28 client.py:117] confirm_model_loaded: Mixtral-8x7B, 5e02d095-06e6-4b32-9749-9b337729b97c
INFO 10-15 15:27:28 client.py:125] Model loaded
DEBUG 10-15 15:27:28 lpllm.py:1048] j: load cost 0.4510228633880615 s waiting cost 0.0015521049499511719 s
DEBUG 10-15 15:27:28 lpllm.py:924] 
DEBUG 10-15 15:27:28 lpllm.py:924] decoder loop j: 62 cur_layer_idx: 31 layer_attn_idx: 31 layer_mlp_idx: 30
DEBUG 10-15 15:27:28 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:28 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:28 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:28 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:28 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:28 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:28 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:28 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:28 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:28 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:28 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:28 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:28 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:28 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:28 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:28 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:28 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:28 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:28 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:28 lpllm.py:2265] GPU2CPU move cost 0.000603 seconds
DEBUG 10-15 15:27:28 lpmodule.py:340] layer_idx 31 attn layer_idx: 31
DEBUG 10-15 15:27:28 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:28 lpmodule.py:364] decoder_attn_batch update batch_dim 240-300
DEBUG 10-15 15:27:28 StaticCacheLen.py:84] static cache update layer_idx: 31, start_batch: 240, end_batch: 300
DEBUG 10-15 15:27:28 lpmodule.py:368] update for kv cache 240-300 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:28 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:28 lpmodule.py:374] update past key value cost 0.022272 seconds
DEBUG 10-15 15:27:28 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:28 lpmodule.py:227] layer idx 30
DEBUG 10-15 15:27:28 lpmodule.py:399] repeat qkv cost 0.030548 seconds
DEBUG 10-15 15:27:28 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:28 lpmodule.py:433] dot attn cost 0.032920 seconds
DEBUG 10-15 15:27:28 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:28 lpmodule.py:444] time cost move to cuda:1 0.026480674743652344 s
DEBUG 10-15 15:27:28 lpllm.py:2283] CPU attn cost 0.138844 seconds if batch True
DEBUG 10-15 15:27:28 lpllm.py:2292] deal attn result cost 0.000003 seconds
DEBUG 10-15 15:27:28 lpllm.py:2294] CPU compute cost 0.139734 seconds
DEBUG 10-15 15:27:28 lpllm.py:2312] free cost 0.000078 seconds
DEBUG 10-15 15:27:28 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:28 lpllm.py:1774] update state cost 3.552436828613281e-05 s
DEBUG 10-15 15:27:28 lpllm.py:1743] restore layer func cost 0.0008230209350585938 s
DEBUG 10-15 15:27:28 lpllm.py:511] restore layer cost 0.001079559326171875 s
DEBUG 10-15 15:27:28 lpllm.py:512] reset_next_layer_need: layer_attn_idx=31, layer_mlp_idx=31, j_loc=62
DEBUG 10-15 15:27:28 lpllm.py:1037] reset layer cost 0.0011568069458007812 s
DEBUG 10-15 15:27:28 lpllm.py:1038] have reset next layer layer_attn 31 layer_mlp 31 
DEBUG 10-15 15:27:28 lpllm.py:924] 
DEBUG 10-15 15:27:28 lpllm.py:924] decoder loop j: 63 cur_layer_idx: 31 layer_attn_idx: 31 layer_mlp_idx: 31
DEBUG 10-15 15:27:28 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:28 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:28 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:28 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:28 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:28 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:28 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:28 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:28 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:28 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:28 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:28 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:28 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:28 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:28 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:28 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:28 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:28 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:28 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:28 lpllm.py:2265] GPU2CPU move cost 0.000617 seconds
DEBUG 10-15 15:27:28 lpmodule.py:340] layer_idx 31 attn layer_idx: 31
DEBUG 10-15 15:27:28 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:28 lpmodule.py:364] decoder_attn_batch update batch_dim 300-360
DEBUG 10-15 15:27:29 StaticCacheLen.py:84] static cache update layer_idx: 31, start_batch: 300, end_batch: 360
DEBUG 10-15 15:27:29 lpmodule.py:368] update for kv cache 300-360 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:29 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:29 lpmodule.py:374] update past key value cost 0.023800 seconds
DEBUG 10-15 15:27:29 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:29 lpmodule.py:227] layer idx 31
DEBUG 10-15 15:27:29 lpmodule.py:399] repeat qkv cost 0.029535 seconds
DEBUG 10-15 15:27:29 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:29 lpmodule.py:433] dot attn cost 0.032830 seconds
DEBUG 10-15 15:27:29 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:29 lpmodule.py:444] time cost move to cuda:1 0.023924589157104492 s
DEBUG 10-15 15:27:29 lpllm.py:2283] CPU attn cost 0.136138 seconds if batch True
DEBUG 10-15 15:27:29 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:29 lpllm.py:2294] CPU compute cost 0.137037 seconds
DEBUG 10-15 15:27:29 lpllm.py:2312] free cost 0.000081 seconds
DEBUG 10-15 15:27:29 lpllm.py:924] 
DEBUG 10-15 15:27:29 lpllm.py:924] decoder loop j: 64 cur_layer_idx: 32 layer_attn_idx: 32 layer_mlp_idx: 31
DEBUG 10-15 15:27:29 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:29 lpmodule.py:227] layer idx 31
DEBUG 10-15 15:27:29 lpllm.py:1085] last_mlp_output_chunk shape: torch.Size([60, 512, 4096]), mlp_output_chunk shape: torch.Size([60, 512, 4096])
DEBUG 10-15 15:27:29 lpllm.py:1086] last_mlp_output_chunk len 1, mlp_output_chunk len 1
DEBUG 10-15 15:27:29 lpllm.py:618] decoders batch for 2 cost 14.839293241500854 s
DEBUG 10-15 15:27:29 lpllm.py:848] hidden_states_chunks1 shape: torch.Size([60, 512, 4096]), hidden_states_chunks1 length: 1
DEBUG 10-15 15:27:29 lpllm.py:849] hidden_states_chunks2 shape: torch.Size([60, 512, 4096]), hidden_states_chunks2 length: 1
DEBUG 10-15 15:27:29 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:29 client.py:72] load_into_gpu: Mixtral-8x7B, 51c76eea-0cf3-4cc2-a4a1-88f2fda0c0dc
INFO 10-15 15:27:29 client.py:113] Model loaded: Mixtral-8x7B, 51c76eea-0cf3-4cc2-a4a1-88f2fda0c0dc
DEBUG 10-15 15:27:29 lpllm.py:1743] restore layer func cost 0.0010023117065429688 s
INFO 10-15 15:27:29 client.py:117] confirm_model_loaded: Mixtral-8x7B, 51c76eea-0cf3-4cc2-a4a1-88f2fda0c0dc
INFO 10-15 15:27:29 client.py:125] Model loaded
DEBUG 10-15 15:27:29 lpllm.py:422] prepare layer cost 0.27295970916748047 s
DEBUG 10-15 15:27:29 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed650>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x003\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x96\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:29 client.py:72] load_into_gpu: Mixtral-8x7B, 9b7d5ee7-9f4b-43aa-829c-d13ff252a328
INFO 10-15 15:27:29 client.py:113] Model loaded: Mixtral-8x7B, 9b7d5ee7-9f4b-43aa-829c-d13ff252a328
DEBUG 10-15 15:27:29 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:29 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:29 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:29 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:29 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:29 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:29 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:29 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:29 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:29 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:29 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:29 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:29 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:29 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:29 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:29 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:29 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:29 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:29 lpllm.py:924] 
DEBUG 10-15 15:27:29 lpllm.py:924] decoder loop j: 1 cur_layer_idx: 0 layer_attn_idx: 0 layer_mlp_idx: 0
DEBUG 10-15 15:27:29 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:29 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:29 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:29 lpllm.py:2265] GPU2CPU move cost 0.000638 seconds
DEBUG 10-15 15:27:29 lpmodule.py:340] layer_idx 0 attn layer_idx: 0
DEBUG 10-15 15:27:29 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:29 lpmodule.py:364] decoder_attn_batch update batch_dim 360-420
DEBUG 10-15 15:27:29 StaticCacheLen.py:84] static cache update layer_idx: 0, start_batch: 360, end_batch: 420
DEBUG 10-15 15:27:29 lpmodule.py:368] update for kv cache 360-420 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:29 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:29 lpmodule.py:374] update past key value cost 0.025735 seconds
DEBUG 10-15 15:27:29 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:29 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:29 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:29 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:29 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:29 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:29 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:29 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:29 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:29 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:29 lpmodule.py:399] repeat qkv cost 0.031483 seconds
DEBUG 10-15 15:27:29 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:29 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:29 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:29 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:29 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:29 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:29 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:29 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:29 lpmodule.py:433] dot attn cost 0.033038 seconds
DEBUG 10-15 15:27:29 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:29 lpmodule.py:444] time cost move to cuda:1 0.023719072341918945 s
DEBUG 10-15 15:27:29 lpllm.py:2283] CPU attn cost 0.139816 seconds if batch True
DEBUG 10-15 15:27:29 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:29 lpllm.py:2294] CPU compute cost 0.140764 seconds
DEBUG 10-15 15:27:29 lpllm.py:2312] free cost 0.000080 seconds
DEBUG 10-15 15:27:29 lpllm.py:2265] GPU2CPU move cost 0.000262 seconds
DEBUG 10-15 15:27:29 lpmodule.py:340] layer_idx 0 attn layer_idx: 0
DEBUG 10-15 15:27:29 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:29 lpmodule.py:364] decoder_attn_batch update batch_dim 420-480
DEBUG 10-15 15:27:29 lpmodule.py:227] layer idx 0
DEBUG 10-15 15:27:29 StaticCacheLen.py:84] static cache update layer_idx: 0, start_batch: 420, end_batch: 480
DEBUG 10-15 15:27:29 lpmodule.py:368] update for kv cache 420-480 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:29 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:29 lpmodule.py:374] update past key value cost 0.025474 seconds
DEBUG 10-15 15:27:29 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:29 lpmodule.py:399] repeat qkv cost 0.031102 seconds
DEBUG 10-15 15:27:29 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:29 lpmodule.py:433] dot attn cost 0.032571 seconds
DEBUG 10-15 15:27:29 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:29 lpmodule.py:444] time cost move to cuda:1 0.024100065231323242 s
DEBUG 10-15 15:27:29 lpllm.py:2283] CPU attn cost 0.139378 seconds if batch True
DEBUG 10-15 15:27:29 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:29 lpllm.py:2294] CPU compute cost 0.139827 seconds
DEBUG 10-15 15:27:29 lpllm.py:2312] free cost 0.000077 seconds
DEBUG 10-15 15:27:29 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:29 lpllm.py:1774] update state cost 3.886222839355469e-05 s
DEBUG 10-15 15:27:29 lpllm.py:1743] restore layer func cost 0.0004119873046875 s
DEBUG 10-15 15:27:29 lpllm.py:511] restore layer cost 0.0006644725799560547 s
DEBUG 10-15 15:27:29 lpllm.py:512] reset_next_layer_need: layer_attn_idx=1, layer_mlp_idx=0, j_loc=1
DEBUG 10-15 15:27:29 lpllm.py:1037] reset layer cost 0.0007398128509521484 s
DEBUG 10-15 15:27:29 lpllm.py:1038] have reset next layer layer_attn 1 layer_mlp 0 
DEBUG 10-15 15:27:29 lpllm.py:1044] j: 1 waiting the layer with layer_idx 1 before wait time 0.35935091972351074 s
INFO 10-15 15:27:29 client.py:117] confirm_model_loaded: Mixtral-8x7B, 9b7d5ee7-9f4b-43aa-829c-d13ff252a328
INFO 10-15 15:27:30 client.py:125] Model loaded
DEBUG 10-15 15:27:30 lpllm.py:1048] j: load cost 0.4150717258453369 s waiting cost 0.055708885192871094 s
DEBUG 10-15 15:27:30 lpllm.py:924] 
DEBUG 10-15 15:27:30 lpllm.py:924] decoder loop j: 2 cur_layer_idx: 1 layer_attn_idx: 1 layer_mlp_idx: 0
DEBUG 10-15 15:27:30 lpllm.py:933] start load next layer cur_layer_idx: 2
DEBUG 10-15 15:27:30 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed6e0>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x004\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x98\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:30 client.py:72] load_into_gpu: Mixtral-8x7B, f59994cd-b5c2-4d45-bbbf-089e928d19a5
INFO 10-15 15:27:30 client.py:113] Model loaded: Mixtral-8x7B, f59994cd-b5c2-4d45-bbbf-089e928d19a5
DEBUG 10-15 15:27:30 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:30 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:30 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:30 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:30 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:30 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:30 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:30 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:30 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:30 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:30 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:30 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:30 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:30 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:30 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:30 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:30 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:30 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:30 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:30 lpllm.py:2265] GPU2CPU move cost 0.000622 seconds
DEBUG 10-15 15:27:30 lpmodule.py:340] layer_idx 1 attn layer_idx: 1
DEBUG 10-15 15:27:30 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:30 lpmodule.py:364] decoder_attn_batch update batch_dim 360-420
DEBUG 10-15 15:27:30 StaticCacheLen.py:84] static cache update layer_idx: 1, start_batch: 360, end_batch: 420
DEBUG 10-15 15:27:30 lpmodule.py:368] update for kv cache 360-420 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:30 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:30 lpmodule.py:374] update past key value cost 0.026868 seconds
DEBUG 10-15 15:27:30 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:30 lpmodule.py:227] layer idx 0
DEBUG 10-15 15:27:30 lpmodule.py:399] repeat qkv cost 0.031294 seconds
DEBUG 10-15 15:27:30 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:30 lpmodule.py:433] dot attn cost 0.033525 seconds
DEBUG 10-15 15:27:30 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:30 lpmodule.py:444] time cost move to cuda:1 0.024196624755859375 s
DEBUG 10-15 15:27:30 lpllm.py:2283] CPU attn cost 0.141637 seconds if batch True
DEBUG 10-15 15:27:30 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:27:30 lpllm.py:2294] CPU compute cost 0.142594 seconds
DEBUG 10-15 15:27:30 lpllm.py:2312] free cost 0.000077 seconds
DEBUG 10-15 15:27:30 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:30 lpllm.py:1774] update state cost 3.5762786865234375e-05 s
DEBUG 10-15 15:27:30 lpllm.py:1743] restore layer func cost 0.0008487701416015625 s
DEBUG 10-15 15:27:30 lpllm.py:511] restore layer cost 0.0011129379272460938 s
DEBUG 10-15 15:27:30 lpllm.py:512] reset_next_layer_need: layer_attn_idx=1, layer_mlp_idx=1, j_loc=2
DEBUG 10-15 15:27:30 lpllm.py:1037] reset layer cost 0.0011925697326660156 s
DEBUG 10-15 15:27:30 lpllm.py:1038] have reset next layer layer_attn 1 layer_mlp 1 
DEBUG 10-15 15:27:30 lpllm.py:924] 
DEBUG 10-15 15:27:30 lpllm.py:924] decoder loop j: 3 cur_layer_idx: 1 layer_attn_idx: 1 layer_mlp_idx: 1
DEBUG 10-15 15:27:30 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:30 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:30 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:30 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:30 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:30 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:30 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:30 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:30 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:30 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:30 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:30 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:30 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:30 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:30 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:30 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:30 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:30 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:30 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:30 lpllm.py:2265] GPU2CPU move cost 0.000578 seconds
DEBUG 10-15 15:27:30 lpmodule.py:340] layer_idx 1 attn layer_idx: 1
DEBUG 10-15 15:27:30 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:30 lpmodule.py:364] decoder_attn_batch update batch_dim 420-480
DEBUG 10-15 15:27:30 StaticCacheLen.py:84] static cache update layer_idx: 1, start_batch: 420, end_batch: 480
DEBUG 10-15 15:27:30 lpmodule.py:368] update for kv cache 420-480 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:30 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:30 lpmodule.py:374] update past key value cost 0.024783 seconds
DEBUG 10-15 15:27:30 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:30 lpmodule.py:227] layer idx 1
DEBUG 10-15 15:27:30 lpmodule.py:399] repeat qkv cost 0.029951 seconds
DEBUG 10-15 15:27:30 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:30 lpmodule.py:433] dot attn cost 0.032166 seconds
DEBUG 10-15 15:27:30 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:30 lpmodule.py:444] time cost move to cuda:1 0.026131868362426758 s
DEBUG 10-15 15:27:30 lpllm.py:2283] CPU attn cost 0.139141 seconds if batch True
DEBUG 10-15 15:27:30 lpllm.py:2292] deal attn result cost 0.000003 seconds
DEBUG 10-15 15:27:30 lpllm.py:2294] CPU compute cost 0.140000 seconds
DEBUG 10-15 15:27:30 lpllm.py:2312] free cost 0.000076 seconds
DEBUG 10-15 15:27:30 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:30 lpllm.py:1774] update state cost 3.743171691894531e-05 s
DEBUG 10-15 15:27:30 lpllm.py:1743] restore layer func cost 0.0003979206085205078 s
DEBUG 10-15 15:27:30 lpllm.py:511] restore layer cost 0.0006418228149414062 s
DEBUG 10-15 15:27:30 lpllm.py:512] reset_next_layer_need: layer_attn_idx=2, layer_mlp_idx=1, j_loc=3
DEBUG 10-15 15:27:30 lpllm.py:1037] reset layer cost 0.0007152557373046875 s
DEBUG 10-15 15:27:30 lpllm.py:1038] have reset next layer layer_attn 2 layer_mlp 1 
DEBUG 10-15 15:27:30 lpllm.py:1044] j: 3 waiting the layer with layer_idx 2 before wait time 0.45381832122802734 s
INFO 10-15 15:27:30 client.py:117] confirm_model_loaded: Mixtral-8x7B, f59994cd-b5c2-4d45-bbbf-089e928d19a5
INFO 10-15 15:27:30 client.py:125] Model loaded
DEBUG 10-15 15:27:30 lpllm.py:1048] j: load cost 0.4556002616882324 s waiting cost 0.0017676353454589844 s
DEBUG 10-15 15:27:30 lpllm.py:924] 
DEBUG 10-15 15:27:30 lpllm.py:924] decoder loop j: 4 cur_layer_idx: 2 layer_attn_idx: 2 layer_mlp_idx: 1
DEBUG 10-15 15:27:30 lpllm.py:933] start load next layer cur_layer_idx: 3
DEBUG 10-15 15:27:30 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:30 client.py:72] load_into_gpu: Mixtral-8x7B, e0abd12c-8d4b-4a5e-bfd0-8542c901055b
INFO 10-15 15:27:30 client.py:113] Model loaded: Mixtral-8x7B, e0abd12c-8d4b-4a5e-bfd0-8542c901055b
DEBUG 10-15 15:27:30 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:30 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:30 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:30 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:30 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:30 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:30 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:30 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:30 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:30 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:30 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:30 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:30 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:30 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:30 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:30 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:30 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:30 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:30 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:30 lpllm.py:2265] GPU2CPU move cost 0.000613 seconds
DEBUG 10-15 15:27:30 lpmodule.py:340] layer_idx 2 attn layer_idx: 2
DEBUG 10-15 15:27:30 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:30 lpmodule.py:364] decoder_attn_batch update batch_dim 360-420
DEBUG 10-15 15:27:30 StaticCacheLen.py:84] static cache update layer_idx: 2, start_batch: 360, end_batch: 420
DEBUG 10-15 15:27:30 lpmodule.py:368] update for kv cache 360-420 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:30 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:30 lpmodule.py:374] update past key value cost 0.023747 seconds
DEBUG 10-15 15:27:30 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:30 lpmodule.py:227] layer idx 1
DEBUG 10-15 15:27:30 lpmodule.py:399] repeat qkv cost 0.030655 seconds
DEBUG 10-15 15:27:30 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:30 lpmodule.py:433] dot attn cost 0.033486 seconds
DEBUG 10-15 15:27:30 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:30 lpmodule.py:444] time cost move to cuda:1 0.023873329162597656 s
DEBUG 10-15 15:27:30 lpllm.py:2283] CPU attn cost 0.137438 seconds if batch True
DEBUG 10-15 15:27:30 lpllm.py:2292] deal attn result cost 0.000003 seconds
DEBUG 10-15 15:27:30 lpllm.py:2294] CPU compute cost 0.138363 seconds
DEBUG 10-15 15:27:30 lpllm.py:2312] free cost 0.000080 seconds
DEBUG 10-15 15:27:30 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:30 lpllm.py:1774] update state cost 2.1457672119140625e-05 s
DEBUG 10-15 15:27:30 lpllm.py:1743] restore layer func cost 0.0007982254028320312 s
DEBUG 10-15 15:27:30 lpllm.py:511] restore layer cost 0.0010287761688232422 s
DEBUG 10-15 15:27:30 lpllm.py:512] reset_next_layer_need: layer_attn_idx=2, layer_mlp_idx=2, j_loc=4
DEBUG 10-15 15:27:30 lpllm.py:1037] reset layer cost 0.0011026859283447266 s
DEBUG 10-15 15:27:30 lpllm.py:1038] have reset next layer layer_attn 2 layer_mlp 2 
DEBUG 10-15 15:27:30 lpllm.py:924] 
DEBUG 10-15 15:27:30 lpllm.py:924] decoder loop j: 5 cur_layer_idx: 2 layer_attn_idx: 2 layer_mlp_idx: 2
DEBUG 10-15 15:27:30 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:30 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:30 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:30 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:30 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:30 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:30 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:30 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:30 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:30 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:30 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:30 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:30 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:30 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:30 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:30 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:30 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:30 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:30 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:30 lpllm.py:2265] GPU2CPU move cost 0.000603 seconds
DEBUG 10-15 15:27:30 lpmodule.py:340] layer_idx 2 attn layer_idx: 2
DEBUG 10-15 15:27:30 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:30 lpmodule.py:364] decoder_attn_batch update batch_dim 420-480
DEBUG 10-15 15:27:30 StaticCacheLen.py:84] static cache update layer_idx: 2, start_batch: 420, end_batch: 480
DEBUG 10-15 15:27:30 lpmodule.py:368] update for kv cache 420-480 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:30 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:30 lpmodule.py:374] update past key value cost 0.025120 seconds
DEBUG 10-15 15:27:30 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:30 lpmodule.py:227] layer idx 2
DEBUG 10-15 15:27:30 lpmodule.py:399] repeat qkv cost 0.032188 seconds
DEBUG 10-15 15:27:30 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:30 lpmodule.py:433] dot attn cost 0.039410 seconds
DEBUG 10-15 15:27:30 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:30 lpmodule.py:444] time cost move to cuda:1 0.023701190948486328 s
DEBUG 10-15 15:27:30 lpllm.py:2283] CPU attn cost 0.146351 seconds if batch True
DEBUG 10-15 15:27:30 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:30 lpllm.py:2294] CPU compute cost 0.147263 seconds
DEBUG 10-15 15:27:30 lpllm.py:2312] free cost 0.000078 seconds
DEBUG 10-15 15:27:30 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:30 lpllm.py:1774] update state cost 2.1219253540039062e-05 s
DEBUG 10-15 15:27:30 lpllm.py:1743] restore layer func cost 0.00037741661071777344 s
DEBUG 10-15 15:27:30 lpllm.py:511] restore layer cost 0.0006084442138671875 s
DEBUG 10-15 15:27:30 lpllm.py:512] reset_next_layer_need: layer_attn_idx=3, layer_mlp_idx=2, j_loc=5
DEBUG 10-15 15:27:30 lpllm.py:1037] reset layer cost 0.0006785392761230469 s
DEBUG 10-15 15:27:30 lpllm.py:1038] have reset next layer layer_attn 3 layer_mlp 2 
DEBUG 10-15 15:27:30 lpllm.py:1044] j: 5 waiting the layer with layer_idx 3 before wait time 0.45474743843078613 s
INFO 10-15 15:27:30 client.py:117] confirm_model_loaded: Mixtral-8x7B, e0abd12c-8d4b-4a5e-bfd0-8542c901055b
INFO 10-15 15:27:30 client.py:125] Model loaded
DEBUG 10-15 15:27:30 lpllm.py:1048] j: load cost 0.4564664363861084 s waiting cost 0.0017046928405761719 s
DEBUG 10-15 15:27:30 lpllm.py:924] 
DEBUG 10-15 15:27:30 lpllm.py:924] decoder loop j: 6 cur_layer_idx: 3 layer_attn_idx: 3 layer_mlp_idx: 2
DEBUG 10-15 15:27:30 lpllm.py:933] start load next layer cur_layer_idx: 4
DEBUG 10-15 15:27:30 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed650>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x003\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x96\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:30 client.py:72] load_into_gpu: Mixtral-8x7B, 7c2989f4-7c3d-4e4a-b325-d75c0dac9ce3
INFO 10-15 15:27:30 client.py:113] Model loaded: Mixtral-8x7B, 7c2989f4-7c3d-4e4a-b325-d75c0dac9ce3
DEBUG 10-15 15:27:30 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:30 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:30 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:30 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:30 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:30 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:30 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:30 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:30 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:30 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:30 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:30 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:30 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:30 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:30 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:30 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:30 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:30 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:30 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:30 lpllm.py:2265] GPU2CPU move cost 0.000600 seconds
DEBUG 10-15 15:27:30 lpmodule.py:340] layer_idx 3 attn layer_idx: 3
DEBUG 10-15 15:27:30 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:30 lpmodule.py:364] decoder_attn_batch update batch_dim 360-420
DEBUG 10-15 15:27:30 StaticCacheLen.py:84] static cache update layer_idx: 3, start_batch: 360, end_batch: 420
DEBUG 10-15 15:27:30 lpmodule.py:368] update for kv cache 360-420 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:30 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:30 lpmodule.py:374] update past key value cost 0.025101 seconds
DEBUG 10-15 15:27:30 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:30 lpmodule.py:227] layer idx 2
DEBUG 10-15 15:27:31 lpmodule.py:399] repeat qkv cost 0.031064 seconds
DEBUG 10-15 15:27:31 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:31 lpmodule.py:433] dot attn cost 0.032819 seconds
DEBUG 10-15 15:27:31 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:31 lpmodule.py:444] time cost move to cuda:1 0.02356266975402832 s
DEBUG 10-15 15:27:31 lpllm.py:2283] CPU attn cost 0.138226 seconds if batch True
DEBUG 10-15 15:27:31 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:31 lpllm.py:2294] CPU compute cost 0.139136 seconds
DEBUG 10-15 15:27:31 lpllm.py:2312] free cost 0.000093 seconds
DEBUG 10-15 15:27:31 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:31 lpllm.py:1774] update state cost 3.5762786865234375e-05 s
DEBUG 10-15 15:27:31 lpllm.py:1743] restore layer func cost 0.0008575916290283203 s
DEBUG 10-15 15:27:31 lpllm.py:511] restore layer cost 0.0011000633239746094 s
DEBUG 10-15 15:27:31 lpllm.py:512] reset_next_layer_need: layer_attn_idx=3, layer_mlp_idx=3, j_loc=6
DEBUG 10-15 15:27:31 lpllm.py:1037] reset layer cost 0.0011773109436035156 s
DEBUG 10-15 15:27:31 lpllm.py:1038] have reset next layer layer_attn 3 layer_mlp 3 
DEBUG 10-15 15:27:31 lpllm.py:924] 
DEBUG 10-15 15:27:31 lpllm.py:924] decoder loop j: 7 cur_layer_idx: 3 layer_attn_idx: 3 layer_mlp_idx: 3
DEBUG 10-15 15:27:31 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:31 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:31 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:31 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:31 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:31 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:31 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:31 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:31 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:31 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:31 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:31 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:31 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:31 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:31 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:31 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:31 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:31 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:31 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:31 lpllm.py:2265] GPU2CPU move cost 0.000489 seconds
DEBUG 10-15 15:27:31 lpmodule.py:340] layer_idx 3 attn layer_idx: 3
DEBUG 10-15 15:27:31 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:31 lpmodule.py:364] decoder_attn_batch update batch_dim 420-480
DEBUG 10-15 15:27:31 StaticCacheLen.py:84] static cache update layer_idx: 3, start_batch: 420, end_batch: 480
DEBUG 10-15 15:27:31 lpmodule.py:368] update for kv cache 420-480 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:31 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:31 lpmodule.py:374] update past key value cost 0.024155 seconds
DEBUG 10-15 15:27:31 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:31 lpmodule.py:227] layer idx 3
DEBUG 10-15 15:27:31 lpmodule.py:399] repeat qkv cost 0.031224 seconds
DEBUG 10-15 15:27:31 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:31 lpmodule.py:433] dot attn cost 0.033278 seconds
DEBUG 10-15 15:27:31 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:31 lpmodule.py:444] time cost move to cuda:1 0.023938417434692383 s
DEBUG 10-15 15:27:31 lpllm.py:2283] CPU attn cost 0.138130 seconds if batch True
DEBUG 10-15 15:27:31 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:31 lpllm.py:2294] CPU compute cost 0.138890 seconds
DEBUG 10-15 15:27:31 lpllm.py:2312] free cost 0.000079 seconds
DEBUG 10-15 15:27:31 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:31 lpllm.py:1774] update state cost 4.100799560546875e-05 s
DEBUG 10-15 15:27:31 lpllm.py:1743] restore layer func cost 0.00040912628173828125 s
DEBUG 10-15 15:27:31 lpllm.py:511] restore layer cost 0.0006568431854248047 s
DEBUG 10-15 15:27:31 lpllm.py:512] reset_next_layer_need: layer_attn_idx=4, layer_mlp_idx=3, j_loc=7
DEBUG 10-15 15:27:31 lpllm.py:1037] reset layer cost 0.0007309913635253906 s
DEBUG 10-15 15:27:31 lpllm.py:1038] have reset next layer layer_attn 4 layer_mlp 3 
DEBUG 10-15 15:27:31 lpllm.py:1044] j: 7 waiting the layer with layer_idx 4 before wait time 0.44458723068237305 s
INFO 10-15 15:27:31 client.py:117] confirm_model_loaded: Mixtral-8x7B, 7c2989f4-7c3d-4e4a-b325-d75c0dac9ce3
INFO 10-15 15:27:31 client.py:125] Model loaded
DEBUG 10-15 15:27:31 lpllm.py:1048] j: load cost 0.44643545150756836 s waiting cost 0.001832723617553711 s
DEBUG 10-15 15:27:31 lpllm.py:924] 
DEBUG 10-15 15:27:31 lpllm.py:924] decoder loop j: 8 cur_layer_idx: 4 layer_attn_idx: 4 layer_mlp_idx: 3
DEBUG 10-15 15:27:31 lpllm.py:933] start load next layer cur_layer_idx: 5
DEBUG 10-15 15:27:31 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed6e0>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x004\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x98\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:31 client.py:72] load_into_gpu: Mixtral-8x7B, 3c0c81a8-ce86-4df4-84a0-b39fad4ba034
INFO 10-15 15:27:31 client.py:113] Model loaded: Mixtral-8x7B, 3c0c81a8-ce86-4df4-84a0-b39fad4ba034
DEBUG 10-15 15:27:31 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:31 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:31 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:31 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:31 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:31 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:31 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:31 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:31 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:31 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:31 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:31 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:31 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:31 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:31 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:31 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:31 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:31 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:31 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:31 lpllm.py:2265] GPU2CPU move cost 0.000448 seconds
DEBUG 10-15 15:27:31 lpmodule.py:340] layer_idx 4 attn layer_idx: 4
DEBUG 10-15 15:27:31 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:31 lpmodule.py:364] decoder_attn_batch update batch_dim 360-420
DEBUG 10-15 15:27:31 StaticCacheLen.py:84] static cache update layer_idx: 4, start_batch: 360, end_batch: 420
DEBUG 10-15 15:27:31 lpmodule.py:368] update for kv cache 360-420 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:31 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:31 lpmodule.py:374] update past key value cost 0.027070 seconds
DEBUG 10-15 15:27:31 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:31 lpmodule.py:227] layer idx 3
DEBUG 10-15 15:27:31 lpmodule.py:399] repeat qkv cost 0.030789 seconds
DEBUG 10-15 15:27:31 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:31 lpmodule.py:433] dot attn cost 0.033967 seconds
DEBUG 10-15 15:27:31 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:31 lpmodule.py:444] time cost move to cuda:1 0.02375006675720215 s
DEBUG 10-15 15:27:31 lpllm.py:2283] CPU attn cost 0.140818 seconds if batch True
DEBUG 10-15 15:27:31 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:31 lpllm.py:2294] CPU compute cost 0.141550 seconds
DEBUG 10-15 15:27:31 lpllm.py:2312] free cost 0.000080 seconds
DEBUG 10-15 15:27:31 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:31 lpllm.py:1774] update state cost 3.600120544433594e-05 s
DEBUG 10-15 15:27:31 lpllm.py:1743] restore layer func cost 0.0008203983306884766 s
DEBUG 10-15 15:27:31 lpllm.py:511] restore layer cost 0.0010559558868408203 s
DEBUG 10-15 15:27:31 lpllm.py:512] reset_next_layer_need: layer_attn_idx=4, layer_mlp_idx=4, j_loc=8
DEBUG 10-15 15:27:31 lpllm.py:1037] reset layer cost 0.0011334419250488281 s
DEBUG 10-15 15:27:31 lpllm.py:1038] have reset next layer layer_attn 4 layer_mlp 4 
DEBUG 10-15 15:27:31 lpllm.py:924] 
DEBUG 10-15 15:27:31 lpllm.py:924] decoder loop j: 9 cur_layer_idx: 4 layer_attn_idx: 4 layer_mlp_idx: 4
DEBUG 10-15 15:27:31 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:31 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:31 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:31 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:31 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:31 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:31 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:31 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:31 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:31 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:31 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:31 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:31 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:31 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:31 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:31 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:31 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:31 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:31 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:31 lpllm.py:2265] GPU2CPU move cost 0.000317 seconds
DEBUG 10-15 15:27:31 lpmodule.py:340] layer_idx 4 attn layer_idx: 4
DEBUG 10-15 15:27:31 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:31 lpmodule.py:364] decoder_attn_batch update batch_dim 420-480
DEBUG 10-15 15:27:31 StaticCacheLen.py:84] static cache update layer_idx: 4, start_batch: 420, end_batch: 480
DEBUG 10-15 15:27:31 lpmodule.py:368] update for kv cache 420-480 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:31 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:31 lpmodule.py:374] update past key value cost 0.022763 seconds
DEBUG 10-15 15:27:31 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:31 lpmodule.py:227] layer idx 4
DEBUG 10-15 15:27:31 lpmodule.py:399] repeat qkv cost 0.031791 seconds
DEBUG 10-15 15:27:31 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:31 lpmodule.py:433] dot attn cost 0.032557 seconds
DEBUG 10-15 15:27:31 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:31 lpmodule.py:444] time cost move to cuda:1 0.023936033248901367 s
DEBUG 10-15 15:27:31 lpllm.py:2283] CPU attn cost 0.136805 seconds if batch True
DEBUG 10-15 15:27:31 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:31 lpllm.py:2294] CPU compute cost 0.137338 seconds
DEBUG 10-15 15:27:31 lpllm.py:2312] free cost 0.000079 seconds
DEBUG 10-15 15:27:31 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:31 lpllm.py:1774] update state cost 3.9577484130859375e-05 s
DEBUG 10-15 15:27:31 lpllm.py:1743] restore layer func cost 0.00041031837463378906 s
DEBUG 10-15 15:27:31 lpllm.py:511] restore layer cost 0.0006563663482666016 s
DEBUG 10-15 15:27:31 lpllm.py:512] reset_next_layer_need: layer_attn_idx=5, layer_mlp_idx=4, j_loc=9
DEBUG 10-15 15:27:31 lpllm.py:1037] reset layer cost 0.0007266998291015625 s
DEBUG 10-15 15:27:31 lpllm.py:1038] have reset next layer layer_attn 5 layer_mlp 4 
DEBUG 10-15 15:27:31 lpllm.py:1044] j: 9 waiting the layer with layer_idx 5 before wait time 0.44586753845214844 s
INFO 10-15 15:27:31 client.py:117] confirm_model_loaded: Mixtral-8x7B, 3c0c81a8-ce86-4df4-84a0-b39fad4ba034
INFO 10-15 15:27:31 client.py:125] Model loaded
DEBUG 10-15 15:27:31 lpllm.py:1048] j: load cost 0.4476132392883301 s waiting cost 0.0017292499542236328 s
DEBUG 10-15 15:27:31 lpllm.py:924] 
DEBUG 10-15 15:27:31 lpllm.py:924] decoder loop j: 10 cur_layer_idx: 5 layer_attn_idx: 5 layer_mlp_idx: 4
DEBUG 10-15 15:27:31 lpllm.py:933] start load next layer cur_layer_idx: 6
DEBUG 10-15 15:27:31 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:31 client.py:72] load_into_gpu: Mixtral-8x7B, 4fee353a-769d-4b1f-bf16-87e1419c3bc7
INFO 10-15 15:27:31 client.py:113] Model loaded: Mixtral-8x7B, 4fee353a-769d-4b1f-bf16-87e1419c3bc7
DEBUG 10-15 15:27:31 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:31 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:31 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:31 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:31 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:31 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:31 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:31 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:31 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:31 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:31 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:31 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:31 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:31 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:31 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:31 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:31 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:31 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:31 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:31 lpllm.py:2265] GPU2CPU move cost 0.000584 seconds
DEBUG 10-15 15:27:31 lpmodule.py:340] layer_idx 5 attn layer_idx: 5
DEBUG 10-15 15:27:31 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:31 lpmodule.py:364] decoder_attn_batch update batch_dim 360-420
DEBUG 10-15 15:27:31 StaticCacheLen.py:84] static cache update layer_idx: 5, start_batch: 360, end_batch: 420
DEBUG 10-15 15:27:31 lpmodule.py:368] update for kv cache 360-420 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:31 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:31 lpmodule.py:374] update past key value cost 0.023955 seconds
DEBUG 10-15 15:27:31 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:31 lpmodule.py:227] layer idx 4
DEBUG 10-15 15:27:31 lpmodule.py:399] repeat qkv cost 0.031940 seconds
DEBUG 10-15 15:27:31 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:31 lpmodule.py:433] dot attn cost 0.033258 seconds
DEBUG 10-15 15:27:31 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:31 lpmodule.py:444] time cost move to cuda:1 0.02373361587524414 s
DEBUG 10-15 15:27:31 lpllm.py:2283] CPU attn cost 0.138293 seconds if batch True
DEBUG 10-15 15:27:31 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:31 lpllm.py:2294] CPU compute cost 0.139175 seconds
DEBUG 10-15 15:27:31 lpllm.py:2312] free cost 0.000090 seconds
DEBUG 10-15 15:27:32 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:32 lpllm.py:1774] update state cost 3.62396240234375e-05 s
DEBUG 10-15 15:27:32 lpllm.py:1743] restore layer func cost 0.0008449554443359375 s
DEBUG 10-15 15:27:32 lpllm.py:511] restore layer cost 0.0011060237884521484 s
DEBUG 10-15 15:27:32 lpllm.py:512] reset_next_layer_need: layer_attn_idx=5, layer_mlp_idx=5, j_loc=10
DEBUG 10-15 15:27:32 lpllm.py:1037] reset layer cost 0.0011844635009765625 s
DEBUG 10-15 15:27:32 lpllm.py:1038] have reset next layer layer_attn 5 layer_mlp 5 
DEBUG 10-15 15:27:32 lpllm.py:924] 
DEBUG 10-15 15:27:32 lpllm.py:924] decoder loop j: 11 cur_layer_idx: 5 layer_attn_idx: 5 layer_mlp_idx: 5
DEBUG 10-15 15:27:32 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:32 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:32 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:32 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:32 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:32 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:32 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:32 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:32 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:32 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:32 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:32 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:32 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:32 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:32 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:32 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:32 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:32 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:32 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:32 lpllm.py:2265] GPU2CPU move cost 0.000580 seconds
DEBUG 10-15 15:27:32 lpmodule.py:340] layer_idx 5 attn layer_idx: 5
DEBUG 10-15 15:27:32 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:32 lpmodule.py:364] decoder_attn_batch update batch_dim 420-480
DEBUG 10-15 15:27:32 StaticCacheLen.py:84] static cache update layer_idx: 5, start_batch: 420, end_batch: 480
DEBUG 10-15 15:27:32 lpmodule.py:368] update for kv cache 420-480 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:32 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:32 lpmodule.py:374] update past key value cost 0.025448 seconds
DEBUG 10-15 15:27:32 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:32 lpmodule.py:227] layer idx 5
DEBUG 10-15 15:27:32 lpmodule.py:399] repeat qkv cost 0.031540 seconds
DEBUG 10-15 15:27:32 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:32 lpmodule.py:433] dot attn cost 0.033115 seconds
DEBUG 10-15 15:27:32 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:32 lpmodule.py:444] time cost move to cuda:1 0.02349066734313965 s
DEBUG 10-15 15:27:32 lpllm.py:2283] CPU attn cost 0.138745 seconds if batch True
DEBUG 10-15 15:27:32 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:32 lpllm.py:2294] CPU compute cost 0.139622 seconds
DEBUG 10-15 15:27:32 lpllm.py:2312] free cost 0.000078 seconds
DEBUG 10-15 15:27:32 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:32 lpllm.py:1774] update state cost 3.9577484130859375e-05 s
DEBUG 10-15 15:27:32 lpllm.py:1743] restore layer func cost 0.0004036426544189453 s
DEBUG 10-15 15:27:32 lpllm.py:511] restore layer cost 0.0006561279296875 s
DEBUG 10-15 15:27:32 lpllm.py:512] reset_next_layer_need: layer_attn_idx=6, layer_mlp_idx=5, j_loc=11
DEBUG 10-15 15:27:32 lpllm.py:1037] reset layer cost 0.0007297992706298828 s
DEBUG 10-15 15:27:32 lpllm.py:1038] have reset next layer layer_attn 6 layer_mlp 5 
DEBUG 10-15 15:27:32 lpllm.py:1044] j: 11 waiting the layer with layer_idx 6 before wait time 0.4448826313018799 s
INFO 10-15 15:27:32 client.py:117] confirm_model_loaded: Mixtral-8x7B, 4fee353a-769d-4b1f-bf16-87e1419c3bc7
INFO 10-15 15:27:32 client.py:125] Model loaded
DEBUG 10-15 15:27:32 lpllm.py:1048] j: load cost 0.44671177864074707 s waiting cost 0.001814126968383789 s
DEBUG 10-15 15:27:32 lpllm.py:924] 
DEBUG 10-15 15:27:32 lpllm.py:924] decoder loop j: 12 cur_layer_idx: 6 layer_attn_idx: 6 layer_mlp_idx: 5
DEBUG 10-15 15:27:32 lpllm.py:933] start load next layer cur_layer_idx: 7
DEBUG 10-15 15:27:32 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed650>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x003\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x96\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:32 client.py:72] load_into_gpu: Mixtral-8x7B, 7ab2d528-eaa1-48ea-894b-0c169968e4b0
INFO 10-15 15:27:32 client.py:113] Model loaded: Mixtral-8x7B, 7ab2d528-eaa1-48ea-894b-0c169968e4b0
DEBUG 10-15 15:27:32 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:32 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:32 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:32 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:32 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:32 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:32 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:32 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:32 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:32 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:32 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:32 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:32 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:32 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:32 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:32 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:32 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:32 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:32 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:32 lpllm.py:2265] GPU2CPU move cost 0.000705 seconds
DEBUG 10-15 15:27:32 lpmodule.py:340] layer_idx 6 attn layer_idx: 6
DEBUG 10-15 15:27:32 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:32 lpmodule.py:364] decoder_attn_batch update batch_dim 360-420
DEBUG 10-15 15:27:32 StaticCacheLen.py:84] static cache update layer_idx: 6, start_batch: 360, end_batch: 420
DEBUG 10-15 15:27:32 lpmodule.py:368] update for kv cache 360-420 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:32 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:32 lpmodule.py:374] update past key value cost 0.024521 seconds
DEBUG 10-15 15:27:32 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:32 lpmodule.py:227] layer idx 5
DEBUG 10-15 15:27:32 lpmodule.py:399] repeat qkv cost 0.030217 seconds
DEBUG 10-15 15:27:32 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:32 lpmodule.py:433] dot attn cost 0.038392 seconds
DEBUG 10-15 15:27:32 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:32 lpmodule.py:444] time cost move to cuda:1 0.023446083068847656 s
DEBUG 10-15 15:27:32 lpllm.py:2283] CPU attn cost 0.142400 seconds if batch True
DEBUG 10-15 15:27:32 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:27:32 lpllm.py:2294] CPU compute cost 0.143385 seconds
DEBUG 10-15 15:27:32 lpllm.py:2312] free cost 0.000103 seconds
DEBUG 10-15 15:27:32 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:32 lpllm.py:1774] update state cost 3.719329833984375e-05 s
DEBUG 10-15 15:27:32 lpllm.py:1743] restore layer func cost 0.0008268356323242188 s
DEBUG 10-15 15:27:32 lpllm.py:511] restore layer cost 0.0010845661163330078 s
DEBUG 10-15 15:27:32 lpllm.py:512] reset_next_layer_need: layer_attn_idx=6, layer_mlp_idx=6, j_loc=12
DEBUG 10-15 15:27:32 lpllm.py:1037] reset layer cost 0.0011608600616455078 s
DEBUG 10-15 15:27:32 lpllm.py:1038] have reset next layer layer_attn 6 layer_mlp 6 
DEBUG 10-15 15:27:32 lpllm.py:924] 
DEBUG 10-15 15:27:32 lpllm.py:924] decoder loop j: 13 cur_layer_idx: 6 layer_attn_idx: 6 layer_mlp_idx: 6
DEBUG 10-15 15:27:32 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:32 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:32 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:32 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:32 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:32 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:32 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:32 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:32 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:32 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:32 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:32 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:32 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:32 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:32 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:32 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:32 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:32 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:32 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:32 lpllm.py:2265] GPU2CPU move cost 0.000585 seconds
DEBUG 10-15 15:27:32 lpmodule.py:340] layer_idx 6 attn layer_idx: 6
DEBUG 10-15 15:27:32 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:32 lpmodule.py:364] decoder_attn_batch update batch_dim 420-480
DEBUG 10-15 15:27:32 StaticCacheLen.py:84] static cache update layer_idx: 6, start_batch: 420, end_batch: 480
DEBUG 10-15 15:27:32 lpmodule.py:368] update for kv cache 420-480 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:32 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:32 lpmodule.py:374] update past key value cost 0.024421 seconds
DEBUG 10-15 15:27:32 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:32 lpmodule.py:227] layer idx 6
DEBUG 10-15 15:27:32 lpmodule.py:399] repeat qkv cost 0.030734 seconds
DEBUG 10-15 15:27:32 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:32 lpmodule.py:433] dot attn cost 0.032796 seconds
DEBUG 10-15 15:27:32 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:32 lpmodule.py:444] time cost move to cuda:1 0.02504873275756836 s
DEBUG 10-15 15:27:32 lpllm.py:2283] CPU attn cost 0.138321 seconds if batch True
DEBUG 10-15 15:27:32 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:27:32 lpllm.py:2294] CPU compute cost 0.139185 seconds
DEBUG 10-15 15:27:32 lpllm.py:2312] free cost 0.000072 seconds
DEBUG 10-15 15:27:32 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:32 lpllm.py:1774] update state cost 2.193450927734375e-05 s
DEBUG 10-15 15:27:32 lpllm.py:1743] restore layer func cost 0.00038504600524902344 s
DEBUG 10-15 15:27:32 lpllm.py:511] restore layer cost 0.0006186962127685547 s
DEBUG 10-15 15:27:32 lpllm.py:512] reset_next_layer_need: layer_attn_idx=7, layer_mlp_idx=6, j_loc=13
DEBUG 10-15 15:27:32 lpllm.py:1037] reset layer cost 0.0006928443908691406 s
DEBUG 10-15 15:27:32 lpllm.py:1038] have reset next layer layer_attn 7 layer_mlp 6 
DEBUG 10-15 15:27:32 lpllm.py:1044] j: 13 waiting the layer with layer_idx 7 before wait time 0.4540412425994873 s
INFO 10-15 15:27:32 client.py:117] confirm_model_loaded: Mixtral-8x7B, 7ab2d528-eaa1-48ea-894b-0c169968e4b0
INFO 10-15 15:27:32 client.py:125] Model loaded
DEBUG 10-15 15:27:32 lpllm.py:1048] j: load cost 0.45584964752197266 s waiting cost 0.0017938613891601562 s
DEBUG 10-15 15:27:32 lpllm.py:924] 
DEBUG 10-15 15:27:32 lpllm.py:924] decoder loop j: 14 cur_layer_idx: 7 layer_attn_idx: 7 layer_mlp_idx: 6
DEBUG 10-15 15:27:32 lpllm.py:933] start load next layer cur_layer_idx: 8
DEBUG 10-15 15:27:32 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed6e0>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x004\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x98\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:32 client.py:72] load_into_gpu: Mixtral-8x7B, 1a5d877d-f568-412f-87f4-70cf7f3b6e63
INFO 10-15 15:27:32 client.py:113] Model loaded: Mixtral-8x7B, 1a5d877d-f568-412f-87f4-70cf7f3b6e63
DEBUG 10-15 15:27:32 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:32 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:32 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:32 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:32 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:32 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:32 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:32 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:32 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:32 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:32 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:32 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:32 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:32 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:32 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:32 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:32 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:32 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:32 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:32 lpllm.py:2265] GPU2CPU move cost 0.000592 seconds
DEBUG 10-15 15:27:32 lpmodule.py:340] layer_idx 7 attn layer_idx: 7
DEBUG 10-15 15:27:32 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:32 lpmodule.py:364] decoder_attn_batch update batch_dim 360-420
DEBUG 10-15 15:27:32 StaticCacheLen.py:84] static cache update layer_idx: 7, start_batch: 360, end_batch: 420
DEBUG 10-15 15:27:32 lpmodule.py:368] update for kv cache 360-420 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:32 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:32 lpmodule.py:374] update past key value cost 0.024191 seconds
DEBUG 10-15 15:27:32 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:32 lpmodule.py:227] layer idx 6
DEBUG 10-15 15:27:32 lpmodule.py:399] repeat qkv cost 0.031403 seconds
DEBUG 10-15 15:27:32 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:32 lpmodule.py:433] dot attn cost 0.039370 seconds
DEBUG 10-15 15:27:32 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:32 lpmodule.py:444] time cost move to cuda:1 0.026268720626831055 s
DEBUG 10-15 15:27:32 lpllm.py:2283] CPU attn cost 0.146383 seconds if batch True
DEBUG 10-15 15:27:32 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:32 lpllm.py:2294] CPU compute cost 0.147289 seconds
DEBUG 10-15 15:27:32 lpllm.py:2312] free cost 0.000077 seconds
DEBUG 10-15 15:27:32 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:32 lpllm.py:1774] update state cost 3.790855407714844e-05 s
DEBUG 10-15 15:27:32 lpllm.py:1743] restore layer func cost 0.0008449554443359375 s
DEBUG 10-15 15:27:32 lpllm.py:511] restore layer cost 0.0010972023010253906 s
DEBUG 10-15 15:27:32 lpllm.py:512] reset_next_layer_need: layer_attn_idx=7, layer_mlp_idx=7, j_loc=14
DEBUG 10-15 15:27:32 lpllm.py:1037] reset layer cost 0.0011737346649169922 s
DEBUG 10-15 15:27:32 lpllm.py:1038] have reset next layer layer_attn 7 layer_mlp 7 
DEBUG 10-15 15:27:32 lpllm.py:924] 
DEBUG 10-15 15:27:32 lpllm.py:924] decoder loop j: 15 cur_layer_idx: 7 layer_attn_idx: 7 layer_mlp_idx: 7
DEBUG 10-15 15:27:32 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:32 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:32 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:32 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:32 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:32 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:32 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:32 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:32 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:32 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:32 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:32 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:32 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:32 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:32 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:32 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:32 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:32 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:32 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:32 lpllm.py:2265] GPU2CPU move cost 0.000641 seconds
DEBUG 10-15 15:27:32 lpmodule.py:340] layer_idx 7 attn layer_idx: 7
DEBUG 10-15 15:27:32 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:32 lpmodule.py:364] decoder_attn_batch update batch_dim 420-480
DEBUG 10-15 15:27:33 StaticCacheLen.py:84] static cache update layer_idx: 7, start_batch: 420, end_batch: 480
DEBUG 10-15 15:27:33 lpmodule.py:368] update for kv cache 420-480 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:33 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:33 lpmodule.py:374] update past key value cost 0.023939 seconds
DEBUG 10-15 15:27:33 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:33 lpmodule.py:227] layer idx 7
DEBUG 10-15 15:27:33 lpmodule.py:399] repeat qkv cost 0.030513 seconds
DEBUG 10-15 15:27:33 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:33 lpmodule.py:433] dot attn cost 0.033129 seconds
DEBUG 10-15 15:27:33 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:33 lpmodule.py:444] time cost move to cuda:1 0.023366451263427734 s
DEBUG 10-15 15:27:33 lpllm.py:2283] CPU attn cost 0.136657 seconds if batch True
DEBUG 10-15 15:27:33 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:33 lpllm.py:2294] CPU compute cost 0.137587 seconds
DEBUG 10-15 15:27:33 lpllm.py:2312] free cost 0.000074 seconds
DEBUG 10-15 15:27:33 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:33 lpllm.py:1774] update state cost 2.1696090698242188e-05 s
DEBUG 10-15 15:27:33 lpllm.py:1743] restore layer func cost 0.0003998279571533203 s
DEBUG 10-15 15:27:33 lpllm.py:511] restore layer cost 0.0006339550018310547 s
DEBUG 10-15 15:27:33 lpllm.py:512] reset_next_layer_need: layer_attn_idx=8, layer_mlp_idx=7, j_loc=15
DEBUG 10-15 15:27:33 lpllm.py:1037] reset layer cost 0.0007069110870361328 s
DEBUG 10-15 15:27:33 lpllm.py:1038] have reset next layer layer_attn 8 layer_mlp 7 
DEBUG 10-15 15:27:33 lpllm.py:1044] j: 15 waiting the layer with layer_idx 8 before wait time 0.45012545585632324 s
INFO 10-15 15:27:33 client.py:117] confirm_model_loaded: Mixtral-8x7B, 1a5d877d-f568-412f-87f4-70cf7f3b6e63
INFO 10-15 15:27:33 client.py:125] Model loaded
DEBUG 10-15 15:27:33 lpllm.py:1048] j: load cost 0.4519643783569336 s waiting cost 0.0018236637115478516 s
DEBUG 10-15 15:27:33 lpllm.py:924] 
DEBUG 10-15 15:27:33 lpllm.py:924] decoder loop j: 16 cur_layer_idx: 8 layer_attn_idx: 8 layer_mlp_idx: 7
DEBUG 10-15 15:27:33 lpllm.py:933] start load next layer cur_layer_idx: 9
DEBUG 10-15 15:27:33 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:33 client.py:72] load_into_gpu: Mixtral-8x7B, 70a19b5e-f7b3-43fb-81ff-4e4f9e9f09cd
INFO 10-15 15:27:33 client.py:113] Model loaded: Mixtral-8x7B, 70a19b5e-f7b3-43fb-81ff-4e4f9e9f09cd
DEBUG 10-15 15:27:33 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:33 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:33 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:33 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:33 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:33 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:33 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:33 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:33 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:33 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:33 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:33 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:33 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:33 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:33 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:33 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:33 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:33 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:33 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:33 lpllm.py:2265] GPU2CPU move cost 0.000575 seconds
DEBUG 10-15 15:27:33 lpmodule.py:340] layer_idx 8 attn layer_idx: 8
DEBUG 10-15 15:27:33 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:33 lpmodule.py:364] decoder_attn_batch update batch_dim 360-420
DEBUG 10-15 15:27:33 StaticCacheLen.py:84] static cache update layer_idx: 8, start_batch: 360, end_batch: 420
DEBUG 10-15 15:27:33 lpmodule.py:368] update for kv cache 360-420 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:33 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:33 lpmodule.py:374] update past key value cost 0.023104 seconds
DEBUG 10-15 15:27:33 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:33 lpmodule.py:227] layer idx 7
DEBUG 10-15 15:27:33 lpmodule.py:399] repeat qkv cost 0.032757 seconds
DEBUG 10-15 15:27:33 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:33 lpmodule.py:433] dot attn cost 0.033366 seconds
DEBUG 10-15 15:27:33 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:33 lpmodule.py:444] time cost move to cuda:1 0.024011850357055664 s
DEBUG 10-15 15:27:33 lpllm.py:2283] CPU attn cost 0.138520 seconds if batch True
DEBUG 10-15 15:27:33 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:33 lpllm.py:2294] CPU compute cost 0.139387 seconds
DEBUG 10-15 15:27:33 lpllm.py:2312] free cost 0.000075 seconds
DEBUG 10-15 15:27:33 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:33 lpllm.py:1774] update state cost 2.0265579223632812e-05 s
DEBUG 10-15 15:27:33 lpllm.py:1743] restore layer func cost 0.0007944107055664062 s
DEBUG 10-15 15:27:33 lpllm.py:511] restore layer cost 0.0010325908660888672 s
DEBUG 10-15 15:27:33 lpllm.py:512] reset_next_layer_need: layer_attn_idx=8, layer_mlp_idx=8, j_loc=16
DEBUG 10-15 15:27:33 lpllm.py:1037] reset layer cost 0.0011138916015625 s
DEBUG 10-15 15:27:33 lpllm.py:1038] have reset next layer layer_attn 8 layer_mlp 8 
DEBUG 10-15 15:27:33 lpllm.py:924] 
DEBUG 10-15 15:27:33 lpllm.py:924] decoder loop j: 17 cur_layer_idx: 8 layer_attn_idx: 8 layer_mlp_idx: 8
DEBUG 10-15 15:27:33 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:33 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:33 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:33 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:33 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:33 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:33 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:33 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:33 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:33 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:33 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:33 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:33 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:33 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:33 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:33 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:33 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:33 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:33 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:33 lpllm.py:2265] GPU2CPU move cost 0.000614 seconds
DEBUG 10-15 15:27:33 lpmodule.py:340] layer_idx 8 attn layer_idx: 8
DEBUG 10-15 15:27:33 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:33 lpmodule.py:364] decoder_attn_batch update batch_dim 420-480
DEBUG 10-15 15:27:33 StaticCacheLen.py:84] static cache update layer_idx: 8, start_batch: 420, end_batch: 480
DEBUG 10-15 15:27:33 lpmodule.py:368] update for kv cache 420-480 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:33 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:33 lpmodule.py:374] update past key value cost 0.024642 seconds
DEBUG 10-15 15:27:33 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:33 lpmodule.py:227] layer idx 8
DEBUG 10-15 15:27:33 lpmodule.py:399] repeat qkv cost 0.030592 seconds
DEBUG 10-15 15:27:33 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:33 lpmodule.py:433] dot attn cost 0.031890 seconds
DEBUG 10-15 15:27:33 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:33 lpmodule.py:444] time cost move to cuda:1 0.026653051376342773 s
DEBUG 10-15 15:27:33 lpllm.py:2283] CPU attn cost 0.138994 seconds if batch True
DEBUG 10-15 15:27:33 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:33 lpllm.py:2294] CPU compute cost 0.139904 seconds
DEBUG 10-15 15:27:33 lpllm.py:2312] free cost 0.000076 seconds
DEBUG 10-15 15:27:33 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:33 lpllm.py:1774] update state cost 3.790855407714844e-05 s
DEBUG 10-15 15:27:33 lpllm.py:1743] restore layer func cost 0.0003960132598876953 s
DEBUG 10-15 15:27:33 lpllm.py:511] restore layer cost 0.0006566047668457031 s
DEBUG 10-15 15:27:33 lpllm.py:512] reset_next_layer_need: layer_attn_idx=9, layer_mlp_idx=8, j_loc=17
DEBUG 10-15 15:27:33 lpllm.py:1037] reset layer cost 0.0007288455963134766 s
DEBUG 10-15 15:27:33 lpllm.py:1038] have reset next layer layer_attn 9 layer_mlp 8 
DEBUG 10-15 15:27:33 lpllm.py:1044] j: 17 waiting the layer with layer_idx 9 before wait time 0.4512002468109131 s
INFO 10-15 15:27:33 client.py:117] confirm_model_loaded: Mixtral-8x7B, 70a19b5e-f7b3-43fb-81ff-4e4f9e9f09cd
INFO 10-15 15:27:33 client.py:125] Model loaded
DEBUG 10-15 15:27:33 lpllm.py:1048] j: load cost 0.45290136337280273 s waiting cost 0.00168609619140625 s
DEBUG 10-15 15:27:33 lpllm.py:924] 
DEBUG 10-15 15:27:33 lpllm.py:924] decoder loop j: 18 cur_layer_idx: 9 layer_attn_idx: 9 layer_mlp_idx: 8
DEBUG 10-15 15:27:33 lpllm.py:933] start load next layer cur_layer_idx: 10
DEBUG 10-15 15:27:33 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed650>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x003\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x96\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:33 client.py:72] load_into_gpu: Mixtral-8x7B, 5a88366c-7f58-45e3-aaf3-13b30b7c0f0b
INFO 10-15 15:27:33 client.py:113] Model loaded: Mixtral-8x7B, 5a88366c-7f58-45e3-aaf3-13b30b7c0f0b
DEBUG 10-15 15:27:33 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:33 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:33 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:33 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:33 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:33 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:33 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:33 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:33 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:33 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:33 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:33 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:33 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:33 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:33 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:33 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:33 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:33 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:33 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:33 lpllm.py:2265] GPU2CPU move cost 0.000619 seconds
DEBUG 10-15 15:27:33 lpmodule.py:340] layer_idx 9 attn layer_idx: 9
DEBUG 10-15 15:27:33 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:33 lpmodule.py:364] decoder_attn_batch update batch_dim 360-420
DEBUG 10-15 15:27:33 StaticCacheLen.py:84] static cache update layer_idx: 9, start_batch: 360, end_batch: 420
DEBUG 10-15 15:27:33 lpmodule.py:368] update for kv cache 360-420 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:33 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:33 lpmodule.py:374] update past key value cost 0.024557 seconds
DEBUG 10-15 15:27:33 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:33 lpmodule.py:227] layer idx 8
DEBUG 10-15 15:27:33 lpmodule.py:399] repeat qkv cost 0.031211 seconds
DEBUG 10-15 15:27:33 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:33 lpmodule.py:433] dot attn cost 0.035143 seconds
DEBUG 10-15 15:27:33 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:33 lpmodule.py:444] time cost move to cuda:1 0.02441263198852539 s
DEBUG 10-15 15:27:33 lpllm.py:2283] CPU attn cost 0.141627 seconds if batch True
DEBUG 10-15 15:27:33 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:33 lpllm.py:2294] CPU compute cost 0.142550 seconds
DEBUG 10-15 15:27:33 lpllm.py:2312] free cost 0.000072 seconds
DEBUG 10-15 15:27:33 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:33 lpllm.py:1774] update state cost 3.4332275390625e-05 s
DEBUG 10-15 15:27:33 lpllm.py:1743] restore layer func cost 0.0008296966552734375 s
DEBUG 10-15 15:27:33 lpllm.py:511] restore layer cost 0.0010852813720703125 s
DEBUG 10-15 15:27:33 lpllm.py:512] reset_next_layer_need: layer_attn_idx=9, layer_mlp_idx=9, j_loc=18
DEBUG 10-15 15:27:33 lpllm.py:1037] reset layer cost 0.0011591911315917969 s
DEBUG 10-15 15:27:33 lpllm.py:1038] have reset next layer layer_attn 9 layer_mlp 9 
DEBUG 10-15 15:27:33 lpllm.py:924] 
DEBUG 10-15 15:27:33 lpllm.py:924] decoder loop j: 19 cur_layer_idx: 9 layer_attn_idx: 9 layer_mlp_idx: 9
DEBUG 10-15 15:27:33 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:33 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:33 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:33 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:33 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:33 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:33 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:33 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:33 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:33 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:33 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:33 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:33 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:33 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:33 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:33 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:33 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:33 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:33 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:33 lpllm.py:2265] GPU2CPU move cost 0.000594 seconds
DEBUG 10-15 15:27:33 lpmodule.py:340] layer_idx 9 attn layer_idx: 9
DEBUG 10-15 15:27:33 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:33 lpmodule.py:364] decoder_attn_batch update batch_dim 420-480
DEBUG 10-15 15:27:33 StaticCacheLen.py:84] static cache update layer_idx: 9, start_batch: 420, end_batch: 480
DEBUG 10-15 15:27:33 lpmodule.py:368] update for kv cache 420-480 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:33 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:33 lpmodule.py:374] update past key value cost 0.024445 seconds
DEBUG 10-15 15:27:33 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:33 lpmodule.py:227] layer idx 9
DEBUG 10-15 15:27:33 lpmodule.py:399] repeat qkv cost 0.031290 seconds
DEBUG 10-15 15:27:33 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:33 lpmodule.py:433] dot attn cost 0.039087 seconds
DEBUG 10-15 15:27:33 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:33 lpmodule.py:444] time cost move to cuda:1 0.02352595329284668 s
DEBUG 10-15 15:27:34 lpllm.py:2283] CPU attn cost 0.143283 seconds if batch True
DEBUG 10-15 15:27:34 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:34 lpllm.py:2294] CPU compute cost 0.144161 seconds
DEBUG 10-15 15:27:34 lpllm.py:2312] free cost 0.000075 seconds
DEBUG 10-15 15:27:34 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:34 lpllm.py:1774] update state cost 2.1696090698242188e-05 s
DEBUG 10-15 15:27:34 lpllm.py:1743] restore layer func cost 0.0003886222839355469 s
DEBUG 10-15 15:27:34 lpllm.py:511] restore layer cost 0.0006191730499267578 s
DEBUG 10-15 15:27:34 lpllm.py:512] reset_next_layer_need: layer_attn_idx=10, layer_mlp_idx=9, j_loc=19
DEBUG 10-15 15:27:34 lpllm.py:1037] reset layer cost 0.0006878376007080078 s
DEBUG 10-15 15:27:34 lpllm.py:1038] have reset next layer layer_attn 10 layer_mlp 9 
DEBUG 10-15 15:27:34 lpllm.py:1044] j: 19 waiting the layer with layer_idx 10 before wait time 0.4423692226409912 s
INFO 10-15 15:27:34 client.py:117] confirm_model_loaded: Mixtral-8x7B, 5a88366c-7f58-45e3-aaf3-13b30b7c0f0b
INFO 10-15 15:27:34 client.py:125] Model loaded
DEBUG 10-15 15:27:34 lpllm.py:1048] j: load cost 0.44383907318115234 s waiting cost 0.0014545917510986328 s
DEBUG 10-15 15:27:34 lpllm.py:924] 
DEBUG 10-15 15:27:34 lpllm.py:924] decoder loop j: 20 cur_layer_idx: 10 layer_attn_idx: 10 layer_mlp_idx: 9
DEBUG 10-15 15:27:34 lpllm.py:933] start load next layer cur_layer_idx: 11
DEBUG 10-15 15:27:34 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed6e0>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x004\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x98\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:34 client.py:72] load_into_gpu: Mixtral-8x7B, cbe2e7b3-2d72-47ad-8b85-d0e4c1330bbe
INFO 10-15 15:27:34 client.py:113] Model loaded: Mixtral-8x7B, cbe2e7b3-2d72-47ad-8b85-d0e4c1330bbe
DEBUG 10-15 15:27:34 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:34 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:34 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:34 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:34 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:34 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:34 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:34 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:34 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:34 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:34 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:34 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:34 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:34 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:34 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:34 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:34 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:34 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:34 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:34 lpllm.py:2265] GPU2CPU move cost 0.000616 seconds
DEBUG 10-15 15:27:34 lpmodule.py:340] layer_idx 10 attn layer_idx: 10
DEBUG 10-15 15:27:34 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:34 lpmodule.py:364] decoder_attn_batch update batch_dim 360-420
DEBUG 10-15 15:27:34 StaticCacheLen.py:84] static cache update layer_idx: 10, start_batch: 360, end_batch: 420
DEBUG 10-15 15:27:34 lpmodule.py:368] update for kv cache 360-420 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:34 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:34 lpmodule.py:374] update past key value cost 0.024213 seconds
DEBUG 10-15 15:27:34 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:34 lpmodule.py:227] layer idx 9
DEBUG 10-15 15:27:34 lpmodule.py:399] repeat qkv cost 0.031201 seconds
DEBUG 10-15 15:27:34 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:34 lpmodule.py:433] dot attn cost 0.043444 seconds
DEBUG 10-15 15:27:34 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:34 lpmodule.py:444] time cost move to cuda:1 0.023020505905151367 s
DEBUG 10-15 15:27:34 lpllm.py:2283] CPU attn cost 0.147398 seconds if batch True
DEBUG 10-15 15:27:34 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:34 lpllm.py:2294] CPU compute cost 0.148303 seconds
DEBUG 10-15 15:27:34 lpllm.py:2312] free cost 0.000074 seconds
DEBUG 10-15 15:27:34 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:34 lpllm.py:1774] update state cost 2.1457672119140625e-05 s
DEBUG 10-15 15:27:34 lpllm.py:1743] restore layer func cost 0.0008249282836914062 s
DEBUG 10-15 15:27:34 lpllm.py:511] restore layer cost 0.0010585784912109375 s
DEBUG 10-15 15:27:34 lpllm.py:512] reset_next_layer_need: layer_attn_idx=10, layer_mlp_idx=10, j_loc=20
DEBUG 10-15 15:27:34 lpllm.py:1037] reset layer cost 0.0011353492736816406 s
DEBUG 10-15 15:27:34 lpllm.py:1038] have reset next layer layer_attn 10 layer_mlp 10 
DEBUG 10-15 15:27:34 lpllm.py:924] 
DEBUG 10-15 15:27:34 lpllm.py:924] decoder loop j: 21 cur_layer_idx: 10 layer_attn_idx: 10 layer_mlp_idx: 10
DEBUG 10-15 15:27:34 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:34 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:34 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:34 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:34 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:34 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:34 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:34 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:34 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:34 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:34 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:34 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:34 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:34 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:34 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:34 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:34 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:34 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:34 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:34 lpllm.py:2265] GPU2CPU move cost 0.000568 seconds
DEBUG 10-15 15:27:34 lpmodule.py:340] layer_idx 10 attn layer_idx: 10
DEBUG 10-15 15:27:34 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:34 lpmodule.py:364] decoder_attn_batch update batch_dim 420-480
DEBUG 10-15 15:27:34 StaticCacheLen.py:84] static cache update layer_idx: 10, start_batch: 420, end_batch: 480
DEBUG 10-15 15:27:34 lpmodule.py:368] update for kv cache 420-480 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:34 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:34 lpmodule.py:374] update past key value cost 0.024803 seconds
DEBUG 10-15 15:27:34 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:34 lpmodule.py:227] layer idx 10
DEBUG 10-15 15:27:34 lpmodule.py:399] repeat qkv cost 0.032069 seconds
DEBUG 10-15 15:27:34 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:34 lpmodule.py:433] dot attn cost 0.043010 seconds
DEBUG 10-15 15:27:34 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:34 lpmodule.py:444] time cost move to cuda:1 0.022804737091064453 s
DEBUG 10-15 15:27:34 lpllm.py:2283] CPU attn cost 0.146990 seconds if batch True
DEBUG 10-15 15:27:34 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:34 lpllm.py:2294] CPU compute cost 0.147836 seconds
DEBUG 10-15 15:27:34 lpllm.py:2312] free cost 0.000076 seconds
DEBUG 10-15 15:27:34 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:34 lpllm.py:1774] update state cost 2.1696090698242188e-05 s
DEBUG 10-15 15:27:34 lpllm.py:1743] restore layer func cost 0.0004038810729980469 s
DEBUG 10-15 15:27:34 lpllm.py:511] restore layer cost 0.000640869140625 s
DEBUG 10-15 15:27:34 lpllm.py:512] reset_next_layer_need: layer_attn_idx=11, layer_mlp_idx=10, j_loc=21
DEBUG 10-15 15:27:34 lpllm.py:1037] reset layer cost 0.0007152557373046875 s
DEBUG 10-15 15:27:34 lpllm.py:1038] have reset next layer layer_attn 11 layer_mlp 10 
DEBUG 10-15 15:27:34 lpllm.py:1044] j: 21 waiting the layer with layer_idx 11 before wait time 0.4454202651977539 s
INFO 10-15 15:27:34 client.py:117] confirm_model_loaded: Mixtral-8x7B, cbe2e7b3-2d72-47ad-8b85-d0e4c1330bbe
INFO 10-15 15:27:34 client.py:125] Model loaded
DEBUG 10-15 15:27:34 lpllm.py:1048] j: load cost 0.4472332000732422 s waiting cost 0.0017979145050048828 s
DEBUG 10-15 15:27:34 lpllm.py:924] 
DEBUG 10-15 15:27:34 lpllm.py:924] decoder loop j: 22 cur_layer_idx: 11 layer_attn_idx: 11 layer_mlp_idx: 10
DEBUG 10-15 15:27:34 lpllm.py:933] start load next layer cur_layer_idx: 12
DEBUG 10-15 15:27:34 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:34 client.py:72] load_into_gpu: Mixtral-8x7B, 785164f0-a9ea-43fd-a2ed-17fccfea5888
INFO 10-15 15:27:34 client.py:113] Model loaded: Mixtral-8x7B, 785164f0-a9ea-43fd-a2ed-17fccfea5888
DEBUG 10-15 15:27:34 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:34 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:34 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:34 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:34 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:34 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:34 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:34 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:34 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:34 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:34 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:34 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:34 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:34 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:34 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:34 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:34 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:34 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:34 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:34 lpllm.py:2265] GPU2CPU move cost 0.000609 seconds
DEBUG 10-15 15:27:34 lpmodule.py:340] layer_idx 11 attn layer_idx: 11
DEBUG 10-15 15:27:34 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:34 lpmodule.py:364] decoder_attn_batch update batch_dim 360-420
DEBUG 10-15 15:27:34 StaticCacheLen.py:84] static cache update layer_idx: 11, start_batch: 360, end_batch: 420
DEBUG 10-15 15:27:34 lpmodule.py:368] update for kv cache 360-420 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:34 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:34 lpmodule.py:374] update past key value cost 0.024038 seconds
DEBUG 10-15 15:27:34 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:34 lpmodule.py:227] layer idx 10
DEBUG 10-15 15:27:34 lpmodule.py:399] repeat qkv cost 0.032056 seconds
DEBUG 10-15 15:27:34 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:34 lpmodule.py:433] dot attn cost 0.043373 seconds
DEBUG 10-15 15:27:34 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:34 lpmodule.py:444] time cost move to cuda:1 0.023072481155395508 s
DEBUG 10-15 15:27:34 lpllm.py:2283] CPU attn cost 0.146659 seconds if batch True
DEBUG 10-15 15:27:34 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:34 lpllm.py:2294] CPU compute cost 0.147553 seconds
DEBUG 10-15 15:27:34 lpllm.py:2312] free cost 0.000076 seconds
DEBUG 10-15 15:27:34 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:34 lpllm.py:1774] update state cost 3.600120544433594e-05 s
DEBUG 10-15 15:27:34 lpllm.py:1743] restore layer func cost 0.0008289813995361328 s
DEBUG 10-15 15:27:34 lpllm.py:511] restore layer cost 0.0010862350463867188 s
DEBUG 10-15 15:27:34 lpllm.py:512] reset_next_layer_need: layer_attn_idx=11, layer_mlp_idx=11, j_loc=22
DEBUG 10-15 15:27:34 lpllm.py:1037] reset layer cost 0.0011625289916992188 s
DEBUG 10-15 15:27:34 lpllm.py:1038] have reset next layer layer_attn 11 layer_mlp 11 
DEBUG 10-15 15:27:34 lpllm.py:924] 
DEBUG 10-15 15:27:34 lpllm.py:924] decoder loop j: 23 cur_layer_idx: 11 layer_attn_idx: 11 layer_mlp_idx: 11
DEBUG 10-15 15:27:34 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:34 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:34 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:34 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:34 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:34 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:34 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:34 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:34 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:34 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:34 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:34 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:34 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:34 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:34 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:34 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:34 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:34 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:34 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:34 lpllm.py:2265] GPU2CPU move cost 0.000588 seconds
DEBUG 10-15 15:27:34 lpmodule.py:340] layer_idx 11 attn layer_idx: 11
DEBUG 10-15 15:27:34 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:34 lpmodule.py:364] decoder_attn_batch update batch_dim 420-480
DEBUG 10-15 15:27:34 StaticCacheLen.py:84] static cache update layer_idx: 11, start_batch: 420, end_batch: 480
DEBUG 10-15 15:27:34 lpmodule.py:368] update for kv cache 420-480 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:34 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:34 lpmodule.py:374] update past key value cost 0.025092 seconds
DEBUG 10-15 15:27:34 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:34 lpmodule.py:227] layer idx 11
DEBUG 10-15 15:27:34 lpmodule.py:399] repeat qkv cost 0.030938 seconds
DEBUG 10-15 15:27:34 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:34 lpmodule.py:433] dot attn cost 0.033121 seconds
DEBUG 10-15 15:27:34 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:34 lpmodule.py:444] time cost move to cuda:1 0.02327132225036621 s
DEBUG 10-15 15:27:34 lpllm.py:2283] CPU attn cost 0.136615 seconds if batch True
DEBUG 10-15 15:27:34 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:34 lpllm.py:2294] CPU compute cost 0.137514 seconds
DEBUG 10-15 15:27:34 lpllm.py:2312] free cost 0.000075 seconds
DEBUG 10-15 15:27:34 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:34 lpllm.py:1774] update state cost 4.1484832763671875e-05 s
DEBUG 10-15 15:27:34 lpllm.py:1743] restore layer func cost 0.0004010200500488281 s
DEBUG 10-15 15:27:34 lpllm.py:511] restore layer cost 0.0006625652313232422 s
DEBUG 10-15 15:27:34 lpllm.py:512] reset_next_layer_need: layer_attn_idx=12, layer_mlp_idx=11, j_loc=23
DEBUG 10-15 15:27:34 lpllm.py:1037] reset layer cost 0.000736236572265625 s
DEBUG 10-15 15:27:34 lpllm.py:1038] have reset next layer layer_attn 12 layer_mlp 11 
DEBUG 10-15 15:27:34 lpllm.py:1044] j: 23 waiting the layer with layer_idx 12 before wait time 0.4505271911621094 s
INFO 10-15 15:27:34 client.py:117] confirm_model_loaded: Mixtral-8x7B, 785164f0-a9ea-43fd-a2ed-17fccfea5888
INFO 10-15 15:27:34 client.py:125] Model loaded
DEBUG 10-15 15:27:34 lpllm.py:1048] j: load cost 0.45229649543762207 s waiting cost 0.0017549991607666016 s
DEBUG 10-15 15:27:34 lpllm.py:924] 
DEBUG 10-15 15:27:34 lpllm.py:924] decoder loop j: 24 cur_layer_idx: 12 layer_attn_idx: 12 layer_mlp_idx: 11
DEBUG 10-15 15:27:34 lpllm.py:933] start load next layer cur_layer_idx: 13
DEBUG 10-15 15:27:34 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed650>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x003\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x96\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:34 client.py:72] load_into_gpu: Mixtral-8x7B, abb5adf7-ea9d-4ecb-8ff8-8a36171bba2e
INFO 10-15 15:27:34 client.py:113] Model loaded: Mixtral-8x7B, abb5adf7-ea9d-4ecb-8ff8-8a36171bba2e
DEBUG 10-15 15:27:34 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:34 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:34 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:34 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:34 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:34 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:34 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:34 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:34 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:34 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:34 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:34 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:34 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:34 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:34 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:34 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:34 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:34 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:35 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:35 lpllm.py:2265] GPU2CPU move cost 0.000542 seconds
DEBUG 10-15 15:27:35 lpmodule.py:340] layer_idx 12 attn layer_idx: 12
DEBUG 10-15 15:27:35 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:35 lpmodule.py:364] decoder_attn_batch update batch_dim 360-420
DEBUG 10-15 15:27:35 StaticCacheLen.py:84] static cache update layer_idx: 12, start_batch: 360, end_batch: 420
DEBUG 10-15 15:27:35 lpmodule.py:368] update for kv cache 360-420 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:35 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:35 lpmodule.py:374] update past key value cost 0.023499 seconds
DEBUG 10-15 15:27:35 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:35 lpmodule.py:227] layer idx 11
DEBUG 10-15 15:27:35 lpmodule.py:399] repeat qkv cost 0.030224 seconds
DEBUG 10-15 15:27:35 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:35 lpmodule.py:433] dot attn cost 0.033434 seconds
DEBUG 10-15 15:27:35 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:35 lpmodule.py:444] time cost move to cuda:1 0.022753477096557617 s
DEBUG 10-15 15:27:35 lpllm.py:2283] CPU attn cost 0.134153 seconds if batch True
DEBUG 10-15 15:27:35 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:35 lpllm.py:2294] CPU compute cost 0.134952 seconds
DEBUG 10-15 15:27:35 lpllm.py:2312] free cost 0.000075 seconds
DEBUG 10-15 15:27:35 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:35 lpllm.py:1774] update state cost 2.7418136596679688e-05 s
DEBUG 10-15 15:27:35 lpllm.py:1743] restore layer func cost 0.0008330345153808594 s
DEBUG 10-15 15:27:35 lpllm.py:511] restore layer cost 0.0011136531829833984 s
DEBUG 10-15 15:27:35 lpllm.py:512] reset_next_layer_need: layer_attn_idx=12, layer_mlp_idx=12, j_loc=24
DEBUG 10-15 15:27:35 lpllm.py:1037] reset layer cost 0.0012040138244628906 s
DEBUG 10-15 15:27:35 lpllm.py:1038] have reset next layer layer_attn 12 layer_mlp 12 
DEBUG 10-15 15:27:35 lpllm.py:924] 
DEBUG 10-15 15:27:35 lpllm.py:924] decoder loop j: 25 cur_layer_idx: 12 layer_attn_idx: 12 layer_mlp_idx: 12
DEBUG 10-15 15:27:35 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:35 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:35 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:35 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:35 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:35 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:35 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:35 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:35 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:35 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:35 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:35 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:35 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:35 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:35 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:35 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:35 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:35 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:35 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:35 lpllm.py:2265] GPU2CPU move cost 0.000675 seconds
DEBUG 10-15 15:27:35 lpmodule.py:340] layer_idx 12 attn layer_idx: 12
DEBUG 10-15 15:27:35 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:35 lpmodule.py:364] decoder_attn_batch update batch_dim 420-480
DEBUG 10-15 15:27:35 StaticCacheLen.py:84] static cache update layer_idx: 12, start_batch: 420, end_batch: 480
DEBUG 10-15 15:27:35 lpmodule.py:368] update for kv cache 420-480 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:35 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:35 lpmodule.py:374] update past key value cost 0.025034 seconds
DEBUG 10-15 15:27:35 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:35 lpmodule.py:227] layer idx 12
DEBUG 10-15 15:27:35 lpmodule.py:399] repeat qkv cost 0.030872 seconds
DEBUG 10-15 15:27:35 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:35 lpmodule.py:433] dot attn cost 0.033243 seconds
DEBUG 10-15 15:27:35 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:35 lpmodule.py:444] time cost move to cuda:1 0.022889137268066406 s
DEBUG 10-15 15:27:35 lpllm.py:2283] CPU attn cost 0.136578 seconds if batch True
DEBUG 10-15 15:27:35 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:35 lpllm.py:2294] CPU compute cost 0.137566 seconds
DEBUG 10-15 15:27:35 lpllm.py:2312] free cost 0.000076 seconds
DEBUG 10-15 15:27:35 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:35 lpllm.py:1774] update state cost 2.3603439331054688e-05 s
DEBUG 10-15 15:27:35 lpllm.py:1743] restore layer func cost 0.00039124488830566406 s
DEBUG 10-15 15:27:35 lpllm.py:511] restore layer cost 0.0006246566772460938 s
DEBUG 10-15 15:27:35 lpllm.py:512] reset_next_layer_need: layer_attn_idx=13, layer_mlp_idx=12, j_loc=25
DEBUG 10-15 15:27:35 lpllm.py:1037] reset layer cost 0.0006978511810302734 s
DEBUG 10-15 15:27:35 lpllm.py:1038] have reset next layer layer_attn 13 layer_mlp 12 
DEBUG 10-15 15:27:35 lpllm.py:1044] j: 25 waiting the layer with layer_idx 13 before wait time 0.44608163833618164 s
INFO 10-15 15:27:35 client.py:117] confirm_model_loaded: Mixtral-8x7B, abb5adf7-ea9d-4ecb-8ff8-8a36171bba2e
INFO 10-15 15:27:35 client.py:125] Model loaded
DEBUG 10-15 15:27:35 lpllm.py:1048] j: load cost 0.447721004486084 s waiting cost 0.0016241073608398438 s
DEBUG 10-15 15:27:35 lpllm.py:924] 
DEBUG 10-15 15:27:35 lpllm.py:924] decoder loop j: 26 cur_layer_idx: 13 layer_attn_idx: 13 layer_mlp_idx: 12
DEBUG 10-15 15:27:35 lpllm.py:933] start load next layer cur_layer_idx: 14
DEBUG 10-15 15:27:35 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed6e0>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x004\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x98\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:35 client.py:72] load_into_gpu: Mixtral-8x7B, f9671b5a-8088-41e3-8e54-69fc7ba178ce
INFO 10-15 15:27:35 client.py:113] Model loaded: Mixtral-8x7B, f9671b5a-8088-41e3-8e54-69fc7ba178ce
DEBUG 10-15 15:27:35 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:35 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:35 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:35 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:35 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:35 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:35 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:35 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:35 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:35 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:35 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:35 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:35 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:35 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:35 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:35 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:35 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:35 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:35 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:35 lpllm.py:2265] GPU2CPU move cost 0.000569 seconds
DEBUG 10-15 15:27:35 lpmodule.py:340] layer_idx 13 attn layer_idx: 13
DEBUG 10-15 15:27:35 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:35 lpmodule.py:364] decoder_attn_batch update batch_dim 360-420
DEBUG 10-15 15:27:35 StaticCacheLen.py:84] static cache update layer_idx: 13, start_batch: 360, end_batch: 420
DEBUG 10-15 15:27:35 lpmodule.py:368] update for kv cache 360-420 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:35 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:35 lpmodule.py:374] update past key value cost 0.023852 seconds
DEBUG 10-15 15:27:35 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:35 lpmodule.py:227] layer idx 12
DEBUG 10-15 15:27:35 lpmodule.py:399] repeat qkv cost 0.030610 seconds
DEBUG 10-15 15:27:35 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:35 lpmodule.py:433] dot attn cost 0.033212 seconds
DEBUG 10-15 15:27:35 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:35 lpmodule.py:444] time cost move to cuda:1 0.025214672088623047 s
DEBUG 10-15 15:27:35 lpllm.py:2283] CPU attn cost 0.137239 seconds if batch True
DEBUG 10-15 15:27:35 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:35 lpllm.py:2294] CPU compute cost 0.138096 seconds
DEBUG 10-15 15:27:35 lpllm.py:2312] free cost 0.000073 seconds
DEBUG 10-15 15:27:35 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:35 lpllm.py:1774] update state cost 3.790855407714844e-05 s
DEBUG 10-15 15:27:35 lpllm.py:1743] restore layer func cost 0.0008318424224853516 s
DEBUG 10-15 15:27:35 lpllm.py:511] restore layer cost 0.0010905265808105469 s
DEBUG 10-15 15:27:35 lpllm.py:512] reset_next_layer_need: layer_attn_idx=13, layer_mlp_idx=13, j_loc=26
DEBUG 10-15 15:27:35 lpllm.py:1037] reset layer cost 0.0011677742004394531 s
DEBUG 10-15 15:27:35 lpllm.py:1038] have reset next layer layer_attn 13 layer_mlp 13 
DEBUG 10-15 15:27:35 lpllm.py:924] 
DEBUG 10-15 15:27:35 lpllm.py:924] decoder loop j: 27 cur_layer_idx: 13 layer_attn_idx: 13 layer_mlp_idx: 13
DEBUG 10-15 15:27:35 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:35 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:35 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:35 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:35 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:35 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:35 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:35 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:35 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:35 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:35 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:35 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:35 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:35 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:35 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:35 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:35 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:35 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:35 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:35 lpllm.py:2265] GPU2CPU move cost 0.000598 seconds
DEBUG 10-15 15:27:35 lpmodule.py:340] layer_idx 13 attn layer_idx: 13
DEBUG 10-15 15:27:35 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:35 lpmodule.py:364] decoder_attn_batch update batch_dim 420-480
DEBUG 10-15 15:27:35 StaticCacheLen.py:84] static cache update layer_idx: 13, start_batch: 420, end_batch: 480
DEBUG 10-15 15:27:35 lpmodule.py:368] update for kv cache 420-480 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:35 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:35 lpmodule.py:374] update past key value cost 0.025211 seconds
DEBUG 10-15 15:27:35 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:35 lpmodule.py:227] layer idx 13
DEBUG 10-15 15:27:35 lpmodule.py:399] repeat qkv cost 0.030941 seconds
DEBUG 10-15 15:27:35 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:35 lpmodule.py:433] dot attn cost 0.032484 seconds
DEBUG 10-15 15:27:35 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:35 lpmodule.py:444] time cost move to cuda:1 0.023106098175048828 s
DEBUG 10-15 15:27:35 lpllm.py:2283] CPU attn cost 0.136067 seconds if batch True
DEBUG 10-15 15:27:35 lpllm.py:2292] deal attn result cost 0.000003 seconds
DEBUG 10-15 15:27:35 lpllm.py:2294] CPU compute cost 0.136943 seconds
DEBUG 10-15 15:27:35 lpllm.py:2312] free cost 0.000088 seconds
DEBUG 10-15 15:27:35 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:35 lpllm.py:1774] update state cost 3.790855407714844e-05 s
DEBUG 10-15 15:27:35 lpllm.py:1743] restore layer func cost 0.0003921985626220703 s
DEBUG 10-15 15:27:35 lpllm.py:511] restore layer cost 0.0006399154663085938 s
DEBUG 10-15 15:27:35 lpllm.py:512] reset_next_layer_need: layer_attn_idx=14, layer_mlp_idx=13, j_loc=27
DEBUG 10-15 15:27:35 lpllm.py:1037] reset layer cost 0.0007147789001464844 s
DEBUG 10-15 15:27:35 lpllm.py:1038] have reset next layer layer_attn 14 layer_mlp 13 
DEBUG 10-15 15:27:35 lpllm.py:1044] j: 27 waiting the layer with layer_idx 14 before wait time 0.44534945487976074 s
INFO 10-15 15:27:35 client.py:117] confirm_model_loaded: Mixtral-8x7B, f9671b5a-8088-41e3-8e54-69fc7ba178ce
INFO 10-15 15:27:35 client.py:125] Model loaded
DEBUG 10-15 15:27:35 lpllm.py:1048] j: load cost 0.44711995124816895 s waiting cost 0.0017554759979248047 s
DEBUG 10-15 15:27:35 lpllm.py:924] 
DEBUG 10-15 15:27:35 lpllm.py:924] decoder loop j: 28 cur_layer_idx: 14 layer_attn_idx: 14 layer_mlp_idx: 13
DEBUG 10-15 15:27:35 lpllm.py:933] start load next layer cur_layer_idx: 15
DEBUG 10-15 15:27:35 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:35 client.py:72] load_into_gpu: Mixtral-8x7B, 1cc6b270-abce-4a32-840a-6e7f71a9c962
INFO 10-15 15:27:35 client.py:113] Model loaded: Mixtral-8x7B, 1cc6b270-abce-4a32-840a-6e7f71a9c962
DEBUG 10-15 15:27:35 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:35 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:35 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:35 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:35 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:35 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:35 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:35 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:35 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:35 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:35 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:35 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:35 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:35 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:35 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:35 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:35 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:35 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:35 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:35 lpllm.py:2265] GPU2CPU move cost 0.000587 seconds
DEBUG 10-15 15:27:35 lpmodule.py:340] layer_idx 14 attn layer_idx: 14
DEBUG 10-15 15:27:35 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:35 lpmodule.py:364] decoder_attn_batch update batch_dim 360-420
DEBUG 10-15 15:27:35 StaticCacheLen.py:84] static cache update layer_idx: 14, start_batch: 360, end_batch: 420
DEBUG 10-15 15:27:35 lpmodule.py:368] update for kv cache 360-420 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:35 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:35 lpmodule.py:374] update past key value cost 0.025224 seconds
DEBUG 10-15 15:27:35 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:35 lpmodule.py:227] layer idx 13
DEBUG 10-15 15:27:35 lpmodule.py:399] repeat qkv cost 0.030766 seconds
DEBUG 10-15 15:27:35 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:35 lpmodule.py:433] dot attn cost 0.032635 seconds
DEBUG 10-15 15:27:35 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:36 lpmodule.py:444] time cost move to cuda:1 0.023313522338867188 s
DEBUG 10-15 15:27:36 lpllm.py:2283] CPU attn cost 0.136209 seconds if batch True
DEBUG 10-15 15:27:36 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:36 lpllm.py:2294] CPU compute cost 0.137071 seconds
DEBUG 10-15 15:27:36 lpllm.py:2312] free cost 0.000074 seconds
DEBUG 10-15 15:27:36 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:36 lpllm.py:1774] update state cost 3.743171691894531e-05 s
DEBUG 10-15 15:27:36 lpllm.py:1743] restore layer func cost 0.0008165836334228516 s
DEBUG 10-15 15:27:36 lpllm.py:511] restore layer cost 0.0010612010955810547 s
DEBUG 10-15 15:27:36 lpllm.py:512] reset_next_layer_need: layer_attn_idx=14, layer_mlp_idx=14, j_loc=28
DEBUG 10-15 15:27:36 lpllm.py:1037] reset layer cost 0.00113677978515625 s
DEBUG 10-15 15:27:36 lpllm.py:1038] have reset next layer layer_attn 14 layer_mlp 14 
DEBUG 10-15 15:27:36 lpllm.py:924] 
DEBUG 10-15 15:27:36 lpllm.py:924] decoder loop j: 29 cur_layer_idx: 14 layer_attn_idx: 14 layer_mlp_idx: 14
DEBUG 10-15 15:27:36 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:36 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:36 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:36 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:36 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:36 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:36 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:36 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:36 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:36 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:36 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:36 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:36 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:36 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:36 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:36 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:36 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:36 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:36 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:36 lpllm.py:2265] GPU2CPU move cost 0.000567 seconds
DEBUG 10-15 15:27:36 lpmodule.py:340] layer_idx 14 attn layer_idx: 14
DEBUG 10-15 15:27:36 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:36 lpmodule.py:364] decoder_attn_batch update batch_dim 420-480
DEBUG 10-15 15:27:36 StaticCacheLen.py:84] static cache update layer_idx: 14, start_batch: 420, end_batch: 480
DEBUG 10-15 15:27:36 lpmodule.py:368] update for kv cache 420-480 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:36 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:36 lpmodule.py:374] update past key value cost 0.024659 seconds
DEBUG 10-15 15:27:36 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:36 lpmodule.py:227] layer idx 14
DEBUG 10-15 15:27:36 lpmodule.py:399] repeat qkv cost 0.031199 seconds
DEBUG 10-15 15:27:36 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:36 lpmodule.py:433] dot attn cost 0.032618 seconds
DEBUG 10-15 15:27:36 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:36 lpmodule.py:444] time cost move to cuda:1 0.023104190826416016 s
DEBUG 10-15 15:27:36 lpllm.py:2283] CPU attn cost 0.136819 seconds if batch True
DEBUG 10-15 15:27:36 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:36 lpllm.py:2294] CPU compute cost 0.137661 seconds
DEBUG 10-15 15:27:36 lpllm.py:2312] free cost 0.000076 seconds
DEBUG 10-15 15:27:36 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:36 lpllm.py:1774] update state cost 3.814697265625e-05 s
DEBUG 10-15 15:27:36 lpllm.py:1743] restore layer func cost 0.00039458274841308594 s
DEBUG 10-15 15:27:36 lpllm.py:511] restore layer cost 0.0006372928619384766 s
DEBUG 10-15 15:27:36 lpllm.py:512] reset_next_layer_need: layer_attn_idx=15, layer_mlp_idx=14, j_loc=29
DEBUG 10-15 15:27:36 lpllm.py:1037] reset layer cost 0.0007119178771972656 s
DEBUG 10-15 15:27:36 lpllm.py:1038] have reset next layer layer_attn 15 layer_mlp 14 
DEBUG 10-15 15:27:36 lpllm.py:1044] j: 29 waiting the layer with layer_idx 15 before wait time 0.44754528999328613 s
INFO 10-15 15:27:36 client.py:117] confirm_model_loaded: Mixtral-8x7B, 1cc6b270-abce-4a32-840a-6e7f71a9c962
INFO 10-15 15:27:36 client.py:125] Model loaded
DEBUG 10-15 15:27:36 lpllm.py:1048] j: load cost 0.4492685794830322 s waiting cost 0.001707315444946289 s
DEBUG 10-15 15:27:36 lpllm.py:924] 
DEBUG 10-15 15:27:36 lpllm.py:924] decoder loop j: 30 cur_layer_idx: 15 layer_attn_idx: 15 layer_mlp_idx: 14
DEBUG 10-15 15:27:36 lpllm.py:933] start load next layer cur_layer_idx: 16
DEBUG 10-15 15:27:36 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed650>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x003\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x96\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:36 client.py:72] load_into_gpu: Mixtral-8x7B, cdead9c7-96df-4db9-bec4-712c2d184331
INFO 10-15 15:27:36 client.py:113] Model loaded: Mixtral-8x7B, cdead9c7-96df-4db9-bec4-712c2d184331
DEBUG 10-15 15:27:36 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:36 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:36 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:36 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:36 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:36 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:36 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:36 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:36 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:36 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:36 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:36 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:36 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:36 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:36 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:36 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:36 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:36 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:36 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:36 lpllm.py:2265] GPU2CPU move cost 0.000617 seconds
DEBUG 10-15 15:27:36 lpmodule.py:340] layer_idx 15 attn layer_idx: 15
DEBUG 10-15 15:27:36 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:36 lpmodule.py:364] decoder_attn_batch update batch_dim 360-420
DEBUG 10-15 15:27:36 StaticCacheLen.py:84] static cache update layer_idx: 15, start_batch: 360, end_batch: 420
DEBUG 10-15 15:27:36 lpmodule.py:368] update for kv cache 360-420 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:36 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:36 lpmodule.py:374] update past key value cost 0.024415 seconds
DEBUG 10-15 15:27:36 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:36 lpmodule.py:227] layer idx 14
DEBUG 10-15 15:27:36 lpmodule.py:399] repeat qkv cost 0.032285 seconds
DEBUG 10-15 15:27:36 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:36 lpmodule.py:433] dot attn cost 0.033674 seconds
DEBUG 10-15 15:27:36 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:36 lpmodule.py:444] time cost move to cuda:1 0.02292466163635254 s
DEBUG 10-15 15:27:36 lpllm.py:2283] CPU attn cost 0.136984 seconds if batch True
DEBUG 10-15 15:27:36 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:27:36 lpllm.py:2294] CPU compute cost 0.137882 seconds
DEBUG 10-15 15:27:36 lpllm.py:2312] free cost 0.000077 seconds
DEBUG 10-15 15:27:36 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:36 lpllm.py:1774] update state cost 3.886222839355469e-05 s
DEBUG 10-15 15:27:36 lpllm.py:1743] restore layer func cost 0.0008292198181152344 s
DEBUG 10-15 15:27:36 lpllm.py:511] restore layer cost 0.001077890396118164 s
DEBUG 10-15 15:27:36 lpllm.py:512] reset_next_layer_need: layer_attn_idx=15, layer_mlp_idx=15, j_loc=30
DEBUG 10-15 15:27:36 lpllm.py:1037] reset layer cost 0.001154184341430664 s
DEBUG 10-15 15:27:36 lpllm.py:1038] have reset next layer layer_attn 15 layer_mlp 15 
DEBUG 10-15 15:27:36 lpllm.py:924] 
DEBUG 10-15 15:27:36 lpllm.py:924] decoder loop j: 31 cur_layer_idx: 15 layer_attn_idx: 15 layer_mlp_idx: 15
DEBUG 10-15 15:27:36 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:36 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:36 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:36 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:36 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:36 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:36 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:36 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:36 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:36 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:36 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:36 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:36 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:36 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:36 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:36 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:36 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:36 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:36 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:36 lpllm.py:2265] GPU2CPU move cost 0.000567 seconds
DEBUG 10-15 15:27:36 lpmodule.py:340] layer_idx 15 attn layer_idx: 15
DEBUG 10-15 15:27:36 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:36 lpmodule.py:364] decoder_attn_batch update batch_dim 420-480
DEBUG 10-15 15:27:36 StaticCacheLen.py:84] static cache update layer_idx: 15, start_batch: 420, end_batch: 480
DEBUG 10-15 15:27:36 lpmodule.py:368] update for kv cache 420-480 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:36 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:36 lpmodule.py:374] update past key value cost 0.023574 seconds
DEBUG 10-15 15:27:36 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:36 lpmodule.py:227] layer idx 15
DEBUG 10-15 15:27:36 lpmodule.py:399] repeat qkv cost 0.031754 seconds
DEBUG 10-15 15:27:36 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:36 lpmodule.py:433] dot attn cost 0.034201 seconds
DEBUG 10-15 15:27:36 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:36 lpmodule.py:444] time cost move to cuda:1 0.02313518524169922 s
DEBUG 10-15 15:27:36 lpllm.py:2283] CPU attn cost 0.136895 seconds if batch True
DEBUG 10-15 15:27:36 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:27:36 lpllm.py:2294] CPU compute cost 0.137748 seconds
DEBUG 10-15 15:27:36 lpllm.py:2312] free cost 0.000075 seconds
DEBUG 10-15 15:27:36 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:36 lpllm.py:1774] update state cost 2.3603439331054688e-05 s
DEBUG 10-15 15:27:36 lpllm.py:1743] restore layer func cost 0.0003838539123535156 s
DEBUG 10-15 15:27:36 lpllm.py:511] restore layer cost 0.0006244182586669922 s
DEBUG 10-15 15:27:36 lpllm.py:512] reset_next_layer_need: layer_attn_idx=16, layer_mlp_idx=15, j_loc=31
DEBUG 10-15 15:27:36 lpllm.py:1037] reset layer cost 0.0006968975067138672 s
DEBUG 10-15 15:27:36 lpllm.py:1038] have reset next layer layer_attn 16 layer_mlp 15 
DEBUG 10-15 15:27:36 lpllm.py:1044] j: 31 waiting the layer with layer_idx 16 before wait time 0.45011162757873535 s
INFO 10-15 15:27:36 client.py:117] confirm_model_loaded: Mixtral-8x7B, cdead9c7-96df-4db9-bec4-712c2d184331
INFO 10-15 15:27:36 client.py:125] Model loaded
DEBUG 10-15 15:27:36 lpllm.py:1048] j: load cost 0.451962947845459 s waiting cost 0.0018362998962402344 s
DEBUG 10-15 15:27:36 lpllm.py:924] 
DEBUG 10-15 15:27:36 lpllm.py:924] decoder loop j: 32 cur_layer_idx: 16 layer_attn_idx: 16 layer_mlp_idx: 15
DEBUG 10-15 15:27:36 lpllm.py:933] start load next layer cur_layer_idx: 17
DEBUG 10-15 15:27:36 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed6e0>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x004\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x98\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:36 client.py:72] load_into_gpu: Mixtral-8x7B, b74135e0-2932-4c0b-8cb3-0c84eaebe5d1
INFO 10-15 15:27:36 client.py:113] Model loaded: Mixtral-8x7B, b74135e0-2932-4c0b-8cb3-0c84eaebe5d1
DEBUG 10-15 15:27:36 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:36 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:36 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:36 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:36 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:36 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:36 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:36 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:36 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:36 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:36 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:36 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:36 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:36 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:36 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:36 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:36 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:36 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:36 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:36 lpllm.py:2265] GPU2CPU move cost 0.000571 seconds
DEBUG 10-15 15:27:36 lpmodule.py:340] layer_idx 16 attn layer_idx: 16
DEBUG 10-15 15:27:36 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:36 lpmodule.py:364] decoder_attn_batch update batch_dim 360-420
DEBUG 10-15 15:27:36 StaticCacheLen.py:84] static cache update layer_idx: 16, start_batch: 360, end_batch: 420
DEBUG 10-15 15:27:36 lpmodule.py:368] update for kv cache 360-420 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:36 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:36 lpmodule.py:374] update past key value cost 0.023782 seconds
DEBUG 10-15 15:27:36 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:36 lpmodule.py:227] layer idx 15
DEBUG 10-15 15:27:36 lpmodule.py:399] repeat qkv cost 0.032378 seconds
DEBUG 10-15 15:27:36 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:36 lpmodule.py:433] dot attn cost 0.037981 seconds
DEBUG 10-15 15:27:36 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:36 lpmodule.py:444] time cost move to cuda:1 0.022948741912841797 s
DEBUG 10-15 15:27:36 lpllm.py:2283] CPU attn cost 0.140921 seconds if batch True
DEBUG 10-15 15:27:36 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:27:36 lpllm.py:2294] CPU compute cost 0.141781 seconds
DEBUG 10-15 15:27:36 lpllm.py:2312] free cost 0.000077 seconds
DEBUG 10-15 15:27:37 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:37 lpllm.py:1774] update state cost 2.1219253540039062e-05 s
DEBUG 10-15 15:27:37 lpllm.py:1743] restore layer func cost 0.0007884502410888672 s
DEBUG 10-15 15:27:37 lpllm.py:511] restore layer cost 0.001018524169921875 s
DEBUG 10-15 15:27:37 lpllm.py:512] reset_next_layer_need: layer_attn_idx=16, layer_mlp_idx=16, j_loc=32
DEBUG 10-15 15:27:37 lpllm.py:1037] reset layer cost 0.0010938644409179688 s
DEBUG 10-15 15:27:37 lpllm.py:1038] have reset next layer layer_attn 16 layer_mlp 16 
DEBUG 10-15 15:27:37 lpllm.py:924] 
DEBUG 10-15 15:27:37 lpllm.py:924] decoder loop j: 33 cur_layer_idx: 16 layer_attn_idx: 16 layer_mlp_idx: 16
DEBUG 10-15 15:27:37 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:37 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:37 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:37 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:37 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:37 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:37 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:37 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:37 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:37 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:37 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:37 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:37 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:37 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:37 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:37 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:37 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:37 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:37 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:37 lpllm.py:2265] GPU2CPU move cost 0.000617 seconds
DEBUG 10-15 15:27:37 lpmodule.py:340] layer_idx 16 attn layer_idx: 16
DEBUG 10-15 15:27:37 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:37 lpmodule.py:364] decoder_attn_batch update batch_dim 420-480
DEBUG 10-15 15:27:37 StaticCacheLen.py:84] static cache update layer_idx: 16, start_batch: 420, end_batch: 480
DEBUG 10-15 15:27:37 lpmodule.py:368] update for kv cache 420-480 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:37 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:37 lpmodule.py:374] update past key value cost 0.024046 seconds
DEBUG 10-15 15:27:37 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:37 lpmodule.py:227] layer idx 16
DEBUG 10-15 15:27:37 lpmodule.py:399] repeat qkv cost 0.031412 seconds
DEBUG 10-15 15:27:37 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:37 lpmodule.py:433] dot attn cost 0.032120 seconds
DEBUG 10-15 15:27:37 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:37 lpmodule.py:444] time cost move to cuda:1 0.024126052856445312 s
DEBUG 10-15 15:27:37 lpllm.py:2283] CPU attn cost 0.138292 seconds if batch True
DEBUG 10-15 15:27:37 lpllm.py:2292] deal attn result cost 0.000003 seconds
DEBUG 10-15 15:27:37 lpllm.py:2294] CPU compute cost 0.139187 seconds
DEBUG 10-15 15:27:37 lpllm.py:2312] free cost 0.000073 seconds
DEBUG 10-15 15:27:37 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:37 lpllm.py:1774] update state cost 3.9577484130859375e-05 s
DEBUG 10-15 15:27:37 lpllm.py:1743] restore layer func cost 0.0003826618194580078 s
DEBUG 10-15 15:27:37 lpllm.py:511] restore layer cost 0.0006239414215087891 s
DEBUG 10-15 15:27:37 lpllm.py:512] reset_next_layer_need: layer_attn_idx=17, layer_mlp_idx=16, j_loc=33
DEBUG 10-15 15:27:37 lpllm.py:1037] reset layer cost 0.0006952285766601562 s
DEBUG 10-15 15:27:37 lpllm.py:1038] have reset next layer layer_attn 17 layer_mlp 16 
DEBUG 10-15 15:27:37 lpllm.py:1044] j: 33 waiting the layer with layer_idx 17 before wait time 0.461763858795166 s
INFO 10-15 15:27:37 client.py:117] confirm_model_loaded: Mixtral-8x7B, b74135e0-2932-4c0b-8cb3-0c84eaebe5d1
INFO 10-15 15:27:37 client.py:125] Model loaded
DEBUG 10-15 15:27:37 lpllm.py:1048] j: load cost 0.46350622177124023 s waiting cost 0.0017268657684326172 s
DEBUG 10-15 15:27:37 lpllm.py:924] 
DEBUG 10-15 15:27:37 lpllm.py:924] decoder loop j: 34 cur_layer_idx: 17 layer_attn_idx: 17 layer_mlp_idx: 16
DEBUG 10-15 15:27:37 lpllm.py:933] start load next layer cur_layer_idx: 18
DEBUG 10-15 15:27:37 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:37 client.py:72] load_into_gpu: Mixtral-8x7B, e1bee293-3c64-46ac-aff0-b88faa41b66c
INFO 10-15 15:27:37 client.py:113] Model loaded: Mixtral-8x7B, e1bee293-3c64-46ac-aff0-b88faa41b66c
DEBUG 10-15 15:27:37 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:37 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:37 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:37 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:37 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:37 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:37 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:37 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:37 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:37 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:37 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:37 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:37 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:37 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:37 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:37 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:37 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:37 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:37 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:37 lpllm.py:2265] GPU2CPU move cost 0.000578 seconds
DEBUG 10-15 15:27:37 lpmodule.py:340] layer_idx 17 attn layer_idx: 17
DEBUG 10-15 15:27:37 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:37 lpmodule.py:364] decoder_attn_batch update batch_dim 360-420
DEBUG 10-15 15:27:37 StaticCacheLen.py:84] static cache update layer_idx: 17, start_batch: 360, end_batch: 420
DEBUG 10-15 15:27:37 lpmodule.py:368] update for kv cache 360-420 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:37 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:37 lpmodule.py:374] update past key value cost 0.024641 seconds
DEBUG 10-15 15:27:37 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:37 lpmodule.py:227] layer idx 16
DEBUG 10-15 15:27:37 lpmodule.py:399] repeat qkv cost 0.031014 seconds
DEBUG 10-15 15:27:37 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:37 lpmodule.py:433] dot attn cost 0.033565 seconds
DEBUG 10-15 15:27:37 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:37 lpmodule.py:444] time cost move to cuda:1 0.024305105209350586 s
DEBUG 10-15 15:27:37 lpllm.py:2283] CPU attn cost 0.139141 seconds if batch True
DEBUG 10-15 15:27:37 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:37 lpllm.py:2294] CPU compute cost 0.140012 seconds
DEBUG 10-15 15:27:37 lpllm.py:2312] free cost 0.000074 seconds
DEBUG 10-15 15:27:37 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:37 lpllm.py:1774] update state cost 3.695487976074219e-05 s
DEBUG 10-15 15:27:37 lpllm.py:1743] restore layer func cost 0.0008170604705810547 s
DEBUG 10-15 15:27:37 lpllm.py:511] restore layer cost 0.0010654926300048828 s
DEBUG 10-15 15:27:37 lpllm.py:512] reset_next_layer_need: layer_attn_idx=17, layer_mlp_idx=17, j_loc=34
DEBUG 10-15 15:27:37 lpllm.py:1037] reset layer cost 0.001146554946899414 s
DEBUG 10-15 15:27:37 lpllm.py:1038] have reset next layer layer_attn 17 layer_mlp 17 
DEBUG 10-15 15:27:37 lpllm.py:924] 
DEBUG 10-15 15:27:37 lpllm.py:924] decoder loop j: 35 cur_layer_idx: 17 layer_attn_idx: 17 layer_mlp_idx: 17
DEBUG 10-15 15:27:37 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:37 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:37 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:37 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:37 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:37 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:37 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:37 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:37 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:37 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:37 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:37 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:37 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:37 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:37 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:37 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:37 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:37 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:37 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:37 lpllm.py:2265] GPU2CPU move cost 0.000618 seconds
DEBUG 10-15 15:27:37 lpmodule.py:340] layer_idx 17 attn layer_idx: 17
DEBUG 10-15 15:27:37 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:37 lpmodule.py:364] decoder_attn_batch update batch_dim 420-480
DEBUG 10-15 15:27:37 StaticCacheLen.py:84] static cache update layer_idx: 17, start_batch: 420, end_batch: 480
DEBUG 10-15 15:27:37 lpmodule.py:368] update for kv cache 420-480 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:37 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:37 lpmodule.py:374] update past key value cost 0.026992 seconds
DEBUG 10-15 15:27:37 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:37 lpmodule.py:227] layer idx 17
DEBUG 10-15 15:27:37 lpmodule.py:399] repeat qkv cost 0.029354 seconds
DEBUG 10-15 15:27:37 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:37 lpmodule.py:433] dot attn cost 0.032981 seconds
DEBUG 10-15 15:27:37 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:37 lpmodule.py:444] time cost move to cuda:1 0.023587942123413086 s
DEBUG 10-15 15:27:37 lpllm.py:2283] CPU attn cost 0.138369 seconds if batch True
DEBUG 10-15 15:27:37 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:37 lpllm.py:2294] CPU compute cost 0.139280 seconds
DEBUG 10-15 15:27:37 lpllm.py:2312] free cost 0.000076 seconds
DEBUG 10-15 15:27:37 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:37 lpllm.py:1774] update state cost 2.2172927856445312e-05 s
DEBUG 10-15 15:27:37 lpllm.py:1743] restore layer func cost 0.0003895759582519531 s
DEBUG 10-15 15:27:37 lpllm.py:511] restore layer cost 0.0006308555603027344 s
DEBUG 10-15 15:27:37 lpllm.py:512] reset_next_layer_need: layer_attn_idx=18, layer_mlp_idx=17, j_loc=35
DEBUG 10-15 15:27:37 lpllm.py:1037] reset layer cost 0.0007042884826660156 s
DEBUG 10-15 15:27:37 lpllm.py:1038] have reset next layer layer_attn 18 layer_mlp 17 
DEBUG 10-15 15:27:37 lpllm.py:1044] j: 35 waiting the layer with layer_idx 18 before wait time 0.4555542469024658 s
INFO 10-15 15:27:37 client.py:117] confirm_model_loaded: Mixtral-8x7B, e1bee293-3c64-46ac-aff0-b88faa41b66c
INFO 10-15 15:27:37 client.py:125] Model loaded
DEBUG 10-15 15:27:37 lpllm.py:1048] j: load cost 0.4574134349822998 s waiting cost 0.001844167709350586 s
DEBUG 10-15 15:27:37 lpllm.py:924] 
DEBUG 10-15 15:27:37 lpllm.py:924] decoder loop j: 36 cur_layer_idx: 18 layer_attn_idx: 18 layer_mlp_idx: 17
DEBUG 10-15 15:27:37 lpllm.py:933] start load next layer cur_layer_idx: 19
DEBUG 10-15 15:27:37 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed650>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x003\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x96\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:37 client.py:72] load_into_gpu: Mixtral-8x7B, 98725d97-c23f-4d76-a65e-00ded252a7f0
INFO 10-15 15:27:37 client.py:113] Model loaded: Mixtral-8x7B, 98725d97-c23f-4d76-a65e-00ded252a7f0
DEBUG 10-15 15:27:37 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:37 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:37 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:37 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:37 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:37 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:37 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:37 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:37 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:37 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:37 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:37 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:37 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:37 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:37 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:37 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:37 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:37 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:37 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:37 lpllm.py:2265] GPU2CPU move cost 0.000603 seconds
DEBUG 10-15 15:27:37 lpmodule.py:340] layer_idx 18 attn layer_idx: 18
DEBUG 10-15 15:27:37 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:37 lpmodule.py:364] decoder_attn_batch update batch_dim 360-420
DEBUG 10-15 15:27:37 StaticCacheLen.py:84] static cache update layer_idx: 18, start_batch: 360, end_batch: 420
DEBUG 10-15 15:27:37 lpmodule.py:368] update for kv cache 360-420 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:37 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:37 lpmodule.py:374] update past key value cost 0.023914 seconds
DEBUG 10-15 15:27:37 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:37 lpmodule.py:227] layer idx 17
DEBUG 10-15 15:27:37 lpmodule.py:399] repeat qkv cost 0.031540 seconds
DEBUG 10-15 15:27:37 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:37 lpmodule.py:433] dot attn cost 0.033192 seconds
DEBUG 10-15 15:27:37 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:37 lpmodule.py:444] time cost move to cuda:1 0.02342963218688965 s
DEBUG 10-15 15:27:37 lpllm.py:2283] CPU attn cost 0.137215 seconds if batch True
DEBUG 10-15 15:27:37 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:27:37 lpllm.py:2294] CPU compute cost 0.138114 seconds
DEBUG 10-15 15:27:37 lpllm.py:2312] free cost 0.000074 seconds
DEBUG 10-15 15:27:37 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:37 lpllm.py:1774] update state cost 3.62396240234375e-05 s
DEBUG 10-15 15:27:37 lpllm.py:1743] restore layer func cost 0.0008294582366943359 s
DEBUG 10-15 15:27:37 lpllm.py:511] restore layer cost 0.001081705093383789 s
DEBUG 10-15 15:27:37 lpllm.py:512] reset_next_layer_need: layer_attn_idx=18, layer_mlp_idx=18, j_loc=36
DEBUG 10-15 15:27:37 lpllm.py:1037] reset layer cost 0.001155853271484375 s
DEBUG 10-15 15:27:37 lpllm.py:1038] have reset next layer layer_attn 18 layer_mlp 18 
DEBUG 10-15 15:27:37 lpllm.py:924] 
DEBUG 10-15 15:27:37 lpllm.py:924] decoder loop j: 37 cur_layer_idx: 18 layer_attn_idx: 18 layer_mlp_idx: 18
DEBUG 10-15 15:27:37 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:37 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:37 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:37 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:37 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:37 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:37 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:37 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:37 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:37 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:37 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:37 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:37 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:37 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:37 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:37 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:37 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:37 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:37 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:37 lpllm.py:2265] GPU2CPU move cost 0.000579 seconds
DEBUG 10-15 15:27:37 lpmodule.py:340] layer_idx 18 attn layer_idx: 18
DEBUG 10-15 15:27:37 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:37 lpmodule.py:364] decoder_attn_batch update batch_dim 420-480
DEBUG 10-15 15:27:37 StaticCacheLen.py:84] static cache update layer_idx: 18, start_batch: 420, end_batch: 480
DEBUG 10-15 15:27:37 lpmodule.py:368] update for kv cache 420-480 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:37 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:37 lpmodule.py:374] update past key value cost 0.024125 seconds
DEBUG 10-15 15:27:37 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:37 lpmodule.py:227] layer idx 18
DEBUG 10-15 15:27:38 lpmodule.py:399] repeat qkv cost 0.032422 seconds
DEBUG 10-15 15:27:38 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:38 lpmodule.py:433] dot attn cost 0.042708 seconds
DEBUG 10-15 15:27:38 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:38 lpmodule.py:444] time cost move to cuda:1 0.023502111434936523 s
DEBUG 10-15 15:27:38 lpllm.py:2283] CPU attn cost 0.147509 seconds if batch True
DEBUG 10-15 15:27:38 lpllm.py:2292] deal attn result cost 0.000003 seconds
DEBUG 10-15 15:27:38 lpllm.py:2294] CPU compute cost 0.148375 seconds
DEBUG 10-15 15:27:38 lpllm.py:2312] free cost 0.000073 seconds
DEBUG 10-15 15:27:38 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:38 lpllm.py:1774] update state cost 3.814697265625e-05 s
DEBUG 10-15 15:27:38 lpllm.py:1743] restore layer func cost 0.0003879070281982422 s
DEBUG 10-15 15:27:38 lpllm.py:511] restore layer cost 0.0006310939788818359 s
DEBUG 10-15 15:27:38 lpllm.py:512] reset_next_layer_need: layer_attn_idx=19, layer_mlp_idx=18, j_loc=37
DEBUG 10-15 15:27:38 lpllm.py:1037] reset layer cost 0.0007123947143554688 s
DEBUG 10-15 15:27:38 lpllm.py:1038] have reset next layer layer_attn 19 layer_mlp 18 
DEBUG 10-15 15:27:38 lpllm.py:1044] j: 37 waiting the layer with layer_idx 19 before wait time 0.4575507640838623 s
INFO 10-15 15:27:38 client.py:117] confirm_model_loaded: Mixtral-8x7B, 98725d97-c23f-4d76-a65e-00ded252a7f0
INFO 10-15 15:27:38 client.py:125] Model loaded
DEBUG 10-15 15:27:38 lpllm.py:1048] j: load cost 0.45926713943481445 s waiting cost 0.0017011165618896484 s
DEBUG 10-15 15:27:38 lpllm.py:924] 
DEBUG 10-15 15:27:38 lpllm.py:924] decoder loop j: 38 cur_layer_idx: 19 layer_attn_idx: 19 layer_mlp_idx: 18
DEBUG 10-15 15:27:38 lpllm.py:933] start load next layer cur_layer_idx: 20
DEBUG 10-15 15:27:38 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed6e0>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x004\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x98\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:38 client.py:72] load_into_gpu: Mixtral-8x7B, 740489af-36ac-434a-b73b-5b6c99647f9c
INFO 10-15 15:27:38 client.py:113] Model loaded: Mixtral-8x7B, 740489af-36ac-434a-b73b-5b6c99647f9c
DEBUG 10-15 15:27:38 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:38 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:38 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:38 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:38 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:38 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:38 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:38 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:38 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:38 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:38 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:38 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:38 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:38 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:38 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:38 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:38 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:38 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:38 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:38 lpllm.py:2265] GPU2CPU move cost 0.000616 seconds
DEBUG 10-15 15:27:38 lpmodule.py:340] layer_idx 19 attn layer_idx: 19
DEBUG 10-15 15:27:38 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:38 lpmodule.py:364] decoder_attn_batch update batch_dim 360-420
DEBUG 10-15 15:27:38 StaticCacheLen.py:84] static cache update layer_idx: 19, start_batch: 360, end_batch: 420
DEBUG 10-15 15:27:38 lpmodule.py:368] update for kv cache 360-420 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:38 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:38 lpmodule.py:374] update past key value cost 0.023841 seconds
DEBUG 10-15 15:27:38 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:38 lpmodule.py:227] layer idx 18
DEBUG 10-15 15:27:38 lpmodule.py:399] repeat qkv cost 0.030628 seconds
DEBUG 10-15 15:27:38 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:38 lpmodule.py:433] dot attn cost 0.034782 seconds
DEBUG 10-15 15:27:38 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:38 lpmodule.py:444] time cost move to cuda:1 0.023611783981323242 s
DEBUG 10-15 15:27:38 lpllm.py:2283] CPU attn cost 0.137566 seconds if batch True
DEBUG 10-15 15:27:38 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:38 lpllm.py:2294] CPU compute cost 0.138470 seconds
DEBUG 10-15 15:27:38 lpllm.py:2312] free cost 0.000075 seconds
DEBUG 10-15 15:27:38 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:38 lpllm.py:1774] update state cost 3.504753112792969e-05 s
DEBUG 10-15 15:27:38 lpllm.py:1743] restore layer func cost 0.0008215904235839844 s
DEBUG 10-15 15:27:38 lpllm.py:511] restore layer cost 0.0010790824890136719 s
DEBUG 10-15 15:27:38 lpllm.py:512] reset_next_layer_need: layer_attn_idx=19, layer_mlp_idx=19, j_loc=38
DEBUG 10-15 15:27:38 lpllm.py:1037] reset layer cost 0.0011589527130126953 s
DEBUG 10-15 15:27:38 lpllm.py:1038] have reset next layer layer_attn 19 layer_mlp 19 
DEBUG 10-15 15:27:38 lpllm.py:924] 
DEBUG 10-15 15:27:38 lpllm.py:924] decoder loop j: 39 cur_layer_idx: 19 layer_attn_idx: 19 layer_mlp_idx: 19
DEBUG 10-15 15:27:38 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:38 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:38 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:38 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:38 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:38 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:38 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:38 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:38 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:38 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:38 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:38 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:38 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:38 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:38 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:38 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:38 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:38 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:38 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:38 lpllm.py:2265] GPU2CPU move cost 0.000576 seconds
DEBUG 10-15 15:27:38 lpmodule.py:340] layer_idx 19 attn layer_idx: 19
DEBUG 10-15 15:27:38 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:38 lpmodule.py:364] decoder_attn_batch update batch_dim 420-480
DEBUG 10-15 15:27:38 StaticCacheLen.py:84] static cache update layer_idx: 19, start_batch: 420, end_batch: 480
DEBUG 10-15 15:27:38 lpmodule.py:368] update for kv cache 420-480 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:38 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:38 lpmodule.py:374] update past key value cost 0.025642 seconds
DEBUG 10-15 15:27:38 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:38 lpmodule.py:227] layer idx 19
DEBUG 10-15 15:27:38 lpmodule.py:399] repeat qkv cost 0.035017 seconds
DEBUG 10-15 15:27:38 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:38 lpmodule.py:433] dot attn cost 0.032642 seconds
DEBUG 10-15 15:27:38 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:38 lpmodule.py:444] time cost move to cuda:1 0.02321624755859375 s
DEBUG 10-15 15:27:38 lpllm.py:2283] CPU attn cost 0.141477 seconds if batch True
DEBUG 10-15 15:27:38 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:27:38 lpllm.py:2294] CPU compute cost 0.142329 seconds
DEBUG 10-15 15:27:38 lpllm.py:2312] free cost 0.000074 seconds
DEBUG 10-15 15:27:38 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:38 lpllm.py:1774] update state cost 4.0531158447265625e-05 s
DEBUG 10-15 15:27:38 lpllm.py:1743] restore layer func cost 0.0004069805145263672 s
DEBUG 10-15 15:27:38 lpllm.py:511] restore layer cost 0.0006580352783203125 s
DEBUG 10-15 15:27:38 lpllm.py:512] reset_next_layer_need: layer_attn_idx=20, layer_mlp_idx=19, j_loc=39
DEBUG 10-15 15:27:38 lpllm.py:1037] reset layer cost 0.0007348060607910156 s
DEBUG 10-15 15:27:38 lpllm.py:1038] have reset next layer layer_attn 20 layer_mlp 19 
DEBUG 10-15 15:27:38 lpllm.py:1044] j: 39 waiting the layer with layer_idx 20 before wait time 0.4466397762298584 s
INFO 10-15 15:27:38 client.py:117] confirm_model_loaded: Mixtral-8x7B, 740489af-36ac-434a-b73b-5b6c99647f9c
INFO 10-15 15:27:38 client.py:125] Model loaded
DEBUG 10-15 15:27:38 lpllm.py:1048] j: load cost 0.4483611583709717 s waiting cost 0.0017056465148925781 s
DEBUG 10-15 15:27:38 lpllm.py:924] 
DEBUG 10-15 15:27:38 lpllm.py:924] decoder loop j: 40 cur_layer_idx: 20 layer_attn_idx: 20 layer_mlp_idx: 19
DEBUG 10-15 15:27:38 lpllm.py:933] start load next layer cur_layer_idx: 21
DEBUG 10-15 15:27:38 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:38 client.py:72] load_into_gpu: Mixtral-8x7B, da996128-5ce7-45ba-b3dc-5288f0fd9030
INFO 10-15 15:27:38 client.py:113] Model loaded: Mixtral-8x7B, da996128-5ce7-45ba-b3dc-5288f0fd9030
DEBUG 10-15 15:27:38 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:38 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:38 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:38 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:38 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:38 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:38 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:38 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:38 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:38 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:38 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:38 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:38 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:38 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:38 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:38 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:38 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:38 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:38 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:38 lpllm.py:2265] GPU2CPU move cost 0.000689 seconds
DEBUG 10-15 15:27:38 lpmodule.py:340] layer_idx 20 attn layer_idx: 20
DEBUG 10-15 15:27:38 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:38 lpmodule.py:364] decoder_attn_batch update batch_dim 360-420
DEBUG 10-15 15:27:38 StaticCacheLen.py:84] static cache update layer_idx: 20, start_batch: 360, end_batch: 420
DEBUG 10-15 15:27:38 lpmodule.py:368] update for kv cache 360-420 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:38 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:38 lpmodule.py:374] update past key value cost 0.023527 seconds
DEBUG 10-15 15:27:38 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:38 lpmodule.py:227] layer idx 19
DEBUG 10-15 15:27:38 lpmodule.py:399] repeat qkv cost 0.030591 seconds
DEBUG 10-15 15:27:38 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:38 lpmodule.py:433] dot attn cost 0.032540 seconds
DEBUG 10-15 15:27:38 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:38 lpmodule.py:444] time cost move to cuda:1 0.02378225326538086 s
DEBUG 10-15 15:27:38 lpllm.py:2283] CPU attn cost 0.135321 seconds if batch True
DEBUG 10-15 15:27:38 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:38 lpllm.py:2294] CPU compute cost 0.136341 seconds
DEBUG 10-15 15:27:38 lpllm.py:2312] free cost 0.000076 seconds
DEBUG 10-15 15:27:38 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:38 lpllm.py:1774] update state cost 3.719329833984375e-05 s
DEBUG 10-15 15:27:38 lpllm.py:1743] restore layer func cost 0.0008113384246826172 s
DEBUG 10-15 15:27:38 lpllm.py:511] restore layer cost 0.0010690689086914062 s
DEBUG 10-15 15:27:38 lpllm.py:512] reset_next_layer_need: layer_attn_idx=20, layer_mlp_idx=20, j_loc=40
DEBUG 10-15 15:27:38 lpllm.py:1037] reset layer cost 0.001148223876953125 s
DEBUG 10-15 15:27:38 lpllm.py:1038] have reset next layer layer_attn 20 layer_mlp 20 
DEBUG 10-15 15:27:38 lpllm.py:924] 
DEBUG 10-15 15:27:38 lpllm.py:924] decoder loop j: 41 cur_layer_idx: 20 layer_attn_idx: 20 layer_mlp_idx: 20
DEBUG 10-15 15:27:38 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:38 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:38 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:38 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:38 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:38 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:38 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:38 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:38 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:38 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:38 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:38 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:38 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:38 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:38 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:38 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:38 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:38 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:38 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:38 lpllm.py:2265] GPU2CPU move cost 0.000572 seconds
DEBUG 10-15 15:27:38 lpmodule.py:340] layer_idx 20 attn layer_idx: 20
DEBUG 10-15 15:27:38 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:38 lpmodule.py:364] decoder_attn_batch update batch_dim 420-480
DEBUG 10-15 15:27:38 StaticCacheLen.py:84] static cache update layer_idx: 20, start_batch: 420, end_batch: 480
DEBUG 10-15 15:27:38 lpmodule.py:368] update for kv cache 420-480 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:38 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:38 lpmodule.py:374] update past key value cost 0.025427 seconds
DEBUG 10-15 15:27:38 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:38 lpmodule.py:227] layer idx 20
DEBUG 10-15 15:27:38 lpmodule.py:399] repeat qkv cost 0.031115 seconds
DEBUG 10-15 15:27:38 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:38 lpmodule.py:433] dot attn cost 0.033275 seconds
DEBUG 10-15 15:27:38 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:38 lpmodule.py:444] time cost move to cuda:1 0.025892257690429688 s
DEBUG 10-15 15:27:38 lpllm.py:2283] CPU attn cost 0.140987 seconds if batch True
DEBUG 10-15 15:27:38 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:38 lpllm.py:2294] CPU compute cost 0.141843 seconds
DEBUG 10-15 15:27:38 lpllm.py:2312] free cost 0.000072 seconds
DEBUG 10-15 15:27:39 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:39 lpllm.py:1774] update state cost 4.601478576660156e-05 s
DEBUG 10-15 15:27:39 lpllm.py:1743] restore layer func cost 0.0003840923309326172 s
DEBUG 10-15 15:27:39 lpllm.py:511] restore layer cost 0.0006382465362548828 s
DEBUG 10-15 15:27:39 lpllm.py:512] reset_next_layer_need: layer_attn_idx=21, layer_mlp_idx=20, j_loc=41
DEBUG 10-15 15:27:39 lpllm.py:1037] reset layer cost 0.0007107257843017578 s
DEBUG 10-15 15:27:39 lpllm.py:1038] have reset next layer layer_attn 21 layer_mlp 20 
DEBUG 10-15 15:27:39 lpllm.py:1044] j: 41 waiting the layer with layer_idx 21 before wait time 0.44627976417541504 s
INFO 10-15 15:27:39 client.py:117] confirm_model_loaded: Mixtral-8x7B, da996128-5ce7-45ba-b3dc-5288f0fd9030
INFO 10-15 15:27:39 client.py:125] Model loaded
DEBUG 10-15 15:27:39 lpllm.py:1048] j: load cost 0.4481163024902344 s waiting cost 0.0018215179443359375 s
DEBUG 10-15 15:27:39 lpllm.py:924] 
DEBUG 10-15 15:27:39 lpllm.py:924] decoder loop j: 42 cur_layer_idx: 21 layer_attn_idx: 21 layer_mlp_idx: 20
DEBUG 10-15 15:27:39 lpllm.py:933] start load next layer cur_layer_idx: 22
DEBUG 10-15 15:27:39 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed650>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x003\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x96\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:39 client.py:72] load_into_gpu: Mixtral-8x7B, 9745b83d-9d0d-4afa-8261-d5067acd294f
INFO 10-15 15:27:39 client.py:113] Model loaded: Mixtral-8x7B, 9745b83d-9d0d-4afa-8261-d5067acd294f
DEBUG 10-15 15:27:39 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:39 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:39 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:39 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:39 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:39 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:39 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:39 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:39 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:39 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:39 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:39 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:39 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:39 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:39 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:39 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:39 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:39 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:39 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:39 lpllm.py:2265] GPU2CPU move cost 0.000613 seconds
DEBUG 10-15 15:27:39 lpmodule.py:340] layer_idx 21 attn layer_idx: 21
DEBUG 10-15 15:27:39 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:39 lpmodule.py:364] decoder_attn_batch update batch_dim 360-420
DEBUG 10-15 15:27:39 StaticCacheLen.py:84] static cache update layer_idx: 21, start_batch: 360, end_batch: 420
DEBUG 10-15 15:27:39 lpmodule.py:368] update for kv cache 360-420 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:39 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:39 lpmodule.py:374] update past key value cost 0.023727 seconds
DEBUG 10-15 15:27:39 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:39 lpmodule.py:227] layer idx 20
DEBUG 10-15 15:27:39 lpmodule.py:399] repeat qkv cost 0.031285 seconds
DEBUG 10-15 15:27:39 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:39 lpmodule.py:433] dot attn cost 0.033338 seconds
DEBUG 10-15 15:27:39 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:39 lpmodule.py:444] time cost move to cuda:1 0.02376389503479004 s
DEBUG 10-15 15:27:39 lpllm.py:2283] CPU attn cost 0.136981 seconds if batch True
DEBUG 10-15 15:27:39 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:39 lpllm.py:2294] CPU compute cost 0.137870 seconds
DEBUG 10-15 15:27:39 lpllm.py:2312] free cost 0.000077 seconds
DEBUG 10-15 15:27:39 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:39 lpllm.py:1774] update state cost 3.719329833984375e-05 s
DEBUG 10-15 15:27:39 lpllm.py:1743] restore layer func cost 0.0008234977722167969 s
DEBUG 10-15 15:27:39 lpllm.py:511] restore layer cost 0.0010709762573242188 s
DEBUG 10-15 15:27:39 lpllm.py:512] reset_next_layer_need: layer_attn_idx=21, layer_mlp_idx=21, j_loc=42
DEBUG 10-15 15:27:39 lpllm.py:1037] reset layer cost 0.0011539459228515625 s
DEBUG 10-15 15:27:39 lpllm.py:1038] have reset next layer layer_attn 21 layer_mlp 21 
DEBUG 10-15 15:27:39 lpllm.py:924] 
DEBUG 10-15 15:27:39 lpllm.py:924] decoder loop j: 43 cur_layer_idx: 21 layer_attn_idx: 21 layer_mlp_idx: 21
DEBUG 10-15 15:27:39 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:39 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:39 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:39 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:39 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:39 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:39 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:39 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:39 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:39 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:39 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:39 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:39 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:39 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:39 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:39 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:39 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:39 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:39 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:39 lpllm.py:2265] GPU2CPU move cost 0.000617 seconds
DEBUG 10-15 15:27:39 lpmodule.py:340] layer_idx 21 attn layer_idx: 21
DEBUG 10-15 15:27:39 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:39 lpmodule.py:364] decoder_attn_batch update batch_dim 420-480
DEBUG 10-15 15:27:39 StaticCacheLen.py:84] static cache update layer_idx: 21, start_batch: 420, end_batch: 480
DEBUG 10-15 15:27:39 lpmodule.py:368] update for kv cache 420-480 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:39 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:39 lpmodule.py:374] update past key value cost 0.024870 seconds
DEBUG 10-15 15:27:39 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:39 lpmodule.py:227] layer idx 21
DEBUG 10-15 15:27:39 lpmodule.py:399] repeat qkv cost 0.030987 seconds
DEBUG 10-15 15:27:39 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:39 lpmodule.py:433] dot attn cost 0.032527 seconds
DEBUG 10-15 15:27:39 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:39 lpmodule.py:444] time cost move to cuda:1 0.02378249168395996 s
DEBUG 10-15 15:27:39 lpllm.py:2283] CPU attn cost 0.137273 seconds if batch True
DEBUG 10-15 15:27:39 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:39 lpllm.py:2294] CPU compute cost 0.138179 seconds
DEBUG 10-15 15:27:39 lpllm.py:2312] free cost 0.000075 seconds
DEBUG 10-15 15:27:39 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:39 lpllm.py:1774] update state cost 2.3126602172851562e-05 s
DEBUG 10-15 15:27:39 lpllm.py:1743] restore layer func cost 0.000392913818359375 s
DEBUG 10-15 15:27:39 lpllm.py:511] restore layer cost 0.0006189346313476562 s
DEBUG 10-15 15:27:39 lpllm.py:512] reset_next_layer_need: layer_attn_idx=22, layer_mlp_idx=21, j_loc=43
DEBUG 10-15 15:27:39 lpllm.py:1037] reset layer cost 0.0006909370422363281 s
DEBUG 10-15 15:27:39 lpllm.py:1038] have reset next layer layer_attn 22 layer_mlp 21 
DEBUG 10-15 15:27:39 lpllm.py:1044] j: 43 waiting the layer with layer_idx 22 before wait time 0.4499354362487793 s
INFO 10-15 15:27:39 client.py:117] confirm_model_loaded: Mixtral-8x7B, 9745b83d-9d0d-4afa-8261-d5067acd294f
INFO 10-15 15:27:39 client.py:125] Model loaded
DEBUG 10-15 15:27:39 lpllm.py:1048] j: load cost 0.4517862796783447 s waiting cost 0.001836538314819336 s
DEBUG 10-15 15:27:39 lpllm.py:924] 
DEBUG 10-15 15:27:39 lpllm.py:924] decoder loop j: 44 cur_layer_idx: 22 layer_attn_idx: 22 layer_mlp_idx: 21
DEBUG 10-15 15:27:39 lpllm.py:933] start load next layer cur_layer_idx: 23
DEBUG 10-15 15:27:39 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed6e0>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x004\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x98\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:39 client.py:72] load_into_gpu: Mixtral-8x7B, f437adc9-c1b1-4b8c-9ad9-a3a448cd37a4
INFO 10-15 15:27:39 client.py:113] Model loaded: Mixtral-8x7B, f437adc9-c1b1-4b8c-9ad9-a3a448cd37a4
DEBUG 10-15 15:27:39 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:39 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:39 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:39 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:39 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:39 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:39 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:39 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:39 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:39 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:39 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:39 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:39 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:39 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:39 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:39 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:39 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:39 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:39 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:39 lpllm.py:2265] GPU2CPU move cost 0.000570 seconds
DEBUG 10-15 15:27:39 lpmodule.py:340] layer_idx 22 attn layer_idx: 22
DEBUG 10-15 15:27:39 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:39 lpmodule.py:364] decoder_attn_batch update batch_dim 360-420
DEBUG 10-15 15:27:39 StaticCacheLen.py:84] static cache update layer_idx: 22, start_batch: 360, end_batch: 420
DEBUG 10-15 15:27:39 lpmodule.py:368] update for kv cache 360-420 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:39 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:39 lpmodule.py:374] update past key value cost 0.022892 seconds
DEBUG 10-15 15:27:39 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:39 lpmodule.py:227] layer idx 21
DEBUG 10-15 15:27:39 lpmodule.py:399] repeat qkv cost 0.031703 seconds
DEBUG 10-15 15:27:39 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:39 lpmodule.py:433] dot attn cost 0.033252 seconds
DEBUG 10-15 15:27:39 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:39 lpmodule.py:444] time cost move to cuda:1 0.023564577102661133 s
DEBUG 10-15 15:27:39 lpllm.py:2283] CPU attn cost 0.136561 seconds if batch True
DEBUG 10-15 15:27:39 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:27:39 lpllm.py:2294] CPU compute cost 0.137413 seconds
DEBUG 10-15 15:27:39 lpllm.py:2312] free cost 0.000077 seconds
DEBUG 10-15 15:27:39 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:39 lpllm.py:1774] update state cost 2.1457672119140625e-05 s
DEBUG 10-15 15:27:39 lpllm.py:1743] restore layer func cost 0.0007932186126708984 s
DEBUG 10-15 15:27:39 lpllm.py:511] restore layer cost 0.0010249614715576172 s
DEBUG 10-15 15:27:39 lpllm.py:512] reset_next_layer_need: layer_attn_idx=22, layer_mlp_idx=22, j_loc=44
DEBUG 10-15 15:27:39 lpllm.py:1037] reset layer cost 0.0010995864868164062 s
DEBUG 10-15 15:27:39 lpllm.py:1038] have reset next layer layer_attn 22 layer_mlp 22 
DEBUG 10-15 15:27:39 lpllm.py:924] 
DEBUG 10-15 15:27:39 lpllm.py:924] decoder loop j: 45 cur_layer_idx: 22 layer_attn_idx: 22 layer_mlp_idx: 22
DEBUG 10-15 15:27:39 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:39 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:39 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:39 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:39 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:39 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:39 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:39 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:39 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:39 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:39 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:39 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:39 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:39 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:39 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:39 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:39 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:39 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:39 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:39 lpllm.py:2265] GPU2CPU move cost 0.000631 seconds
DEBUG 10-15 15:27:39 lpmodule.py:340] layer_idx 22 attn layer_idx: 22
DEBUG 10-15 15:27:39 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:39 lpmodule.py:364] decoder_attn_batch update batch_dim 420-480
DEBUG 10-15 15:27:39 StaticCacheLen.py:84] static cache update layer_idx: 22, start_batch: 420, end_batch: 480
DEBUG 10-15 15:27:39 lpmodule.py:368] update for kv cache 420-480 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:39 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:39 lpmodule.py:374] update past key value cost 0.024764 seconds
DEBUG 10-15 15:27:39 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:39 lpmodule.py:227] layer idx 22
DEBUG 10-15 15:27:39 lpmodule.py:399] repeat qkv cost 0.031376 seconds
DEBUG 10-15 15:27:39 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:39 lpmodule.py:433] dot attn cost 0.043181 seconds
DEBUG 10-15 15:27:39 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:39 lpmodule.py:444] time cost move to cuda:1 0.02345442771911621 s
DEBUG 10-15 15:27:39 lpllm.py:2283] CPU attn cost 0.147851 seconds if batch True
DEBUG 10-15 15:27:39 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:39 lpllm.py:2294] CPU compute cost 0.148766 seconds
DEBUG 10-15 15:27:39 lpllm.py:2312] free cost 0.000072 seconds
DEBUG 10-15 15:27:39 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:39 lpllm.py:1774] update state cost 3.695487976074219e-05 s
DEBUG 10-15 15:27:39 lpllm.py:1743] restore layer func cost 0.0003955364227294922 s
DEBUG 10-15 15:27:39 lpllm.py:511] restore layer cost 0.0006339550018310547 s
DEBUG 10-15 15:27:39 lpllm.py:512] reset_next_layer_need: layer_attn_idx=23, layer_mlp_idx=22, j_loc=45
DEBUG 10-15 15:27:39 lpllm.py:1037] reset layer cost 0.0007088184356689453 s
DEBUG 10-15 15:27:39 lpllm.py:1038] have reset next layer layer_attn 23 layer_mlp 22 
DEBUG 10-15 15:27:39 lpllm.py:1044] j: 45 waiting the layer with layer_idx 23 before wait time 0.45580291748046875 s
INFO 10-15 15:27:39 client.py:117] confirm_model_loaded: Mixtral-8x7B, f437adc9-c1b1-4b8c-9ad9-a3a448cd37a4
INFO 10-15 15:27:39 client.py:125] Model loaded
DEBUG 10-15 15:27:39 lpllm.py:1048] j: load cost 0.4576232433319092 s waiting cost 0.0018045902252197266 s
DEBUG 10-15 15:27:39 lpllm.py:924] 
DEBUG 10-15 15:27:39 lpllm.py:924] decoder loop j: 46 cur_layer_idx: 23 layer_attn_idx: 23 layer_mlp_idx: 22
DEBUG 10-15 15:27:39 lpllm.py:933] start load next layer cur_layer_idx: 24
DEBUG 10-15 15:27:39 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:39 client.py:72] load_into_gpu: Mixtral-8x7B, 252f89bc-4faa-46da-8700-99888159a62e
INFO 10-15 15:27:39 client.py:113] Model loaded: Mixtral-8x7B, 252f89bc-4faa-46da-8700-99888159a62e
DEBUG 10-15 15:27:39 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:39 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:39 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:39 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:39 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:39 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:39 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:39 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:39 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:39 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:39 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:39 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:39 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:39 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:39 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:39 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:39 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:39 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:39 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:39 lpllm.py:2265] GPU2CPU move cost 0.000569 seconds
DEBUG 10-15 15:27:39 lpmodule.py:340] layer_idx 23 attn layer_idx: 23
DEBUG 10-15 15:27:39 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:39 lpmodule.py:364] decoder_attn_batch update batch_dim 360-420
DEBUG 10-15 15:27:40 StaticCacheLen.py:84] static cache update layer_idx: 23, start_batch: 360, end_batch: 420
DEBUG 10-15 15:27:40 lpmodule.py:368] update for kv cache 360-420 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:40 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:40 lpmodule.py:374] update past key value cost 0.024148 seconds
DEBUG 10-15 15:27:40 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:40 lpmodule.py:227] layer idx 22
DEBUG 10-15 15:27:40 lpmodule.py:399] repeat qkv cost 0.031404 seconds
DEBUG 10-15 15:27:40 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:40 lpmodule.py:433] dot attn cost 0.032873 seconds
DEBUG 10-15 15:27:40 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:40 lpmodule.py:444] time cost move to cuda:1 0.0253908634185791 s
DEBUG 10-15 15:27:40 lpllm.py:2283] CPU attn cost 0.138968 seconds if batch True
DEBUG 10-15 15:27:40 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:40 lpllm.py:2294] CPU compute cost 0.139844 seconds
DEBUG 10-15 15:27:40 lpllm.py:2312] free cost 0.000074 seconds
DEBUG 10-15 15:27:40 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:40 lpllm.py:1774] update state cost 3.695487976074219e-05 s
DEBUG 10-15 15:27:40 lpllm.py:1743] restore layer func cost 0.0008287429809570312 s
DEBUG 10-15 15:27:40 lpllm.py:511] restore layer cost 0.0010783672332763672 s
DEBUG 10-15 15:27:40 lpllm.py:512] reset_next_layer_need: layer_attn_idx=23, layer_mlp_idx=23, j_loc=46
DEBUG 10-15 15:27:40 lpllm.py:1037] reset layer cost 0.0011522769927978516 s
DEBUG 10-15 15:27:40 lpllm.py:1038] have reset next layer layer_attn 23 layer_mlp 23 
DEBUG 10-15 15:27:40 lpllm.py:924] 
DEBUG 10-15 15:27:40 lpllm.py:924] decoder loop j: 47 cur_layer_idx: 23 layer_attn_idx: 23 layer_mlp_idx: 23
DEBUG 10-15 15:27:40 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:40 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:40 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:40 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:40 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:40 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:40 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:40 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:40 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:40 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:40 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:40 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:40 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:40 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:40 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:40 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:40 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:40 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:40 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:40 lpllm.py:2265] GPU2CPU move cost 0.000573 seconds
DEBUG 10-15 15:27:40 lpmodule.py:340] layer_idx 23 attn layer_idx: 23
DEBUG 10-15 15:27:40 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:40 lpmodule.py:364] decoder_attn_batch update batch_dim 420-480
DEBUG 10-15 15:27:40 StaticCacheLen.py:84] static cache update layer_idx: 23, start_batch: 420, end_batch: 480
DEBUG 10-15 15:27:40 lpmodule.py:368] update for kv cache 420-480 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:40 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:40 lpmodule.py:374] update past key value cost 0.025219 seconds
DEBUG 10-15 15:27:40 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:40 lpmodule.py:227] layer idx 23
DEBUG 10-15 15:27:40 lpmodule.py:399] repeat qkv cost 0.030455 seconds
DEBUG 10-15 15:27:40 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:40 lpmodule.py:433] dot attn cost 0.033387 seconds
DEBUG 10-15 15:27:40 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:40 lpmodule.py:444] time cost move to cuda:1 0.023743391036987305 s
DEBUG 10-15 15:27:40 lpllm.py:2283] CPU attn cost 0.137861 seconds if batch True
DEBUG 10-15 15:27:40 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:40 lpllm.py:2294] CPU compute cost 0.138721 seconds
DEBUG 10-15 15:27:40 lpllm.py:2312] free cost 0.000086 seconds
DEBUG 10-15 15:27:40 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:40 lpllm.py:1774] update state cost 3.7670135498046875e-05 s
DEBUG 10-15 15:27:40 lpllm.py:1743] restore layer func cost 0.0003962516784667969 s
DEBUG 10-15 15:27:40 lpllm.py:511] restore layer cost 0.0006430149078369141 s
DEBUG 10-15 15:27:40 lpllm.py:512] reset_next_layer_need: layer_attn_idx=24, layer_mlp_idx=23, j_loc=47
DEBUG 10-15 15:27:40 lpllm.py:1037] reset layer cost 0.0007150173187255859 s
DEBUG 10-15 15:27:40 lpllm.py:1038] have reset next layer layer_attn 24 layer_mlp 23 
DEBUG 10-15 15:27:40 lpllm.py:1044] j: 47 waiting the layer with layer_idx 24 before wait time 0.44873976707458496 s
INFO 10-15 15:27:40 client.py:117] confirm_model_loaded: Mixtral-8x7B, 252f89bc-4faa-46da-8700-99888159a62e
INFO 10-15 15:27:40 client.py:125] Model loaded
DEBUG 10-15 15:27:40 lpllm.py:1048] j: load cost 0.4504854679107666 s waiting cost 0.0017290115356445312 s
DEBUG 10-15 15:27:40 lpllm.py:924] 
DEBUG 10-15 15:27:40 lpllm.py:924] decoder loop j: 48 cur_layer_idx: 24 layer_attn_idx: 24 layer_mlp_idx: 23
DEBUG 10-15 15:27:40 lpllm.py:933] start load next layer cur_layer_idx: 25
DEBUG 10-15 15:27:40 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed650>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x003\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x96\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:40 client.py:72] load_into_gpu: Mixtral-8x7B, b57d3c66-d969-4fa3-a420-4542b32ad3a3
INFO 10-15 15:27:40 client.py:113] Model loaded: Mixtral-8x7B, b57d3c66-d969-4fa3-a420-4542b32ad3a3
DEBUG 10-15 15:27:40 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:40 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:40 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:40 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:40 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:40 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:40 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:40 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:40 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:40 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:40 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:40 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:40 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:40 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:40 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:40 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:40 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:40 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:40 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:40 lpllm.py:2265] GPU2CPU move cost 0.000644 seconds
DEBUG 10-15 15:27:40 lpmodule.py:340] layer_idx 24 attn layer_idx: 24
DEBUG 10-15 15:27:40 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:40 lpmodule.py:364] decoder_attn_batch update batch_dim 360-420
DEBUG 10-15 15:27:40 StaticCacheLen.py:84] static cache update layer_idx: 24, start_batch: 360, end_batch: 420
DEBUG 10-15 15:27:40 lpmodule.py:368] update for kv cache 360-420 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:40 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:40 lpmodule.py:374] update past key value cost 0.024900 seconds
DEBUG 10-15 15:27:40 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:40 lpmodule.py:227] layer idx 23
DEBUG 10-15 15:27:40 lpmodule.py:399] repeat qkv cost 0.030269 seconds
DEBUG 10-15 15:27:40 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:40 lpmodule.py:433] dot attn cost 0.033356 seconds
DEBUG 10-15 15:27:40 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:40 lpmodule.py:444] time cost move to cuda:1 0.023493289947509766 s
DEBUG 10-15 15:27:40 lpllm.py:2283] CPU attn cost 0.137539 seconds if batch True
DEBUG 10-15 15:27:40 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:40 lpllm.py:2294] CPU compute cost 0.138492 seconds
DEBUG 10-15 15:27:40 lpllm.py:2312] free cost 0.000075 seconds
DEBUG 10-15 15:27:40 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:40 lpllm.py:1774] update state cost 3.6716461181640625e-05 s
DEBUG 10-15 15:27:40 lpllm.py:1743] restore layer func cost 0.0008153915405273438 s
DEBUG 10-15 15:27:40 lpllm.py:511] restore layer cost 0.0010743141174316406 s
DEBUG 10-15 15:27:40 lpllm.py:512] reset_next_layer_need: layer_attn_idx=24, layer_mlp_idx=24, j_loc=48
DEBUG 10-15 15:27:40 lpllm.py:1037] reset layer cost 0.0011508464813232422 s
DEBUG 10-15 15:27:40 lpllm.py:1038] have reset next layer layer_attn 24 layer_mlp 24 
DEBUG 10-15 15:27:40 lpllm.py:924] 
DEBUG 10-15 15:27:40 lpllm.py:924] decoder loop j: 49 cur_layer_idx: 24 layer_attn_idx: 24 layer_mlp_idx: 24
DEBUG 10-15 15:27:40 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:40 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:40 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:40 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:40 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:40 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:40 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:40 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:40 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:40 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:40 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:40 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:40 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:40 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:40 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:40 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:40 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:40 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:40 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:40 lpllm.py:2265] GPU2CPU move cost 0.000582 seconds
DEBUG 10-15 15:27:40 lpmodule.py:340] layer_idx 24 attn layer_idx: 24
DEBUG 10-15 15:27:40 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:40 lpmodule.py:364] decoder_attn_batch update batch_dim 420-480
DEBUG 10-15 15:27:40 StaticCacheLen.py:84] static cache update layer_idx: 24, start_batch: 420, end_batch: 480
DEBUG 10-15 15:27:40 lpmodule.py:368] update for kv cache 420-480 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:40 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:40 lpmodule.py:374] update past key value cost 0.026401 seconds
DEBUG 10-15 15:27:40 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:40 lpmodule.py:227] layer idx 24
DEBUG 10-15 15:27:40 lpmodule.py:399] repeat qkv cost 0.030308 seconds
DEBUG 10-15 15:27:40 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:40 lpmodule.py:433] dot attn cost 0.033053 seconds
DEBUG 10-15 15:27:40 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:40 lpmodule.py:444] time cost move to cuda:1 0.02348947525024414 s
DEBUG 10-15 15:27:40 lpllm.py:2283] CPU attn cost 0.138623 seconds if batch True
DEBUG 10-15 15:27:40 lpllm.py:2292] deal attn result cost 0.000003 seconds
DEBUG 10-15 15:27:40 lpllm.py:2294] CPU compute cost 0.139495 seconds
DEBUG 10-15 15:27:40 lpllm.py:2312] free cost 0.000076 seconds
DEBUG 10-15 15:27:40 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:40 lpllm.py:1774] update state cost 2.2649765014648438e-05 s
DEBUG 10-15 15:27:40 lpllm.py:1743] restore layer func cost 0.0004241466522216797 s
DEBUG 10-15 15:27:40 lpllm.py:511] restore layer cost 0.0006468296051025391 s
DEBUG 10-15 15:27:40 lpllm.py:512] reset_next_layer_need: layer_attn_idx=25, layer_mlp_idx=24, j_loc=49
DEBUG 10-15 15:27:40 lpllm.py:1037] reset layer cost 0.0007226467132568359 s
DEBUG 10-15 15:27:40 lpllm.py:1038] have reset next layer layer_attn 25 layer_mlp 24 
DEBUG 10-15 15:27:40 lpllm.py:1044] j: 49 waiting the layer with layer_idx 25 before wait time 0.449674129486084 s
INFO 10-15 15:27:40 client.py:117] confirm_model_loaded: Mixtral-8x7B, b57d3c66-d969-4fa3-a420-4542b32ad3a3
INFO 10-15 15:27:40 client.py:125] Model loaded
DEBUG 10-15 15:27:40 lpllm.py:1048] j: load cost 0.45132899284362793 s waiting cost 0.0016388893127441406 s
DEBUG 10-15 15:27:40 lpllm.py:924] 
DEBUG 10-15 15:27:40 lpllm.py:924] decoder loop j: 50 cur_layer_idx: 25 layer_attn_idx: 25 layer_mlp_idx: 24
DEBUG 10-15 15:27:40 lpllm.py:933] start load next layer cur_layer_idx: 26
DEBUG 10-15 15:27:40 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed6e0>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x004\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x98\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:40 client.py:72] load_into_gpu: Mixtral-8x7B, eea10381-ad77-425c-ab0f-4570f66d230c
INFO 10-15 15:27:40 client.py:113] Model loaded: Mixtral-8x7B, eea10381-ad77-425c-ab0f-4570f66d230c
DEBUG 10-15 15:27:40 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:40 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:40 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:40 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:40 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:40 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:40 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:40 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:40 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:40 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:40 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:40 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:40 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:40 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:40 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:40 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:40 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:40 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:40 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:40 lpllm.py:2265] GPU2CPU move cost 0.000492 seconds
DEBUG 10-15 15:27:40 lpmodule.py:340] layer_idx 25 attn layer_idx: 25
DEBUG 10-15 15:27:40 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:40 lpmodule.py:364] decoder_attn_batch update batch_dim 360-420
DEBUG 10-15 15:27:40 StaticCacheLen.py:84] static cache update layer_idx: 25, start_batch: 360, end_batch: 420
DEBUG 10-15 15:27:40 lpmodule.py:368] update for kv cache 360-420 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:40 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:40 lpmodule.py:374] update past key value cost 0.023133 seconds
DEBUG 10-15 15:27:40 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:41 lpmodule.py:227] layer idx 24
DEBUG 10-15 15:27:41 lpmodule.py:399] repeat qkv cost 0.031016 seconds
DEBUG 10-15 15:27:41 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:41 lpmodule.py:433] dot attn cost 0.033044 seconds
DEBUG 10-15 15:27:41 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:41 lpmodule.py:444] time cost move to cuda:1 0.02349400520324707 s
DEBUG 10-15 15:27:41 lpllm.py:2283] CPU attn cost 0.140275 seconds if batch True
DEBUG 10-15 15:27:41 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:41 lpllm.py:2294] CPU compute cost 0.141073 seconds
DEBUG 10-15 15:27:41 lpllm.py:2312] free cost 0.000076 seconds
DEBUG 10-15 15:27:41 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:41 lpllm.py:1774] update state cost 3.6716461181640625e-05 s
DEBUG 10-15 15:27:41 lpllm.py:1743] restore layer func cost 0.000865936279296875 s
DEBUG 10-15 15:27:41 lpllm.py:511] restore layer cost 0.0011188983917236328 s
DEBUG 10-15 15:27:41 lpllm.py:512] reset_next_layer_need: layer_attn_idx=25, layer_mlp_idx=25, j_loc=50
DEBUG 10-15 15:27:41 lpllm.py:1037] reset layer cost 0.0011975765228271484 s
DEBUG 10-15 15:27:41 lpllm.py:1038] have reset next layer layer_attn 25 layer_mlp 25 
DEBUG 10-15 15:27:41 lpllm.py:924] 
DEBUG 10-15 15:27:41 lpllm.py:924] decoder loop j: 51 cur_layer_idx: 25 layer_attn_idx: 25 layer_mlp_idx: 25
DEBUG 10-15 15:27:41 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:41 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:41 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:41 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:41 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:41 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:41 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:41 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:41 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:41 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:41 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:41 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:41 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:41 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:41 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:41 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:41 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:41 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:41 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:41 lpllm.py:2265] GPU2CPU move cost 0.000374 seconds
DEBUG 10-15 15:27:41 lpmodule.py:340] layer_idx 25 attn layer_idx: 25
DEBUG 10-15 15:27:41 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:41 lpmodule.py:364] decoder_attn_batch update batch_dim 420-480
DEBUG 10-15 15:27:41 StaticCacheLen.py:84] static cache update layer_idx: 25, start_batch: 420, end_batch: 480
DEBUG 10-15 15:27:41 lpmodule.py:368] update for kv cache 420-480 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:41 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:41 lpmodule.py:374] update past key value cost 0.024599 seconds
DEBUG 10-15 15:27:41 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:41 lpmodule.py:227] layer idx 25
DEBUG 10-15 15:27:41 lpmodule.py:399] repeat qkv cost 0.029953 seconds
DEBUG 10-15 15:27:41 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:41 lpmodule.py:433] dot attn cost 0.033461 seconds
DEBUG 10-15 15:27:41 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:41 lpmodule.py:444] time cost move to cuda:1 0.02411818504333496 s
DEBUG 10-15 15:27:41 lpllm.py:2283] CPU attn cost 0.137382 seconds if batch True
DEBUG 10-15 15:27:41 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:27:41 lpllm.py:2294] CPU compute cost 0.138015 seconds
DEBUG 10-15 15:27:41 lpllm.py:2312] free cost 0.000076 seconds
DEBUG 10-15 15:27:41 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:41 lpllm.py:1774] update state cost 2.2649765014648438e-05 s
DEBUG 10-15 15:27:41 lpllm.py:1743] restore layer func cost 0.00037598609924316406 s
DEBUG 10-15 15:27:41 lpllm.py:511] restore layer cost 0.0006000995635986328 s
DEBUG 10-15 15:27:41 lpllm.py:512] reset_next_layer_need: layer_attn_idx=26, layer_mlp_idx=25, j_loc=51
DEBUG 10-15 15:27:41 lpllm.py:1037] reset layer cost 0.0006699562072753906 s
DEBUG 10-15 15:27:41 lpllm.py:1038] have reset next layer layer_attn 26 layer_mlp 25 
DEBUG 10-15 15:27:41 lpllm.py:1044] j: 51 waiting the layer with layer_idx 26 before wait time 0.5309696197509766 s
INFO 10-15 15:27:41 client.py:117] confirm_model_loaded: Mixtral-8x7B, eea10381-ad77-425c-ab0f-4570f66d230c
INFO 10-15 15:27:41 client.py:125] Model loaded
DEBUG 10-15 15:27:41 lpllm.py:1048] j: load cost 0.5327551364898682 s waiting cost 0.0017714500427246094 s
DEBUG 10-15 15:27:41 lpllm.py:924] 
DEBUG 10-15 15:27:41 lpllm.py:924] decoder loop j: 52 cur_layer_idx: 26 layer_attn_idx: 26 layer_mlp_idx: 25
DEBUG 10-15 15:27:41 lpllm.py:933] start load next layer cur_layer_idx: 27
DEBUG 10-15 15:27:41 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:41 client.py:72] load_into_gpu: Mixtral-8x7B, e8fb4407-3381-43bd-8d67-8e12ec7cb23c
INFO 10-15 15:27:41 client.py:113] Model loaded: Mixtral-8x7B, e8fb4407-3381-43bd-8d67-8e12ec7cb23c
DEBUG 10-15 15:27:41 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:41 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:41 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:41 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:41 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:41 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:41 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:41 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:41 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:41 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:41 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:41 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:41 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:41 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:41 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:41 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:41 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:41 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:41 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:41 lpllm.py:2265] GPU2CPU move cost 0.000462 seconds
DEBUG 10-15 15:27:41 lpmodule.py:340] layer_idx 26 attn layer_idx: 26
DEBUG 10-15 15:27:41 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:41 lpmodule.py:364] decoder_attn_batch update batch_dim 360-420
DEBUG 10-15 15:27:41 StaticCacheLen.py:84] static cache update layer_idx: 26, start_batch: 360, end_batch: 420
DEBUG 10-15 15:27:41 lpmodule.py:368] update for kv cache 360-420 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:41 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:41 lpmodule.py:374] update past key value cost 0.023939 seconds
DEBUG 10-15 15:27:41 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:41 lpmodule.py:227] layer idx 25
DEBUG 10-15 15:27:41 lpmodule.py:399] repeat qkv cost 0.031029 seconds
DEBUG 10-15 15:27:41 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:41 lpmodule.py:433] dot attn cost 0.033577 seconds
DEBUG 10-15 15:27:41 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:41 lpmodule.py:444] time cost move to cuda:1 0.024025917053222656 s
DEBUG 10-15 15:27:41 lpllm.py:2283] CPU attn cost 0.138052 seconds if batch True
DEBUG 10-15 15:27:41 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:41 lpllm.py:2294] CPU compute cost 0.138782 seconds
DEBUG 10-15 15:27:41 lpllm.py:2312] free cost 0.000076 seconds
DEBUG 10-15 15:27:41 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:41 lpllm.py:1774] update state cost 3.743171691894531e-05 s
DEBUG 10-15 15:27:41 lpllm.py:1743] restore layer func cost 0.000823974609375 s
DEBUG 10-15 15:27:41 lpllm.py:511] restore layer cost 0.001073598861694336 s
DEBUG 10-15 15:27:41 lpllm.py:512] reset_next_layer_need: layer_attn_idx=26, layer_mlp_idx=26, j_loc=52
DEBUG 10-15 15:27:41 lpllm.py:1037] reset layer cost 0.0011479854583740234 s
DEBUG 10-15 15:27:41 lpllm.py:1038] have reset next layer layer_attn 26 layer_mlp 26 
DEBUG 10-15 15:27:41 lpllm.py:924] 
DEBUG 10-15 15:27:41 lpllm.py:924] decoder loop j: 53 cur_layer_idx: 26 layer_attn_idx: 26 layer_mlp_idx: 26
DEBUG 10-15 15:27:41 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:41 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:41 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:41 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:41 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:41 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:41 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:41 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:41 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:41 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:41 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:41 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:41 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:41 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:41 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:41 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:41 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:41 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:41 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:41 lpllm.py:2265] GPU2CPU move cost 0.000619 seconds
DEBUG 10-15 15:27:41 lpmodule.py:340] layer_idx 26 attn layer_idx: 26
DEBUG 10-15 15:27:41 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:41 lpmodule.py:364] decoder_attn_batch update batch_dim 420-480
DEBUG 10-15 15:27:41 StaticCacheLen.py:84] static cache update layer_idx: 26, start_batch: 420, end_batch: 480
DEBUG 10-15 15:27:41 lpmodule.py:368] update for kv cache 420-480 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:41 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:41 lpmodule.py:374] update past key value cost 0.028426 seconds
DEBUG 10-15 15:27:41 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:41 lpmodule.py:227] layer idx 26
DEBUG 10-15 15:27:41 lpmodule.py:399] repeat qkv cost 0.030767 seconds
DEBUG 10-15 15:27:41 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:41 lpmodule.py:433] dot attn cost 0.033700 seconds
DEBUG 10-15 15:27:41 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:41 lpmodule.py:444] time cost move to cuda:1 0.023175716400146484 s
DEBUG 10-15 15:27:41 lpllm.py:2283] CPU attn cost 0.141524 seconds if batch True
DEBUG 10-15 15:27:41 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:41 lpllm.py:2294] CPU compute cost 0.142428 seconds
DEBUG 10-15 15:27:41 lpllm.py:2312] free cost 0.000078 seconds
DEBUG 10-15 15:27:41 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:41 lpllm.py:1774] update state cost 3.814697265625e-05 s
DEBUG 10-15 15:27:41 lpllm.py:1743] restore layer func cost 0.0003871917724609375 s
DEBUG 10-15 15:27:41 lpllm.py:511] restore layer cost 0.0006175041198730469 s
DEBUG 10-15 15:27:41 lpllm.py:512] reset_next_layer_need: layer_attn_idx=27, layer_mlp_idx=26, j_loc=53
DEBUG 10-15 15:27:41 lpllm.py:1037] reset layer cost 0.0006895065307617188 s
DEBUG 10-15 15:27:41 lpllm.py:1038] have reset next layer layer_attn 27 layer_mlp 26 
DEBUG 10-15 15:27:41 lpllm.py:1044] j: 53 waiting the layer with layer_idx 27 before wait time 0.4587399959564209 s
INFO 10-15 15:27:41 client.py:117] confirm_model_loaded: Mixtral-8x7B, e8fb4407-3381-43bd-8d67-8e12ec7cb23c
INFO 10-15 15:27:41 client.py:125] Model loaded
DEBUG 10-15 15:27:41 lpllm.py:1048] j: load cost 0.46048951148986816 s waiting cost 0.0017342567443847656 s
DEBUG 10-15 15:27:41 lpllm.py:924] 
DEBUG 10-15 15:27:41 lpllm.py:924] decoder loop j: 54 cur_layer_idx: 27 layer_attn_idx: 27 layer_mlp_idx: 26
DEBUG 10-15 15:27:41 lpllm.py:933] start load next layer cur_layer_idx: 28
DEBUG 10-15 15:27:41 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed650>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x003\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x96\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:41 client.py:72] load_into_gpu: Mixtral-8x7B, 5f3e0167-39a8-4949-b4f1-fe012c24f58d
INFO 10-15 15:27:41 client.py:113] Model loaded: Mixtral-8x7B, 5f3e0167-39a8-4949-b4f1-fe012c24f58d
DEBUG 10-15 15:27:41 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:41 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:41 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:41 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:41 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:41 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:41 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:41 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:41 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:41 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:41 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:41 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:41 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:41 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:41 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:41 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:41 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:41 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:41 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:41 lpllm.py:2265] GPU2CPU move cost 0.000580 seconds
DEBUG 10-15 15:27:41 lpmodule.py:340] layer_idx 27 attn layer_idx: 27
DEBUG 10-15 15:27:41 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:41 lpmodule.py:364] decoder_attn_batch update batch_dim 360-420
DEBUG 10-15 15:27:41 StaticCacheLen.py:84] static cache update layer_idx: 27, start_batch: 360, end_batch: 420
DEBUG 10-15 15:27:41 lpmodule.py:368] update for kv cache 360-420 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:41 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:41 lpmodule.py:374] update past key value cost 0.023318 seconds
DEBUG 10-15 15:27:41 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:41 lpmodule.py:227] layer idx 26
DEBUG 10-15 15:27:41 lpmodule.py:399] repeat qkv cost 0.031454 seconds
DEBUG 10-15 15:27:41 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:41 lpmodule.py:433] dot attn cost 0.033630 seconds
DEBUG 10-15 15:27:41 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:41 lpmodule.py:444] time cost move to cuda:1 0.023955106735229492 s
DEBUG 10-15 15:27:42 lpllm.py:2283] CPU attn cost 0.138038 seconds if batch True
DEBUG 10-15 15:27:42 lpllm.py:2292] deal attn result cost 0.000003 seconds
DEBUG 10-15 15:27:42 lpllm.py:2294] CPU compute cost 0.138912 seconds
DEBUG 10-15 15:27:42 lpllm.py:2312] free cost 0.000075 seconds
DEBUG 10-15 15:27:42 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:42 lpllm.py:1774] update state cost 2.1457672119140625e-05 s
DEBUG 10-15 15:27:42 lpllm.py:1743] restore layer func cost 0.0007958412170410156 s
DEBUG 10-15 15:27:42 lpllm.py:511] restore layer cost 0.0010344982147216797 s
DEBUG 10-15 15:27:42 lpllm.py:512] reset_next_layer_need: layer_attn_idx=27, layer_mlp_idx=27, j_loc=54
DEBUG 10-15 15:27:42 lpllm.py:1037] reset layer cost 0.001108407974243164 s
DEBUG 10-15 15:27:42 lpllm.py:1038] have reset next layer layer_attn 27 layer_mlp 27 
DEBUG 10-15 15:27:42 lpllm.py:924] 
DEBUG 10-15 15:27:42 lpllm.py:924] decoder loop j: 55 cur_layer_idx: 27 layer_attn_idx: 27 layer_mlp_idx: 27
DEBUG 10-15 15:27:42 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:42 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:42 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:42 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:42 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:42 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:42 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:42 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:42 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:42 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:42 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:42 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:42 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:42 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:42 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:42 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:42 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:42 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:42 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:42 lpllm.py:2265] GPU2CPU move cost 0.000509 seconds
DEBUG 10-15 15:27:42 lpmodule.py:340] layer_idx 27 attn layer_idx: 27
DEBUG 10-15 15:27:42 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:42 lpmodule.py:364] decoder_attn_batch update batch_dim 420-480
DEBUG 10-15 15:27:42 StaticCacheLen.py:84] static cache update layer_idx: 27, start_batch: 420, end_batch: 480
DEBUG 10-15 15:27:42 lpmodule.py:368] update for kv cache 420-480 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:42 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:42 lpmodule.py:374] update past key value cost 0.023583 seconds
DEBUG 10-15 15:27:42 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:42 lpmodule.py:227] layer idx 27
DEBUG 10-15 15:27:42 lpmodule.py:399] repeat qkv cost 0.031361 seconds
DEBUG 10-15 15:27:42 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:42 lpmodule.py:433] dot attn cost 0.038686 seconds
DEBUG 10-15 15:27:42 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:42 lpmodule.py:444] time cost move to cuda:1 0.029757976531982422 s
DEBUG 10-15 15:27:42 lpllm.py:2283] CPU attn cost 0.148383 seconds if batch True
DEBUG 10-15 15:27:42 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:42 lpllm.py:2294] CPU compute cost 0.149124 seconds
DEBUG 10-15 15:27:42 lpllm.py:2312] free cost 0.000075 seconds
DEBUG 10-15 15:27:42 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:42 lpllm.py:1774] update state cost 2.4318695068359375e-05 s
DEBUG 10-15 15:27:42 lpllm.py:1743] restore layer func cost 0.0003752708435058594 s
DEBUG 10-15 15:27:42 lpllm.py:511] restore layer cost 0.0005939006805419922 s
DEBUG 10-15 15:27:42 lpllm.py:512] reset_next_layer_need: layer_attn_idx=28, layer_mlp_idx=27, j_loc=55
DEBUG 10-15 15:27:42 lpllm.py:1037] reset layer cost 0.0006644725799560547 s
DEBUG 10-15 15:27:42 lpllm.py:1038] have reset next layer layer_attn 28 layer_mlp 27 
DEBUG 10-15 15:27:42 lpllm.py:1044] j: 55 waiting the layer with layer_idx 28 before wait time 0.4511733055114746 s
INFO 10-15 15:27:42 client.py:117] confirm_model_loaded: Mixtral-8x7B, 5f3e0167-39a8-4949-b4f1-fe012c24f58d
INFO 10-15 15:27:42 client.py:125] Model loaded
DEBUG 10-15 15:27:42 lpllm.py:1048] j: load cost 0.4529294967651367 s waiting cost 0.0017428398132324219 s
DEBUG 10-15 15:27:42 lpllm.py:924] 
DEBUG 10-15 15:27:42 lpllm.py:924] decoder loop j: 56 cur_layer_idx: 28 layer_attn_idx: 28 layer_mlp_idx: 27
DEBUG 10-15 15:27:42 lpllm.py:933] start load next layer cur_layer_idx: 29
DEBUG 10-15 15:27:42 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed6e0>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x004\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x98\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:42 client.py:72] load_into_gpu: Mixtral-8x7B, bf02d460-f007-423f-b897-e581e744163d
INFO 10-15 15:27:42 client.py:113] Model loaded: Mixtral-8x7B, bf02d460-f007-423f-b897-e581e744163d
DEBUG 10-15 15:27:42 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:42 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:42 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:42 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:42 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:42 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:42 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:42 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:42 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:42 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:42 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:42 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:42 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:42 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:42 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:42 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:42 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:42 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:42 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:42 lpllm.py:2265] GPU2CPU move cost 0.000610 seconds
DEBUG 10-15 15:27:42 lpmodule.py:340] layer_idx 28 attn layer_idx: 28
DEBUG 10-15 15:27:42 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:42 lpmodule.py:364] decoder_attn_batch update batch_dim 360-420
DEBUG 10-15 15:27:42 StaticCacheLen.py:84] static cache update layer_idx: 28, start_batch: 360, end_batch: 420
DEBUG 10-15 15:27:42 lpmodule.py:368] update for kv cache 360-420 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:42 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:42 lpmodule.py:374] update past key value cost 0.024761 seconds
DEBUG 10-15 15:27:42 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:42 lpmodule.py:227] layer idx 27
DEBUG 10-15 15:27:42 lpmodule.py:399] repeat qkv cost 0.032641 seconds
DEBUG 10-15 15:27:42 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:42 lpmodule.py:433] dot attn cost 0.032979 seconds
DEBUG 10-15 15:27:42 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:42 lpmodule.py:444] time cost move to cuda:1 0.023931264877319336 s
DEBUG 10-15 15:27:42 lpllm.py:2283] CPU attn cost 0.140436 seconds if batch True
DEBUG 10-15 15:27:42 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:42 lpllm.py:2294] CPU compute cost 0.141331 seconds
DEBUG 10-15 15:27:42 lpllm.py:2312] free cost 0.000073 seconds
DEBUG 10-15 15:27:42 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:42 lpllm.py:1774] update state cost 3.743171691894531e-05 s
DEBUG 10-15 15:27:42 lpllm.py:1743] restore layer func cost 0.0008494853973388672 s
DEBUG 10-15 15:27:42 lpllm.py:511] restore layer cost 0.0011000633239746094 s
DEBUG 10-15 15:27:42 lpllm.py:512] reset_next_layer_need: layer_attn_idx=28, layer_mlp_idx=28, j_loc=56
DEBUG 10-15 15:27:42 lpllm.py:1037] reset layer cost 0.001176595687866211 s
DEBUG 10-15 15:27:42 lpllm.py:1038] have reset next layer layer_attn 28 layer_mlp 28 
DEBUG 10-15 15:27:42 lpllm.py:924] 
DEBUG 10-15 15:27:42 lpllm.py:924] decoder loop j: 57 cur_layer_idx: 28 layer_attn_idx: 28 layer_mlp_idx: 28
DEBUG 10-15 15:27:42 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:42 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:42 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:42 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:42 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:42 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:42 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:42 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:42 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:42 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:42 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:42 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:42 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:42 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:42 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:42 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:42 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:42 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:42 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:42 lpllm.py:2265] GPU2CPU move cost 0.000583 seconds
DEBUG 10-15 15:27:42 lpmodule.py:340] layer_idx 28 attn layer_idx: 28
DEBUG 10-15 15:27:42 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:42 lpmodule.py:364] decoder_attn_batch update batch_dim 420-480
DEBUG 10-15 15:27:42 StaticCacheLen.py:84] static cache update layer_idx: 28, start_batch: 420, end_batch: 480
DEBUG 10-15 15:27:42 lpmodule.py:368] update for kv cache 420-480 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:42 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:42 lpmodule.py:374] update past key value cost 0.024805 seconds
DEBUG 10-15 15:27:42 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:42 lpmodule.py:227] layer idx 28
DEBUG 10-15 15:27:42 lpmodule.py:399] repeat qkv cost 0.031840 seconds
DEBUG 10-15 15:27:42 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:42 lpmodule.py:433] dot attn cost 0.033253 seconds
DEBUG 10-15 15:27:42 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:42 lpmodule.py:444] time cost move to cuda:1 0.023348093032836914 s
DEBUG 10-15 15:27:42 lpllm.py:2283] CPU attn cost 0.138908 seconds if batch True
DEBUG 10-15 15:27:42 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:42 lpllm.py:2294] CPU compute cost 0.139792 seconds
DEBUG 10-15 15:27:42 lpllm.py:2312] free cost 0.000075 seconds
DEBUG 10-15 15:27:42 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:42 lpllm.py:1774] update state cost 3.910064697265625e-05 s
DEBUG 10-15 15:27:42 lpllm.py:1743] restore layer func cost 0.00040221214294433594 s
DEBUG 10-15 15:27:42 lpllm.py:511] restore layer cost 0.0006430149078369141 s
DEBUG 10-15 15:27:42 lpllm.py:512] reset_next_layer_need: layer_attn_idx=29, layer_mlp_idx=28, j_loc=57
DEBUG 10-15 15:27:42 lpllm.py:1037] reset layer cost 0.0007154941558837891 s
DEBUG 10-15 15:27:42 lpllm.py:1038] have reset next layer layer_attn 29 layer_mlp 28 
DEBUG 10-15 15:27:42 lpllm.py:1044] j: 57 waiting the layer with layer_idx 29 before wait time 0.45445847511291504 s
INFO 10-15 15:27:42 client.py:117] confirm_model_loaded: Mixtral-8x7B, bf02d460-f007-423f-b897-e581e744163d
INFO 10-15 15:27:42 client.py:125] Model loaded
DEBUG 10-15 15:27:42 lpllm.py:1048] j: load cost 0.4563419818878174 s waiting cost 0.0018689632415771484 s
DEBUG 10-15 15:27:42 lpllm.py:924] 
DEBUG 10-15 15:27:42 lpllm.py:924] decoder loop j: 58 cur_layer_idx: 29 layer_attn_idx: 29 layer_mlp_idx: 28
DEBUG 10-15 15:27:42 lpllm.py:933] start load next layer cur_layer_idx: 30
DEBUG 10-15 15:27:42 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:42 client.py:72] load_into_gpu: Mixtral-8x7B, 9a343cd8-1a66-4592-83c9-a971e2f53171
INFO 10-15 15:27:42 client.py:113] Model loaded: Mixtral-8x7B, 9a343cd8-1a66-4592-83c9-a971e2f53171
DEBUG 10-15 15:27:42 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:42 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:42 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:42 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:42 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:42 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:42 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:42 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:42 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:42 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:42 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:42 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:42 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:42 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:42 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:42 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:42 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:42 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:42 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:42 lpllm.py:2265] GPU2CPU move cost 0.000616 seconds
DEBUG 10-15 15:27:42 lpmodule.py:340] layer_idx 29 attn layer_idx: 29
DEBUG 10-15 15:27:42 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:42 lpmodule.py:364] decoder_attn_batch update batch_dim 360-420
DEBUG 10-15 15:27:42 StaticCacheLen.py:84] static cache update layer_idx: 29, start_batch: 360, end_batch: 420
DEBUG 10-15 15:27:42 lpmodule.py:368] update for kv cache 360-420 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:42 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:42 lpmodule.py:374] update past key value cost 0.023110 seconds
DEBUG 10-15 15:27:42 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:42 lpmodule.py:227] layer idx 28
DEBUG 10-15 15:27:42 lpmodule.py:399] repeat qkv cost 0.030795 seconds
DEBUG 10-15 15:27:42 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:42 lpmodule.py:433] dot attn cost 0.035096 seconds
DEBUG 10-15 15:27:42 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:42 lpmodule.py:444] time cost move to cuda:1 0.023525714874267578 s
DEBUG 10-15 15:27:42 lpllm.py:2283] CPU attn cost 0.138012 seconds if batch True
DEBUG 10-15 15:27:42 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:42 lpllm.py:2294] CPU compute cost 0.138917 seconds
DEBUG 10-15 15:27:42 lpllm.py:2312] free cost 0.000073 seconds
DEBUG 10-15 15:27:42 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:42 lpllm.py:1774] update state cost 3.695487976074219e-05 s
DEBUG 10-15 15:27:42 lpllm.py:1743] restore layer func cost 0.000823974609375 s
DEBUG 10-15 15:27:42 lpllm.py:511] restore layer cost 0.0010724067687988281 s
DEBUG 10-15 15:27:42 lpllm.py:512] reset_next_layer_need: layer_attn_idx=29, layer_mlp_idx=29, j_loc=58
DEBUG 10-15 15:27:42 lpllm.py:1037] reset layer cost 0.0011453628540039062 s
DEBUG 10-15 15:27:42 lpllm.py:1038] have reset next layer layer_attn 29 layer_mlp 29 
DEBUG 10-15 15:27:42 lpllm.py:924] 
DEBUG 10-15 15:27:42 lpllm.py:924] decoder loop j: 59 cur_layer_idx: 29 layer_attn_idx: 29 layer_mlp_idx: 29
DEBUG 10-15 15:27:42 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:42 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:42 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:43 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:43 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:43 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:43 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:43 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:43 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:43 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:43 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:43 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:43 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:43 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:43 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:43 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:43 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:43 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:43 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:43 lpllm.py:2265] GPU2CPU move cost 0.000572 seconds
DEBUG 10-15 15:27:43 lpmodule.py:340] layer_idx 29 attn layer_idx: 29
DEBUG 10-15 15:27:43 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:43 lpmodule.py:364] decoder_attn_batch update batch_dim 420-480
DEBUG 10-15 15:27:43 StaticCacheLen.py:84] static cache update layer_idx: 29, start_batch: 420, end_batch: 480
DEBUG 10-15 15:27:43 lpmodule.py:368] update for kv cache 420-480 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:43 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:43 lpmodule.py:374] update past key value cost 0.024860 seconds
DEBUG 10-15 15:27:43 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:43 lpmodule.py:227] layer idx 29
DEBUG 10-15 15:27:43 lpmodule.py:399] repeat qkv cost 0.032647 seconds
DEBUG 10-15 15:27:43 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:43 lpmodule.py:433] dot attn cost 0.043138 seconds
DEBUG 10-15 15:27:43 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:43 lpmodule.py:444] time cost move to cuda:1 0.02284717559814453 s
DEBUG 10-15 15:27:43 lpllm.py:2283] CPU attn cost 0.148358 seconds if batch True
DEBUG 10-15 15:27:43 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:43 lpllm.py:2294] CPU compute cost 0.149208 seconds
DEBUG 10-15 15:27:43 lpllm.py:2312] free cost 0.000076 seconds
DEBUG 10-15 15:27:43 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:43 lpllm.py:1774] update state cost 3.8623809814453125e-05 s
DEBUG 10-15 15:27:43 lpllm.py:1743] restore layer func cost 0.00039315223693847656 s
DEBUG 10-15 15:27:43 lpllm.py:511] restore layer cost 0.0006403923034667969 s
DEBUG 10-15 15:27:43 lpllm.py:512] reset_next_layer_need: layer_attn_idx=30, layer_mlp_idx=29, j_loc=59
DEBUG 10-15 15:27:43 lpllm.py:1037] reset layer cost 0.0007171630859375 s
DEBUG 10-15 15:27:43 lpllm.py:1038] have reset next layer layer_attn 30 layer_mlp 29 
DEBUG 10-15 15:27:43 lpllm.py:1044] j: 59 waiting the layer with layer_idx 30 before wait time 0.46108269691467285 s
INFO 10-15 15:27:43 client.py:117] confirm_model_loaded: Mixtral-8x7B, 9a343cd8-1a66-4592-83c9-a971e2f53171
INFO 10-15 15:27:43 client.py:125] Model loaded
DEBUG 10-15 15:27:43 lpllm.py:1048] j: load cost 0.4628932476043701 s waiting cost 0.0017957687377929688 s
DEBUG 10-15 15:27:43 lpllm.py:924] 
DEBUG 10-15 15:27:43 lpllm.py:924] decoder loop j: 60 cur_layer_idx: 30 layer_attn_idx: 30 layer_mlp_idx: 29
DEBUG 10-15 15:27:43 lpllm.py:933] start load next layer cur_layer_idx: 31
DEBUG 10-15 15:27:43 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed650>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x003\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x96\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:43 client.py:72] load_into_gpu: Mixtral-8x7B, 14e16974-9aa6-4e7e-b7a6-645164cb9726
INFO 10-15 15:27:43 client.py:113] Model loaded: Mixtral-8x7B, 14e16974-9aa6-4e7e-b7a6-645164cb9726
DEBUG 10-15 15:27:43 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:43 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:43 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:43 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:43 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:43 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:43 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:43 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:43 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:43 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:43 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:43 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:43 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:43 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:43 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:43 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:43 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:43 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:43 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:43 lpllm.py:2265] GPU2CPU move cost 0.000533 seconds
DEBUG 10-15 15:27:43 lpmodule.py:340] layer_idx 30 attn layer_idx: 30
DEBUG 10-15 15:27:43 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:43 lpmodule.py:364] decoder_attn_batch update batch_dim 360-420
DEBUG 10-15 15:27:43 StaticCacheLen.py:84] static cache update layer_idx: 30, start_batch: 360, end_batch: 420
DEBUG 10-15 15:27:43 lpmodule.py:368] update for kv cache 360-420 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:43 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:43 lpmodule.py:374] update past key value cost 0.023745 seconds
DEBUG 10-15 15:27:43 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:43 lpmodule.py:227] layer idx 29
DEBUG 10-15 15:27:43 lpmodule.py:399] repeat qkv cost 0.031983 seconds
DEBUG 10-15 15:27:43 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:43 lpmodule.py:433] dot attn cost 0.033798 seconds
DEBUG 10-15 15:27:43 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:43 lpmodule.py:444] time cost move to cuda:1 0.02393317222595215 s
DEBUG 10-15 15:27:43 lpllm.py:2283] CPU attn cost 0.138294 seconds if batch True
DEBUG 10-15 15:27:43 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:43 lpllm.py:2294] CPU compute cost 0.139050 seconds
DEBUG 10-15 15:27:43 lpllm.py:2312] free cost 0.000074 seconds
DEBUG 10-15 15:27:43 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:43 lpllm.py:1774] update state cost 2.1457672119140625e-05 s
DEBUG 10-15 15:27:43 lpllm.py:1743] restore layer func cost 0.0007810592651367188 s
DEBUG 10-15 15:27:43 lpllm.py:511] restore layer cost 0.0010037422180175781 s
DEBUG 10-15 15:27:43 lpllm.py:512] reset_next_layer_need: layer_attn_idx=30, layer_mlp_idx=30, j_loc=60
DEBUG 10-15 15:27:43 lpllm.py:1037] reset layer cost 0.0010743141174316406 s
DEBUG 10-15 15:27:43 lpllm.py:1038] have reset next layer layer_attn 30 layer_mlp 30 
DEBUG 10-15 15:27:43 lpllm.py:924] 
DEBUG 10-15 15:27:43 lpllm.py:924] decoder loop j: 61 cur_layer_idx: 30 layer_attn_idx: 30 layer_mlp_idx: 30
DEBUG 10-15 15:27:43 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:43 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:43 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:43 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:43 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:43 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:43 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:43 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:43 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:43 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:43 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:43 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:43 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:43 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:43 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:43 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:43 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:43 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:43 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:43 lpllm.py:2265] GPU2CPU move cost 0.000614 seconds
DEBUG 10-15 15:27:43 lpmodule.py:340] layer_idx 30 attn layer_idx: 30
DEBUG 10-15 15:27:43 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:43 lpmodule.py:364] decoder_attn_batch update batch_dim 420-480
DEBUG 10-15 15:27:43 StaticCacheLen.py:84] static cache update layer_idx: 30, start_batch: 420, end_batch: 480
DEBUG 10-15 15:27:43 lpmodule.py:368] update for kv cache 420-480 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:43 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:43 lpmodule.py:374] update past key value cost 0.027033 seconds
DEBUG 10-15 15:27:43 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:43 lpmodule.py:227] layer idx 30
DEBUG 10-15 15:27:43 lpmodule.py:399] repeat qkv cost 0.030421 seconds
DEBUG 10-15 15:27:43 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:43 lpmodule.py:433] dot attn cost 0.033397 seconds
DEBUG 10-15 15:27:43 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:43 lpmodule.py:444] time cost move to cuda:1 0.023751497268676758 s
DEBUG 10-15 15:27:43 lpllm.py:2283] CPU attn cost 0.139441 seconds if batch True
DEBUG 10-15 15:27:43 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:43 lpllm.py:2294] CPU compute cost 0.140342 seconds
DEBUG 10-15 15:27:43 lpllm.py:2312] free cost 0.000073 seconds
DEBUG 10-15 15:27:43 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:43 lpllm.py:1774] update state cost 2.2649765014648438e-05 s
DEBUG 10-15 15:27:43 lpllm.py:1743] restore layer func cost 0.00038623809814453125 s
DEBUG 10-15 15:27:43 lpllm.py:511] restore layer cost 0.0006110668182373047 s
DEBUG 10-15 15:27:43 lpllm.py:512] reset_next_layer_need: layer_attn_idx=31, layer_mlp_idx=30, j_loc=61
DEBUG 10-15 15:27:43 lpllm.py:1037] reset layer cost 0.0006830692291259766 s
DEBUG 10-15 15:27:43 lpllm.py:1038] have reset next layer layer_attn 31 layer_mlp 30 
DEBUG 10-15 15:27:43 lpllm.py:1044] j: 61 waiting the layer with layer_idx 31 before wait time 0.457974910736084 s
INFO 10-15 15:27:43 client.py:117] confirm_model_loaded: Mixtral-8x7B, 14e16974-9aa6-4e7e-b7a6-645164cb9726
INFO 10-15 15:27:43 client.py:125] Model loaded
DEBUG 10-15 15:27:43 lpllm.py:1048] j: load cost 0.45981478691101074 s waiting cost 0.0018248558044433594 s
DEBUG 10-15 15:27:43 lpllm.py:924] 
DEBUG 10-15 15:27:43 lpllm.py:924] decoder loop j: 62 cur_layer_idx: 31 layer_attn_idx: 31 layer_mlp_idx: 30
DEBUG 10-15 15:27:43 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:43 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:43 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:43 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:43 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:43 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:43 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:43 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:43 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:43 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:43 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:43 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:43 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:43 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:43 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:43 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:43 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:43 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:43 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:43 lpllm.py:2265] GPU2CPU move cost 0.000580 seconds
DEBUG 10-15 15:27:43 lpmodule.py:340] layer_idx 31 attn layer_idx: 31
DEBUG 10-15 15:27:43 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:43 lpmodule.py:364] decoder_attn_batch update batch_dim 360-420
DEBUG 10-15 15:27:43 StaticCacheLen.py:84] static cache update layer_idx: 31, start_batch: 360, end_batch: 420
DEBUG 10-15 15:27:43 lpmodule.py:368] update for kv cache 360-420 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:43 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:43 lpmodule.py:374] update past key value cost 0.022155 seconds
DEBUG 10-15 15:27:43 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:43 lpmodule.py:227] layer idx 30
DEBUG 10-15 15:27:43 lpmodule.py:399] repeat qkv cost 0.030365 seconds
DEBUG 10-15 15:27:43 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:43 lpmodule.py:433] dot attn cost 0.031849 seconds
DEBUG 10-15 15:27:43 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:43 lpmodule.py:444] time cost move to cuda:1 0.024146080017089844 s
DEBUG 10-15 15:27:43 lpllm.py:2283] CPU attn cost 0.134163 seconds if batch True
DEBUG 10-15 15:27:43 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:27:43 lpllm.py:2294] CPU compute cost 0.135032 seconds
DEBUG 10-15 15:27:43 lpllm.py:2312] free cost 0.000077 seconds
DEBUG 10-15 15:27:43 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:43 lpllm.py:1774] update state cost 2.09808349609375e-05 s
DEBUG 10-15 15:27:43 lpllm.py:1743] restore layer func cost 0.0007851123809814453 s
DEBUG 10-15 15:27:43 lpllm.py:511] restore layer cost 0.00101470947265625 s
DEBUG 10-15 15:27:43 lpllm.py:512] reset_next_layer_need: layer_attn_idx=31, layer_mlp_idx=31, j_loc=62
DEBUG 10-15 15:27:43 lpllm.py:1037] reset layer cost 0.0010838508605957031 s
DEBUG 10-15 15:27:43 lpllm.py:1038] have reset next layer layer_attn 31 layer_mlp 31 
DEBUG 10-15 15:27:43 lpllm.py:924] 
DEBUG 10-15 15:27:43 lpllm.py:924] decoder loop j: 63 cur_layer_idx: 31 layer_attn_idx: 31 layer_mlp_idx: 31
DEBUG 10-15 15:27:43 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:43 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:43 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:43 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:43 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:43 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:43 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:43 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:43 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:43 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:43 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:43 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:43 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:43 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:43 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:43 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:43 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:43 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:43 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:43 lpllm.py:2265] GPU2CPU move cost 0.000622 seconds
DEBUG 10-15 15:27:43 lpmodule.py:340] layer_idx 31 attn layer_idx: 31
DEBUG 10-15 15:27:43 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:43 lpmodule.py:364] decoder_attn_batch update batch_dim 420-480
DEBUG 10-15 15:27:43 StaticCacheLen.py:84] static cache update layer_idx: 31, start_batch: 420, end_batch: 480
DEBUG 10-15 15:27:43 lpmodule.py:368] update for kv cache 420-480 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:43 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:43 lpmodule.py:374] update past key value cost 0.027650 seconds
DEBUG 10-15 15:27:43 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:43 lpmodule.py:227] layer idx 31
DEBUG 10-15 15:27:43 lpmodule.py:399] repeat qkv cost 0.030154 seconds
DEBUG 10-15 15:27:43 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:44 lpmodule.py:433] dot attn cost 0.033047 seconds
DEBUG 10-15 15:27:44 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:44 lpmodule.py:444] time cost move to cuda:1 0.023554563522338867 s
DEBUG 10-15 15:27:44 lpllm.py:2283] CPU attn cost 0.139902 seconds if batch True
DEBUG 10-15 15:27:44 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:44 lpllm.py:2294] CPU compute cost 0.140824 seconds
DEBUG 10-15 15:27:44 lpllm.py:2312] free cost 0.000086 seconds
DEBUG 10-15 15:27:44 lpllm.py:924] 
DEBUG 10-15 15:27:44 lpllm.py:924] decoder loop j: 64 cur_layer_idx: 32 layer_attn_idx: 32 layer_mlp_idx: 31
DEBUG 10-15 15:27:44 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:44 lpmodule.py:227] layer idx 31
DEBUG 10-15 15:27:44 lpllm.py:1085] last_mlp_output_chunk shape: torch.Size([60, 512, 4096]), mlp_output_chunk shape: torch.Size([60, 512, 4096])
DEBUG 10-15 15:27:44 lpllm.py:1086] last_mlp_output_chunk len 1, mlp_output_chunk len 1
DEBUG 10-15 15:27:44 lpllm.py:618] decoders batch for 3 cost 14.961395025253296 s
DEBUG 10-15 15:27:44 lpllm.py:848] hidden_states_chunks1 shape: torch.Size([60, 512, 4096]), hidden_states_chunks1 length: 1
DEBUG 10-15 15:27:44 lpllm.py:849] hidden_states_chunks2 shape: torch.Size([60, 512, 4096]), hidden_states_chunks2 length: 1
DEBUG 10-15 15:27:44 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed6e0>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x004\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x98\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:44 client.py:72] load_into_gpu: Mixtral-8x7B, 2a1f3049-ec69-474f-8ee7-ecd401c5b9f2
INFO 10-15 15:27:44 client.py:113] Model loaded: Mixtral-8x7B, 2a1f3049-ec69-474f-8ee7-ecd401c5b9f2
DEBUG 10-15 15:27:44 lpllm.py:1743] restore layer func cost 0.0009379386901855469 s
INFO 10-15 15:27:44 client.py:117] confirm_model_loaded: Mixtral-8x7B, 2a1f3049-ec69-474f-8ee7-ecd401c5b9f2
INFO 10-15 15:27:44 client.py:125] Model loaded
DEBUG 10-15 15:27:44 lpllm.py:422] prepare layer cost 0.27282261848449707 s
DEBUG 10-15 15:27:44 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:44 client.py:72] load_into_gpu: Mixtral-8x7B, 6cd87ca8-191a-44b6-be8f-3ec6a600016a
INFO 10-15 15:27:44 client.py:113] Model loaded: Mixtral-8x7B, 6cd87ca8-191a-44b6-be8f-3ec6a600016a
DEBUG 10-15 15:27:44 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:44 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:44 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:44 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:44 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:44 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:44 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:44 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:44 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:44 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:44 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:44 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:44 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:44 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:44 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:44 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:44 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:44 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:44 lpllm.py:924] 
DEBUG 10-15 15:27:44 lpllm.py:924] decoder loop j: 1 cur_layer_idx: 0 layer_attn_idx: 0 layer_mlp_idx: 0
DEBUG 10-15 15:27:44 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:44 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:44 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:44 lpllm.py:2265] GPU2CPU move cost 0.000679 seconds
DEBUG 10-15 15:27:44 lpmodule.py:340] layer_idx 0 attn layer_idx: 0
DEBUG 10-15 15:27:44 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:44 lpmodule.py:364] decoder_attn_batch update batch_dim 480-540
DEBUG 10-15 15:27:44 StaticCacheLen.py:84] static cache update layer_idx: 0, start_batch: 480, end_batch: 540
DEBUG 10-15 15:27:44 lpmodule.py:368] update for kv cache 480-540 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:44 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:44 lpmodule.py:374] update past key value cost 0.025545 seconds
DEBUG 10-15 15:27:44 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:44 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:44 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:44 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:44 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:44 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:44 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:44 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:44 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:44 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:44 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:44 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:44 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:44 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:44 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:44 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:44 lpmodule.py:399] repeat qkv cost 0.031809 seconds
DEBUG 10-15 15:27:44 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:44 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:44 lpmodule.py:433] dot attn cost 0.046929 seconds
DEBUG 10-15 15:27:44 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:44 lpmodule.py:444] time cost move to cuda:1 0.02365732192993164 s
DEBUG 10-15 15:27:44 lpllm.py:2283] CPU attn cost 0.153298 seconds if batch True
DEBUG 10-15 15:27:44 lpllm.py:2292] deal attn result cost 0.000019 seconds
DEBUG 10-15 15:27:44 lpllm.py:2294] CPU compute cost 0.154352 seconds
DEBUG 10-15 15:27:44 lpllm.py:2312] free cost 0.000169 seconds
DEBUG 10-15 15:27:44 lpllm.py:2265] GPU2CPU move cost 0.000606 seconds
DEBUG 10-15 15:27:44 lpmodule.py:340] layer_idx 0 attn layer_idx: 0
DEBUG 10-15 15:27:44 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:44 lpmodule.py:364] decoder_attn_batch update batch_dim 540-600
DEBUG 10-15 15:27:44 lpmodule.py:227] layer idx 0
DEBUG 10-15 15:27:44 StaticCacheLen.py:84] static cache update layer_idx: 0, start_batch: 540, end_batch: 600
DEBUG 10-15 15:27:44 lpmodule.py:368] update for kv cache 540-600 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:44 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:44 lpmodule.py:374] update past key value cost 0.026031 seconds
DEBUG 10-15 15:27:44 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:44 lpmodule.py:399] repeat qkv cost 0.029970 seconds
DEBUG 10-15 15:27:44 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:44 lpmodule.py:433] dot attn cost 0.034319 seconds
DEBUG 10-15 15:27:44 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:44 lpmodule.py:444] time cost move to cuda:1 0.025600671768188477 s
DEBUG 10-15 15:27:44 lpllm.py:2283] CPU attn cost 0.140484 seconds if batch True
DEBUG 10-15 15:27:44 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:44 lpllm.py:2294] CPU compute cost 0.141295 seconds
DEBUG 10-15 15:27:44 lpllm.py:2312] free cost 0.000071 seconds
DEBUG 10-15 15:27:44 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:44 lpllm.py:1774] update state cost 5.14984130859375e-05 s
DEBUG 10-15 15:27:44 lpllm.py:1743] restore layer func cost 0.0005133152008056641 s
DEBUG 10-15 15:27:44 lpllm.py:511] restore layer cost 0.0008103847503662109 s
DEBUG 10-15 15:27:44 lpllm.py:512] reset_next_layer_need: layer_attn_idx=1, layer_mlp_idx=0, j_loc=1
DEBUG 10-15 15:27:44 lpllm.py:1037] reset layer cost 0.0009348392486572266 s
DEBUG 10-15 15:27:44 lpllm.py:1038] have reset next layer layer_attn 1 layer_mlp 0 
DEBUG 10-15 15:27:44 lpllm.py:1044] j: 1 waiting the layer with layer_idx 1 before wait time 0.3693821430206299 s
INFO 10-15 15:27:44 client.py:117] confirm_model_loaded: Mixtral-8x7B, 6cd87ca8-191a-44b6-be8f-3ec6a600016a
INFO 10-15 15:27:44 client.py:125] Model loaded
DEBUG 10-15 15:27:44 lpllm.py:1048] j: load cost 0.3711264133453369 s waiting cost 0.0017271041870117188 s
DEBUG 10-15 15:27:44 lpllm.py:924] 
DEBUG 10-15 15:27:44 lpllm.py:924] decoder loop j: 2 cur_layer_idx: 1 layer_attn_idx: 1 layer_mlp_idx: 0
DEBUG 10-15 15:27:44 lpllm.py:933] start load next layer cur_layer_idx: 2
DEBUG 10-15 15:27:44 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed650>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x003\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x96\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:44 client.py:72] load_into_gpu: Mixtral-8x7B, 8b4e9da9-585f-4ee7-8016-a8f1eeea7b10
INFO 10-15 15:27:44 client.py:113] Model loaded: Mixtral-8x7B, 8b4e9da9-585f-4ee7-8016-a8f1eeea7b10
DEBUG 10-15 15:27:44 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:44 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:44 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:44 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:44 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:44 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:44 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:44 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:44 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:44 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:44 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:44 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:44 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:44 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:44 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:44 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:44 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:44 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:44 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:44 lpllm.py:2265] GPU2CPU move cost 0.000616 seconds
DEBUG 10-15 15:27:44 lpmodule.py:340] layer_idx 1 attn layer_idx: 1
DEBUG 10-15 15:27:44 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:44 lpmodule.py:364] decoder_attn_batch update batch_dim 480-540
DEBUG 10-15 15:27:44 StaticCacheLen.py:84] static cache update layer_idx: 1, start_batch: 480, end_batch: 540
DEBUG 10-15 15:27:44 lpmodule.py:368] update for kv cache 480-540 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:44 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:44 lpmodule.py:374] update past key value cost 0.022708 seconds
DEBUG 10-15 15:27:44 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:44 lpmodule.py:227] layer idx 0
DEBUG 10-15 15:27:45 lpmodule.py:399] repeat qkv cost 0.031429 seconds
DEBUG 10-15 15:27:45 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:45 lpmodule.py:433] dot attn cost 0.046050 seconds
DEBUG 10-15 15:27:45 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:45 lpmodule.py:444] time cost move to cuda:1 0.024118900299072266 s
DEBUG 10-15 15:27:45 lpllm.py:2283] CPU attn cost 0.148717 seconds if batch True
DEBUG 10-15 15:27:45 lpllm.py:2292] deal attn result cost 0.000008 seconds
DEBUG 10-15 15:27:45 lpllm.py:2294] CPU compute cost 0.149640 seconds
DEBUG 10-15 15:27:45 lpllm.py:2312] free cost 0.000101 seconds
DEBUG 10-15 15:27:45 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:45 lpllm.py:1774] update state cost 2.956390380859375e-05 s
DEBUG 10-15 15:27:45 lpllm.py:1743] restore layer func cost 0.0011081695556640625 s
DEBUG 10-15 15:27:45 lpllm.py:511] restore layer cost 0.001383066177368164 s
DEBUG 10-15 15:27:45 lpllm.py:512] reset_next_layer_need: layer_attn_idx=1, layer_mlp_idx=1, j_loc=2
DEBUG 10-15 15:27:45 lpllm.py:1037] reset layer cost 0.0015141963958740234 s
DEBUG 10-15 15:27:45 lpllm.py:1038] have reset next layer layer_attn 1 layer_mlp 1 
DEBUG 10-15 15:27:45 lpllm.py:924] 
DEBUG 10-15 15:27:45 lpllm.py:924] decoder loop j: 3 cur_layer_idx: 1 layer_attn_idx: 1 layer_mlp_idx: 1
DEBUG 10-15 15:27:45 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:45 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:45 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:45 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:45 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:45 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:45 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:45 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:45 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:45 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:45 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:45 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:45 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:45 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:45 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:45 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:45 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:45 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:45 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:45 lpllm.py:2265] GPU2CPU move cost 0.000550 seconds
DEBUG 10-15 15:27:45 lpmodule.py:340] layer_idx 1 attn layer_idx: 1
DEBUG 10-15 15:27:45 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:45 lpmodule.py:364] decoder_attn_batch update batch_dim 540-600
DEBUG 10-15 15:27:45 StaticCacheLen.py:84] static cache update layer_idx: 1, start_batch: 540, end_batch: 600
DEBUG 10-15 15:27:45 lpmodule.py:368] update for kv cache 540-600 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:45 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:45 lpmodule.py:374] update past key value cost 0.025804 seconds
DEBUG 10-15 15:27:45 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:45 lpmodule.py:227] layer idx 1
DEBUG 10-15 15:27:45 lpmodule.py:399] repeat qkv cost 0.029451 seconds
DEBUG 10-15 15:27:45 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:45 lpmodule.py:433] dot attn cost 0.032990 seconds
DEBUG 10-15 15:27:45 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:45 lpmodule.py:444] time cost move to cuda:1 0.02399897575378418 s
DEBUG 10-15 15:27:45 lpllm.py:2283] CPU attn cost 0.136706 seconds if batch True
DEBUG 10-15 15:27:45 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:45 lpllm.py:2294] CPU compute cost 0.137520 seconds
DEBUG 10-15 15:27:45 lpllm.py:2312] free cost 0.000085 seconds
DEBUG 10-15 15:27:45 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:45 lpllm.py:1774] update state cost 3.2901763916015625e-05 s
DEBUG 10-15 15:27:45 lpllm.py:1743] restore layer func cost 0.0005109310150146484 s
DEBUG 10-15 15:27:45 lpllm.py:511] restore layer cost 0.0008103847503662109 s
DEBUG 10-15 15:27:45 lpllm.py:512] reset_next_layer_need: layer_attn_idx=2, layer_mlp_idx=1, j_loc=3
DEBUG 10-15 15:27:45 lpllm.py:1037] reset layer cost 0.0009045600891113281 s
DEBUG 10-15 15:27:45 lpllm.py:1038] have reset next layer layer_attn 2 layer_mlp 1 
DEBUG 10-15 15:27:45 lpllm.py:1044] j: 3 waiting the layer with layer_idx 2 before wait time 0.45555996894836426 s
INFO 10-15 15:27:45 client.py:117] confirm_model_loaded: Mixtral-8x7B, 8b4e9da9-585f-4ee7-8016-a8f1eeea7b10
INFO 10-15 15:27:45 client.py:125] Model loaded
DEBUG 10-15 15:27:45 lpllm.py:1048] j: load cost 0.4574000835418701 s waiting cost 0.0018208026885986328 s
DEBUG 10-15 15:27:45 lpllm.py:924] 
DEBUG 10-15 15:27:45 lpllm.py:924] decoder loop j: 4 cur_layer_idx: 2 layer_attn_idx: 2 layer_mlp_idx: 1
DEBUG 10-15 15:27:45 lpllm.py:933] start load next layer cur_layer_idx: 3
DEBUG 10-15 15:27:45 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed6e0>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x004\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x98\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:45 client.py:72] load_into_gpu: Mixtral-8x7B, 4812b1e4-0d46-455e-b30b-1bf3c1760226
INFO 10-15 15:27:45 client.py:113] Model loaded: Mixtral-8x7B, 4812b1e4-0d46-455e-b30b-1bf3c1760226
DEBUG 10-15 15:27:45 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:45 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:45 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:45 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:45 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:45 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:45 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:45 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:45 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:45 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:45 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:45 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:45 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:45 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:45 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:45 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:45 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:45 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:45 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:45 lpllm.py:2265] GPU2CPU move cost 0.000637 seconds
DEBUG 10-15 15:27:45 lpmodule.py:340] layer_idx 2 attn layer_idx: 2
DEBUG 10-15 15:27:45 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:45 lpmodule.py:364] decoder_attn_batch update batch_dim 480-540
DEBUG 10-15 15:27:45 StaticCacheLen.py:84] static cache update layer_idx: 2, start_batch: 480, end_batch: 540
DEBUG 10-15 15:27:45 lpmodule.py:368] update for kv cache 480-540 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:45 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:45 lpmodule.py:374] update past key value cost 0.023520 seconds
DEBUG 10-15 15:27:45 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:45 lpmodule.py:227] layer idx 1
DEBUG 10-15 15:27:45 lpmodule.py:399] repeat qkv cost 0.030224 seconds
DEBUG 10-15 15:27:45 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:45 lpmodule.py:433] dot attn cost 0.034820 seconds
DEBUG 10-15 15:27:45 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:45 lpmodule.py:444] time cost move to cuda:1 0.024082422256469727 s
DEBUG 10-15 15:27:45 lpllm.py:2283] CPU attn cost 0.137155 seconds if batch True
DEBUG 10-15 15:27:45 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:45 lpllm.py:2294] CPU compute cost 0.138079 seconds
DEBUG 10-15 15:27:45 lpllm.py:2312] free cost 0.000074 seconds
DEBUG 10-15 15:27:45 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:45 lpllm.py:1774] update state cost 2.86102294921875e-05 s
DEBUG 10-15 15:27:45 lpllm.py:1743] restore layer func cost 0.0010325908660888672 s
DEBUG 10-15 15:27:45 lpllm.py:511] restore layer cost 0.0013158321380615234 s
DEBUG 10-15 15:27:45 lpllm.py:512] reset_next_layer_need: layer_attn_idx=2, layer_mlp_idx=2, j_loc=4
DEBUG 10-15 15:27:45 lpllm.py:1037] reset layer cost 0.0014104843139648438 s
DEBUG 10-15 15:27:45 lpllm.py:1038] have reset next layer layer_attn 2 layer_mlp 2 
DEBUG 10-15 15:27:45 lpllm.py:924] 
DEBUG 10-15 15:27:45 lpllm.py:924] decoder loop j: 5 cur_layer_idx: 2 layer_attn_idx: 2 layer_mlp_idx: 2
DEBUG 10-15 15:27:45 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:45 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:45 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:45 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:45 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:45 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:45 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:45 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:45 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:45 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:45 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:45 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:45 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:45 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:45 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:45 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:45 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:45 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:45 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:45 lpllm.py:2265] GPU2CPU move cost 0.000315 seconds
DEBUG 10-15 15:27:45 lpmodule.py:340] layer_idx 2 attn layer_idx: 2
DEBUG 10-15 15:27:45 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:45 lpmodule.py:364] decoder_attn_batch update batch_dim 540-600
DEBUG 10-15 15:27:45 StaticCacheLen.py:84] static cache update layer_idx: 2, start_batch: 540, end_batch: 600
DEBUG 10-15 15:27:45 lpmodule.py:368] update for kv cache 540-600 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:45 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:45 lpmodule.py:374] update past key value cost 0.024640 seconds
DEBUG 10-15 15:27:45 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:45 lpmodule.py:227] layer idx 2
DEBUG 10-15 15:27:45 lpmodule.py:399] repeat qkv cost 0.030257 seconds
DEBUG 10-15 15:27:45 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:45 lpmodule.py:433] dot attn cost 0.037573 seconds
DEBUG 10-15 15:27:45 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:45 lpmodule.py:444] time cost move to cuda:1 0.023959636688232422 s
DEBUG 10-15 15:27:45 lpllm.py:2283] CPU attn cost 0.141418 seconds if batch True
DEBUG 10-15 15:27:45 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:45 lpllm.py:2294] CPU compute cost 0.141952 seconds
DEBUG 10-15 15:27:45 lpllm.py:2312] free cost 0.000075 seconds
DEBUG 10-15 15:27:45 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:45 lpllm.py:1774] update state cost 3.0040740966796875e-05 s
DEBUG 10-15 15:27:45 lpllm.py:1743] restore layer func cost 0.00045299530029296875 s
DEBUG 10-15 15:27:45 lpllm.py:511] restore layer cost 0.0007114410400390625 s
DEBUG 10-15 15:27:45 lpllm.py:512] reset_next_layer_need: layer_attn_idx=3, layer_mlp_idx=2, j_loc=5
DEBUG 10-15 15:27:45 lpllm.py:1037] reset layer cost 0.0008220672607421875 s
DEBUG 10-15 15:27:45 lpllm.py:1038] have reset next layer layer_attn 3 layer_mlp 2 
DEBUG 10-15 15:27:45 lpllm.py:1044] j: 5 waiting the layer with layer_idx 3 before wait time 0.45289182662963867 s
INFO 10-15 15:27:45 client.py:117] confirm_model_loaded: Mixtral-8x7B, 4812b1e4-0d46-455e-b30b-1bf3c1760226
INFO 10-15 15:27:45 client.py:125] Model loaded
DEBUG 10-15 15:27:45 lpllm.py:1048] j: load cost 0.4547128677368164 s waiting cost 0.0018012523651123047 s
DEBUG 10-15 15:27:45 lpllm.py:924] 
DEBUG 10-15 15:27:45 lpllm.py:924] decoder loop j: 6 cur_layer_idx: 3 layer_attn_idx: 3 layer_mlp_idx: 2
DEBUG 10-15 15:27:45 lpllm.py:933] start load next layer cur_layer_idx: 4
DEBUG 10-15 15:27:45 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:45 client.py:72] load_into_gpu: Mixtral-8x7B, f2db2468-95ea-4c62-a1f7-4063d5b161f9
INFO 10-15 15:27:45 client.py:113] Model loaded: Mixtral-8x7B, f2db2468-95ea-4c62-a1f7-4063d5b161f9
DEBUG 10-15 15:27:45 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:45 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:45 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:45 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:45 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:45 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:45 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:45 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:45 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:45 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:45 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:45 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:45 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:45 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:45 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:45 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:45 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:45 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:45 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:45 lpllm.py:2265] GPU2CPU move cost 0.000392 seconds
DEBUG 10-15 15:27:45 lpmodule.py:340] layer_idx 3 attn layer_idx: 3
DEBUG 10-15 15:27:45 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:45 lpmodule.py:364] decoder_attn_batch update batch_dim 480-540
DEBUG 10-15 15:27:45 StaticCacheLen.py:84] static cache update layer_idx: 3, start_batch: 480, end_batch: 540
DEBUG 10-15 15:27:45 lpmodule.py:368] update for kv cache 480-540 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:45 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:45 lpmodule.py:374] update past key value cost 0.021978 seconds
DEBUG 10-15 15:27:45 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:45 lpmodule.py:227] layer idx 2
DEBUG 10-15 15:27:45 lpmodule.py:399] repeat qkv cost 0.029598 seconds
DEBUG 10-15 15:27:45 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:45 lpmodule.py:433] dot attn cost 0.033687 seconds
DEBUG 10-15 15:27:45 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:45 lpmodule.py:444] time cost move to cuda:1 0.024176597595214844 s
DEBUG 10-15 15:27:46 lpllm.py:2283] CPU attn cost 0.134719 seconds if batch True
DEBUG 10-15 15:27:46 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:46 lpllm.py:2294] CPU compute cost 0.135339 seconds
DEBUG 10-15 15:27:46 lpllm.py:2312] free cost 0.000076 seconds
DEBUG 10-15 15:27:46 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:46 lpllm.py:1774] update state cost 2.3126602172851562e-05 s
DEBUG 10-15 15:27:46 lpllm.py:1743] restore layer func cost 0.0010523796081542969 s
DEBUG 10-15 15:27:46 lpllm.py:511] restore layer cost 0.0013077259063720703 s
DEBUG 10-15 15:27:46 lpllm.py:512] reset_next_layer_need: layer_attn_idx=3, layer_mlp_idx=3, j_loc=6
DEBUG 10-15 15:27:46 lpllm.py:1037] reset layer cost 0.0013842582702636719 s
DEBUG 10-15 15:27:46 lpllm.py:1038] have reset next layer layer_attn 3 layer_mlp 3 
DEBUG 10-15 15:27:46 lpllm.py:924] 
DEBUG 10-15 15:27:46 lpllm.py:924] decoder loop j: 7 cur_layer_idx: 3 layer_attn_idx: 3 layer_mlp_idx: 3
DEBUG 10-15 15:27:46 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:46 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:46 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:46 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:46 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:46 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:46 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:46 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:46 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:46 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:46 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:46 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:46 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:46 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:46 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:46 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:46 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:46 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:46 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:46 lpllm.py:2265] GPU2CPU move cost 0.000605 seconds
DEBUG 10-15 15:27:46 lpmodule.py:340] layer_idx 3 attn layer_idx: 3
DEBUG 10-15 15:27:46 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:46 lpmodule.py:364] decoder_attn_batch update batch_dim 540-600
DEBUG 10-15 15:27:46 StaticCacheLen.py:84] static cache update layer_idx: 3, start_batch: 540, end_batch: 600
DEBUG 10-15 15:27:46 lpmodule.py:368] update for kv cache 540-600 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:46 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:46 lpmodule.py:374] update past key value cost 0.027113 seconds
DEBUG 10-15 15:27:46 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:46 lpmodule.py:227] layer idx 3
DEBUG 10-15 15:27:46 lpmodule.py:399] repeat qkv cost 0.029709 seconds
DEBUG 10-15 15:27:46 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:46 lpmodule.py:433] dot attn cost 0.034302 seconds
DEBUG 10-15 15:27:46 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:46 lpmodule.py:444] time cost move to cuda:1 0.023311138153076172 s
DEBUG 10-15 15:27:46 lpllm.py:2283] CPU attn cost 0.139157 seconds if batch True
DEBUG 10-15 15:27:46 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:46 lpllm.py:2294] CPU compute cost 0.140062 seconds
DEBUG 10-15 15:27:46 lpllm.py:2312] free cost 0.000072 seconds
DEBUG 10-15 15:27:46 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:46 lpllm.py:1774] update state cost 2.3603439331054688e-05 s
DEBUG 10-15 15:27:46 lpllm.py:1743] restore layer func cost 0.0004277229309082031 s
DEBUG 10-15 15:27:46 lpllm.py:511] restore layer cost 0.0006787776947021484 s
DEBUG 10-15 15:27:46 lpllm.py:512] reset_next_layer_need: layer_attn_idx=4, layer_mlp_idx=3, j_loc=7
DEBUG 10-15 15:27:46 lpllm.py:1037] reset layer cost 0.0007619857788085938 s
DEBUG 10-15 15:27:46 lpllm.py:1038] have reset next layer layer_attn 4 layer_mlp 3 
DEBUG 10-15 15:27:46 lpllm.py:1044] j: 7 waiting the layer with layer_idx 4 before wait time 0.4439876079559326 s
INFO 10-15 15:27:46 client.py:117] confirm_model_loaded: Mixtral-8x7B, f2db2468-95ea-4c62-a1f7-4063d5b161f9
INFO 10-15 15:27:46 client.py:125] Model loaded
DEBUG 10-15 15:27:46 lpllm.py:1048] j: load cost 0.4457402229309082 s waiting cost 0.0017361640930175781 s
DEBUG 10-15 15:27:46 lpllm.py:924] 
DEBUG 10-15 15:27:46 lpllm.py:924] decoder loop j: 8 cur_layer_idx: 4 layer_attn_idx: 4 layer_mlp_idx: 3
DEBUG 10-15 15:27:46 lpllm.py:933] start load next layer cur_layer_idx: 5
DEBUG 10-15 15:27:46 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed650>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x003\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x96\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:46 client.py:72] load_into_gpu: Mixtral-8x7B, 0f4c0696-4c0e-4ffe-9fb9-c5273fe2a340
INFO 10-15 15:27:46 client.py:113] Model loaded: Mixtral-8x7B, 0f4c0696-4c0e-4ffe-9fb9-c5273fe2a340
DEBUG 10-15 15:27:46 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:46 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:46 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:46 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:46 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:46 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:46 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:46 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:46 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:46 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:46 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:46 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:46 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:46 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:46 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:46 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:46 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:46 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:46 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:46 lpllm.py:2265] GPU2CPU move cost 0.000574 seconds
DEBUG 10-15 15:27:46 lpmodule.py:340] layer_idx 4 attn layer_idx: 4
DEBUG 10-15 15:27:46 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:46 lpmodule.py:364] decoder_attn_batch update batch_dim 480-540
DEBUG 10-15 15:27:46 StaticCacheLen.py:84] static cache update layer_idx: 4, start_batch: 480, end_batch: 540
DEBUG 10-15 15:27:46 lpmodule.py:368] update for kv cache 480-540 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:46 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:46 lpmodule.py:374] update past key value cost 0.021637 seconds
DEBUG 10-15 15:27:46 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:46 lpmodule.py:227] layer idx 3
DEBUG 10-15 15:27:46 lpmodule.py:399] repeat qkv cost 0.030199 seconds
DEBUG 10-15 15:27:46 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:46 lpmodule.py:433] dot attn cost 0.040642 seconds
DEBUG 10-15 15:27:46 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:46 lpmodule.py:444] time cost move to cuda:1 0.02610921859741211 s
DEBUG 10-15 15:27:46 lpllm.py:2283] CPU attn cost 0.145963 seconds if batch True
DEBUG 10-15 15:27:46 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:27:46 lpllm.py:2294] CPU compute cost 0.146820 seconds
DEBUG 10-15 15:27:46 lpllm.py:2312] free cost 0.000081 seconds
DEBUG 10-15 15:27:46 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:46 lpllm.py:1774] update state cost 2.1696090698242188e-05 s
DEBUG 10-15 15:27:46 lpllm.py:1743] restore layer func cost 0.000993967056274414 s
DEBUG 10-15 15:27:46 lpllm.py:511] restore layer cost 0.001252889633178711 s
DEBUG 10-15 15:27:46 lpllm.py:512] reset_next_layer_need: layer_attn_idx=4, layer_mlp_idx=4, j_loc=8
DEBUG 10-15 15:27:46 lpllm.py:1037] reset layer cost 0.0013484954833984375 s
DEBUG 10-15 15:27:46 lpllm.py:1038] have reset next layer layer_attn 4 layer_mlp 4 
DEBUG 10-15 15:27:46 lpllm.py:924] 
DEBUG 10-15 15:27:46 lpllm.py:924] decoder loop j: 9 cur_layer_idx: 4 layer_attn_idx: 4 layer_mlp_idx: 4
DEBUG 10-15 15:27:46 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:46 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:46 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:46 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:46 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:46 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:46 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:46 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:46 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:46 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:46 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:46 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:46 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:46 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:46 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:46 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:46 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:46 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:46 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:46 lpllm.py:2265] GPU2CPU move cost 0.000573 seconds
DEBUG 10-15 15:27:46 lpmodule.py:340] layer_idx 4 attn layer_idx: 4
DEBUG 10-15 15:27:46 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:46 lpmodule.py:364] decoder_attn_batch update batch_dim 540-600
DEBUG 10-15 15:27:46 StaticCacheLen.py:84] static cache update layer_idx: 4, start_batch: 540, end_batch: 600
DEBUG 10-15 15:27:46 lpmodule.py:368] update for kv cache 540-600 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:46 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:46 lpmodule.py:374] update past key value cost 0.025190 seconds
DEBUG 10-15 15:27:46 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:46 lpmodule.py:227] layer idx 4
DEBUG 10-15 15:27:46 lpmodule.py:399] repeat qkv cost 0.029806 seconds
DEBUG 10-15 15:27:46 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:46 lpmodule.py:433] dot attn cost 0.036010 seconds
DEBUG 10-15 15:27:46 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:46 lpmodule.py:444] time cost move to cuda:1 0.023695945739746094 s
DEBUG 10-15 15:27:46 lpllm.py:2283] CPU attn cost 0.139127 seconds if batch True
DEBUG 10-15 15:27:46 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:46 lpllm.py:2294] CPU compute cost 0.139984 seconds
DEBUG 10-15 15:27:46 lpllm.py:2312] free cost 0.000075 seconds
DEBUG 10-15 15:27:46 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:46 lpllm.py:1774] update state cost 2.4557113647460938e-05 s
DEBUG 10-15 15:27:46 lpllm.py:1743] restore layer func cost 0.00043487548828125 s
DEBUG 10-15 15:27:46 lpllm.py:511] restore layer cost 0.0006706714630126953 s
DEBUG 10-15 15:27:46 lpllm.py:512] reset_next_layer_need: layer_attn_idx=5, layer_mlp_idx=4, j_loc=9
DEBUG 10-15 15:27:46 lpllm.py:1037] reset layer cost 0.0007507801055908203 s
DEBUG 10-15 15:27:46 lpllm.py:1038] have reset next layer layer_attn 5 layer_mlp 4 
DEBUG 10-15 15:27:46 lpllm.py:1044] j: 9 waiting the layer with layer_idx 5 before wait time 0.4459850788116455 s
INFO 10-15 15:27:46 client.py:117] confirm_model_loaded: Mixtral-8x7B, 0f4c0696-4c0e-4ffe-9fb9-c5273fe2a340
INFO 10-15 15:27:46 client.py:125] Model loaded
DEBUG 10-15 15:27:46 lpllm.py:1048] j: load cost 0.44770383834838867 s waiting cost 0.0017032623291015625 s
DEBUG 10-15 15:27:46 lpllm.py:924] 
DEBUG 10-15 15:27:46 lpllm.py:924] decoder loop j: 10 cur_layer_idx: 5 layer_attn_idx: 5 layer_mlp_idx: 4
DEBUG 10-15 15:27:46 lpllm.py:933] start load next layer cur_layer_idx: 6
DEBUG 10-15 15:27:46 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed6e0>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x004\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x98\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:46 client.py:72] load_into_gpu: Mixtral-8x7B, a053ed87-4d3a-4aae-8b70-78b06ad504c5
INFO 10-15 15:27:46 client.py:113] Model loaded: Mixtral-8x7B, a053ed87-4d3a-4aae-8b70-78b06ad504c5
DEBUG 10-15 15:27:46 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:46 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:46 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:46 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:46 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:46 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:46 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:46 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:46 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:46 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:46 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:46 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:46 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:46 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:46 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:46 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:46 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:46 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:46 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:46 lpllm.py:2265] GPU2CPU move cost 0.000621 seconds
DEBUG 10-15 15:27:46 lpmodule.py:340] layer_idx 5 attn layer_idx: 5
DEBUG 10-15 15:27:46 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:46 lpmodule.py:364] decoder_attn_batch update batch_dim 480-540
DEBUG 10-15 15:27:46 StaticCacheLen.py:84] static cache update layer_idx: 5, start_batch: 480, end_batch: 540
DEBUG 10-15 15:27:46 lpmodule.py:368] update for kv cache 480-540 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:46 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:46 lpmodule.py:374] update past key value cost 0.022943 seconds
DEBUG 10-15 15:27:46 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:46 lpmodule.py:227] layer idx 4
DEBUG 10-15 15:27:46 lpmodule.py:399] repeat qkv cost 0.030187 seconds
DEBUG 10-15 15:27:46 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:46 lpmodule.py:433] dot attn cost 0.039737 seconds
DEBUG 10-15 15:27:46 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:46 lpmodule.py:444] time cost move to cuda:1 0.023528575897216797 s
DEBUG 10-15 15:27:46 lpllm.py:2283] CPU attn cost 0.140981 seconds if batch True
DEBUG 10-15 15:27:46 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:27:46 lpllm.py:2294] CPU compute cost 0.141887 seconds
DEBUG 10-15 15:27:46 lpllm.py:2312] free cost 0.000073 seconds
DEBUG 10-15 15:27:46 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:46 lpllm.py:1774] update state cost 2.4318695068359375e-05 s
DEBUG 10-15 15:27:46 lpllm.py:1743] restore layer func cost 0.000997781753540039 s
DEBUG 10-15 15:27:46 lpllm.py:511] restore layer cost 0.0012519359588623047 s
DEBUG 10-15 15:27:46 lpllm.py:512] reset_next_layer_need: layer_attn_idx=5, layer_mlp_idx=5, j_loc=10
DEBUG 10-15 15:27:46 lpllm.py:1037] reset layer cost 0.0013468265533447266 s
DEBUG 10-15 15:27:46 lpllm.py:1038] have reset next layer layer_attn 5 layer_mlp 5 
DEBUG 10-15 15:27:46 lpllm.py:924] 
DEBUG 10-15 15:27:46 lpllm.py:924] decoder loop j: 11 cur_layer_idx: 5 layer_attn_idx: 5 layer_mlp_idx: 5
DEBUG 10-15 15:27:46 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:46 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:46 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:46 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:46 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:46 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:46 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:46 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:46 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:46 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:46 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:46 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:46 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:46 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:46 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:46 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:46 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:46 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:46 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:46 lpllm.py:2265] GPU2CPU move cost 0.000571 seconds
DEBUG 10-15 15:27:46 lpmodule.py:340] layer_idx 5 attn layer_idx: 5
DEBUG 10-15 15:27:46 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:46 lpmodule.py:364] decoder_attn_batch update batch_dim 540-600
DEBUG 10-15 15:27:47 StaticCacheLen.py:84] static cache update layer_idx: 5, start_batch: 540, end_batch: 600
DEBUG 10-15 15:27:47 lpmodule.py:368] update for kv cache 540-600 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:47 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:47 lpmodule.py:374] update past key value cost 0.027931 seconds
DEBUG 10-15 15:27:47 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:47 lpmodule.py:227] layer idx 5
DEBUG 10-15 15:27:47 lpmodule.py:399] repeat qkv cost 0.029750 seconds
DEBUG 10-15 15:27:47 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:47 lpmodule.py:433] dot attn cost 0.045022 seconds
DEBUG 10-15 15:27:47 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:47 lpmodule.py:444] time cost move to cuda:1 0.022458553314208984 s
DEBUG 10-15 15:27:47 lpllm.py:2283] CPU attn cost 0.148634 seconds if batch True
DEBUG 10-15 15:27:47 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:47 lpllm.py:2294] CPU compute cost 0.149494 seconds
DEBUG 10-15 15:27:47 lpllm.py:2312] free cost 0.000072 seconds
DEBUG 10-15 15:27:47 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:47 lpllm.py:1774] update state cost 2.4557113647460938e-05 s
DEBUG 10-15 15:27:47 lpllm.py:1743] restore layer func cost 0.00045228004455566406 s
DEBUG 10-15 15:27:47 lpllm.py:511] restore layer cost 0.0006854534149169922 s
DEBUG 10-15 15:27:47 lpllm.py:512] reset_next_layer_need: layer_attn_idx=6, layer_mlp_idx=5, j_loc=11
DEBUG 10-15 15:27:47 lpllm.py:1037] reset layer cost 0.0007624626159667969 s
DEBUG 10-15 15:27:47 lpllm.py:1038] have reset next layer layer_attn 6 layer_mlp 5 
DEBUG 10-15 15:27:47 lpllm.py:1044] j: 11 waiting the layer with layer_idx 6 before wait time 0.45528078079223633 s
INFO 10-15 15:27:47 client.py:117] confirm_model_loaded: Mixtral-8x7B, a053ed87-4d3a-4aae-8b70-78b06ad504c5
INFO 10-15 15:27:47 client.py:125] Model loaded
DEBUG 10-15 15:27:47 lpllm.py:1048] j: load cost 0.45696139335632324 s waiting cost 0.0016646385192871094 s
DEBUG 10-15 15:27:47 lpllm.py:924] 
DEBUG 10-15 15:27:47 lpllm.py:924] decoder loop j: 12 cur_layer_idx: 6 layer_attn_idx: 6 layer_mlp_idx: 5
DEBUG 10-15 15:27:47 lpllm.py:933] start load next layer cur_layer_idx: 7
DEBUG 10-15 15:27:47 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:47 client.py:72] load_into_gpu: Mixtral-8x7B, fdfe36ed-e31b-48be-8514-42c97bd90da8
INFO 10-15 15:27:47 client.py:113] Model loaded: Mixtral-8x7B, fdfe36ed-e31b-48be-8514-42c97bd90da8
DEBUG 10-15 15:27:47 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:47 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:47 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:47 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:47 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:47 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:47 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:47 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:47 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:47 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:47 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:47 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:47 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:47 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:47 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:47 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:47 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:47 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:47 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:47 lpllm.py:2265] GPU2CPU move cost 0.000607 seconds
DEBUG 10-15 15:27:47 lpmodule.py:340] layer_idx 6 attn layer_idx: 6
DEBUG 10-15 15:27:47 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:47 lpmodule.py:364] decoder_attn_batch update batch_dim 480-540
DEBUG 10-15 15:27:47 StaticCacheLen.py:84] static cache update layer_idx: 6, start_batch: 480, end_batch: 540
DEBUG 10-15 15:27:47 lpmodule.py:368] update for kv cache 480-540 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:47 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:47 lpmodule.py:374] update past key value cost 0.023020 seconds
DEBUG 10-15 15:27:47 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:47 lpmodule.py:227] layer idx 5
DEBUG 10-15 15:27:47 lpmodule.py:399] repeat qkv cost 0.030466 seconds
DEBUG 10-15 15:27:47 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:47 lpmodule.py:433] dot attn cost 0.036933 seconds
DEBUG 10-15 15:27:47 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:47 lpmodule.py:444] time cost move to cuda:1 0.02325606346130371 s
DEBUG 10-15 15:27:47 lpllm.py:2283] CPU attn cost 0.138443 seconds if batch True
DEBUG 10-15 15:27:47 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:47 lpllm.py:2294] CPU compute cost 0.139344 seconds
DEBUG 10-15 15:27:47 lpllm.py:2312] free cost 0.000089 seconds
DEBUG 10-15 15:27:47 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:47 lpllm.py:1774] update state cost 2.2649765014648438e-05 s
DEBUG 10-15 15:27:47 lpllm.py:1743] restore layer func cost 0.001007080078125 s
DEBUG 10-15 15:27:47 lpllm.py:511] restore layer cost 0.0012469291687011719 s
DEBUG 10-15 15:27:47 lpllm.py:512] reset_next_layer_need: layer_attn_idx=6, layer_mlp_idx=6, j_loc=12
DEBUG 10-15 15:27:47 lpllm.py:1037] reset layer cost 0.001340627670288086 s
DEBUG 10-15 15:27:47 lpllm.py:1038] have reset next layer layer_attn 6 layer_mlp 6 
DEBUG 10-15 15:27:47 lpllm.py:924] 
DEBUG 10-15 15:27:47 lpllm.py:924] decoder loop j: 13 cur_layer_idx: 6 layer_attn_idx: 6 layer_mlp_idx: 6
DEBUG 10-15 15:27:47 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:47 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:47 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:47 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:47 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:47 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:47 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:47 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:47 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:47 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:47 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:47 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:47 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:47 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:47 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:47 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:47 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:47 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:47 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:47 lpllm.py:2265] GPU2CPU move cost 0.000579 seconds
DEBUG 10-15 15:27:47 lpmodule.py:340] layer_idx 6 attn layer_idx: 6
DEBUG 10-15 15:27:47 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:47 lpmodule.py:364] decoder_attn_batch update batch_dim 540-600
DEBUG 10-15 15:27:47 StaticCacheLen.py:84] static cache update layer_idx: 6, start_batch: 540, end_batch: 600
DEBUG 10-15 15:27:47 lpmodule.py:368] update for kv cache 540-600 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:47 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:47 lpmodule.py:374] update past key value cost 0.027041 seconds
DEBUG 10-15 15:27:47 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:47 lpmodule.py:227] layer idx 6
DEBUG 10-15 15:27:47 lpmodule.py:399] repeat qkv cost 0.031207 seconds
DEBUG 10-15 15:27:47 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:47 lpmodule.py:433] dot attn cost 0.037135 seconds
DEBUG 10-15 15:27:47 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:47 lpmodule.py:444] time cost move to cuda:1 0.023602962493896484 s
DEBUG 10-15 15:27:47 lpllm.py:2283] CPU attn cost 0.143576 seconds if batch True
DEBUG 10-15 15:27:47 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:47 lpllm.py:2294] CPU compute cost 0.144431 seconds
DEBUG 10-15 15:27:47 lpllm.py:2312] free cost 0.000072 seconds
DEBUG 10-15 15:27:47 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:47 lpllm.py:1774] update state cost 2.4557113647460938e-05 s
DEBUG 10-15 15:27:47 lpllm.py:1743] restore layer func cost 0.0004544258117675781 s
DEBUG 10-15 15:27:47 lpllm.py:511] restore layer cost 0.0006961822509765625 s
DEBUG 10-15 15:27:47 lpllm.py:512] reset_next_layer_need: layer_attn_idx=7, layer_mlp_idx=6, j_loc=13
DEBUG 10-15 15:27:47 lpllm.py:1037] reset layer cost 0.0007877349853515625 s
DEBUG 10-15 15:27:47 lpllm.py:1038] have reset next layer layer_attn 7 layer_mlp 6 
DEBUG 10-15 15:27:47 lpllm.py:1044] j: 13 waiting the layer with layer_idx 7 before wait time 0.44843149185180664 s
INFO 10-15 15:27:47 client.py:117] confirm_model_loaded: Mixtral-8x7B, fdfe36ed-e31b-48be-8514-42c97bd90da8
INFO 10-15 15:27:47 client.py:125] Model loaded
DEBUG 10-15 15:27:47 lpllm.py:1048] j: load cost 0.4501650333404541 s waiting cost 0.0017163753509521484 s
DEBUG 10-15 15:27:47 lpllm.py:924] 
DEBUG 10-15 15:27:47 lpllm.py:924] decoder loop j: 14 cur_layer_idx: 7 layer_attn_idx: 7 layer_mlp_idx: 6
DEBUG 10-15 15:27:47 lpllm.py:933] start load next layer cur_layer_idx: 8
DEBUG 10-15 15:27:47 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed650>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x003\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x96\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:47 client.py:72] load_into_gpu: Mixtral-8x7B, 950fc2b3-be34-42c1-b154-151e11f22fe7
INFO 10-15 15:27:47 client.py:113] Model loaded: Mixtral-8x7B, 950fc2b3-be34-42c1-b154-151e11f22fe7
DEBUG 10-15 15:27:47 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:47 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:47 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:47 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:47 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:47 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:47 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:47 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:47 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:47 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:47 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:47 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:47 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:47 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:47 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:47 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:47 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:47 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:47 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:47 lpllm.py:2265] GPU2CPU move cost 0.000570 seconds
DEBUG 10-15 15:27:47 lpmodule.py:340] layer_idx 7 attn layer_idx: 7
DEBUG 10-15 15:27:47 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:47 lpmodule.py:364] decoder_attn_batch update batch_dim 480-540
DEBUG 10-15 15:27:47 StaticCacheLen.py:84] static cache update layer_idx: 7, start_batch: 480, end_batch: 540
DEBUG 10-15 15:27:47 lpmodule.py:368] update for kv cache 480-540 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:47 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:47 lpmodule.py:374] update past key value cost 0.021883 seconds
DEBUG 10-15 15:27:47 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:47 lpmodule.py:227] layer idx 6
DEBUG 10-15 15:27:47 lpmodule.py:399] repeat qkv cost 0.030548 seconds
DEBUG 10-15 15:27:47 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:47 lpmodule.py:433] dot attn cost 0.036569 seconds
DEBUG 10-15 15:27:47 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:47 lpmodule.py:444] time cost move to cuda:1 0.022820472717285156 s
DEBUG 10-15 15:27:47 lpllm.py:2283] CPU attn cost 0.136144 seconds if batch True
DEBUG 10-15 15:27:47 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:47 lpllm.py:2294] CPU compute cost 0.137025 seconds
DEBUG 10-15 15:27:47 lpllm.py:2312] free cost 0.000074 seconds
DEBUG 10-15 15:27:47 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:47 lpllm.py:1774] update state cost 2.5033950805664062e-05 s
DEBUG 10-15 15:27:47 lpllm.py:1743] restore layer func cost 0.0010340213775634766 s
DEBUG 10-15 15:27:47 lpllm.py:511] restore layer cost 0.0012924671173095703 s
DEBUG 10-15 15:27:47 lpllm.py:512] reset_next_layer_need: layer_attn_idx=7, layer_mlp_idx=7, j_loc=14
DEBUG 10-15 15:27:47 lpllm.py:1037] reset layer cost 0.0014078617095947266 s
DEBUG 10-15 15:27:47 lpllm.py:1038] have reset next layer layer_attn 7 layer_mlp 7 
DEBUG 10-15 15:27:47 lpllm.py:924] 
DEBUG 10-15 15:27:47 lpllm.py:924] decoder loop j: 15 cur_layer_idx: 7 layer_attn_idx: 7 layer_mlp_idx: 7
DEBUG 10-15 15:27:47 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:47 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:47 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:47 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:47 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:47 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:47 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:47 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:47 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:47 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:47 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:47 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:47 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:47 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:47 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:47 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:47 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:47 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:47 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:47 lpllm.py:2265] GPU2CPU move cost 0.000621 seconds
DEBUG 10-15 15:27:47 lpmodule.py:340] layer_idx 7 attn layer_idx: 7
DEBUG 10-15 15:27:47 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:47 lpmodule.py:364] decoder_attn_batch update batch_dim 540-600
DEBUG 10-15 15:27:47 StaticCacheLen.py:84] static cache update layer_idx: 7, start_batch: 540, end_batch: 600
DEBUG 10-15 15:27:47 lpmodule.py:368] update for kv cache 540-600 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:47 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:47 lpmodule.py:374] update past key value cost 0.026125 seconds
DEBUG 10-15 15:27:47 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:47 lpmodule.py:227] layer idx 7
DEBUG 10-15 15:27:47 lpmodule.py:399] repeat qkv cost 0.029198 seconds
DEBUG 10-15 15:27:47 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:47 lpmodule.py:433] dot attn cost 0.035573 seconds
DEBUG 10-15 15:27:47 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:48 lpmodule.py:444] time cost move to cuda:1 0.023305892944335938 s
DEBUG 10-15 15:27:48 lpllm.py:2283] CPU attn cost 0.138124 seconds if batch True
DEBUG 10-15 15:27:48 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:48 lpllm.py:2294] CPU compute cost 0.139025 seconds
DEBUG 10-15 15:27:48 lpllm.py:2312] free cost 0.000071 seconds
DEBUG 10-15 15:27:48 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:48 lpllm.py:1774] update state cost 3.0040740966796875e-05 s
DEBUG 10-15 15:27:48 lpllm.py:1743] restore layer func cost 0.0004646778106689453 s
DEBUG 10-15 15:27:48 lpllm.py:511] restore layer cost 0.0007336139678955078 s
DEBUG 10-15 15:27:48 lpllm.py:512] reset_next_layer_need: layer_attn_idx=8, layer_mlp_idx=7, j_loc=15
DEBUG 10-15 15:27:48 lpllm.py:1037] reset layer cost 0.0008258819580078125 s
DEBUG 10-15 15:27:48 lpllm.py:1038] have reset next layer layer_attn 8 layer_mlp 7 
DEBUG 10-15 15:27:48 lpllm.py:1044] j: 15 waiting the layer with layer_idx 8 before wait time 0.44637107849121094 s
INFO 10-15 15:27:48 client.py:117] confirm_model_loaded: Mixtral-8x7B, 950fc2b3-be34-42c1-b154-151e11f22fe7
INFO 10-15 15:27:48 client.py:125] Model loaded
DEBUG 10-15 15:27:48 lpllm.py:1048] j: load cost 0.4480855464935303 s waiting cost 0.0016932487487792969 s
DEBUG 10-15 15:27:48 lpllm.py:924] 
DEBUG 10-15 15:27:48 lpllm.py:924] decoder loop j: 16 cur_layer_idx: 8 layer_attn_idx: 8 layer_mlp_idx: 7
DEBUG 10-15 15:27:48 lpllm.py:933] start load next layer cur_layer_idx: 9
DEBUG 10-15 15:27:48 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed6e0>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x004\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x98\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:48 client.py:72] load_into_gpu: Mixtral-8x7B, 8fa86a1e-b66d-4b04-a296-56db77cbb9d2
INFO 10-15 15:27:48 client.py:113] Model loaded: Mixtral-8x7B, 8fa86a1e-b66d-4b04-a296-56db77cbb9d2
DEBUG 10-15 15:27:48 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:48 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:48 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:48 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:48 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:48 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:48 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:48 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:48 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:48 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:48 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:48 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:48 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:48 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:48 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:48 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:48 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:48 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:48 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:48 lpllm.py:2265] GPU2CPU move cost 0.000427 seconds
DEBUG 10-15 15:27:48 lpmodule.py:340] layer_idx 8 attn layer_idx: 8
DEBUG 10-15 15:27:48 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:48 lpmodule.py:364] decoder_attn_batch update batch_dim 480-540
DEBUG 10-15 15:27:48 StaticCacheLen.py:84] static cache update layer_idx: 8, start_batch: 480, end_batch: 540
DEBUG 10-15 15:27:48 lpmodule.py:368] update for kv cache 480-540 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:48 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:48 lpmodule.py:374] update past key value cost 0.020823 seconds
DEBUG 10-15 15:27:48 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:48 lpmodule.py:227] layer idx 7
DEBUG 10-15 15:27:48 lpmodule.py:399] repeat qkv cost 0.029999 seconds
DEBUG 10-15 15:27:48 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:48 lpmodule.py:433] dot attn cost 0.033498 seconds
DEBUG 10-15 15:27:48 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:48 lpmodule.py:444] time cost move to cuda:1 0.02354884147644043 s
DEBUG 10-15 15:27:48 lpllm.py:2283] CPU attn cost 0.133486 seconds if batch True
DEBUG 10-15 15:27:48 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:48 lpllm.py:2294] CPU compute cost 0.134159 seconds
DEBUG 10-15 15:27:48 lpllm.py:2312] free cost 0.000076 seconds
DEBUG 10-15 15:27:48 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:48 lpllm.py:1774] update state cost 2.7179718017578125e-05 s
DEBUG 10-15 15:27:48 lpllm.py:1743] restore layer func cost 0.0009534358978271484 s
DEBUG 10-15 15:27:48 lpllm.py:511] restore layer cost 0.0012249946594238281 s
DEBUG 10-15 15:27:48 lpllm.py:512] reset_next_layer_need: layer_attn_idx=8, layer_mlp_idx=8, j_loc=16
DEBUG 10-15 15:27:48 lpllm.py:1037] reset layer cost 0.001317739486694336 s
DEBUG 10-15 15:27:48 lpllm.py:1038] have reset next layer layer_attn 8 layer_mlp 8 
DEBUG 10-15 15:27:48 lpllm.py:924] 
DEBUG 10-15 15:27:48 lpllm.py:924] decoder loop j: 17 cur_layer_idx: 8 layer_attn_idx: 8 layer_mlp_idx: 8
DEBUG 10-15 15:27:48 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:48 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:48 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:48 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:48 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:48 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:48 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:48 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:48 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:48 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:48 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:48 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:48 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:48 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:48 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:48 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:48 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:48 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:48 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:48 lpllm.py:2265] GPU2CPU move cost 0.000644 seconds
DEBUG 10-15 15:27:48 lpmodule.py:340] layer_idx 8 attn layer_idx: 8
DEBUG 10-15 15:27:48 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:48 lpmodule.py:364] decoder_attn_batch update batch_dim 540-600
DEBUG 10-15 15:27:48 StaticCacheLen.py:84] static cache update layer_idx: 8, start_batch: 540, end_batch: 600
DEBUG 10-15 15:27:48 lpmodule.py:368] update for kv cache 540-600 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:48 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:48 lpmodule.py:374] update past key value cost 0.027888 seconds
DEBUG 10-15 15:27:48 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:48 lpmodule.py:227] layer idx 8
DEBUG 10-15 15:27:48 lpmodule.py:399] repeat qkv cost 0.028104 seconds
DEBUG 10-15 15:27:48 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:48 lpmodule.py:433] dot attn cost 0.034540 seconds
DEBUG 10-15 15:27:48 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:48 lpmodule.py:444] time cost move to cuda:1 0.023562908172607422 s
DEBUG 10-15 15:27:48 lpllm.py:2283] CPU attn cost 0.139393 seconds if batch True
DEBUG 10-15 15:27:48 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:48 lpllm.py:2294] CPU compute cost 0.140331 seconds
DEBUG 10-15 15:27:48 lpllm.py:2312] free cost 0.000074 seconds
DEBUG 10-15 15:27:48 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:48 lpllm.py:1774] update state cost 2.5272369384765625e-05 s
DEBUG 10-15 15:27:48 lpllm.py:1743] restore layer func cost 0.0004494190216064453 s
DEBUG 10-15 15:27:48 lpllm.py:511] restore layer cost 0.0006842613220214844 s
DEBUG 10-15 15:27:48 lpllm.py:512] reset_next_layer_need: layer_attn_idx=9, layer_mlp_idx=8, j_loc=17
DEBUG 10-15 15:27:48 lpllm.py:1037] reset layer cost 0.0007774829864501953 s
DEBUG 10-15 15:27:48 lpllm.py:1038] have reset next layer layer_attn 9 layer_mlp 8 
DEBUG 10-15 15:27:48 lpllm.py:1044] j: 17 waiting the layer with layer_idx 9 before wait time 0.4431643486022949 s
INFO 10-15 15:27:48 client.py:117] confirm_model_loaded: Mixtral-8x7B, 8fa86a1e-b66d-4b04-a296-56db77cbb9d2
INFO 10-15 15:27:48 client.py:125] Model loaded
DEBUG 10-15 15:27:48 lpllm.py:1048] j: load cost 0.44490861892700195 s waiting cost 0.0017275810241699219 s
DEBUG 10-15 15:27:48 lpllm.py:924] 
DEBUG 10-15 15:27:48 lpllm.py:924] decoder loop j: 18 cur_layer_idx: 9 layer_attn_idx: 9 layer_mlp_idx: 8
DEBUG 10-15 15:27:48 lpllm.py:933] start load next layer cur_layer_idx: 10
DEBUG 10-15 15:27:48 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:48 client.py:72] load_into_gpu: Mixtral-8x7B, 84295578-9a6a-444d-b369-f9decdacda6d
INFO 10-15 15:27:48 client.py:113] Model loaded: Mixtral-8x7B, 84295578-9a6a-444d-b369-f9decdacda6d
DEBUG 10-15 15:27:48 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:48 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:48 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:48 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:48 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:48 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:48 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:48 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:48 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:48 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:48 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:48 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:48 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:48 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:48 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:48 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:48 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:48 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:48 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:48 lpllm.py:2265] GPU2CPU move cost 0.000578 seconds
DEBUG 10-15 15:27:48 lpmodule.py:340] layer_idx 9 attn layer_idx: 9
DEBUG 10-15 15:27:48 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:48 lpmodule.py:364] decoder_attn_batch update batch_dim 480-540
DEBUG 10-15 15:27:48 StaticCacheLen.py:84] static cache update layer_idx: 9, start_batch: 480, end_batch: 540
DEBUG 10-15 15:27:48 lpmodule.py:368] update for kv cache 480-540 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:48 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:48 lpmodule.py:374] update past key value cost 0.023081 seconds
DEBUG 10-15 15:27:48 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:48 lpmodule.py:227] layer idx 8
DEBUG 10-15 15:27:48 lpmodule.py:399] repeat qkv cost 0.029216 seconds
DEBUG 10-15 15:27:48 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:48 lpmodule.py:433] dot attn cost 0.034741 seconds
DEBUG 10-15 15:27:48 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:48 lpmodule.py:444] time cost move to cuda:1 0.02328181266784668 s
DEBUG 10-15 15:27:48 lpllm.py:2283] CPU attn cost 0.134551 seconds if batch True
DEBUG 10-15 15:27:48 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:48 lpllm.py:2294] CPU compute cost 0.135416 seconds
DEBUG 10-15 15:27:48 lpllm.py:2312] free cost 0.000072 seconds
DEBUG 10-15 15:27:48 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:48 lpllm.py:1774] update state cost 2.1219253540039062e-05 s
DEBUG 10-15 15:27:48 lpllm.py:1743] restore layer func cost 0.0009753704071044922 s
DEBUG 10-15 15:27:48 lpllm.py:511] restore layer cost 0.0012142658233642578 s
DEBUG 10-15 15:27:48 lpllm.py:512] reset_next_layer_need: layer_attn_idx=9, layer_mlp_idx=9, j_loc=18
DEBUG 10-15 15:27:48 lpllm.py:1037] reset layer cost 0.0012900829315185547 s
DEBUG 10-15 15:27:48 lpllm.py:1038] have reset next layer layer_attn 9 layer_mlp 9 
DEBUG 10-15 15:27:48 lpllm.py:924] 
DEBUG 10-15 15:27:48 lpllm.py:924] decoder loop j: 19 cur_layer_idx: 9 layer_attn_idx: 9 layer_mlp_idx: 9
DEBUG 10-15 15:27:48 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:48 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:48 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:48 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:48 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:48 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:48 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:48 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:48 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:48 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:48 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:48 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:48 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:48 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:48 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:48 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:48 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:48 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:48 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:48 lpllm.py:2265] GPU2CPU move cost 0.000567 seconds
DEBUG 10-15 15:27:48 lpmodule.py:340] layer_idx 9 attn layer_idx: 9
DEBUG 10-15 15:27:48 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:48 lpmodule.py:364] decoder_attn_batch update batch_dim 540-600
DEBUG 10-15 15:27:48 StaticCacheLen.py:84] static cache update layer_idx: 9, start_batch: 540, end_batch: 600
DEBUG 10-15 15:27:48 lpmodule.py:368] update for kv cache 540-600 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:48 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:48 lpmodule.py:374] update past key value cost 0.025095 seconds
DEBUG 10-15 15:27:48 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:48 lpmodule.py:227] layer idx 9
DEBUG 10-15 15:27:48 lpmodule.py:399] repeat qkv cost 0.027982 seconds
DEBUG 10-15 15:27:48 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:48 lpmodule.py:433] dot attn cost 0.039444 seconds
DEBUG 10-15 15:27:48 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:48 lpmodule.py:444] time cost move to cuda:1 0.023409605026245117 s
DEBUG 10-15 15:27:48 lpllm.py:2283] CPU attn cost 0.140000 seconds if batch True
DEBUG 10-15 15:27:48 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:48 lpllm.py:2294] CPU compute cost 0.140851 seconds
DEBUG 10-15 15:27:48 lpllm.py:2312] free cost 0.000073 seconds
DEBUG 10-15 15:27:48 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:48 lpllm.py:1774] update state cost 2.5987625122070312e-05 s
DEBUG 10-15 15:27:48 lpllm.py:1743] restore layer func cost 0.00043392181396484375 s
DEBUG 10-15 15:27:48 lpllm.py:511] restore layer cost 0.0006728172302246094 s
DEBUG 10-15 15:27:48 lpllm.py:512] reset_next_layer_need: layer_attn_idx=10, layer_mlp_idx=9, j_loc=19
DEBUG 10-15 15:27:48 lpllm.py:1037] reset layer cost 0.0007479190826416016 s
DEBUG 10-15 15:27:48 lpllm.py:1038] have reset next layer layer_attn 10 layer_mlp 9 
DEBUG 10-15 15:27:48 lpllm.py:1044] j: 19 waiting the layer with layer_idx 10 before wait time 0.4504516124725342 s
INFO 10-15 15:27:48 client.py:117] confirm_model_loaded: Mixtral-8x7B, 84295578-9a6a-444d-b369-f9decdacda6d
INFO 10-15 15:27:48 client.py:125] Model loaded
DEBUG 10-15 15:27:48 lpllm.py:1048] j: load cost 0.45217156410217285 s waiting cost 0.0017049312591552734 s
DEBUG 10-15 15:27:48 lpllm.py:924] 
DEBUG 10-15 15:27:48 lpllm.py:924] decoder loop j: 20 cur_layer_idx: 10 layer_attn_idx: 10 layer_mlp_idx: 9
DEBUG 10-15 15:27:48 lpllm.py:933] start load next layer cur_layer_idx: 11
DEBUG 10-15 15:27:48 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed650>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x003\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x96\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:48 client.py:72] load_into_gpu: Mixtral-8x7B, b1cdb6f8-7461-4d97-af51-adb4c3edb69a
INFO 10-15 15:27:49 client.py:113] Model loaded: Mixtral-8x7B, b1cdb6f8-7461-4d97-af51-adb4c3edb69a
DEBUG 10-15 15:27:49 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:49 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:49 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:49 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:49 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:49 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:49 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:49 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:49 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:49 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:49 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:49 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:49 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:49 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:49 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:49 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:49 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:49 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:49 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:49 lpllm.py:2265] GPU2CPU move cost 0.000616 seconds
DEBUG 10-15 15:27:49 lpmodule.py:340] layer_idx 10 attn layer_idx: 10
DEBUG 10-15 15:27:49 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:49 lpmodule.py:364] decoder_attn_batch update batch_dim 480-540
DEBUG 10-15 15:27:49 StaticCacheLen.py:84] static cache update layer_idx: 10, start_batch: 480, end_batch: 540
DEBUG 10-15 15:27:49 lpmodule.py:368] update for kv cache 480-540 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:49 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:49 lpmodule.py:374] update past key value cost 0.023673 seconds
DEBUG 10-15 15:27:49 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:49 lpmodule.py:227] layer idx 9
DEBUG 10-15 15:27:49 lpmodule.py:399] repeat qkv cost 0.030142 seconds
DEBUG 10-15 15:27:49 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:49 lpmodule.py:433] dot attn cost 0.036647 seconds
DEBUG 10-15 15:27:49 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:49 lpmodule.py:444] time cost move to cuda:1 0.026047229766845703 s
DEBUG 10-15 15:27:49 lpllm.py:2283] CPU attn cost 0.140394 seconds if batch True
DEBUG 10-15 15:27:49 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:27:49 lpllm.py:2294] CPU compute cost 0.141298 seconds
DEBUG 10-15 15:27:49 lpllm.py:2312] free cost 0.000077 seconds
DEBUG 10-15 15:27:49 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:49 lpllm.py:1774] update state cost 2.2649765014648438e-05 s
DEBUG 10-15 15:27:49 lpllm.py:1743] restore layer func cost 0.0010042190551757812 s
DEBUG 10-15 15:27:49 lpllm.py:511] restore layer cost 0.001247406005859375 s
DEBUG 10-15 15:27:49 lpllm.py:512] reset_next_layer_need: layer_attn_idx=10, layer_mlp_idx=10, j_loc=20
DEBUG 10-15 15:27:49 lpllm.py:1037] reset layer cost 0.0013396739959716797 s
DEBUG 10-15 15:27:49 lpllm.py:1038] have reset next layer layer_attn 10 layer_mlp 10 
DEBUG 10-15 15:27:49 lpllm.py:924] 
DEBUG 10-15 15:27:49 lpllm.py:924] decoder loop j: 21 cur_layer_idx: 10 layer_attn_idx: 10 layer_mlp_idx: 10
DEBUG 10-15 15:27:49 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:49 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:49 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:49 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:49 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:49 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:49 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:49 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:49 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:49 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:49 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:49 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:49 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:49 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:49 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:49 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:49 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:49 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:49 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:49 lpllm.py:2265] GPU2CPU move cost 0.000594 seconds
DEBUG 10-15 15:27:49 lpmodule.py:340] layer_idx 10 attn layer_idx: 10
DEBUG 10-15 15:27:49 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:49 lpmodule.py:364] decoder_attn_batch update batch_dim 540-600
DEBUG 10-15 15:27:49 StaticCacheLen.py:84] static cache update layer_idx: 10, start_batch: 540, end_batch: 600
DEBUG 10-15 15:27:49 lpmodule.py:368] update for kv cache 540-600 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:49 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:49 lpmodule.py:374] update past key value cost 0.026235 seconds
DEBUG 10-15 15:27:49 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:49 lpmodule.py:227] layer idx 10
DEBUG 10-15 15:27:49 lpmodule.py:399] repeat qkv cost 0.028689 seconds
DEBUG 10-15 15:27:49 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:49 lpmodule.py:433] dot attn cost 0.036168 seconds
DEBUG 10-15 15:27:49 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:49 lpmodule.py:444] time cost move to cuda:1 0.026400327682495117 s
DEBUG 10-15 15:27:49 lpllm.py:2283] CPU attn cost 0.141697 seconds if batch True
DEBUG 10-15 15:27:49 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:49 lpllm.py:2294] CPU compute cost 0.142572 seconds
DEBUG 10-15 15:27:49 lpllm.py:2312] free cost 0.000075 seconds
DEBUG 10-15 15:27:49 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:49 lpllm.py:1774] update state cost 2.47955322265625e-05 s
DEBUG 10-15 15:27:49 lpllm.py:1743] restore layer func cost 0.0004382133483886719 s
DEBUG 10-15 15:27:49 lpllm.py:511] restore layer cost 0.0006799697875976562 s
DEBUG 10-15 15:27:49 lpllm.py:512] reset_next_layer_need: layer_attn_idx=11, layer_mlp_idx=10, j_loc=21
DEBUG 10-15 15:27:49 lpllm.py:1037] reset layer cost 0.0007719993591308594 s
DEBUG 10-15 15:27:49 lpllm.py:1038] have reset next layer layer_attn 11 layer_mlp 10 
DEBUG 10-15 15:27:49 lpllm.py:1044] j: 21 waiting the layer with layer_idx 11 before wait time 0.46427273750305176 s
INFO 10-15 15:27:49 client.py:117] confirm_model_loaded: Mixtral-8x7B, b1cdb6f8-7461-4d97-af51-adb4c3edb69a
INFO 10-15 15:27:49 client.py:125] Model loaded
DEBUG 10-15 15:27:49 lpllm.py:1048] j: load cost 0.46604442596435547 s waiting cost 0.0017552375793457031 s
DEBUG 10-15 15:27:49 lpllm.py:924] 
DEBUG 10-15 15:27:49 lpllm.py:924] decoder loop j: 22 cur_layer_idx: 11 layer_attn_idx: 11 layer_mlp_idx: 10
DEBUG 10-15 15:27:49 lpllm.py:933] start load next layer cur_layer_idx: 12
DEBUG 10-15 15:27:49 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed6e0>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x004\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x98\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:49 client.py:72] load_into_gpu: Mixtral-8x7B, 52a071fc-a424-46bc-8479-8c6373bf5f50
INFO 10-15 15:27:49 client.py:113] Model loaded: Mixtral-8x7B, 52a071fc-a424-46bc-8479-8c6373bf5f50
DEBUG 10-15 15:27:49 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:49 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:49 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:49 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:49 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:49 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:49 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:49 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:49 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:49 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:49 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:49 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:49 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:49 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:49 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:49 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:49 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:49 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:49 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:49 lpllm.py:2265] GPU2CPU move cost 0.000618 seconds
DEBUG 10-15 15:27:49 lpmodule.py:340] layer_idx 11 attn layer_idx: 11
DEBUG 10-15 15:27:49 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:49 lpmodule.py:364] decoder_attn_batch update batch_dim 480-540
DEBUG 10-15 15:27:49 StaticCacheLen.py:84] static cache update layer_idx: 11, start_batch: 480, end_batch: 540
DEBUG 10-15 15:27:49 lpmodule.py:368] update for kv cache 480-540 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:49 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:49 lpmodule.py:374] update past key value cost 0.025327 seconds
DEBUG 10-15 15:27:49 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:49 lpmodule.py:227] layer idx 10
DEBUG 10-15 15:27:49 lpmodule.py:399] repeat qkv cost 0.029263 seconds
DEBUG 10-15 15:27:49 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:49 lpmodule.py:433] dot attn cost 0.032784 seconds
DEBUG 10-15 15:27:49 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:49 lpmodule.py:444] time cost move to cuda:1 0.02295994758605957 s
DEBUG 10-15 15:27:49 lpllm.py:2283] CPU attn cost 0.136574 seconds if batch True
DEBUG 10-15 15:27:49 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:27:49 lpllm.py:2294] CPU compute cost 0.137480 seconds
DEBUG 10-15 15:27:49 lpllm.py:2312] free cost 0.000075 seconds
DEBUG 10-15 15:27:49 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:49 lpllm.py:1774] update state cost 1.9788742065429688e-05 s
DEBUG 10-15 15:27:49 lpllm.py:1743] restore layer func cost 0.0008482933044433594 s
DEBUG 10-15 15:27:49 lpllm.py:511] restore layer cost 0.0011012554168701172 s
DEBUG 10-15 15:27:49 lpllm.py:512] reset_next_layer_need: layer_attn_idx=11, layer_mlp_idx=11, j_loc=22
DEBUG 10-15 15:27:49 lpllm.py:1037] reset layer cost 0.0011758804321289062 s
DEBUG 10-15 15:27:49 lpllm.py:1038] have reset next layer layer_attn 11 layer_mlp 11 
DEBUG 10-15 15:27:49 lpllm.py:924] 
DEBUG 10-15 15:27:49 lpllm.py:924] decoder loop j: 23 cur_layer_idx: 11 layer_attn_idx: 11 layer_mlp_idx: 11
DEBUG 10-15 15:27:49 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:49 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:49 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:49 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:49 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:49 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:49 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:49 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:49 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:49 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:49 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:49 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:49 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:49 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:49 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:49 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:49 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:49 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:49 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:49 lpllm.py:2265] GPU2CPU move cost 0.000578 seconds
DEBUG 10-15 15:27:49 lpmodule.py:340] layer_idx 11 attn layer_idx: 11
DEBUG 10-15 15:27:49 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:49 lpmodule.py:364] decoder_attn_batch update batch_dim 540-600
DEBUG 10-15 15:27:49 StaticCacheLen.py:84] static cache update layer_idx: 11, start_batch: 540, end_batch: 600
DEBUG 10-15 15:27:49 lpmodule.py:368] update for kv cache 540-600 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:49 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:49 lpmodule.py:374] update past key value cost 0.025728 seconds
DEBUG 10-15 15:27:49 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:49 lpmodule.py:227] layer idx 11
DEBUG 10-15 15:27:49 lpmodule.py:399] repeat qkv cost 0.029348 seconds
DEBUG 10-15 15:27:49 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:49 lpmodule.py:433] dot attn cost 0.045783 seconds
DEBUG 10-15 15:27:49 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:49 lpmodule.py:444] time cost move to cuda:1 0.023000478744506836 s
DEBUG 10-15 15:27:49 lpllm.py:2283] CPU attn cost 0.148036 seconds if batch True
DEBUG 10-15 15:27:49 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:27:49 lpllm.py:2294] CPU compute cost 0.148891 seconds
DEBUG 10-15 15:27:49 lpllm.py:2312] free cost 0.000072 seconds
DEBUG 10-15 15:27:49 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:49 lpllm.py:1774] update state cost 2.47955322265625e-05 s
DEBUG 10-15 15:27:49 lpllm.py:1743] restore layer func cost 0.00040650367736816406 s
DEBUG 10-15 15:27:49 lpllm.py:511] restore layer cost 0.0006339550018310547 s
DEBUG 10-15 15:27:49 lpllm.py:512] reset_next_layer_need: layer_attn_idx=12, layer_mlp_idx=11, j_loc=23
DEBUG 10-15 15:27:49 lpllm.py:1037] reset layer cost 0.0007236003875732422 s
DEBUG 10-15 15:27:49 lpllm.py:1038] have reset next layer layer_attn 12 layer_mlp 11 
DEBUG 10-15 15:27:49 lpllm.py:1044] j: 23 waiting the layer with layer_idx 12 before wait time 0.44811177253723145 s
INFO 10-15 15:27:49 client.py:117] confirm_model_loaded: Mixtral-8x7B, 52a071fc-a424-46bc-8479-8c6373bf5f50
INFO 10-15 15:27:49 client.py:125] Model loaded
DEBUG 10-15 15:27:49 lpllm.py:1048] j: load cost 0.44989800453186035 s waiting cost 0.0017714500427246094 s
DEBUG 10-15 15:27:49 lpllm.py:924] 
DEBUG 10-15 15:27:49 lpllm.py:924] decoder loop j: 24 cur_layer_idx: 12 layer_attn_idx: 12 layer_mlp_idx: 11
DEBUG 10-15 15:27:49 lpllm.py:933] start load next layer cur_layer_idx: 13
DEBUG 10-15 15:27:49 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:49 client.py:72] load_into_gpu: Mixtral-8x7B, ffd8f4c8-3530-4b57-9fb0-b3d8ce2dec71
INFO 10-15 15:27:49 client.py:113] Model loaded: Mixtral-8x7B, ffd8f4c8-3530-4b57-9fb0-b3d8ce2dec71
DEBUG 10-15 15:27:49 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:49 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:49 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:49 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:49 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:49 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:49 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:49 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:49 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:49 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:49 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:49 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:49 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:49 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:49 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:49 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:49 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:49 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:49 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:49 lpllm.py:2265] GPU2CPU move cost 0.000610 seconds
DEBUG 10-15 15:27:49 lpmodule.py:340] layer_idx 12 attn layer_idx: 12
DEBUG 10-15 15:27:49 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:49 lpmodule.py:364] decoder_attn_batch update batch_dim 480-540
DEBUG 10-15 15:27:49 StaticCacheLen.py:84] static cache update layer_idx: 12, start_batch: 480, end_batch: 540
DEBUG 10-15 15:27:49 lpmodule.py:368] update for kv cache 480-540 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:49 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:49 lpmodule.py:374] update past key value cost 0.023965 seconds
DEBUG 10-15 15:27:49 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:49 lpmodule.py:227] layer idx 11
DEBUG 10-15 15:27:49 lpmodule.py:399] repeat qkv cost 0.028679 seconds
DEBUG 10-15 15:27:49 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:50 lpmodule.py:433] dot attn cost 0.035228 seconds
DEBUG 10-15 15:27:50 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:50 lpmodule.py:444] time cost move to cuda:1 0.023120880126953125 s
DEBUG 10-15 15:27:50 lpllm.py:2283] CPU attn cost 0.135254 seconds if batch True
DEBUG 10-15 15:27:50 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:50 lpllm.py:2294] CPU compute cost 0.136174 seconds
DEBUG 10-15 15:27:50 lpllm.py:2312] free cost 0.000077 seconds
DEBUG 10-15 15:27:50 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:50 lpllm.py:1774] update state cost 2.3126602172851562e-05 s
DEBUG 10-15 15:27:50 lpllm.py:1743] restore layer func cost 0.0008525848388671875 s
DEBUG 10-15 15:27:50 lpllm.py:511] restore layer cost 0.0010955333709716797 s
DEBUG 10-15 15:27:50 lpllm.py:512] reset_next_layer_need: layer_attn_idx=12, layer_mlp_idx=12, j_loc=24
DEBUG 10-15 15:27:50 lpllm.py:1037] reset layer cost 0.0011858940124511719 s
DEBUG 10-15 15:27:50 lpllm.py:1038] have reset next layer layer_attn 12 layer_mlp 12 
DEBUG 10-15 15:27:50 lpllm.py:924] 
DEBUG 10-15 15:27:50 lpllm.py:924] decoder loop j: 25 cur_layer_idx: 12 layer_attn_idx: 12 layer_mlp_idx: 12
DEBUG 10-15 15:27:50 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:50 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:50 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:50 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:50 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:50 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:50 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:50 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:50 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:50 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:50 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:50 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:50 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:50 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:50 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:50 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:50 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:50 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:50 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:50 lpllm.py:2265] GPU2CPU move cost 0.000618 seconds
DEBUG 10-15 15:27:50 lpmodule.py:340] layer_idx 12 attn layer_idx: 12
DEBUG 10-15 15:27:50 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:50 lpmodule.py:364] decoder_attn_batch update batch_dim 540-600
DEBUG 10-15 15:27:50 StaticCacheLen.py:84] static cache update layer_idx: 12, start_batch: 540, end_batch: 600
DEBUG 10-15 15:27:50 lpmodule.py:368] update for kv cache 540-600 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:50 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:50 lpmodule.py:374] update past key value cost 0.024987 seconds
DEBUG 10-15 15:27:50 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:50 lpmodule.py:227] layer idx 12
DEBUG 10-15 15:27:50 lpmodule.py:399] repeat qkv cost 0.030074 seconds
DEBUG 10-15 15:27:50 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:50 lpmodule.py:433] dot attn cost 0.033671 seconds
DEBUG 10-15 15:27:50 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:50 lpmodule.py:444] time cost move to cuda:1 0.023097991943359375 s
DEBUG 10-15 15:27:50 lpllm.py:2283] CPU attn cost 0.135786 seconds if batch True
DEBUG 10-15 15:27:50 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:50 lpllm.py:2294] CPU compute cost 0.136689 seconds
DEBUG 10-15 15:27:50 lpllm.py:2312] free cost 0.000072 seconds
DEBUG 10-15 15:27:50 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:50 lpllm.py:1774] update state cost 4.0531158447265625e-05 s
DEBUG 10-15 15:27:50 lpllm.py:1743] restore layer func cost 0.00039696693420410156 s
DEBUG 10-15 15:27:50 lpllm.py:511] restore layer cost 0.0006372928619384766 s
DEBUG 10-15 15:27:50 lpllm.py:512] reset_next_layer_need: layer_attn_idx=13, layer_mlp_idx=12, j_loc=25
DEBUG 10-15 15:27:50 lpllm.py:1037] reset layer cost 0.0007102489471435547 s
DEBUG 10-15 15:27:50 lpllm.py:1038] have reset next layer layer_attn 13 layer_mlp 12 
DEBUG 10-15 15:27:50 lpllm.py:1044] j: 25 waiting the layer with layer_idx 13 before wait time 0.4421992301940918 s
INFO 10-15 15:27:50 client.py:117] confirm_model_loaded: Mixtral-8x7B, ffd8f4c8-3530-4b57-9fb0-b3d8ce2dec71
INFO 10-15 15:27:50 client.py:125] Model loaded
DEBUG 10-15 15:27:50 lpllm.py:1048] j: load cost 0.44402623176574707 s waiting cost 0.0018100738525390625 s
DEBUG 10-15 15:27:50 lpllm.py:924] 
DEBUG 10-15 15:27:50 lpllm.py:924] decoder loop j: 26 cur_layer_idx: 13 layer_attn_idx: 13 layer_mlp_idx: 12
DEBUG 10-15 15:27:50 lpllm.py:933] start load next layer cur_layer_idx: 14
DEBUG 10-15 15:27:50 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed650>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x003\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x96\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:50 client.py:72] load_into_gpu: Mixtral-8x7B, 493aca7c-0920-4f8d-a7c1-5725198c25cd
INFO 10-15 15:27:50 client.py:113] Model loaded: Mixtral-8x7B, 493aca7c-0920-4f8d-a7c1-5725198c25cd
DEBUG 10-15 15:27:50 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:50 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:50 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:50 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:50 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:50 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:50 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:50 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:50 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:50 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:50 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:50 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:50 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:50 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:50 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:50 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:50 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:50 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:50 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:50 lpllm.py:2265] GPU2CPU move cost 0.000571 seconds
DEBUG 10-15 15:27:50 lpmodule.py:340] layer_idx 13 attn layer_idx: 13
DEBUG 10-15 15:27:50 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:50 lpmodule.py:364] decoder_attn_batch update batch_dim 480-540
DEBUG 10-15 15:27:50 StaticCacheLen.py:84] static cache update layer_idx: 13, start_batch: 480, end_batch: 540
DEBUG 10-15 15:27:50 lpmodule.py:368] update for kv cache 480-540 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:50 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:50 lpmodule.py:374] update past key value cost 0.022253 seconds
DEBUG 10-15 15:27:50 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:50 lpmodule.py:227] layer idx 12
DEBUG 10-15 15:27:50 lpmodule.py:399] repeat qkv cost 0.031414 seconds
DEBUG 10-15 15:27:50 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:50 lpmodule.py:433] dot attn cost 0.045420 seconds
DEBUG 10-15 15:27:50 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:50 lpmodule.py:444] time cost move to cuda:1 0.02292346954345703 s
DEBUG 10-15 15:27:50 lpllm.py:2283] CPU attn cost 0.145737 seconds if batch True
DEBUG 10-15 15:27:50 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:50 lpllm.py:2294] CPU compute cost 0.146581 seconds
DEBUG 10-15 15:27:50 lpllm.py:2312] free cost 0.000074 seconds
DEBUG 10-15 15:27:50 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:50 lpllm.py:1774] update state cost 4.1484832763671875e-05 s
DEBUG 10-15 15:27:50 lpllm.py:1743] restore layer func cost 0.0008420944213867188 s
DEBUG 10-15 15:27:50 lpllm.py:511] restore layer cost 0.001102447509765625 s
DEBUG 10-15 15:27:50 lpllm.py:512] reset_next_layer_need: layer_attn_idx=13, layer_mlp_idx=13, j_loc=26
DEBUG 10-15 15:27:50 lpllm.py:1037] reset layer cost 0.0011792182922363281 s
DEBUG 10-15 15:27:50 lpllm.py:1038] have reset next layer layer_attn 13 layer_mlp 13 
DEBUG 10-15 15:27:50 lpllm.py:924] 
DEBUG 10-15 15:27:50 lpllm.py:924] decoder loop j: 27 cur_layer_idx: 13 layer_attn_idx: 13 layer_mlp_idx: 13
DEBUG 10-15 15:27:50 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:50 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:50 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:50 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:50 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:50 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:50 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:50 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:50 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:50 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:50 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:50 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:50 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:50 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:50 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:50 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:50 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:50 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:50 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:50 lpllm.py:2265] GPU2CPU move cost 0.000576 seconds
DEBUG 10-15 15:27:50 lpmodule.py:340] layer_idx 13 attn layer_idx: 13
DEBUG 10-15 15:27:50 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:50 lpmodule.py:364] decoder_attn_batch update batch_dim 540-600
DEBUG 10-15 15:27:50 StaticCacheLen.py:84] static cache update layer_idx: 13, start_batch: 540, end_batch: 600
DEBUG 10-15 15:27:50 lpmodule.py:368] update for kv cache 540-600 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:50 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:50 lpmodule.py:374] update past key value cost 0.025012 seconds
DEBUG 10-15 15:27:50 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:50 lpmodule.py:227] layer idx 13
DEBUG 10-15 15:27:50 lpmodule.py:399] repeat qkv cost 0.030470 seconds
DEBUG 10-15 15:27:50 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:50 lpmodule.py:433] dot attn cost 0.034237 seconds
DEBUG 10-15 15:27:50 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:50 lpmodule.py:444] time cost move to cuda:1 0.02378058433532715 s
DEBUG 10-15 15:27:50 lpllm.py:2283] CPU attn cost 0.138521 seconds if batch True
DEBUG 10-15 15:27:50 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:50 lpllm.py:2294] CPU compute cost 0.139397 seconds
DEBUG 10-15 15:27:50 lpllm.py:2312] free cost 0.000109 seconds
DEBUG 10-15 15:27:50 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:50 lpllm.py:1774] update state cost 3.814697265625e-05 s
DEBUG 10-15 15:27:50 lpllm.py:1743] restore layer func cost 0.0004012584686279297 s
DEBUG 10-15 15:27:50 lpllm.py:511] restore layer cost 0.0006377696990966797 s
DEBUG 10-15 15:27:50 lpllm.py:512] reset_next_layer_need: layer_attn_idx=14, layer_mlp_idx=13, j_loc=27
DEBUG 10-15 15:27:50 lpllm.py:1037] reset layer cost 0.0007114410400390625 s
DEBUG 10-15 15:27:50 lpllm.py:1038] have reset next layer layer_attn 14 layer_mlp 13 
DEBUG 10-15 15:27:50 lpllm.py:1044] j: 27 waiting the layer with layer_idx 14 before wait time 0.44376468658447266 s
INFO 10-15 15:27:50 client.py:117] confirm_model_loaded: Mixtral-8x7B, 493aca7c-0920-4f8d-a7c1-5725198c25cd
INFO 10-15 15:27:50 client.py:125] Model loaded
DEBUG 10-15 15:27:50 lpllm.py:1048] j: load cost 0.4454655647277832 s waiting cost 0.0016849040985107422 s
DEBUG 10-15 15:27:50 lpllm.py:924] 
DEBUG 10-15 15:27:50 lpllm.py:924] decoder loop j: 28 cur_layer_idx: 14 layer_attn_idx: 14 layer_mlp_idx: 13
DEBUG 10-15 15:27:50 lpllm.py:933] start load next layer cur_layer_idx: 15
DEBUG 10-15 15:27:50 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed6e0>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x004\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x98\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:50 client.py:72] load_into_gpu: Mixtral-8x7B, 36e5b4e1-ef5f-4e04-bdb6-1724559f48e4
INFO 10-15 15:27:50 client.py:113] Model loaded: Mixtral-8x7B, 36e5b4e1-ef5f-4e04-bdb6-1724559f48e4
DEBUG 10-15 15:27:50 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:50 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:50 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:50 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:50 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:50 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:50 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:50 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:50 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:50 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:50 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:50 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:50 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:50 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:50 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:50 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:50 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:50 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:50 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:50 lpllm.py:2265] GPU2CPU move cost 0.000610 seconds
DEBUG 10-15 15:27:50 lpmodule.py:340] layer_idx 14 attn layer_idx: 14
DEBUG 10-15 15:27:50 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:50 lpmodule.py:364] decoder_attn_batch update batch_dim 480-540
DEBUG 10-15 15:27:50 StaticCacheLen.py:84] static cache update layer_idx: 14, start_batch: 480, end_batch: 540
DEBUG 10-15 15:27:50 lpmodule.py:368] update for kv cache 480-540 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:50 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:50 lpmodule.py:374] update past key value cost 0.024351 seconds
DEBUG 10-15 15:27:50 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:50 lpmodule.py:227] layer idx 13
DEBUG 10-15 15:27:50 lpmodule.py:399] repeat qkv cost 0.030839 seconds
DEBUG 10-15 15:27:50 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:50 lpmodule.py:433] dot attn cost 0.033798 seconds
DEBUG 10-15 15:27:50 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:50 lpmodule.py:444] time cost move to cuda:1 0.023118257522583008 s
DEBUG 10-15 15:27:50 lpllm.py:2283] CPU attn cost 0.138307 seconds if batch True
DEBUG 10-15 15:27:50 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:50 lpllm.py:2294] CPU compute cost 0.139238 seconds
DEBUG 10-15 15:27:50 lpllm.py:2312] free cost 0.000078 seconds
DEBUG 10-15 15:27:51 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:51 lpllm.py:1774] update state cost 2.5987625122070312e-05 s
DEBUG 10-15 15:27:51 lpllm.py:1743] restore layer func cost 0.0008332729339599609 s
DEBUG 10-15 15:27:51 lpllm.py:511] restore layer cost 0.0010709762573242188 s
DEBUG 10-15 15:27:51 lpllm.py:512] reset_next_layer_need: layer_attn_idx=14, layer_mlp_idx=14, j_loc=28
DEBUG 10-15 15:27:51 lpllm.py:1037] reset layer cost 0.0011451244354248047 s
DEBUG 10-15 15:27:51 lpllm.py:1038] have reset next layer layer_attn 14 layer_mlp 14 
DEBUG 10-15 15:27:51 lpllm.py:924] 
DEBUG 10-15 15:27:51 lpllm.py:924] decoder loop j: 29 cur_layer_idx: 14 layer_attn_idx: 14 layer_mlp_idx: 14
DEBUG 10-15 15:27:51 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:51 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:51 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:51 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:51 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:51 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:51 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:51 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:51 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:51 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:51 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:51 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:51 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:51 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:51 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:51 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:51 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:51 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:51 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:51 lpllm.py:2265] GPU2CPU move cost 0.000368 seconds
DEBUG 10-15 15:27:51 lpmodule.py:340] layer_idx 14 attn layer_idx: 14
DEBUG 10-15 15:27:51 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:51 lpmodule.py:364] decoder_attn_batch update batch_dim 540-600
DEBUG 10-15 15:27:51 StaticCacheLen.py:84] static cache update layer_idx: 14, start_batch: 540, end_batch: 600
DEBUG 10-15 15:27:51 lpmodule.py:368] update for kv cache 540-600 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:51 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:51 lpmodule.py:374] update past key value cost 0.024508 seconds
DEBUG 10-15 15:27:51 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:51 lpmodule.py:227] layer idx 14
DEBUG 10-15 15:27:51 lpmodule.py:399] repeat qkv cost 0.029958 seconds
DEBUG 10-15 15:27:51 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:51 lpmodule.py:433] dot attn cost 0.035369 seconds
DEBUG 10-15 15:27:51 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:51 lpmodule.py:444] time cost move to cuda:1 0.02372455596923828 s
DEBUG 10-15 15:27:51 lpllm.py:2283] CPU attn cost 0.137759 seconds if batch True
DEBUG 10-15 15:27:51 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:27:51 lpllm.py:2294] CPU compute cost 0.138370 seconds
DEBUG 10-15 15:27:51 lpllm.py:2312] free cost 0.000073 seconds
DEBUG 10-15 15:27:51 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:51 lpllm.py:1774] update state cost 2.5033950805664062e-05 s
DEBUG 10-15 15:27:51 lpllm.py:1743] restore layer func cost 0.0004417896270751953 s
DEBUG 10-15 15:27:51 lpllm.py:511] restore layer cost 0.0006878376007080078 s
DEBUG 10-15 15:27:51 lpllm.py:512] reset_next_layer_need: layer_attn_idx=15, layer_mlp_idx=14, j_loc=29
DEBUG 10-15 15:27:51 lpllm.py:1037] reset layer cost 0.0007789134979248047 s
DEBUG 10-15 15:27:51 lpllm.py:1038] have reset next layer layer_attn 15 layer_mlp 14 
DEBUG 10-15 15:27:51 lpllm.py:1044] j: 29 waiting the layer with layer_idx 15 before wait time 0.44634032249450684 s
INFO 10-15 15:27:51 client.py:117] confirm_model_loaded: Mixtral-8x7B, 36e5b4e1-ef5f-4e04-bdb6-1724559f48e4
INFO 10-15 15:27:51 client.py:125] Model loaded
DEBUG 10-15 15:27:51 lpllm.py:1048] j: load cost 0.4479706287384033 s waiting cost 0.0016143321990966797 s
DEBUG 10-15 15:27:51 lpllm.py:924] 
DEBUG 10-15 15:27:51 lpllm.py:924] decoder loop j: 30 cur_layer_idx: 15 layer_attn_idx: 15 layer_mlp_idx: 14
DEBUG 10-15 15:27:51 lpllm.py:933] start load next layer cur_layer_idx: 16
DEBUG 10-15 15:27:51 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:51 client.py:72] load_into_gpu: Mixtral-8x7B, bc6d19f3-262f-402b-8820-0a3fe2b3ae87
INFO 10-15 15:27:51 client.py:113] Model loaded: Mixtral-8x7B, bc6d19f3-262f-402b-8820-0a3fe2b3ae87
DEBUG 10-15 15:27:51 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:51 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:51 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:51 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:51 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:51 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:51 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:51 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:51 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:51 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:51 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:51 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:51 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:51 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:51 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:51 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:51 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:51 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:51 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:51 lpllm.py:2265] GPU2CPU move cost 0.000336 seconds
DEBUG 10-15 15:27:51 lpmodule.py:340] layer_idx 15 attn layer_idx: 15
DEBUG 10-15 15:27:51 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:51 lpmodule.py:364] decoder_attn_batch update batch_dim 480-540
DEBUG 10-15 15:27:51 StaticCacheLen.py:84] static cache update layer_idx: 15, start_batch: 480, end_batch: 540
DEBUG 10-15 15:27:51 lpmodule.py:368] update for kv cache 480-540 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:51 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:51 lpmodule.py:374] update past key value cost 0.023883 seconds
DEBUG 10-15 15:27:51 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:51 lpmodule.py:227] layer idx 14
DEBUG 10-15 15:27:51 lpmodule.py:399] repeat qkv cost 0.029689 seconds
DEBUG 10-15 15:27:51 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:51 lpmodule.py:433] dot attn cost 0.043713 seconds
DEBUG 10-15 15:27:51 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:51 lpmodule.py:444] time cost move to cuda:1 0.02294135093688965 s
DEBUG 10-15 15:27:51 lpllm.py:2283] CPU attn cost 0.144307 seconds if batch True
DEBUG 10-15 15:27:51 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:51 lpllm.py:2294] CPU compute cost 0.144859 seconds
DEBUG 10-15 15:27:51 lpllm.py:2312] free cost 0.000074 seconds
DEBUG 10-15 15:27:51 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:51 lpllm.py:1774] update state cost 2.6226043701171875e-05 s
DEBUG 10-15 15:27:51 lpllm.py:1743] restore layer func cost 0.0008130073547363281 s
DEBUG 10-15 15:27:51 lpllm.py:511] restore layer cost 0.0010602474212646484 s
DEBUG 10-15 15:27:51 lpllm.py:512] reset_next_layer_need: layer_attn_idx=15, layer_mlp_idx=15, j_loc=30
DEBUG 10-15 15:27:51 lpllm.py:1037] reset layer cost 0.0011553764343261719 s
DEBUG 10-15 15:27:51 lpllm.py:1038] have reset next layer layer_attn 15 layer_mlp 15 
DEBUG 10-15 15:27:51 lpllm.py:924] 
DEBUG 10-15 15:27:51 lpllm.py:924] decoder loop j: 31 cur_layer_idx: 15 layer_attn_idx: 15 layer_mlp_idx: 15
DEBUG 10-15 15:27:51 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:51 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:51 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:51 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:51 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:51 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:51 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:51 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:51 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:51 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:51 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:51 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:51 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:51 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:51 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:51 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:51 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:51 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:51 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:51 lpllm.py:2265] GPU2CPU move cost 0.000570 seconds
DEBUG 10-15 15:27:51 lpmodule.py:340] layer_idx 15 attn layer_idx: 15
DEBUG 10-15 15:27:51 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:51 lpmodule.py:364] decoder_attn_batch update batch_dim 540-600
DEBUG 10-15 15:27:51 StaticCacheLen.py:84] static cache update layer_idx: 15, start_batch: 540, end_batch: 600
DEBUG 10-15 15:27:51 lpmodule.py:368] update for kv cache 540-600 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:51 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:51 lpmodule.py:374] update past key value cost 0.025783 seconds
DEBUG 10-15 15:27:51 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:51 lpmodule.py:227] layer idx 15
DEBUG 10-15 15:27:51 lpmodule.py:399] repeat qkv cost 0.029044 seconds
DEBUG 10-15 15:27:51 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:51 lpmodule.py:433] dot attn cost 0.034283 seconds
DEBUG 10-15 15:27:51 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:51 lpmodule.py:444] time cost move to cuda:1 0.02356719970703125 s
DEBUG 10-15 15:27:51 lpllm.py:2283] CPU attn cost 0.137570 seconds if batch True
DEBUG 10-15 15:27:51 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:51 lpllm.py:2294] CPU compute cost 0.138423 seconds
DEBUG 10-15 15:27:51 lpllm.py:2312] free cost 0.000076 seconds
DEBUG 10-15 15:27:51 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:51 lpllm.py:1774] update state cost 2.956390380859375e-05 s
DEBUG 10-15 15:27:51 lpllm.py:1743] restore layer func cost 0.0004000663757324219 s
DEBUG 10-15 15:27:51 lpllm.py:511] restore layer cost 0.0006468296051025391 s
DEBUG 10-15 15:27:51 lpllm.py:512] reset_next_layer_need: layer_attn_idx=16, layer_mlp_idx=15, j_loc=31
DEBUG 10-15 15:27:51 lpllm.py:1037] reset layer cost 0.0007386207580566406 s
DEBUG 10-15 15:27:51 lpllm.py:1038] have reset next layer layer_attn 16 layer_mlp 15 
DEBUG 10-15 15:27:51 lpllm.py:1044] j: 31 waiting the layer with layer_idx 16 before wait time 0.4501783847808838 s
INFO 10-15 15:27:51 client.py:117] confirm_model_loaded: Mixtral-8x7B, bc6d19f3-262f-402b-8820-0a3fe2b3ae87
INFO 10-15 15:27:51 client.py:125] Model loaded
DEBUG 10-15 15:27:51 lpllm.py:1048] j: load cost 0.45207977294921875 s waiting cost 0.0018842220306396484 s
DEBUG 10-15 15:27:51 lpllm.py:924] 
DEBUG 10-15 15:27:51 lpllm.py:924] decoder loop j: 32 cur_layer_idx: 16 layer_attn_idx: 16 layer_mlp_idx: 15
DEBUG 10-15 15:27:51 lpllm.py:933] start load next layer cur_layer_idx: 17
DEBUG 10-15 15:27:51 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed650>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x003\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x96\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:51 client.py:72] load_into_gpu: Mixtral-8x7B, b5897deb-894d-4981-99bd-ad2ae460551a
INFO 10-15 15:27:51 client.py:113] Model loaded: Mixtral-8x7B, b5897deb-894d-4981-99bd-ad2ae460551a
DEBUG 10-15 15:27:51 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:51 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:51 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:51 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:51 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:51 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:51 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:51 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:51 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:51 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:51 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:51 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:51 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:51 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:51 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:51 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:51 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:51 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:51 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:51 lpllm.py:2265] GPU2CPU move cost 0.000562 seconds
DEBUG 10-15 15:27:51 lpmodule.py:340] layer_idx 16 attn layer_idx: 16
DEBUG 10-15 15:27:51 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:51 lpmodule.py:364] decoder_attn_batch update batch_dim 480-540
DEBUG 10-15 15:27:51 StaticCacheLen.py:84] static cache update layer_idx: 16, start_batch: 480, end_batch: 540
DEBUG 10-15 15:27:51 lpmodule.py:368] update for kv cache 480-540 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:51 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:51 lpmodule.py:374] update past key value cost 0.023056 seconds
DEBUG 10-15 15:27:51 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:51 lpmodule.py:227] layer idx 15
DEBUG 10-15 15:27:51 lpmodule.py:399] repeat qkv cost 0.029936 seconds
DEBUG 10-15 15:27:51 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:51 lpmodule.py:433] dot attn cost 0.044417 seconds
DEBUG 10-15 15:27:51 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:51 lpmodule.py:444] time cost move to cuda:1 0.025480985641479492 s
DEBUG 10-15 15:27:51 lpllm.py:2283] CPU attn cost 0.148558 seconds if batch True
DEBUG 10-15 15:27:51 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:51 lpllm.py:2294] CPU compute cost 0.149396 seconds
DEBUG 10-15 15:27:51 lpllm.py:2312] free cost 0.000074 seconds
DEBUG 10-15 15:27:51 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:51 lpllm.py:1774] update state cost 2.4080276489257812e-05 s
DEBUG 10-15 15:27:51 lpllm.py:1743] restore layer func cost 0.0008237361907958984 s
DEBUG 10-15 15:27:51 lpllm.py:511] restore layer cost 0.0010857582092285156 s
DEBUG 10-15 15:27:51 lpllm.py:512] reset_next_layer_need: layer_attn_idx=16, layer_mlp_idx=16, j_loc=32
DEBUG 10-15 15:27:51 lpllm.py:1037] reset layer cost 0.0011720657348632812 s
DEBUG 10-15 15:27:51 lpllm.py:1038] have reset next layer layer_attn 16 layer_mlp 16 
DEBUG 10-15 15:27:51 lpllm.py:924] 
DEBUG 10-15 15:27:51 lpllm.py:924] decoder loop j: 33 cur_layer_idx: 16 layer_attn_idx: 16 layer_mlp_idx: 16
DEBUG 10-15 15:27:51 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:51 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:51 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:51 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:51 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:51 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:51 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:51 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:51 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:51 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:51 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:51 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:51 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:51 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:51 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:51 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:51 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:51 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:51 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:51 lpllm.py:2265] GPU2CPU move cost 0.000616 seconds
DEBUG 10-15 15:27:51 lpmodule.py:340] layer_idx 16 attn layer_idx: 16
DEBUG 10-15 15:27:51 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:51 lpmodule.py:364] decoder_attn_batch update batch_dim 540-600
DEBUG 10-15 15:27:51 StaticCacheLen.py:84] static cache update layer_idx: 16, start_batch: 540, end_batch: 600
DEBUG 10-15 15:27:51 lpmodule.py:368] update for kv cache 540-600 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:51 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:51 lpmodule.py:374] update past key value cost 0.027786 seconds
DEBUG 10-15 15:27:51 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:51 lpmodule.py:227] layer idx 16
DEBUG 10-15 15:27:52 lpmodule.py:399] repeat qkv cost 0.028484 seconds
DEBUG 10-15 15:27:52 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:52 lpmodule.py:433] dot attn cost 0.036141 seconds
DEBUG 10-15 15:27:52 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:52 lpmodule.py:444] time cost move to cuda:1 0.024399280548095703 s
DEBUG 10-15 15:27:52 lpllm.py:2283] CPU attn cost 0.141088 seconds if batch True
DEBUG 10-15 15:27:52 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:52 lpllm.py:2294] CPU compute cost 0.141979 seconds
DEBUG 10-15 15:27:52 lpllm.py:2312] free cost 0.000075 seconds
DEBUG 10-15 15:27:52 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:52 lpllm.py:1774] update state cost 4.1961669921875e-05 s
DEBUG 10-15 15:27:52 lpllm.py:1743] restore layer func cost 0.0004076957702636719 s
DEBUG 10-15 15:27:52 lpllm.py:511] restore layer cost 0.0006630420684814453 s
DEBUG 10-15 15:27:52 lpllm.py:512] reset_next_layer_need: layer_attn_idx=17, layer_mlp_idx=16, j_loc=33
DEBUG 10-15 15:27:52 lpllm.py:1037] reset layer cost 0.0007359981536865234 s
DEBUG 10-15 15:27:52 lpllm.py:1038] have reset next layer layer_attn 17 layer_mlp 16 
DEBUG 10-15 15:27:52 lpllm.py:1044] j: 33 waiting the layer with layer_idx 17 before wait time 0.4470956325531006 s
INFO 10-15 15:27:52 client.py:117] confirm_model_loaded: Mixtral-8x7B, b5897deb-894d-4981-99bd-ad2ae460551a
INFO 10-15 15:27:52 client.py:125] Model loaded
DEBUG 10-15 15:27:52 lpllm.py:1048] j: load cost 0.4487743377685547 s waiting cost 0.0016632080078125 s
DEBUG 10-15 15:27:52 lpllm.py:924] 
DEBUG 10-15 15:27:52 lpllm.py:924] decoder loop j: 34 cur_layer_idx: 17 layer_attn_idx: 17 layer_mlp_idx: 16
DEBUG 10-15 15:27:52 lpllm.py:933] start load next layer cur_layer_idx: 18
DEBUG 10-15 15:27:52 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed6e0>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x004\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x98\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:52 client.py:72] load_into_gpu: Mixtral-8x7B, 8fba54ba-35f6-4195-ae12-60893e8ebc2a
INFO 10-15 15:27:52 client.py:113] Model loaded: Mixtral-8x7B, 8fba54ba-35f6-4195-ae12-60893e8ebc2a
DEBUG 10-15 15:27:52 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:52 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:52 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:52 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:52 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:52 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:52 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:52 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:52 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:52 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:52 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:52 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:52 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:52 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:52 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:52 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:52 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:52 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:52 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:52 lpllm.py:2265] GPU2CPU move cost 0.000587 seconds
DEBUG 10-15 15:27:52 lpmodule.py:340] layer_idx 17 attn layer_idx: 17
DEBUG 10-15 15:27:52 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:52 lpmodule.py:364] decoder_attn_batch update batch_dim 480-540
DEBUG 10-15 15:27:52 StaticCacheLen.py:84] static cache update layer_idx: 17, start_batch: 480, end_batch: 540
DEBUG 10-15 15:27:52 lpmodule.py:368] update for kv cache 480-540 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:52 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:52 lpmodule.py:374] update past key value cost 0.024511 seconds
DEBUG 10-15 15:27:52 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:52 lpmodule.py:227] layer idx 16
DEBUG 10-15 15:27:52 lpmodule.py:399] repeat qkv cost 0.029359 seconds
DEBUG 10-15 15:27:52 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:52 lpmodule.py:433] dot attn cost 0.039033 seconds
DEBUG 10-15 15:27:52 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:52 lpmodule.py:444] time cost move to cuda:1 0.026454925537109375 s
DEBUG 10-15 15:27:52 lpllm.py:2283] CPU attn cost 0.143050 seconds if batch True
DEBUG 10-15 15:27:52 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:52 lpllm.py:2294] CPU compute cost 0.143932 seconds
DEBUG 10-15 15:27:52 lpllm.py:2312] free cost 0.000074 seconds
DEBUG 10-15 15:27:52 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:52 lpllm.py:1774] update state cost 2.2649765014648438e-05 s
DEBUG 10-15 15:27:52 lpllm.py:1743] restore layer func cost 0.0008096694946289062 s
DEBUG 10-15 15:27:52 lpllm.py:511] restore layer cost 0.001050710678100586 s
DEBUG 10-15 15:27:52 lpllm.py:512] reset_next_layer_need: layer_attn_idx=17, layer_mlp_idx=17, j_loc=34
DEBUG 10-15 15:27:52 lpllm.py:1037] reset layer cost 0.0011246204376220703 s
DEBUG 10-15 15:27:52 lpllm.py:1038] have reset next layer layer_attn 17 layer_mlp 17 
DEBUG 10-15 15:27:52 lpllm.py:924] 
DEBUG 10-15 15:27:52 lpllm.py:924] decoder loop j: 35 cur_layer_idx: 17 layer_attn_idx: 17 layer_mlp_idx: 17
DEBUG 10-15 15:27:52 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:52 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:52 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:52 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:52 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:52 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:52 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:52 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:52 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:52 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:52 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:52 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:52 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:52 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:52 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:52 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:52 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:52 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:52 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:52 lpllm.py:2265] GPU2CPU move cost 0.000624 seconds
DEBUG 10-15 15:27:52 lpmodule.py:340] layer_idx 17 attn layer_idx: 17
DEBUG 10-15 15:27:52 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:52 lpmodule.py:364] decoder_attn_batch update batch_dim 540-600
DEBUG 10-15 15:27:52 StaticCacheLen.py:84] static cache update layer_idx: 17, start_batch: 540, end_batch: 600
DEBUG 10-15 15:27:52 lpmodule.py:368] update for kv cache 540-600 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:52 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:52 lpmodule.py:374] update past key value cost 0.027526 seconds
DEBUG 10-15 15:27:52 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:52 lpmodule.py:227] layer idx 17
DEBUG 10-15 15:27:52 lpmodule.py:399] repeat qkv cost 0.028746 seconds
DEBUG 10-15 15:27:52 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:52 lpmodule.py:433] dot attn cost 0.041412 seconds
DEBUG 10-15 15:27:52 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:52 lpmodule.py:444] time cost move to cuda:1 0.02339315414428711 s
DEBUG 10-15 15:27:52 lpllm.py:2283] CPU attn cost 0.144983 seconds if batch True
DEBUG 10-15 15:27:52 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:52 lpllm.py:2294] CPU compute cost 0.145892 seconds
DEBUG 10-15 15:27:52 lpllm.py:2312] free cost 0.000075 seconds
DEBUG 10-15 15:27:52 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:52 lpllm.py:1774] update state cost 2.3365020751953125e-05 s
DEBUG 10-15 15:27:52 lpllm.py:1743] restore layer func cost 0.0003883838653564453 s
DEBUG 10-15 15:27:52 lpllm.py:511] restore layer cost 0.0006158351898193359 s
DEBUG 10-15 15:27:52 lpllm.py:512] reset_next_layer_need: layer_attn_idx=18, layer_mlp_idx=17, j_loc=35
DEBUG 10-15 15:27:52 lpllm.py:1037] reset layer cost 0.0006885528564453125 s
DEBUG 10-15 15:27:52 lpllm.py:1038] have reset next layer layer_attn 18 layer_mlp 17 
DEBUG 10-15 15:27:52 lpllm.py:1044] j: 35 waiting the layer with layer_idx 18 before wait time 0.44763779640197754 s
INFO 10-15 15:27:52 client.py:117] confirm_model_loaded: Mixtral-8x7B, 8fba54ba-35f6-4195-ae12-60893e8ebc2a
INFO 10-15 15:27:52 client.py:125] Model loaded
DEBUG 10-15 15:27:52 lpllm.py:1048] j: load cost 0.4493286609649658 s waiting cost 0.0016770362854003906 s
DEBUG 10-15 15:27:52 lpllm.py:924] 
DEBUG 10-15 15:27:52 lpllm.py:924] decoder loop j: 36 cur_layer_idx: 18 layer_attn_idx: 18 layer_mlp_idx: 17
DEBUG 10-15 15:27:52 lpllm.py:933] start load next layer cur_layer_idx: 19
DEBUG 10-15 15:27:52 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:52 client.py:72] load_into_gpu: Mixtral-8x7B, a4795460-9e73-41cc-8955-f3d6feefba61
INFO 10-15 15:27:52 client.py:113] Model loaded: Mixtral-8x7B, a4795460-9e73-41cc-8955-f3d6feefba61
DEBUG 10-15 15:27:52 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:52 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:52 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:52 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:52 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:52 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:52 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:52 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:52 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:52 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:52 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:52 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:52 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:52 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:52 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:52 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:52 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:52 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:52 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:52 lpllm.py:2265] GPU2CPU move cost 0.000516 seconds
DEBUG 10-15 15:27:52 lpmodule.py:340] layer_idx 18 attn layer_idx: 18
DEBUG 10-15 15:27:52 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:52 lpmodule.py:364] decoder_attn_batch update batch_dim 480-540
DEBUG 10-15 15:27:52 StaticCacheLen.py:84] static cache update layer_idx: 18, start_batch: 480, end_batch: 540
DEBUG 10-15 15:27:52 lpmodule.py:368] update for kv cache 480-540 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:52 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:52 lpmodule.py:374] update past key value cost 0.022459 seconds
DEBUG 10-15 15:27:52 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:52 lpmodule.py:227] layer idx 17
DEBUG 10-15 15:27:52 lpmodule.py:399] repeat qkv cost 0.028830 seconds
DEBUG 10-15 15:27:52 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:52 lpmodule.py:433] dot attn cost 0.043007 seconds
DEBUG 10-15 15:27:52 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:52 lpmodule.py:444] time cost move to cuda:1 0.023184537887573242 s
DEBUG 10-15 15:27:52 lpllm.py:2283] CPU attn cost 0.142037 seconds if batch True
DEBUG 10-15 15:27:52 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:27:52 lpllm.py:2294] CPU compute cost 0.142782 seconds
DEBUG 10-15 15:27:52 lpllm.py:2312] free cost 0.000073 seconds
DEBUG 10-15 15:27:52 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:52 lpllm.py:1774] update state cost 2.2172927856445312e-05 s
DEBUG 10-15 15:27:52 lpllm.py:1743] restore layer func cost 0.0008251667022705078 s
DEBUG 10-15 15:27:52 lpllm.py:511] restore layer cost 0.0010619163513183594 s
DEBUG 10-15 15:27:52 lpllm.py:512] reset_next_layer_need: layer_attn_idx=18, layer_mlp_idx=18, j_loc=36
DEBUG 10-15 15:27:52 lpllm.py:1037] reset layer cost 0.0011370182037353516 s
DEBUG 10-15 15:27:52 lpllm.py:1038] have reset next layer layer_attn 18 layer_mlp 18 
DEBUG 10-15 15:27:52 lpllm.py:924] 
DEBUG 10-15 15:27:52 lpllm.py:924] decoder loop j: 37 cur_layer_idx: 18 layer_attn_idx: 18 layer_mlp_idx: 18
DEBUG 10-15 15:27:52 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:52 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:52 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:52 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:52 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:52 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:52 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:52 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:52 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:52 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:52 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:52 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:52 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:52 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:52 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:52 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:52 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:52 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:52 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:52 lpllm.py:2265] GPU2CPU move cost 0.000593 seconds
DEBUG 10-15 15:27:52 lpmodule.py:340] layer_idx 18 attn layer_idx: 18
DEBUG 10-15 15:27:52 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:52 lpmodule.py:364] decoder_attn_batch update batch_dim 540-600
DEBUG 10-15 15:27:52 StaticCacheLen.py:84] static cache update layer_idx: 18, start_batch: 540, end_batch: 600
DEBUG 10-15 15:27:52 lpmodule.py:368] update for kv cache 540-600 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:52 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:52 lpmodule.py:374] update past key value cost 0.024980 seconds
DEBUG 10-15 15:27:52 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:52 lpmodule.py:227] layer idx 18
DEBUG 10-15 15:27:52 lpmodule.py:399] repeat qkv cost 0.031543 seconds
DEBUG 10-15 15:27:52 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:52 lpmodule.py:433] dot attn cost 0.033347 seconds
DEBUG 10-15 15:27:52 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:52 lpmodule.py:444] time cost move to cuda:1 0.02586960792541504 s
DEBUG 10-15 15:27:52 lpllm.py:2283] CPU attn cost 0.140084 seconds if batch True
DEBUG 10-15 15:27:52 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:52 lpllm.py:2294] CPU compute cost 0.140959 seconds
DEBUG 10-15 15:27:52 lpllm.py:2312] free cost 0.000073 seconds
DEBUG 10-15 15:27:53 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:53 lpllm.py:1774] update state cost 2.956390380859375e-05 s
DEBUG 10-15 15:27:53 lpllm.py:1743] restore layer func cost 0.0003962516784667969 s
DEBUG 10-15 15:27:53 lpllm.py:511] restore layer cost 0.0006487369537353516 s
DEBUG 10-15 15:27:53 lpllm.py:512] reset_next_layer_need: layer_attn_idx=19, layer_mlp_idx=18, j_loc=37
DEBUG 10-15 15:27:53 lpllm.py:1037] reset layer cost 0.0007321834564208984 s
DEBUG 10-15 15:27:53 lpllm.py:1038] have reset next layer layer_attn 19 layer_mlp 18 
DEBUG 10-15 15:27:53 lpllm.py:1044] j: 37 waiting the layer with layer_idx 19 before wait time 0.451413631439209 s
INFO 10-15 15:27:53 client.py:117] confirm_model_loaded: Mixtral-8x7B, a4795460-9e73-41cc-8955-f3d6feefba61
INFO 10-15 15:27:53 client.py:125] Model loaded
DEBUG 10-15 15:27:53 lpllm.py:1048] j: load cost 0.45310211181640625 s waiting cost 0.0016715526580810547 s
DEBUG 10-15 15:27:53 lpllm.py:924] 
DEBUG 10-15 15:27:53 lpllm.py:924] decoder loop j: 38 cur_layer_idx: 19 layer_attn_idx: 19 layer_mlp_idx: 18
DEBUG 10-15 15:27:53 lpllm.py:933] start load next layer cur_layer_idx: 20
DEBUG 10-15 15:27:53 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed650>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x003\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x96\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:53 client.py:72] load_into_gpu: Mixtral-8x7B, a745e8e0-47c8-44f3-8435-b8e38210f7ac
INFO 10-15 15:27:53 client.py:113] Model loaded: Mixtral-8x7B, a745e8e0-47c8-44f3-8435-b8e38210f7ac
DEBUG 10-15 15:27:53 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:53 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:53 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:53 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:53 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:53 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:53 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:53 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:53 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:53 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:53 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:53 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:53 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:53 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:53 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:53 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:53 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:53 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:53 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:53 lpllm.py:2265] GPU2CPU move cost 0.000615 seconds
DEBUG 10-15 15:27:53 lpmodule.py:340] layer_idx 19 attn layer_idx: 19
DEBUG 10-15 15:27:53 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:53 lpmodule.py:364] decoder_attn_batch update batch_dim 480-540
DEBUG 10-15 15:27:53 StaticCacheLen.py:84] static cache update layer_idx: 19, start_batch: 480, end_batch: 540
DEBUG 10-15 15:27:53 lpmodule.py:368] update for kv cache 480-540 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:53 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:53 lpmodule.py:374] update past key value cost 0.024510 seconds
DEBUG 10-15 15:27:53 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:53 lpmodule.py:227] layer idx 18
DEBUG 10-15 15:27:53 lpmodule.py:399] repeat qkv cost 0.029347 seconds
DEBUG 10-15 15:27:53 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:53 lpmodule.py:433] dot attn cost 0.035901 seconds
DEBUG 10-15 15:27:53 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:53 lpmodule.py:444] time cost move to cuda:1 0.024570226669311523 s
DEBUG 10-15 15:27:53 lpllm.py:2283] CPU attn cost 0.138587 seconds if batch True
DEBUG 10-15 15:27:53 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:27:53 lpllm.py:2294] CPU compute cost 0.139485 seconds
DEBUG 10-15 15:27:53 lpllm.py:2312] free cost 0.000073 seconds
DEBUG 10-15 15:27:53 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:53 lpllm.py:1774] update state cost 2.6941299438476562e-05 s
DEBUG 10-15 15:27:53 lpllm.py:1743] restore layer func cost 0.0008196830749511719 s
DEBUG 10-15 15:27:53 lpllm.py:511] restore layer cost 0.001074075698852539 s
DEBUG 10-15 15:27:53 lpllm.py:512] reset_next_layer_need: layer_attn_idx=19, layer_mlp_idx=19, j_loc=38
DEBUG 10-15 15:27:53 lpllm.py:1037] reset layer cost 0.0011606216430664062 s
DEBUG 10-15 15:27:53 lpllm.py:1038] have reset next layer layer_attn 19 layer_mlp 19 
DEBUG 10-15 15:27:53 lpllm.py:924] 
DEBUG 10-15 15:27:53 lpllm.py:924] decoder loop j: 39 cur_layer_idx: 19 layer_attn_idx: 19 layer_mlp_idx: 19
DEBUG 10-15 15:27:53 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:53 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:53 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:53 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:53 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:53 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:53 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:53 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:53 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:53 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:53 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:53 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:53 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:53 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:53 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:53 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:53 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:53 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:53 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:53 lpllm.py:2265] GPU2CPU move cost 0.000573 seconds
DEBUG 10-15 15:27:53 lpmodule.py:340] layer_idx 19 attn layer_idx: 19
DEBUG 10-15 15:27:53 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:53 lpmodule.py:364] decoder_attn_batch update batch_dim 540-600
DEBUG 10-15 15:27:53 StaticCacheLen.py:84] static cache update layer_idx: 19, start_batch: 540, end_batch: 600
DEBUG 10-15 15:27:53 lpmodule.py:368] update for kv cache 540-600 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:53 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:53 lpmodule.py:374] update past key value cost 0.028277 seconds
DEBUG 10-15 15:27:53 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:53 lpmodule.py:227] layer idx 19
DEBUG 10-15 15:27:53 lpmodule.py:399] repeat qkv cost 0.029562 seconds
DEBUG 10-15 15:27:53 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:53 lpmodule.py:433] dot attn cost 0.041382 seconds
DEBUG 10-15 15:27:53 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:53 lpmodule.py:444] time cost move to cuda:1 0.02442193031311035 s
DEBUG 10-15 15:27:53 lpllm.py:2283] CPU attn cost 0.149217 seconds if batch True
DEBUG 10-15 15:27:53 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:53 lpllm.py:2294] CPU compute cost 0.150074 seconds
DEBUG 10-15 15:27:53 lpllm.py:2312] free cost 0.000072 seconds
DEBUG 10-15 15:27:53 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:53 lpllm.py:1774] update state cost 4.00543212890625e-05 s
DEBUG 10-15 15:27:53 lpllm.py:1743] restore layer func cost 0.0003991127014160156 s
DEBUG 10-15 15:27:53 lpllm.py:511] restore layer cost 0.0006449222564697266 s
DEBUG 10-15 15:27:53 lpllm.py:512] reset_next_layer_need: layer_attn_idx=20, layer_mlp_idx=19, j_loc=39
DEBUG 10-15 15:27:53 lpllm.py:1037] reset layer cost 0.0007147789001464844 s
DEBUG 10-15 15:27:53 lpllm.py:1038] have reset next layer layer_attn 20 layer_mlp 19 
DEBUG 10-15 15:27:53 lpllm.py:1044] j: 39 waiting the layer with layer_idx 20 before wait time 0.4466125965118408 s
INFO 10-15 15:27:53 client.py:117] confirm_model_loaded: Mixtral-8x7B, a745e8e0-47c8-44f3-8435-b8e38210f7ac
INFO 10-15 15:27:53 client.py:125] Model loaded
DEBUG 10-15 15:27:53 lpllm.py:1048] j: load cost 0.44797348976135254 s waiting cost 0.001344919204711914 s
DEBUG 10-15 15:27:53 lpllm.py:924] 
DEBUG 10-15 15:27:53 lpllm.py:924] decoder loop j: 40 cur_layer_idx: 20 layer_attn_idx: 20 layer_mlp_idx: 19
DEBUG 10-15 15:27:53 lpllm.py:933] start load next layer cur_layer_idx: 21
DEBUG 10-15 15:27:53 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed6e0>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x004\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x98\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:53 client.py:72] load_into_gpu: Mixtral-8x7B, 7f1e0b51-f2e4-409d-883d-245354782269
INFO 10-15 15:27:53 client.py:113] Model loaded: Mixtral-8x7B, 7f1e0b51-f2e4-409d-883d-245354782269
DEBUG 10-15 15:27:53 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:53 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:53 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:53 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:53 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:53 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:53 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:53 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:53 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:53 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:53 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:53 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:53 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:53 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:53 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:53 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:53 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:53 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:53 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:53 lpllm.py:2265] GPU2CPU move cost 0.000605 seconds
DEBUG 10-15 15:27:53 lpmodule.py:340] layer_idx 20 attn layer_idx: 20
DEBUG 10-15 15:27:53 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:53 lpmodule.py:364] decoder_attn_batch update batch_dim 480-540
DEBUG 10-15 15:27:53 StaticCacheLen.py:84] static cache update layer_idx: 20, start_batch: 480, end_batch: 540
DEBUG 10-15 15:27:53 lpmodule.py:368] update for kv cache 480-540 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:53 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:53 lpmodule.py:374] update past key value cost 0.023180 seconds
DEBUG 10-15 15:27:53 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:53 lpmodule.py:227] layer idx 19
DEBUG 10-15 15:27:53 lpmodule.py:399] repeat qkv cost 0.030173 seconds
DEBUG 10-15 15:27:53 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:53 lpmodule.py:433] dot attn cost 0.037826 seconds
DEBUG 10-15 15:27:53 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:53 lpmodule.py:444] time cost move to cuda:1 0.023023366928100586 s
DEBUG 10-15 15:27:53 lpllm.py:2283] CPU attn cost 0.138186 seconds if batch True
DEBUG 10-15 15:27:53 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:53 lpllm.py:2294] CPU compute cost 0.139115 seconds
DEBUG 10-15 15:27:53 lpllm.py:2312] free cost 0.000077 seconds
DEBUG 10-15 15:27:53 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:53 lpllm.py:1774] update state cost 2.2649765014648438e-05 s
DEBUG 10-15 15:27:53 lpllm.py:1743] restore layer func cost 0.0008046627044677734 s
DEBUG 10-15 15:27:53 lpllm.py:511] restore layer cost 0.0010433197021484375 s
DEBUG 10-15 15:27:53 lpllm.py:512] reset_next_layer_need: layer_attn_idx=20, layer_mlp_idx=20, j_loc=40
DEBUG 10-15 15:27:53 lpllm.py:1037] reset layer cost 0.0011153221130371094 s
DEBUG 10-15 15:27:53 lpllm.py:1038] have reset next layer layer_attn 20 layer_mlp 20 
DEBUG 10-15 15:27:53 lpllm.py:924] 
DEBUG 10-15 15:27:53 lpllm.py:924] decoder loop j: 41 cur_layer_idx: 20 layer_attn_idx: 20 layer_mlp_idx: 20
DEBUG 10-15 15:27:53 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:53 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:53 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:53 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:53 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:53 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:53 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:53 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:53 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:53 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:53 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:53 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:53 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:53 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:53 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:53 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:53 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:53 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:53 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:53 lpllm.py:2265] GPU2CPU move cost 0.000573 seconds
DEBUG 10-15 15:27:53 lpmodule.py:340] layer_idx 20 attn layer_idx: 20
DEBUG 10-15 15:27:53 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:53 lpmodule.py:364] decoder_attn_batch update batch_dim 540-600
DEBUG 10-15 15:27:53 StaticCacheLen.py:84] static cache update layer_idx: 20, start_batch: 540, end_batch: 600
DEBUG 10-15 15:27:53 lpmodule.py:368] update for kv cache 540-600 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:53 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:53 lpmodule.py:374] update past key value cost 0.025305 seconds
DEBUG 10-15 15:27:53 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:53 lpmodule.py:227] layer idx 20
DEBUG 10-15 15:27:53 lpmodule.py:399] repeat qkv cost 0.030135 seconds
DEBUG 10-15 15:27:53 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:53 lpmodule.py:433] dot attn cost 0.034576 seconds
DEBUG 10-15 15:27:53 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:53 lpmodule.py:444] time cost move to cuda:1 0.02292943000793457 s
DEBUG 10-15 15:27:53 lpllm.py:2283] CPU attn cost 0.137084 seconds if batch True
DEBUG 10-15 15:27:53 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:27:53 lpllm.py:2294] CPU compute cost 0.137939 seconds
DEBUG 10-15 15:27:53 lpllm.py:2312] free cost 0.000073 seconds
DEBUG 10-15 15:27:53 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:53 lpllm.py:1774] update state cost 4.124641418457031e-05 s
DEBUG 10-15 15:27:53 lpllm.py:1743] restore layer func cost 0.00040221214294433594 s
DEBUG 10-15 15:27:53 lpllm.py:511] restore layer cost 0.0006442070007324219 s
DEBUG 10-15 15:27:53 lpllm.py:512] reset_next_layer_need: layer_attn_idx=21, layer_mlp_idx=20, j_loc=41
DEBUG 10-15 15:27:53 lpllm.py:1037] reset layer cost 0.0007200241088867188 s
DEBUG 10-15 15:27:53 lpllm.py:1038] have reset next layer layer_attn 21 layer_mlp 20 
DEBUG 10-15 15:27:53 lpllm.py:1044] j: 41 waiting the layer with layer_idx 21 before wait time 0.45343947410583496 s
INFO 10-15 15:27:53 client.py:117] confirm_model_loaded: Mixtral-8x7B, 7f1e0b51-f2e4-409d-883d-245354782269
INFO 10-15 15:27:53 client.py:125] Model loaded
DEBUG 10-15 15:27:53 lpllm.py:1048] j: load cost 0.45501160621643066 s waiting cost 0.0015566349029541016 s
DEBUG 10-15 15:27:53 lpllm.py:924] 
DEBUG 10-15 15:27:53 lpllm.py:924] decoder loop j: 42 cur_layer_idx: 21 layer_attn_idx: 21 layer_mlp_idx: 20
DEBUG 10-15 15:27:53 lpllm.py:933] start load next layer cur_layer_idx: 22
DEBUG 10-15 15:27:53 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:53 client.py:72] load_into_gpu: Mixtral-8x7B, 6f8ed95f-687e-45ee-82f2-1b08b5cc20a7
INFO 10-15 15:27:53 client.py:113] Model loaded: Mixtral-8x7B, 6f8ed95f-687e-45ee-82f2-1b08b5cc20a7
DEBUG 10-15 15:27:53 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:53 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:53 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:53 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:53 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:53 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:53 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:53 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:53 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:53 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:53 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:53 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:53 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:53 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:53 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:53 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:53 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:53 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:53 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:53 lpllm.py:2265] GPU2CPU move cost 0.000567 seconds
DEBUG 10-15 15:27:53 lpmodule.py:340] layer_idx 21 attn layer_idx: 21
DEBUG 10-15 15:27:53 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:53 lpmodule.py:364] decoder_attn_batch update batch_dim 480-540
DEBUG 10-15 15:27:54 StaticCacheLen.py:84] static cache update layer_idx: 21, start_batch: 480, end_batch: 540
DEBUG 10-15 15:27:54 lpmodule.py:368] update for kv cache 480-540 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:54 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:54 lpmodule.py:374] update past key value cost 0.025452 seconds
DEBUG 10-15 15:27:54 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:54 lpmodule.py:227] layer idx 20
DEBUG 10-15 15:27:54 lpmodule.py:399] repeat qkv cost 0.029568 seconds
DEBUG 10-15 15:27:54 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:54 lpmodule.py:433] dot attn cost 0.036457 seconds
DEBUG 10-15 15:27:54 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:54 lpmodule.py:444] time cost move to cuda:1 0.023307323455810547 s
DEBUG 10-15 15:27:54 lpllm.py:2283] CPU attn cost 0.138628 seconds if batch True
DEBUG 10-15 15:27:54 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:54 lpllm.py:2294] CPU compute cost 0.139483 seconds
DEBUG 10-15 15:27:54 lpllm.py:2312] free cost 0.000081 seconds
DEBUG 10-15 15:27:54 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:54 lpllm.py:1774] update state cost 3.7670135498046875e-05 s
DEBUG 10-15 15:27:54 lpllm.py:1743] restore layer func cost 0.0008349418640136719 s
DEBUG 10-15 15:27:54 lpllm.py:511] restore layer cost 0.0010836124420166016 s
DEBUG 10-15 15:27:54 lpllm.py:512] reset_next_layer_need: layer_attn_idx=21, layer_mlp_idx=21, j_loc=42
DEBUG 10-15 15:27:54 lpllm.py:1037] reset layer cost 0.001157999038696289 s
DEBUG 10-15 15:27:54 lpllm.py:1038] have reset next layer layer_attn 21 layer_mlp 21 
DEBUG 10-15 15:27:54 lpllm.py:924] 
DEBUG 10-15 15:27:54 lpllm.py:924] decoder loop j: 43 cur_layer_idx: 21 layer_attn_idx: 21 layer_mlp_idx: 21
DEBUG 10-15 15:27:54 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:54 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:54 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:54 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:54 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:54 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:54 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:54 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:54 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:54 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:54 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:54 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:54 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:54 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:54 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:54 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:54 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:54 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:54 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:54 lpllm.py:2265] GPU2CPU move cost 0.000622 seconds
DEBUG 10-15 15:27:54 lpmodule.py:340] layer_idx 21 attn layer_idx: 21
DEBUG 10-15 15:27:54 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:54 lpmodule.py:364] decoder_attn_batch update batch_dim 540-600
DEBUG 10-15 15:27:54 StaticCacheLen.py:84] static cache update layer_idx: 21, start_batch: 540, end_batch: 600
DEBUG 10-15 15:27:54 lpmodule.py:368] update for kv cache 540-600 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:54 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:54 lpmodule.py:374] update past key value cost 0.026232 seconds
DEBUG 10-15 15:27:54 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:54 lpmodule.py:227] layer idx 21
DEBUG 10-15 15:27:54 lpmodule.py:399] repeat qkv cost 0.028983 seconds
DEBUG 10-15 15:27:54 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:54 lpmodule.py:433] dot attn cost 0.038850 seconds
DEBUG 10-15 15:27:54 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:54 lpmodule.py:444] time cost move to cuda:1 0.023236751556396484 s
DEBUG 10-15 15:27:54 lpllm.py:2283] CPU attn cost 0.143624 seconds if batch True
DEBUG 10-15 15:27:54 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:27:54 lpllm.py:2294] CPU compute cost 0.144521 seconds
DEBUG 10-15 15:27:54 lpllm.py:2312] free cost 0.000072 seconds
DEBUG 10-15 15:27:54 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:54 lpllm.py:1774] update state cost 3.361701965332031e-05 s
DEBUG 10-15 15:27:54 lpllm.py:1743] restore layer func cost 0.00040078163146972656 s
DEBUG 10-15 15:27:54 lpllm.py:511] restore layer cost 0.0006597042083740234 s
DEBUG 10-15 15:27:54 lpllm.py:512] reset_next_layer_need: layer_attn_idx=22, layer_mlp_idx=21, j_loc=43
DEBUG 10-15 15:27:54 lpllm.py:1037] reset layer cost 0.0007343292236328125 s
DEBUG 10-15 15:27:54 lpllm.py:1038] have reset next layer layer_attn 22 layer_mlp 21 
DEBUG 10-15 15:27:54 lpllm.py:1044] j: 43 waiting the layer with layer_idx 22 before wait time 0.446150541305542 s
INFO 10-15 15:27:54 client.py:117] confirm_model_loaded: Mixtral-8x7B, 6f8ed95f-687e-45ee-82f2-1b08b5cc20a7
INFO 10-15 15:27:54 client.py:125] Model loaded
DEBUG 10-15 15:27:54 lpllm.py:1048] j: load cost 0.4478938579559326 s waiting cost 0.001728057861328125 s
DEBUG 10-15 15:27:54 lpllm.py:924] 
DEBUG 10-15 15:27:54 lpllm.py:924] decoder loop j: 44 cur_layer_idx: 22 layer_attn_idx: 22 layer_mlp_idx: 21
DEBUG 10-15 15:27:54 lpllm.py:933] start load next layer cur_layer_idx: 23
DEBUG 10-15 15:27:54 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed650>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x003\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x96\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:54 client.py:72] load_into_gpu: Mixtral-8x7B, b15bd5e3-3299-48a7-92fc-8925c466c82e
INFO 10-15 15:27:54 client.py:113] Model loaded: Mixtral-8x7B, b15bd5e3-3299-48a7-92fc-8925c466c82e
DEBUG 10-15 15:27:54 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:54 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:54 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:54 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:54 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:54 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:54 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:54 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:54 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:54 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:54 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:54 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:54 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:54 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:54 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:54 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:54 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:54 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:54 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:54 lpllm.py:2265] GPU2CPU move cost 0.000571 seconds
DEBUG 10-15 15:27:54 lpmodule.py:340] layer_idx 22 attn layer_idx: 22
DEBUG 10-15 15:27:54 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:54 lpmodule.py:364] decoder_attn_batch update batch_dim 480-540
DEBUG 10-15 15:27:54 StaticCacheLen.py:84] static cache update layer_idx: 22, start_batch: 480, end_batch: 540
DEBUG 10-15 15:27:54 lpmodule.py:368] update for kv cache 480-540 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:54 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:54 lpmodule.py:374] update past key value cost 0.024231 seconds
DEBUG 10-15 15:27:54 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:54 lpmodule.py:227] layer idx 21
DEBUG 10-15 15:27:54 lpmodule.py:399] repeat qkv cost 0.030071 seconds
DEBUG 10-15 15:27:54 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:54 lpmodule.py:433] dot attn cost 0.035933 seconds
DEBUG 10-15 15:27:54 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:54 lpmodule.py:444] time cost move to cuda:1 0.023231983184814453 s
DEBUG 10-15 15:27:54 lpllm.py:2283] CPU attn cost 0.138019 seconds if batch True
DEBUG 10-15 15:27:54 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:54 lpllm.py:2294] CPU compute cost 0.138874 seconds
DEBUG 10-15 15:27:54 lpllm.py:2312] free cost 0.000075 seconds
DEBUG 10-15 15:27:54 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:54 lpllm.py:1774] update state cost 3.743171691894531e-05 s
DEBUG 10-15 15:27:54 lpllm.py:1743] restore layer func cost 0.0008335113525390625 s
DEBUG 10-15 15:27:54 lpllm.py:511] restore layer cost 0.0010828971862792969 s
DEBUG 10-15 15:27:54 lpllm.py:512] reset_next_layer_need: layer_attn_idx=22, layer_mlp_idx=22, j_loc=44
DEBUG 10-15 15:27:54 lpllm.py:1037] reset layer cost 0.0011563301086425781 s
DEBUG 10-15 15:27:54 lpllm.py:1038] have reset next layer layer_attn 22 layer_mlp 22 
DEBUG 10-15 15:27:54 lpllm.py:924] 
DEBUG 10-15 15:27:54 lpllm.py:924] decoder loop j: 45 cur_layer_idx: 22 layer_attn_idx: 22 layer_mlp_idx: 22
DEBUG 10-15 15:27:54 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:54 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:54 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:54 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:54 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:54 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:54 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:54 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:54 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:54 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:54 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:54 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:54 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:54 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:54 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:54 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:54 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:54 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:54 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:54 lpllm.py:2265] GPU2CPU move cost 0.000603 seconds
DEBUG 10-15 15:27:54 lpmodule.py:340] layer_idx 22 attn layer_idx: 22
DEBUG 10-15 15:27:54 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:54 lpmodule.py:364] decoder_attn_batch update batch_dim 540-600
DEBUG 10-15 15:27:54 StaticCacheLen.py:84] static cache update layer_idx: 22, start_batch: 540, end_batch: 600
DEBUG 10-15 15:27:54 lpmodule.py:368] update for kv cache 540-600 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:54 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:54 lpmodule.py:374] update past key value cost 0.025806 seconds
DEBUG 10-15 15:27:54 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:54 lpmodule.py:227] layer idx 22
DEBUG 10-15 15:27:54 lpmodule.py:399] repeat qkv cost 0.033751 seconds
DEBUG 10-15 15:27:54 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:54 lpmodule.py:433] dot attn cost 0.036648 seconds
DEBUG 10-15 15:27:54 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:54 lpmodule.py:444] time cost move to cuda:1 0.02355790138244629 s
DEBUG 10-15 15:27:54 lpllm.py:2283] CPU attn cost 0.143811 seconds if batch True
DEBUG 10-15 15:27:54 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:27:54 lpllm.py:2294] CPU compute cost 0.144706 seconds
DEBUG 10-15 15:27:54 lpllm.py:2312] free cost 0.000087 seconds
DEBUG 10-15 15:27:54 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:54 lpllm.py:1774] update state cost 2.384185791015625e-05 s
DEBUG 10-15 15:27:54 lpllm.py:1743] restore layer func cost 0.0003809928894042969 s
DEBUG 10-15 15:27:54 lpllm.py:511] restore layer cost 0.0005917549133300781 s
DEBUG 10-15 15:27:54 lpllm.py:512] reset_next_layer_need: layer_attn_idx=23, layer_mlp_idx=22, j_loc=45
DEBUG 10-15 15:27:54 lpllm.py:1037] reset layer cost 0.0006582736968994141 s
DEBUG 10-15 15:27:54 lpllm.py:1038] have reset next layer layer_attn 23 layer_mlp 22 
DEBUG 10-15 15:27:54 lpllm.py:1044] j: 45 waiting the layer with layer_idx 23 before wait time 0.4537832736968994 s
INFO 10-15 15:27:54 client.py:117] confirm_model_loaded: Mixtral-8x7B, b15bd5e3-3299-48a7-92fc-8925c466c82e
INFO 10-15 15:27:54 client.py:125] Model loaded
DEBUG 10-15 15:27:54 lpllm.py:1048] j: load cost 0.45553040504455566 s waiting cost 0.0017325878143310547 s
DEBUG 10-15 15:27:54 lpllm.py:924] 
DEBUG 10-15 15:27:54 lpllm.py:924] decoder loop j: 46 cur_layer_idx: 23 layer_attn_idx: 23 layer_mlp_idx: 22
DEBUG 10-15 15:27:54 lpllm.py:933] start load next layer cur_layer_idx: 24
DEBUG 10-15 15:27:54 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed6e0>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x004\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x98\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:54 client.py:72] load_into_gpu: Mixtral-8x7B, fbf1c0b6-8dfa-4527-8ca0-6decc1b3c566
INFO 10-15 15:27:54 client.py:113] Model loaded: Mixtral-8x7B, fbf1c0b6-8dfa-4527-8ca0-6decc1b3c566
DEBUG 10-15 15:27:54 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:54 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:54 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:54 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:54 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:54 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:54 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:54 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:54 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:54 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:54 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:54 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:54 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:54 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:54 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:54 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:54 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:54 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:54 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:54 lpllm.py:2265] GPU2CPU move cost 0.000569 seconds
DEBUG 10-15 15:27:54 lpmodule.py:340] layer_idx 23 attn layer_idx: 23
DEBUG 10-15 15:27:54 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:54 lpmodule.py:364] decoder_attn_batch update batch_dim 480-540
DEBUG 10-15 15:27:54 StaticCacheLen.py:84] static cache update layer_idx: 23, start_batch: 480, end_batch: 540
DEBUG 10-15 15:27:54 lpmodule.py:368] update for kv cache 480-540 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:54 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:54 lpmodule.py:374] update past key value cost 0.024054 seconds
DEBUG 10-15 15:27:54 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:54 lpmodule.py:227] layer idx 22
DEBUG 10-15 15:27:54 lpmodule.py:399] repeat qkv cost 0.029791 seconds
DEBUG 10-15 15:27:54 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:54 lpmodule.py:433] dot attn cost 0.033714 seconds
DEBUG 10-15 15:27:54 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:55 lpmodule.py:444] time cost move to cuda:1 0.029711008071899414 s
DEBUG 10-15 15:27:55 lpllm.py:2283] CPU attn cost 0.141013 seconds if batch True
DEBUG 10-15 15:27:55 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:55 lpllm.py:2294] CPU compute cost 0.141881 seconds
DEBUG 10-15 15:27:55 lpllm.py:2312] free cost 0.000075 seconds
DEBUG 10-15 15:27:55 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:55 lpllm.py:1774] update state cost 2.1457672119140625e-05 s
DEBUG 10-15 15:27:55 lpllm.py:1743] restore layer func cost 0.0007946491241455078 s
DEBUG 10-15 15:27:55 lpllm.py:511] restore layer cost 0.0010285377502441406 s
DEBUG 10-15 15:27:55 lpllm.py:512] reset_next_layer_need: layer_attn_idx=23, layer_mlp_idx=23, j_loc=46
DEBUG 10-15 15:27:55 lpllm.py:1037] reset layer cost 0.0010976791381835938 s
DEBUG 10-15 15:27:55 lpllm.py:1038] have reset next layer layer_attn 23 layer_mlp 23 
DEBUG 10-15 15:27:55 lpllm.py:924] 
DEBUG 10-15 15:27:55 lpllm.py:924] decoder loop j: 47 cur_layer_idx: 23 layer_attn_idx: 23 layer_mlp_idx: 23
DEBUG 10-15 15:27:55 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:55 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:55 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:55 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:55 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:55 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:55 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:55 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:55 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:55 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:55 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:55 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:55 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:55 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:55 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:55 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:55 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:55 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:55 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:55 lpllm.py:2265] GPU2CPU move cost 0.000570 seconds
DEBUG 10-15 15:27:55 lpmodule.py:340] layer_idx 23 attn layer_idx: 23
DEBUG 10-15 15:27:55 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:55 lpmodule.py:364] decoder_attn_batch update batch_dim 540-600
DEBUG 10-15 15:27:55 StaticCacheLen.py:84] static cache update layer_idx: 23, start_batch: 540, end_batch: 600
DEBUG 10-15 15:27:55 lpmodule.py:368] update for kv cache 540-600 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:55 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:55 lpmodule.py:374] update past key value cost 0.030537 seconds
DEBUG 10-15 15:27:55 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:55 lpmodule.py:227] layer idx 23
DEBUG 10-15 15:27:55 lpmodule.py:399] repeat qkv cost 0.029834 seconds
DEBUG 10-15 15:27:55 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:55 lpmodule.py:433] dot attn cost 0.034732 seconds
DEBUG 10-15 15:27:55 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:55 lpmodule.py:444] time cost move to cuda:1 0.023290395736694336 s
DEBUG 10-15 15:27:55 lpllm.py:2283] CPU attn cost 0.142147 seconds if batch True
DEBUG 10-15 15:27:55 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:55 lpllm.py:2294] CPU compute cost 0.143001 seconds
DEBUG 10-15 15:27:55 lpllm.py:2312] free cost 0.000073 seconds
DEBUG 10-15 15:27:55 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:55 lpllm.py:1774] update state cost 3.838539123535156e-05 s
DEBUG 10-15 15:27:55 lpllm.py:1743] restore layer func cost 0.00040459632873535156 s
DEBUG 10-15 15:27:55 lpllm.py:511] restore layer cost 0.0006461143493652344 s
DEBUG 10-15 15:27:55 lpllm.py:512] reset_next_layer_need: layer_attn_idx=24, layer_mlp_idx=23, j_loc=47
DEBUG 10-15 15:27:55 lpllm.py:1037] reset layer cost 0.0007181167602539062 s
DEBUG 10-15 15:27:55 lpllm.py:1038] have reset next layer layer_attn 24 layer_mlp 23 
DEBUG 10-15 15:27:55 lpllm.py:1044] j: 47 waiting the layer with layer_idx 24 before wait time 0.4609363079071045 s
INFO 10-15 15:27:55 client.py:117] confirm_model_loaded: Mixtral-8x7B, fbf1c0b6-8dfa-4527-8ca0-6decc1b3c566
INFO 10-15 15:27:55 client.py:125] Model loaded
DEBUG 10-15 15:27:55 lpllm.py:1048] j: load cost 0.4626467227935791 s waiting cost 0.0016951560974121094 s
DEBUG 10-15 15:27:55 lpllm.py:924] 
DEBUG 10-15 15:27:55 lpllm.py:924] decoder loop j: 48 cur_layer_idx: 24 layer_attn_idx: 24 layer_mlp_idx: 23
DEBUG 10-15 15:27:55 lpllm.py:933] start load next layer cur_layer_idx: 25
DEBUG 10-15 15:27:55 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:55 client.py:72] load_into_gpu: Mixtral-8x7B, fd005e98-7529-4553-83b6-5089af5da9a0
INFO 10-15 15:27:55 client.py:113] Model loaded: Mixtral-8x7B, fd005e98-7529-4553-83b6-5089af5da9a0
DEBUG 10-15 15:27:55 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:55 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:55 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:55 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:55 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:55 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:55 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:55 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:55 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:55 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:55 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:55 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:55 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:55 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:55 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:55 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:55 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:55 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:55 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:55 lpllm.py:2265] GPU2CPU move cost 0.000608 seconds
DEBUG 10-15 15:27:55 lpmodule.py:340] layer_idx 24 attn layer_idx: 24
DEBUG 10-15 15:27:55 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:55 lpmodule.py:364] decoder_attn_batch update batch_dim 480-540
DEBUG 10-15 15:27:55 StaticCacheLen.py:84] static cache update layer_idx: 24, start_batch: 480, end_batch: 540
DEBUG 10-15 15:27:55 lpmodule.py:368] update for kv cache 480-540 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:55 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:55 lpmodule.py:374] update past key value cost 0.025334 seconds
DEBUG 10-15 15:27:55 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:55 lpmodule.py:227] layer idx 23
DEBUG 10-15 15:27:55 lpmodule.py:399] repeat qkv cost 0.028689 seconds
DEBUG 10-15 15:27:55 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:55 lpmodule.py:433] dot attn cost 0.039120 seconds
DEBUG 10-15 15:27:55 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:55 lpmodule.py:444] time cost move to cuda:1 0.025252580642700195 s
DEBUG 10-15 15:27:55 lpllm.py:2283] CPU attn cost 0.141743 seconds if batch True
DEBUG 10-15 15:27:55 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:55 lpllm.py:2294] CPU compute cost 0.142630 seconds
DEBUG 10-15 15:27:55 lpllm.py:2312] free cost 0.000074 seconds
DEBUG 10-15 15:27:55 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:55 lpllm.py:1774] update state cost 2.6702880859375e-05 s
DEBUG 10-15 15:27:55 lpllm.py:1743] restore layer func cost 0.0008263587951660156 s
DEBUG 10-15 15:27:55 lpllm.py:511] restore layer cost 0.0010750293731689453 s
DEBUG 10-15 15:27:55 lpllm.py:512] reset_next_layer_need: layer_attn_idx=24, layer_mlp_idx=24, j_loc=48
DEBUG 10-15 15:27:55 lpllm.py:1037] reset layer cost 0.0011515617370605469 s
DEBUG 10-15 15:27:55 lpllm.py:1038] have reset next layer layer_attn 24 layer_mlp 24 
DEBUG 10-15 15:27:55 lpllm.py:924] 
DEBUG 10-15 15:27:55 lpllm.py:924] decoder loop j: 49 cur_layer_idx: 24 layer_attn_idx: 24 layer_mlp_idx: 24
DEBUG 10-15 15:27:55 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:55 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:55 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:55 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:55 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:55 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:55 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:55 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:55 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:55 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:55 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:55 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:55 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:55 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:55 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:55 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:55 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:55 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:55 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:55 lpllm.py:2265] GPU2CPU move cost 0.000623 seconds
DEBUG 10-15 15:27:55 lpmodule.py:340] layer_idx 24 attn layer_idx: 24
DEBUG 10-15 15:27:55 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:55 lpmodule.py:364] decoder_attn_batch update batch_dim 540-600
DEBUG 10-15 15:27:55 StaticCacheLen.py:84] static cache update layer_idx: 24, start_batch: 540, end_batch: 600
DEBUG 10-15 15:27:55 lpmodule.py:368] update for kv cache 540-600 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:55 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:55 lpmodule.py:374] update past key value cost 0.024973 seconds
DEBUG 10-15 15:27:55 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:55 lpmodule.py:227] layer idx 24
DEBUG 10-15 15:27:55 lpmodule.py:399] repeat qkv cost 0.030202 seconds
DEBUG 10-15 15:27:55 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:55 lpmodule.py:433] dot attn cost 0.033508 seconds
DEBUG 10-15 15:27:55 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:55 lpmodule.py:444] time cost move to cuda:1 0.023520231246948242 s
DEBUG 10-15 15:27:55 lpllm.py:2283] CPU attn cost 0.135356 seconds if batch True
DEBUG 10-15 15:27:55 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:55 lpllm.py:2294] CPU compute cost 0.136258 seconds
DEBUG 10-15 15:27:55 lpllm.py:2312] free cost 0.000073 seconds
DEBUG 10-15 15:27:55 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:55 lpllm.py:1774] update state cost 2.5510787963867188e-05 s
DEBUG 10-15 15:27:55 lpllm.py:1743] restore layer func cost 0.0004134178161621094 s
DEBUG 10-15 15:27:55 lpllm.py:511] restore layer cost 0.0006442070007324219 s
DEBUG 10-15 15:27:55 lpllm.py:512] reset_next_layer_need: layer_attn_idx=25, layer_mlp_idx=24, j_loc=49
DEBUG 10-15 15:27:55 lpllm.py:1037] reset layer cost 0.0007131099700927734 s
DEBUG 10-15 15:27:55 lpllm.py:1038] have reset next layer layer_attn 25 layer_mlp 24 
DEBUG 10-15 15:27:55 lpllm.py:1044] j: 49 waiting the layer with layer_idx 25 before wait time 0.4494180679321289 s
INFO 10-15 15:27:55 client.py:117] confirm_model_loaded: Mixtral-8x7B, fd005e98-7529-4553-83b6-5089af5da9a0
INFO 10-15 15:27:55 client.py:125] Model loaded
DEBUG 10-15 15:27:55 lpllm.py:1048] j: load cost 0.4510781764984131 s waiting cost 0.0016455650329589844 s
DEBUG 10-15 15:27:55 lpllm.py:924] 
DEBUG 10-15 15:27:55 lpllm.py:924] decoder loop j: 50 cur_layer_idx: 25 layer_attn_idx: 25 layer_mlp_idx: 24
DEBUG 10-15 15:27:55 lpllm.py:933] start load next layer cur_layer_idx: 26
DEBUG 10-15 15:27:55 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed650>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x003\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x96\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:55 client.py:72] load_into_gpu: Mixtral-8x7B, 689ee722-7e47-4040-b4c6-8c843f3075e0
INFO 10-15 15:27:55 client.py:113] Model loaded: Mixtral-8x7B, 689ee722-7e47-4040-b4c6-8c843f3075e0
DEBUG 10-15 15:27:55 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:55 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:55 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:55 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:55 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:55 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:55 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:55 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:55 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:55 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:55 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:55 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:55 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:55 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:55 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:55 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:55 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:55 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:55 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:55 lpllm.py:2265] GPU2CPU move cost 0.000566 seconds
DEBUG 10-15 15:27:55 lpmodule.py:340] layer_idx 25 attn layer_idx: 25
DEBUG 10-15 15:27:55 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:55 lpmodule.py:364] decoder_attn_batch update batch_dim 480-540
DEBUG 10-15 15:27:55 StaticCacheLen.py:84] static cache update layer_idx: 25, start_batch: 480, end_batch: 540
DEBUG 10-15 15:27:55 lpmodule.py:368] update for kv cache 480-540 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:55 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:55 lpmodule.py:374] update past key value cost 0.024041 seconds
DEBUG 10-15 15:27:55 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:55 lpmodule.py:227] layer idx 24
DEBUG 10-15 15:27:55 lpmodule.py:399] repeat qkv cost 0.030704 seconds
DEBUG 10-15 15:27:55 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:55 lpmodule.py:433] dot attn cost 0.038681 seconds
DEBUG 10-15 15:27:55 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:55 lpmodule.py:444] time cost move to cuda:1 0.023174524307250977 s
DEBUG 10-15 15:27:55 lpllm.py:2283] CPU attn cost 0.140309 seconds if batch True
DEBUG 10-15 15:27:55 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:27:55 lpllm.py:2294] CPU compute cost 0.141159 seconds
DEBUG 10-15 15:27:55 lpllm.py:2312] free cost 0.000077 seconds
DEBUG 10-15 15:27:56 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:56 lpllm.py:1774] update state cost 2.1696090698242188e-05 s
DEBUG 10-15 15:27:56 lpllm.py:1743] restore layer func cost 0.0007977485656738281 s
DEBUG 10-15 15:27:56 lpllm.py:511] restore layer cost 0.0010237693786621094 s
DEBUG 10-15 15:27:56 lpllm.py:512] reset_next_layer_need: layer_attn_idx=25, layer_mlp_idx=25, j_loc=50
DEBUG 10-15 15:27:56 lpllm.py:1037] reset layer cost 0.001093149185180664 s
DEBUG 10-15 15:27:56 lpllm.py:1038] have reset next layer layer_attn 25 layer_mlp 25 
DEBUG 10-15 15:27:56 lpllm.py:924] 
DEBUG 10-15 15:27:56 lpllm.py:924] decoder loop j: 51 cur_layer_idx: 25 layer_attn_idx: 25 layer_mlp_idx: 25
DEBUG 10-15 15:27:56 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:56 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:56 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:56 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:56 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:56 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:56 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:56 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:56 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:56 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:56 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:56 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:56 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:56 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:56 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:56 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:56 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:56 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:56 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:56 lpllm.py:2265] GPU2CPU move cost 0.000568 seconds
DEBUG 10-15 15:27:56 lpmodule.py:340] layer_idx 25 attn layer_idx: 25
DEBUG 10-15 15:27:56 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:56 lpmodule.py:364] decoder_attn_batch update batch_dim 540-600
DEBUG 10-15 15:27:56 StaticCacheLen.py:84] static cache update layer_idx: 25, start_batch: 540, end_batch: 600
DEBUG 10-15 15:27:56 lpmodule.py:368] update for kv cache 540-600 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:56 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:56 lpmodule.py:374] update past key value cost 0.026054 seconds
DEBUG 10-15 15:27:56 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:56 lpmodule.py:227] layer idx 25
DEBUG 10-15 15:27:56 lpmodule.py:399] repeat qkv cost 0.032382 seconds
DEBUG 10-15 15:27:56 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:56 lpmodule.py:433] dot attn cost 0.035391 seconds
DEBUG 10-15 15:27:56 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:56 lpmodule.py:444] time cost move to cuda:1 0.023583173751831055 s
DEBUG 10-15 15:27:56 lpllm.py:2283] CPU attn cost 0.141590 seconds if batch True
DEBUG 10-15 15:27:56 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:56 lpllm.py:2294] CPU compute cost 0.142435 seconds
DEBUG 10-15 15:27:56 lpllm.py:2312] free cost 0.000072 seconds
DEBUG 10-15 15:27:56 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:56 lpllm.py:1774] update state cost 2.384185791015625e-05 s
DEBUG 10-15 15:27:56 lpllm.py:1743] restore layer func cost 0.0004012584686279297 s
DEBUG 10-15 15:27:56 lpllm.py:511] restore layer cost 0.0006265640258789062 s
DEBUG 10-15 15:27:56 lpllm.py:512] reset_next_layer_need: layer_attn_idx=26, layer_mlp_idx=25, j_loc=51
DEBUG 10-15 15:27:56 lpllm.py:1037] reset layer cost 0.0006973743438720703 s
DEBUG 10-15 15:27:56 lpllm.py:1038] have reset next layer layer_attn 26 layer_mlp 25 
DEBUG 10-15 15:27:56 lpllm.py:1044] j: 51 waiting the layer with layer_idx 26 before wait time 0.44965171813964844 s
INFO 10-15 15:27:56 client.py:117] confirm_model_loaded: Mixtral-8x7B, 689ee722-7e47-4040-b4c6-8c843f3075e0
INFO 10-15 15:27:56 client.py:125] Model loaded
DEBUG 10-15 15:27:56 lpllm.py:1048] j: load cost 0.45134687423706055 s waiting cost 0.0016796588897705078 s
DEBUG 10-15 15:27:56 lpllm.py:924] 
DEBUG 10-15 15:27:56 lpllm.py:924] decoder loop j: 52 cur_layer_idx: 26 layer_attn_idx: 26 layer_mlp_idx: 25
DEBUG 10-15 15:27:56 lpllm.py:933] start load next layer cur_layer_idx: 27
DEBUG 10-15 15:27:56 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed6e0>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x004\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x98\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:56 client.py:72] load_into_gpu: Mixtral-8x7B, aba0450d-ef1e-4d7d-a107-0f8a4c0c1618
INFO 10-15 15:27:56 client.py:113] Model loaded: Mixtral-8x7B, aba0450d-ef1e-4d7d-a107-0f8a4c0c1618
DEBUG 10-15 15:27:56 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:56 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:56 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:56 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:56 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:56 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:56 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:56 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:56 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:56 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:56 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:56 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:56 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:56 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:56 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:56 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:56 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:56 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:56 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:56 lpllm.py:2265] GPU2CPU move cost 0.000584 seconds
DEBUG 10-15 15:27:56 lpmodule.py:340] layer_idx 26 attn layer_idx: 26
DEBUG 10-15 15:27:56 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:56 lpmodule.py:364] decoder_attn_batch update batch_dim 480-540
DEBUG 10-15 15:27:56 StaticCacheLen.py:84] static cache update layer_idx: 26, start_batch: 480, end_batch: 540
DEBUG 10-15 15:27:56 lpmodule.py:368] update for kv cache 480-540 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:56 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:56 lpmodule.py:374] update past key value cost 0.026077 seconds
DEBUG 10-15 15:27:56 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:56 lpmodule.py:227] layer idx 25
DEBUG 10-15 15:27:56 lpmodule.py:399] repeat qkv cost 0.031161 seconds
DEBUG 10-15 15:27:56 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:56 lpmodule.py:433] dot attn cost 0.039912 seconds
DEBUG 10-15 15:27:56 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:56 lpmodule.py:444] time cost move to cuda:1 0.023418426513671875 s
DEBUG 10-15 15:27:56 lpllm.py:2283] CPU attn cost 0.143948 seconds if batch True
DEBUG 10-15 15:27:56 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:27:56 lpllm.py:2294] CPU compute cost 0.144813 seconds
DEBUG 10-15 15:27:56 lpllm.py:2312] free cost 0.000084 seconds
DEBUG 10-15 15:27:56 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:56 lpllm.py:1774] update state cost 3.814697265625e-05 s
DEBUG 10-15 15:27:56 lpllm.py:1743] restore layer func cost 0.000835418701171875 s
DEBUG 10-15 15:27:56 lpllm.py:511] restore layer cost 0.0010905265808105469 s
DEBUG 10-15 15:27:56 lpllm.py:512] reset_next_layer_need: layer_attn_idx=26, layer_mlp_idx=26, j_loc=52
DEBUG 10-15 15:27:56 lpllm.py:1037] reset layer cost 0.001165628433227539 s
DEBUG 10-15 15:27:56 lpllm.py:1038] have reset next layer layer_attn 26 layer_mlp 26 
DEBUG 10-15 15:27:56 lpllm.py:924] 
DEBUG 10-15 15:27:56 lpllm.py:924] decoder loop j: 53 cur_layer_idx: 26 layer_attn_idx: 26 layer_mlp_idx: 26
DEBUG 10-15 15:27:56 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:56 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:56 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:56 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:56 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:56 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:56 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:56 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:56 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:56 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:56 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:56 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:56 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:56 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:56 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:56 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:56 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:56 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:56 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:56 lpllm.py:2265] GPU2CPU move cost 0.000611 seconds
DEBUG 10-15 15:27:56 lpmodule.py:340] layer_idx 26 attn layer_idx: 26
DEBUG 10-15 15:27:56 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:56 lpmodule.py:364] decoder_attn_batch update batch_dim 540-600
DEBUG 10-15 15:27:56 StaticCacheLen.py:84] static cache update layer_idx: 26, start_batch: 540, end_batch: 600
DEBUG 10-15 15:27:56 lpmodule.py:368] update for kv cache 540-600 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:56 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:56 lpmodule.py:374] update past key value cost 0.027699 seconds
DEBUG 10-15 15:27:56 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:56 lpmodule.py:227] layer idx 26
DEBUG 10-15 15:27:56 lpmodule.py:399] repeat qkv cost 0.029003 seconds
DEBUG 10-15 15:27:56 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:56 lpmodule.py:433] dot attn cost 0.035506 seconds
DEBUG 10-15 15:27:56 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:56 lpmodule.py:444] time cost move to cuda:1 0.026623010635375977 s
DEBUG 10-15 15:27:56 lpllm.py:2283] CPU attn cost 0.142969 seconds if batch True
DEBUG 10-15 15:27:56 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:56 lpllm.py:2294] CPU compute cost 0.143877 seconds
DEBUG 10-15 15:27:56 lpllm.py:2312] free cost 0.000074 seconds
DEBUG 10-15 15:27:56 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:56 lpllm.py:1774] update state cost 4.100799560546875e-05 s
DEBUG 10-15 15:27:56 lpllm.py:1743] restore layer func cost 0.0003948211669921875 s
DEBUG 10-15 15:27:56 lpllm.py:511] restore layer cost 0.0006387233734130859 s
DEBUG 10-15 15:27:56 lpllm.py:512] reset_next_layer_need: layer_attn_idx=27, layer_mlp_idx=26, j_loc=53
DEBUG 10-15 15:27:56 lpllm.py:1037] reset layer cost 0.0007112026214599609 s
DEBUG 10-15 15:27:56 lpllm.py:1038] have reset next layer layer_attn 27 layer_mlp 26 
DEBUG 10-15 15:27:56 lpllm.py:1044] j: 53 waiting the layer with layer_idx 27 before wait time 0.5188262462615967 s
INFO 10-15 15:27:56 client.py:117] confirm_model_loaded: Mixtral-8x7B, aba0450d-ef1e-4d7d-a107-0f8a4c0c1618
INFO 10-15 15:27:56 client.py:125] Model loaded
DEBUG 10-15 15:27:56 lpllm.py:1048] j: load cost 0.5205690860748291 s waiting cost 0.0017271041870117188 s
DEBUG 10-15 15:27:56 lpllm.py:924] 
DEBUG 10-15 15:27:56 lpllm.py:924] decoder loop j: 54 cur_layer_idx: 27 layer_attn_idx: 27 layer_mlp_idx: 26
DEBUG 10-15 15:27:56 lpllm.py:933] start load next layer cur_layer_idx: 28
DEBUG 10-15 15:27:56 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:56 client.py:72] load_into_gpu: Mixtral-8x7B, f400d1f5-fdda-4f9b-bd62-6afab85f9ae4
INFO 10-15 15:27:56 client.py:113] Model loaded: Mixtral-8x7B, f400d1f5-fdda-4f9b-bd62-6afab85f9ae4
DEBUG 10-15 15:27:56 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:56 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:56 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:56 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:56 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:56 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:56 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:56 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:56 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:56 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:56 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:56 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:56 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:56 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:56 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:56 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:56 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:56 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:56 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:56 lpllm.py:2265] GPU2CPU move cost 0.000568 seconds
DEBUG 10-15 15:27:56 lpmodule.py:340] layer_idx 27 attn layer_idx: 27
DEBUG 10-15 15:27:56 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:56 lpmodule.py:364] decoder_attn_batch update batch_dim 480-540
DEBUG 10-15 15:27:56 StaticCacheLen.py:84] static cache update layer_idx: 27, start_batch: 480, end_batch: 540
DEBUG 10-15 15:27:56 lpmodule.py:368] update for kv cache 480-540 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:56 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:56 lpmodule.py:374] update past key value cost 0.026209 seconds
DEBUG 10-15 15:27:56 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:56 lpmodule.py:227] layer idx 26
DEBUG 10-15 15:27:56 lpmodule.py:399] repeat qkv cost 0.030002 seconds
DEBUG 10-15 15:27:56 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:56 lpmodule.py:433] dot attn cost 0.039846 seconds
DEBUG 10-15 15:27:56 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:56 lpmodule.py:444] time cost move to cuda:1 0.023632526397705078 s
DEBUG 10-15 15:27:56 lpllm.py:2283] CPU attn cost 0.143119 seconds if batch True
DEBUG 10-15 15:27:56 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:56 lpllm.py:2294] CPU compute cost 0.143977 seconds
DEBUG 10-15 15:27:56 lpllm.py:2312] free cost 0.000075 seconds
DEBUG 10-15 15:27:56 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:56 lpllm.py:1774] update state cost 2.09808349609375e-05 s
DEBUG 10-15 15:27:56 lpllm.py:1743] restore layer func cost 0.0008103847503662109 s
DEBUG 10-15 15:27:56 lpllm.py:511] restore layer cost 0.001050710678100586 s
DEBUG 10-15 15:27:56 lpllm.py:512] reset_next_layer_need: layer_attn_idx=27, layer_mlp_idx=27, j_loc=54
DEBUG 10-15 15:27:56 lpllm.py:1037] reset layer cost 0.00112152099609375 s
DEBUG 10-15 15:27:56 lpllm.py:1038] have reset next layer layer_attn 27 layer_mlp 27 
DEBUG 10-15 15:27:56 lpllm.py:924] 
DEBUG 10-15 15:27:56 lpllm.py:924] decoder loop j: 55 cur_layer_idx: 27 layer_attn_idx: 27 layer_mlp_idx: 27
DEBUG 10-15 15:27:56 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:56 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:56 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:56 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:56 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:56 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:56 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:56 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:56 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:56 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:56 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:56 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:56 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:56 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:56 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:56 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:56 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:56 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:56 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:57 lpllm.py:2265] GPU2CPU move cost 0.000572 seconds
DEBUG 10-15 15:27:57 lpmodule.py:340] layer_idx 27 attn layer_idx: 27
DEBUG 10-15 15:27:57 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:57 lpmodule.py:364] decoder_attn_batch update batch_dim 540-600
DEBUG 10-15 15:27:57 StaticCacheLen.py:84] static cache update layer_idx: 27, start_batch: 540, end_batch: 600
DEBUG 10-15 15:27:57 lpmodule.py:368] update for kv cache 540-600 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:57 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:57 lpmodule.py:374] update past key value cost 0.027177 seconds
DEBUG 10-15 15:27:57 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:57 lpmodule.py:227] layer idx 27
DEBUG 10-15 15:27:57 lpmodule.py:399] repeat qkv cost 0.028616 seconds
DEBUG 10-15 15:27:57 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:57 lpmodule.py:433] dot attn cost 0.036312 seconds
DEBUG 10-15 15:27:57 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:57 lpmodule.py:444] time cost move to cuda:1 0.023415088653564453 s
DEBUG 10-15 15:27:57 lpllm.py:2283] CPU attn cost 0.139283 seconds if batch True
DEBUG 10-15 15:27:57 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:57 lpllm.py:2294] CPU compute cost 0.140146 seconds
DEBUG 10-15 15:27:57 lpllm.py:2312] free cost 0.000076 seconds
DEBUG 10-15 15:27:57 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:57 lpllm.py:1774] update state cost 2.4318695068359375e-05 s
DEBUG 10-15 15:27:57 lpllm.py:1743] restore layer func cost 0.0003895759582519531 s
DEBUG 10-15 15:27:57 lpllm.py:511] restore layer cost 0.0006194114685058594 s
DEBUG 10-15 15:27:57 lpllm.py:512] reset_next_layer_need: layer_attn_idx=28, layer_mlp_idx=27, j_loc=55
DEBUG 10-15 15:27:57 lpllm.py:1037] reset layer cost 0.0006885528564453125 s
DEBUG 10-15 15:27:57 lpllm.py:1038] have reset next layer layer_attn 28 layer_mlp 27 
DEBUG 10-15 15:27:57 lpllm.py:1044] j: 55 waiting the layer with layer_idx 28 before wait time 0.4558720588684082 s
INFO 10-15 15:27:57 client.py:117] confirm_model_loaded: Mixtral-8x7B, f400d1f5-fdda-4f9b-bd62-6afab85f9ae4
INFO 10-15 15:27:57 client.py:125] Model loaded
DEBUG 10-15 15:27:57 lpllm.py:1048] j: load cost 0.457751989364624 s waiting cost 0.00186920166015625 s
DEBUG 10-15 15:27:57 lpllm.py:924] 
DEBUG 10-15 15:27:57 lpllm.py:924] decoder loop j: 56 cur_layer_idx: 28 layer_attn_idx: 28 layer_mlp_idx: 27
DEBUG 10-15 15:27:57 lpllm.py:933] start load next layer cur_layer_idx: 29
DEBUG 10-15 15:27:57 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed650>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x003\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x96\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:57 client.py:72] load_into_gpu: Mixtral-8x7B, 76eb04b2-2902-4f2c-a808-185de58a08b5
INFO 10-15 15:27:57 client.py:113] Model loaded: Mixtral-8x7B, 76eb04b2-2902-4f2c-a808-185de58a08b5
DEBUG 10-15 15:27:57 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:57 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:57 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:57 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:57 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:57 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:57 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:57 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:57 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:57 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:57 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:57 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:57 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:57 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:57 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:57 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:57 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:57 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:57 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:57 lpllm.py:2265] GPU2CPU move cost 0.000612 seconds
DEBUG 10-15 15:27:57 lpmodule.py:340] layer_idx 28 attn layer_idx: 28
DEBUG 10-15 15:27:57 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:57 lpmodule.py:364] decoder_attn_batch update batch_dim 480-540
DEBUG 10-15 15:27:57 StaticCacheLen.py:84] static cache update layer_idx: 28, start_batch: 480, end_batch: 540
DEBUG 10-15 15:27:57 lpmodule.py:368] update for kv cache 480-540 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:57 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:57 lpmodule.py:374] update past key value cost 0.024555 seconds
DEBUG 10-15 15:27:57 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:57 lpmodule.py:227] layer idx 27
DEBUG 10-15 15:27:57 lpmodule.py:399] repeat qkv cost 0.029141 seconds
DEBUG 10-15 15:27:57 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:57 lpmodule.py:433] dot attn cost 0.035820 seconds
DEBUG 10-15 15:27:57 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:57 lpmodule.py:444] time cost move to cuda:1 0.023008346557617188 s
DEBUG 10-15 15:27:57 lpllm.py:2283] CPU attn cost 0.136131 seconds if batch True
DEBUG 10-15 15:27:57 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:57 lpllm.py:2294] CPU compute cost 0.137032 seconds
DEBUG 10-15 15:27:57 lpllm.py:2312] free cost 0.000076 seconds
DEBUG 10-15 15:27:57 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:57 lpllm.py:1774] update state cost 2.3603439331054688e-05 s
DEBUG 10-15 15:27:57 lpllm.py:1743] restore layer func cost 0.0008301734924316406 s
DEBUG 10-15 15:27:57 lpllm.py:511] restore layer cost 0.0010666847229003906 s
DEBUG 10-15 15:27:57 lpllm.py:512] reset_next_layer_need: layer_attn_idx=28, layer_mlp_idx=28, j_loc=56
DEBUG 10-15 15:27:57 lpllm.py:1037] reset layer cost 0.001138448715209961 s
DEBUG 10-15 15:27:57 lpllm.py:1038] have reset next layer layer_attn 28 layer_mlp 28 
DEBUG 10-15 15:27:57 lpllm.py:924] 
DEBUG 10-15 15:27:57 lpllm.py:924] decoder loop j: 57 cur_layer_idx: 28 layer_attn_idx: 28 layer_mlp_idx: 28
DEBUG 10-15 15:27:57 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:57 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:57 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:57 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:57 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:57 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:57 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:57 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:57 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:57 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:57 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:57 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:57 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:57 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:57 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:57 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:57 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:57 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:57 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:57 lpllm.py:2265] GPU2CPU move cost 0.000570 seconds
DEBUG 10-15 15:27:57 lpmodule.py:340] layer_idx 28 attn layer_idx: 28
DEBUG 10-15 15:27:57 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:57 lpmodule.py:364] decoder_attn_batch update batch_dim 540-600
DEBUG 10-15 15:27:57 StaticCacheLen.py:84] static cache update layer_idx: 28, start_batch: 540, end_batch: 600
DEBUG 10-15 15:27:57 lpmodule.py:368] update for kv cache 540-600 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:57 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:57 lpmodule.py:374] update past key value cost 0.028312 seconds
DEBUG 10-15 15:27:57 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:57 lpmodule.py:227] layer idx 28
DEBUG 10-15 15:27:57 lpmodule.py:399] repeat qkv cost 0.032425 seconds
DEBUG 10-15 15:27:57 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:57 lpmodule.py:433] dot attn cost 0.038830 seconds
DEBUG 10-15 15:27:57 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:57 lpmodule.py:444] time cost move to cuda:1 0.022940874099731445 s
DEBUG 10-15 15:27:57 lpllm.py:2283] CPU attn cost 0.146364 seconds if batch True
DEBUG 10-15 15:27:57 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:57 lpllm.py:2294] CPU compute cost 0.147215 seconds
DEBUG 10-15 15:27:57 lpllm.py:2312] free cost 0.000077 seconds
DEBUG 10-15 15:27:57 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:57 lpllm.py:1774] update state cost 4.1961669921875e-05 s
DEBUG 10-15 15:27:57 lpllm.py:1743] restore layer func cost 0.00040912628173828125 s
DEBUG 10-15 15:27:57 lpllm.py:511] restore layer cost 0.0006542205810546875 s
DEBUG 10-15 15:27:57 lpllm.py:512] reset_next_layer_need: layer_attn_idx=29, layer_mlp_idx=28, j_loc=57
DEBUG 10-15 15:27:57 lpllm.py:1037] reset layer cost 0.0007278919219970703 s
DEBUG 10-15 15:27:57 lpllm.py:1038] have reset next layer layer_attn 29 layer_mlp 28 
DEBUG 10-15 15:27:57 lpllm.py:1044] j: 57 waiting the layer with layer_idx 29 before wait time 0.4549877643585205 s
INFO 10-15 15:27:57 client.py:117] confirm_model_loaded: Mixtral-8x7B, 76eb04b2-2902-4f2c-a808-185de58a08b5
INFO 10-15 15:27:57 client.py:125] Model loaded
DEBUG 10-15 15:27:57 lpllm.py:1048] j: load cost 0.45669102668762207 s waiting cost 0.0016875267028808594 s
DEBUG 10-15 15:27:57 lpllm.py:924] 
DEBUG 10-15 15:27:57 lpllm.py:924] decoder loop j: 58 cur_layer_idx: 29 layer_attn_idx: 29 layer_mlp_idx: 28
DEBUG 10-15 15:27:57 lpllm.py:933] start load next layer cur_layer_idx: 30
DEBUG 10-15 15:27:57 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed6e0>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x004\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x98\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:57 client.py:72] load_into_gpu: Mixtral-8x7B, 31975d2b-1a7e-4d07-8b34-5c54f0f1ed15
INFO 10-15 15:27:57 client.py:113] Model loaded: Mixtral-8x7B, 31975d2b-1a7e-4d07-8b34-5c54f0f1ed15
DEBUG 10-15 15:27:57 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:57 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:57 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:57 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:57 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:57 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:57 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:57 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:57 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:57 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:57 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:57 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:57 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:57 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:57 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:57 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:57 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:57 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:57 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:57 lpllm.py:2265] GPU2CPU move cost 0.000607 seconds
DEBUG 10-15 15:27:57 lpmodule.py:340] layer_idx 29 attn layer_idx: 29
DEBUG 10-15 15:27:57 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:57 lpmodule.py:364] decoder_attn_batch update batch_dim 480-540
DEBUG 10-15 15:27:57 StaticCacheLen.py:84] static cache update layer_idx: 29, start_batch: 480, end_batch: 540
DEBUG 10-15 15:27:57 lpmodule.py:368] update for kv cache 480-540 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:57 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:57 lpmodule.py:374] update past key value cost 0.025097 seconds
DEBUG 10-15 15:27:57 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:57 lpmodule.py:227] layer idx 28
DEBUG 10-15 15:27:57 lpmodule.py:399] repeat qkv cost 0.029089 seconds
DEBUG 10-15 15:27:57 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:57 lpmodule.py:433] dot attn cost 0.039836 seconds
DEBUG 10-15 15:27:57 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:57 lpmodule.py:444] time cost move to cuda:1 0.023149490356445312 s
DEBUG 10-15 15:27:57 lpllm.py:2283] CPU attn cost 0.140734 seconds if batch True
DEBUG 10-15 15:27:57 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:27:57 lpllm.py:2294] CPU compute cost 0.141626 seconds
DEBUG 10-15 15:27:57 lpllm.py:2312] free cost 0.000081 seconds
DEBUG 10-15 15:27:57 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:57 lpllm.py:1774] update state cost 3.647804260253906e-05 s
DEBUG 10-15 15:27:57 lpllm.py:1743] restore layer func cost 0.0008463859558105469 s
DEBUG 10-15 15:27:57 lpllm.py:511] restore layer cost 0.0010921955108642578 s
DEBUG 10-15 15:27:57 lpllm.py:512] reset_next_layer_need: layer_attn_idx=29, layer_mlp_idx=29, j_loc=58
DEBUG 10-15 15:27:57 lpllm.py:1037] reset layer cost 0.0011644363403320312 s
DEBUG 10-15 15:27:57 lpllm.py:1038] have reset next layer layer_attn 29 layer_mlp 29 
DEBUG 10-15 15:27:57 lpllm.py:924] 
DEBUG 10-15 15:27:57 lpllm.py:924] decoder loop j: 59 cur_layer_idx: 29 layer_attn_idx: 29 layer_mlp_idx: 29
DEBUG 10-15 15:27:57 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:57 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:57 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:57 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:57 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:57 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:57 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:57 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:57 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:57 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:57 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:57 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:57 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:57 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:57 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:57 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:57 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:57 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:57 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:57 lpllm.py:2265] GPU2CPU move cost 0.000629 seconds
DEBUG 10-15 15:27:57 lpmodule.py:340] layer_idx 29 attn layer_idx: 29
DEBUG 10-15 15:27:57 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:57 lpmodule.py:364] decoder_attn_batch update batch_dim 540-600
DEBUG 10-15 15:27:57 StaticCacheLen.py:84] static cache update layer_idx: 29, start_batch: 540, end_batch: 600
DEBUG 10-15 15:27:57 lpmodule.py:368] update for kv cache 540-600 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:57 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:57 lpmodule.py:374] update past key value cost 0.027611 seconds
DEBUG 10-15 15:27:57 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:57 lpmodule.py:227] layer idx 29
DEBUG 10-15 15:27:57 lpmodule.py:399] repeat qkv cost 0.029697 seconds
DEBUG 10-15 15:27:57 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:58 lpmodule.py:433] dot attn cost 0.041379 seconds
DEBUG 10-15 15:27:58 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:58 lpmodule.py:444] time cost move to cuda:1 0.023563861846923828 s
DEBUG 10-15 15:27:58 lpllm.py:2283] CPU attn cost 0.147031 seconds if batch True
DEBUG 10-15 15:27:58 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:58 lpllm.py:2294] CPU compute cost 0.147947 seconds
DEBUG 10-15 15:27:58 lpllm.py:2312] free cost 0.000074 seconds
DEBUG 10-15 15:27:58 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:58 lpllm.py:1774] update state cost 4.124641418457031e-05 s
DEBUG 10-15 15:27:58 lpllm.py:1743] restore layer func cost 0.0003998279571533203 s
DEBUG 10-15 15:27:58 lpllm.py:511] restore layer cost 0.0006706714630126953 s
DEBUG 10-15 15:27:58 lpllm.py:512] reset_next_layer_need: layer_attn_idx=30, layer_mlp_idx=29, j_loc=59
DEBUG 10-15 15:27:58 lpllm.py:1037] reset layer cost 0.0007424354553222656 s
DEBUG 10-15 15:27:58 lpllm.py:1038] have reset next layer layer_attn 30 layer_mlp 29 
DEBUG 10-15 15:27:58 lpllm.py:1044] j: 59 waiting the layer with layer_idx 30 before wait time 0.4475271701812744 s
INFO 10-15 15:27:58 client.py:117] confirm_model_loaded: Mixtral-8x7B, 31975d2b-1a7e-4d07-8b34-5c54f0f1ed15
INFO 10-15 15:27:58 client.py:125] Model loaded
DEBUG 10-15 15:27:58 lpllm.py:1048] j: load cost 0.44922661781311035 s waiting cost 0.0016851425170898438 s
DEBUG 10-15 15:27:58 lpllm.py:924] 
DEBUG 10-15 15:27:58 lpllm.py:924] decoder loop j: 60 cur_layer_idx: 30 layer_attn_idx: 30 layer_mlp_idx: 29
DEBUG 10-15 15:27:58 lpllm.py:933] start load next layer cur_layer_idx: 31
DEBUG 10-15 15:27:58 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:58 client.py:72] load_into_gpu: Mixtral-8x7B, 5988f469-0000-4e62-898e-568c2e733afc
INFO 10-15 15:27:58 client.py:113] Model loaded: Mixtral-8x7B, 5988f469-0000-4e62-898e-568c2e733afc
DEBUG 10-15 15:27:58 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:58 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:58 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:58 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:58 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:58 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:58 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:58 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:58 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:58 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:58 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:58 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:58 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:58 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:58 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:58 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:58 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:58 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:58 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:58 lpllm.py:2265] GPU2CPU move cost 0.000573 seconds
DEBUG 10-15 15:27:58 lpmodule.py:340] layer_idx 30 attn layer_idx: 30
DEBUG 10-15 15:27:58 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:58 lpmodule.py:364] decoder_attn_batch update batch_dim 480-540
DEBUG 10-15 15:27:58 StaticCacheLen.py:84] static cache update layer_idx: 30, start_batch: 480, end_batch: 540
DEBUG 10-15 15:27:58 lpmodule.py:368] update for kv cache 480-540 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:58 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:58 lpmodule.py:374] update past key value cost 0.024308 seconds
DEBUG 10-15 15:27:58 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:58 lpmodule.py:227] layer idx 29
DEBUG 10-15 15:27:58 lpmodule.py:399] repeat qkv cost 0.029771 seconds
DEBUG 10-15 15:27:58 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:58 lpmodule.py:433] dot attn cost 0.034890 seconds
DEBUG 10-15 15:27:58 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:58 lpmodule.py:444] time cost move to cuda:1 0.02343010902404785 s
DEBUG 10-15 15:27:58 lpllm.py:2283] CPU attn cost 0.138158 seconds if batch True
DEBUG 10-15 15:27:58 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:58 lpllm.py:2294] CPU compute cost 0.139015 seconds
DEBUG 10-15 15:27:58 lpllm.py:2312] free cost 0.000073 seconds
DEBUG 10-15 15:27:58 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:58 lpllm.py:1774] update state cost 2.6464462280273438e-05 s
DEBUG 10-15 15:27:58 lpllm.py:1743] restore layer func cost 0.0008165836334228516 s
DEBUG 10-15 15:27:58 lpllm.py:511] restore layer cost 0.0010859966278076172 s
DEBUG 10-15 15:27:58 lpllm.py:512] reset_next_layer_need: layer_attn_idx=30, layer_mlp_idx=30, j_loc=60
DEBUG 10-15 15:27:58 lpllm.py:1037] reset layer cost 0.0011718273162841797 s
DEBUG 10-15 15:27:58 lpllm.py:1038] have reset next layer layer_attn 30 layer_mlp 30 
DEBUG 10-15 15:27:58 lpllm.py:924] 
DEBUG 10-15 15:27:58 lpllm.py:924] decoder loop j: 61 cur_layer_idx: 30 layer_attn_idx: 30 layer_mlp_idx: 30
DEBUG 10-15 15:27:58 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:58 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:58 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:58 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:58 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:58 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:58 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:58 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:58 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:58 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:58 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:58 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:58 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:58 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:58 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:58 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:58 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:58 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:58 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:58 lpllm.py:2265] GPU2CPU move cost 0.000655 seconds
DEBUG 10-15 15:27:58 lpmodule.py:340] layer_idx 30 attn layer_idx: 30
DEBUG 10-15 15:27:58 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:58 lpmodule.py:364] decoder_attn_batch update batch_dim 540-600
DEBUG 10-15 15:27:58 StaticCacheLen.py:84] static cache update layer_idx: 30, start_batch: 540, end_batch: 600
DEBUG 10-15 15:27:58 lpmodule.py:368] update for kv cache 540-600 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:58 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:58 lpmodule.py:374] update past key value cost 0.024795 seconds
DEBUG 10-15 15:27:58 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:58 lpmodule.py:227] layer idx 30
DEBUG 10-15 15:27:58 lpmodule.py:399] repeat qkv cost 0.030513 seconds
DEBUG 10-15 15:27:58 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:58 lpmodule.py:433] dot attn cost 0.035561 seconds
DEBUG 10-15 15:27:58 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:58 lpmodule.py:444] time cost move to cuda:1 0.02544999122619629 s
DEBUG 10-15 15:27:58 lpllm.py:2283] CPU attn cost 0.140009 seconds if batch True
DEBUG 10-15 15:27:58 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:58 lpllm.py:2294] CPU compute cost 0.140961 seconds
DEBUG 10-15 15:27:58 lpllm.py:2312] free cost 0.000075 seconds
DEBUG 10-15 15:27:58 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:58 lpllm.py:1774] update state cost 3.266334533691406e-05 s
DEBUG 10-15 15:27:58 lpllm.py:1743] restore layer func cost 0.0003840923309326172 s
DEBUG 10-15 15:27:58 lpllm.py:511] restore layer cost 0.0006814002990722656 s
DEBUG 10-15 15:27:58 lpllm.py:512] reset_next_layer_need: layer_attn_idx=31, layer_mlp_idx=30, j_loc=61
DEBUG 10-15 15:27:58 lpllm.py:1037] reset layer cost 0.0007658004760742188 s
DEBUG 10-15 15:27:58 lpllm.py:1038] have reset next layer layer_attn 31 layer_mlp 30 
DEBUG 10-15 15:27:58 lpllm.py:1044] j: 61 waiting the layer with layer_idx 31 before wait time 0.45294952392578125 s
INFO 10-15 15:27:58 client.py:117] confirm_model_loaded: Mixtral-8x7B, 5988f469-0000-4e62-898e-568c2e733afc
INFO 10-15 15:27:58 client.py:125] Model loaded
DEBUG 10-15 15:27:58 lpllm.py:1048] j: load cost 0.4546377658843994 s waiting cost 0.0016717910766601562 s
DEBUG 10-15 15:27:58 lpllm.py:924] 
DEBUG 10-15 15:27:58 lpllm.py:924] decoder loop j: 62 cur_layer_idx: 31 layer_attn_idx: 31 layer_mlp_idx: 30
DEBUG 10-15 15:27:58 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:58 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:58 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:58 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:58 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:58 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:58 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:58 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:58 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:58 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:58 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:58 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:58 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:58 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:58 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:58 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:58 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:58 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:58 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:58 lpllm.py:2265] GPU2CPU move cost 0.000573 seconds
DEBUG 10-15 15:27:58 lpmodule.py:340] layer_idx 31 attn layer_idx: 31
DEBUG 10-15 15:27:58 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:58 lpmodule.py:364] decoder_attn_batch update batch_dim 480-540
DEBUG 10-15 15:27:58 StaticCacheLen.py:84] static cache update layer_idx: 31, start_batch: 480, end_batch: 540
DEBUG 10-15 15:27:58 lpmodule.py:368] update for kv cache 480-540 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:58 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:58 lpmodule.py:374] update past key value cost 0.025949 seconds
DEBUG 10-15 15:27:58 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:58 lpmodule.py:227] layer idx 30
DEBUG 10-15 15:27:58 lpmodule.py:399] repeat qkv cost 0.034293 seconds
DEBUG 10-15 15:27:58 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:58 lpmodule.py:433] dot attn cost 0.036653 seconds
DEBUG 10-15 15:27:58 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:58 lpmodule.py:444] time cost move to cuda:1 0.02283787727355957 s
DEBUG 10-15 15:27:58 lpllm.py:2283] CPU attn cost 0.143162 seconds if batch True
DEBUG 10-15 15:27:58 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:58 lpllm.py:2294] CPU compute cost 0.144096 seconds
DEBUG 10-15 15:27:58 lpllm.py:2312] free cost 0.000087 seconds
DEBUG 10-15 15:27:58 lpllm.py:503] reset update experts
DEBUG 10-15 15:27:58 lpllm.py:1774] update state cost 2.1457672119140625e-05 s
DEBUG 10-15 15:27:58 lpllm.py:1743] restore layer func cost 0.0008413791656494141 s
DEBUG 10-15 15:27:58 lpllm.py:511] restore layer cost 0.001110076904296875 s
DEBUG 10-15 15:27:58 lpllm.py:512] reset_next_layer_need: layer_attn_idx=31, layer_mlp_idx=31, j_loc=62
DEBUG 10-15 15:27:58 lpllm.py:1037] reset layer cost 0.001184225082397461 s
DEBUG 10-15 15:27:58 lpllm.py:1038] have reset next layer layer_attn 31 layer_mlp 31 
DEBUG 10-15 15:27:58 lpllm.py:924] 
DEBUG 10-15 15:27:58 lpllm.py:924] decoder loop j: 63 cur_layer_idx: 31 layer_attn_idx: 31 layer_mlp_idx: 31
DEBUG 10-15 15:27:58 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:58 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:58 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:58 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:58 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:58 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:58 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:58 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:58 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:58 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:58 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:58 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:58 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:58 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:58 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:58 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:58 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:58 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:58 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:58 lpllm.py:2265] GPU2CPU move cost 0.000608 seconds
DEBUG 10-15 15:27:58 lpmodule.py:340] layer_idx 31 attn layer_idx: 31
DEBUG 10-15 15:27:58 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:58 lpmodule.py:364] decoder_attn_batch update batch_dim 540-600
DEBUG 10-15 15:27:58 StaticCacheLen.py:84] static cache update layer_idx: 31, start_batch: 540, end_batch: 600
DEBUG 10-15 15:27:58 lpmodule.py:368] update for kv cache 540-600 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:58 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:58 lpmodule.py:374] update past key value cost 0.028387 seconds
DEBUG 10-15 15:27:58 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:58 lpmodule.py:227] layer idx 31
DEBUG 10-15 15:27:58 lpmodule.py:399] repeat qkv cost 0.030803 seconds
DEBUG 10-15 15:27:58 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:58 lpmodule.py:433] dot attn cost 0.036506 seconds
DEBUG 10-15 15:27:58 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:58 lpmodule.py:444] time cost move to cuda:1 0.02300548553466797 s
DEBUG 10-15 15:27:58 lpllm.py:2283] CPU attn cost 0.142427 seconds if batch True
DEBUG 10-15 15:27:58 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:27:58 lpllm.py:2294] CPU compute cost 0.143337 seconds
DEBUG 10-15 15:27:58 lpllm.py:2312] free cost 0.000072 seconds
DEBUG 10-15 15:27:59 lpllm.py:924] 
DEBUG 10-15 15:27:59 lpllm.py:924] decoder loop j: 64 cur_layer_idx: 32 layer_attn_idx: 32 layer_mlp_idx: 31
DEBUG 10-15 15:27:59 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:59 lpmodule.py:227] layer idx 31
DEBUG 10-15 15:27:59 lpllm.py:1085] last_mlp_output_chunk shape: torch.Size([60, 512, 4096]), mlp_output_chunk shape: torch.Size([60, 512, 4096])
DEBUG 10-15 15:27:59 lpllm.py:1086] last_mlp_output_chunk len 1, mlp_output_chunk len 1
DEBUG 10-15 15:27:59 lpllm.py:618] decoders batch for 4 cost 14.882421016693115 s
DEBUG 10-15 15:27:59 lpllm.py:848] hidden_states_chunks1 shape: torch.Size([60, 512, 4096]), hidden_states_chunks1 length: 1
DEBUG 10-15 15:27:59 lpllm.py:849] hidden_states_chunks2 shape: torch.Size([60, 512, 4096]), hidden_states_chunks2 length: 1
DEBUG 10-15 15:27:59 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed650>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x003\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x96\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:59 client.py:72] load_into_gpu: Mixtral-8x7B, 168ebe0e-ba8c-4f2d-ba54-7db5d77539d0
INFO 10-15 15:27:59 client.py:113] Model loaded: Mixtral-8x7B, 168ebe0e-ba8c-4f2d-ba54-7db5d77539d0
DEBUG 10-15 15:27:59 lpllm.py:1743] restore layer func cost 0.0009589195251464844 s
INFO 10-15 15:27:59 client.py:117] confirm_model_loaded: Mixtral-8x7B, 168ebe0e-ba8c-4f2d-ba54-7db5d77539d0
INFO 10-15 15:27:59 client.py:125] Model loaded
DEBUG 10-15 15:27:59 lpllm.py:422] prepare layer cost 0.2742898464202881 s
DEBUG 10-15 15:27:59 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed6e0>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x004\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x98\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:59 client.py:72] load_into_gpu: Mixtral-8x7B, 25433743-e01a-457e-8f2b-10f08c26e452
INFO 10-15 15:27:59 client.py:113] Model loaded: Mixtral-8x7B, 25433743-e01a-457e-8f2b-10f08c26e452
DEBUG 10-15 15:27:59 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:59 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:59 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:59 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:59 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:59 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:59 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:59 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:59 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:59 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:59 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:59 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:59 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:59 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:59 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:59 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:59 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:59 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:59 lpllm.py:924] 
DEBUG 10-15 15:27:59 lpllm.py:924] decoder loop j: 1 cur_layer_idx: 0 layer_attn_idx: 0 layer_mlp_idx: 0
DEBUG 10-15 15:27:59 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:59 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:59 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:59 lpllm.py:2265] GPU2CPU move cost 0.000716 seconds
DEBUG 10-15 15:27:59 lpmodule.py:340] layer_idx 0 attn layer_idx: 0
DEBUG 10-15 15:27:59 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:59 lpmodule.py:364] decoder_attn_batch update batch_dim 600-660
DEBUG 10-15 15:27:59 StaticCacheLen.py:84] static cache update layer_idx: 0, start_batch: 600, end_batch: 660
DEBUG 10-15 15:27:59 lpmodule.py:368] update for kv cache 600-660 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:59 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:59 lpmodule.py:374] update past key value cost 0.028083 seconds
DEBUG 10-15 15:27:59 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:59 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:59 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:59 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:59 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:59 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:59 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:59 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:59 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:59 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:59 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:59 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:59 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:59 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:59 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:59 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:59 lpmodule.py:399] repeat qkv cost 0.031270 seconds
DEBUG 10-15 15:27:59 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:59 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:59 lpmodule.py:433] dot attn cost 0.036912 seconds
DEBUG 10-15 15:27:59 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:59 lpmodule.py:444] time cost move to cuda:1 0.02680063247680664 s
DEBUG 10-15 15:27:59 lpllm.py:2283] CPU attn cost 0.153880 seconds if batch True
DEBUG 10-15 15:27:59 lpllm.py:2292] deal attn result cost 0.000003 seconds
DEBUG 10-15 15:27:59 lpllm.py:2294] CPU compute cost 0.154941 seconds
DEBUG 10-15 15:27:59 lpllm.py:2312] free cost 0.000192 seconds
DEBUG 10-15 15:27:59 lpllm.py:2265] GPU2CPU move cost 0.000591 seconds
DEBUG 10-15 15:27:59 lpmodule.py:340] layer_idx 0 attn layer_idx: 0
DEBUG 10-15 15:27:59 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:59 lpmodule.py:364] decoder_attn_batch update batch_dim 660-720
DEBUG 10-15 15:27:59 lpmodule.py:227] layer idx 0
DEBUG 10-15 15:27:59 StaticCacheLen.py:84] static cache update layer_idx: 0, start_batch: 660, end_batch: 720
DEBUG 10-15 15:27:59 lpmodule.py:368] update for kv cache 660-720 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:59 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:59 lpmodule.py:374] update past key value cost 0.024656 seconds
DEBUG 10-15 15:27:59 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:59 lpmodule.py:399] repeat qkv cost 0.030735 seconds
DEBUG 10-15 15:27:59 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:59 lpmodule.py:433] dot attn cost 0.035403 seconds
DEBUG 10-15 15:27:59 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:59 lpmodule.py:444] time cost move to cuda:1 0.026349544525146484 s
DEBUG 10-15 15:27:59 lpllm.py:2283] CPU attn cost 0.145377 seconds if batch True
DEBUG 10-15 15:27:59 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:59 lpllm.py:2294] CPU compute cost 0.146152 seconds
DEBUG 10-15 15:27:59 lpllm.py:2312] free cost 0.000075 seconds
DEBUG 10-15 15:27:59 lpllm.py:498] reset update attn
DEBUG 10-15 15:27:59 lpllm.py:1774] update state cost 3.7670135498046875e-05 s
DEBUG 10-15 15:27:59 lpllm.py:1743] restore layer func cost 0.00048828125 s
DEBUG 10-15 15:27:59 lpllm.py:511] restore layer cost 0.0007524490356445312 s
DEBUG 10-15 15:27:59 lpllm.py:512] reset_next_layer_need: layer_attn_idx=1, layer_mlp_idx=0, j_loc=1
DEBUG 10-15 15:27:59 lpllm.py:1037] reset layer cost 0.0008704662322998047 s
DEBUG 10-15 15:27:59 lpllm.py:1038] have reset next layer layer_attn 1 layer_mlp 0 
DEBUG 10-15 15:27:59 lpllm.py:1044] j: 1 waiting the layer with layer_idx 1 before wait time 0.36890220642089844 s
INFO 10-15 15:27:59 client.py:117] confirm_model_loaded: Mixtral-8x7B, 25433743-e01a-457e-8f2b-10f08c26e452
INFO 10-15 15:27:59 client.py:125] Model loaded
DEBUG 10-15 15:27:59 lpllm.py:1048] j: load cost 0.3706173896789551 s waiting cost 0.0017001628875732422 s
DEBUG 10-15 15:27:59 lpllm.py:924] 
DEBUG 10-15 15:27:59 lpllm.py:924] decoder loop j: 2 cur_layer_idx: 1 layer_attn_idx: 1 layer_mlp_idx: 0
DEBUG 10-15 15:27:59 lpllm.py:933] start load next layer cur_layer_idx: 2
DEBUG 10-15 15:27:59 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:27:59 client.py:72] load_into_gpu: Mixtral-8x7B, 25aaa9c1-52a6-4b6e-b3b2-4be42c0c64e6
INFO 10-15 15:27:59 client.py:113] Model loaded: Mixtral-8x7B, 25aaa9c1-52a6-4b6e-b3b2-4be42c0c64e6
DEBUG 10-15 15:27:59 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:27:59 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:27:59 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:27:59 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:27:59 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:27:59 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:59 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:59 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:59 lpmodule.py:506]         ...,
DEBUG 10-15 15:27:59 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:59 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:27:59 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:27:59 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:27:59 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:27:59 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:27:59 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:27:59 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:59 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:27:59 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:27:59 lpllm.py:2265] GPU2CPU move cost 0.000639 seconds
DEBUG 10-15 15:27:59 lpmodule.py:340] layer_idx 1 attn layer_idx: 1
DEBUG 10-15 15:27:59 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:27:59 lpmodule.py:364] decoder_attn_batch update batch_dim 600-660
DEBUG 10-15 15:27:59 StaticCacheLen.py:84] static cache update layer_idx: 1, start_batch: 600, end_batch: 660
DEBUG 10-15 15:27:59 lpmodule.py:368] update for kv cache 600-660 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:27:59 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:27:59 lpmodule.py:374] update past key value cost 0.026114 seconds
DEBUG 10-15 15:27:59 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:27:59 lpmodule.py:227] layer idx 0
DEBUG 10-15 15:27:59 lpmodule.py:399] repeat qkv cost 0.028990 seconds
DEBUG 10-15 15:27:59 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:27:59 lpmodule.py:433] dot attn cost 0.033651 seconds
DEBUG 10-15 15:27:59 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:27:59 lpmodule.py:444] time cost move to cuda:1 0.02371072769165039 s
DEBUG 10-15 15:27:59 lpllm.py:2283] CPU attn cost 0.136563 seconds if batch True
DEBUG 10-15 15:27:59 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:27:59 lpllm.py:2294] CPU compute cost 0.137501 seconds
DEBUG 10-15 15:27:59 lpllm.py:2312] free cost 0.000075 seconds
DEBUG 10-15 15:28:00 lpllm.py:503] reset update experts
DEBUG 10-15 15:28:00 lpllm.py:1774] update state cost 2.288818359375e-05 s
DEBUG 10-15 15:28:00 lpllm.py:1743] restore layer func cost 0.0008552074432373047 s
DEBUG 10-15 15:28:00 lpllm.py:511] restore layer cost 0.0011031627655029297 s
DEBUG 10-15 15:28:00 lpllm.py:512] reset_next_layer_need: layer_attn_idx=1, layer_mlp_idx=1, j_loc=2
DEBUG 10-15 15:28:00 lpllm.py:1037] reset layer cost 0.0011768341064453125 s
DEBUG 10-15 15:28:00 lpllm.py:1038] have reset next layer layer_attn 1 layer_mlp 1 
DEBUG 10-15 15:28:00 lpllm.py:924] 
DEBUG 10-15 15:28:00 lpllm.py:924] decoder loop j: 3 cur_layer_idx: 1 layer_attn_idx: 1 layer_mlp_idx: 1
DEBUG 10-15 15:28:00 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:00 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:00 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:00 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:00 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:00 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:00 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:00 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:00 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:00 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:00 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:00 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:00 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:00 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:00 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:00 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:00 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:00 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:00 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:00 lpllm.py:2265] GPU2CPU move cost 0.000573 seconds
DEBUG 10-15 15:28:00 lpmodule.py:340] layer_idx 1 attn layer_idx: 1
DEBUG 10-15 15:28:00 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:00 lpmodule.py:364] decoder_attn_batch update batch_dim 660-720
DEBUG 10-15 15:28:00 StaticCacheLen.py:84] static cache update layer_idx: 1, start_batch: 660, end_batch: 720
DEBUG 10-15 15:28:00 lpmodule.py:368] update for kv cache 660-720 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:00 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:00 lpmodule.py:374] update past key value cost 0.025597 seconds
DEBUG 10-15 15:28:00 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:00 lpmodule.py:227] layer idx 1
DEBUG 10-15 15:28:00 lpmodule.py:399] repeat qkv cost 0.030612 seconds
DEBUG 10-15 15:28:00 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:00 lpmodule.py:433] dot attn cost 0.032524 seconds
DEBUG 10-15 15:28:00 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:00 lpmodule.py:444] time cost move to cuda:1 0.023331642150878906 s
DEBUG 10-15 15:28:00 lpllm.py:2283] CPU attn cost 0.136052 seconds if batch True
DEBUG 10-15 15:28:00 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:28:00 lpllm.py:2294] CPU compute cost 0.136896 seconds
DEBUG 10-15 15:28:00 lpllm.py:2312] free cost 0.000078 seconds
DEBUG 10-15 15:28:00 lpllm.py:498] reset update attn
DEBUG 10-15 15:28:00 lpllm.py:1774] update state cost 2.6702880859375e-05 s
DEBUG 10-15 15:28:00 lpllm.py:1743] restore layer func cost 0.0004742145538330078 s
DEBUG 10-15 15:28:00 lpllm.py:511] restore layer cost 0.0007369518280029297 s
DEBUG 10-15 15:28:00 lpllm.py:512] reset_next_layer_need: layer_attn_idx=2, layer_mlp_idx=1, j_loc=3
DEBUG 10-15 15:28:00 lpllm.py:1037] reset layer cost 0.0008347034454345703 s
DEBUG 10-15 15:28:00 lpllm.py:1038] have reset next layer layer_attn 2 layer_mlp 1 
DEBUG 10-15 15:28:00 lpllm.py:1044] j: 3 waiting the layer with layer_idx 2 before wait time 0.4510538578033447 s
INFO 10-15 15:28:00 client.py:117] confirm_model_loaded: Mixtral-8x7B, 25aaa9c1-52a6-4b6e-b3b2-4be42c0c64e6
INFO 10-15 15:28:00 client.py:125] Model loaded
DEBUG 10-15 15:28:00 lpllm.py:1048] j: load cost 0.4527778625488281 s waiting cost 0.0017082691192626953 s
DEBUG 10-15 15:28:00 lpllm.py:924] 
DEBUG 10-15 15:28:00 lpllm.py:924] decoder loop j: 4 cur_layer_idx: 2 layer_attn_idx: 2 layer_mlp_idx: 1
DEBUG 10-15 15:28:00 lpllm.py:933] start load next layer cur_layer_idx: 3
DEBUG 10-15 15:28:00 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed650>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x003\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x96\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:28:00 client.py:72] load_into_gpu: Mixtral-8x7B, 7c1ba581-d955-42db-a576-9ba34c906396
INFO 10-15 15:28:00 client.py:113] Model loaded: Mixtral-8x7B, 7c1ba581-d955-42db-a576-9ba34c906396
DEBUG 10-15 15:28:00 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:00 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:00 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:00 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:00 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:00 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:00 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:00 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:00 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:00 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:00 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:00 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:00 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:00 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:00 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:00 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:00 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:00 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:00 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:00 lpllm.py:2265] GPU2CPU move cost 0.000609 seconds
DEBUG 10-15 15:28:00 lpmodule.py:340] layer_idx 2 attn layer_idx: 2
DEBUG 10-15 15:28:00 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:00 lpmodule.py:364] decoder_attn_batch update batch_dim 600-660
DEBUG 10-15 15:28:00 StaticCacheLen.py:84] static cache update layer_idx: 2, start_batch: 600, end_batch: 660
DEBUG 10-15 15:28:00 lpmodule.py:368] update for kv cache 600-660 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:00 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:00 lpmodule.py:374] update past key value cost 0.024277 seconds
DEBUG 10-15 15:28:00 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:00 lpmodule.py:227] layer idx 1
DEBUG 10-15 15:28:00 lpmodule.py:399] repeat qkv cost 0.028485 seconds
DEBUG 10-15 15:28:00 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:00 lpmodule.py:433] dot attn cost 0.034136 seconds
DEBUG 10-15 15:28:00 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:00 lpmodule.py:444] time cost move to cuda:1 0.023680448532104492 s
DEBUG 10-15 15:28:00 lpllm.py:2283] CPU attn cost 0.137511 seconds if batch True
DEBUG 10-15 15:28:00 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:00 lpllm.py:2294] CPU compute cost 0.138413 seconds
DEBUG 10-15 15:28:00 lpllm.py:2312] free cost 0.000074 seconds
DEBUG 10-15 15:28:00 lpllm.py:503] reset update experts
DEBUG 10-15 15:28:00 lpllm.py:1774] update state cost 2.193450927734375e-05 s
DEBUG 10-15 15:28:00 lpllm.py:1743] restore layer func cost 0.0009469985961914062 s
DEBUG 10-15 15:28:00 lpllm.py:511] restore layer cost 0.0011944770812988281 s
DEBUG 10-15 15:28:00 lpllm.py:512] reset_next_layer_need: layer_attn_idx=2, layer_mlp_idx=2, j_loc=4
DEBUG 10-15 15:28:00 lpllm.py:1037] reset layer cost 0.001271963119506836 s
DEBUG 10-15 15:28:00 lpllm.py:1038] have reset next layer layer_attn 2 layer_mlp 2 
DEBUG 10-15 15:28:00 lpllm.py:924] 
DEBUG 10-15 15:28:00 lpllm.py:924] decoder loop j: 5 cur_layer_idx: 2 layer_attn_idx: 2 layer_mlp_idx: 2
DEBUG 10-15 15:28:00 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:00 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:00 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:00 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:00 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:00 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:00 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:00 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:00 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:00 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:00 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:00 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:00 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:00 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:00 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:00 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:00 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:00 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:00 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:00 lpllm.py:2265] GPU2CPU move cost 0.000580 seconds
DEBUG 10-15 15:28:00 lpmodule.py:340] layer_idx 2 attn layer_idx: 2
DEBUG 10-15 15:28:00 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:00 lpmodule.py:364] decoder_attn_batch update batch_dim 660-720
DEBUG 10-15 15:28:00 StaticCacheLen.py:84] static cache update layer_idx: 2, start_batch: 660, end_batch: 720
DEBUG 10-15 15:28:00 lpmodule.py:368] update for kv cache 660-720 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:00 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:00 lpmodule.py:374] update past key value cost 0.022740 seconds
DEBUG 10-15 15:28:00 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:00 lpmodule.py:227] layer idx 2
DEBUG 10-15 15:28:00 lpmodule.py:399] repeat qkv cost 0.028167 seconds
DEBUG 10-15 15:28:00 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:00 lpmodule.py:433] dot attn cost 0.033901 seconds
DEBUG 10-15 15:28:00 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:00 lpmodule.py:444] time cost move to cuda:1 0.02359461784362793 s
DEBUG 10-15 15:28:00 lpllm.py:2283] CPU attn cost 0.132624 seconds if batch True
DEBUG 10-15 15:28:00 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:00 lpllm.py:2294] CPU compute cost 0.133501 seconds
DEBUG 10-15 15:28:00 lpllm.py:2312] free cost 0.000081 seconds
DEBUG 10-15 15:28:00 lpllm.py:498] reset update attn
DEBUG 10-15 15:28:00 lpllm.py:1774] update state cost 2.7894973754882812e-05 s
DEBUG 10-15 15:28:00 lpllm.py:1743] restore layer func cost 0.00041604042053222656 s
DEBUG 10-15 15:28:00 lpllm.py:511] restore layer cost 0.0006687641143798828 s
DEBUG 10-15 15:28:00 lpllm.py:512] reset_next_layer_need: layer_attn_idx=3, layer_mlp_idx=2, j_loc=5
DEBUG 10-15 15:28:00 lpllm.py:1037] reset layer cost 0.0007410049438476562 s
DEBUG 10-15 15:28:00 lpllm.py:1038] have reset next layer layer_attn 3 layer_mlp 2 
DEBUG 10-15 15:28:00 lpllm.py:1044] j: 5 waiting the layer with layer_idx 3 before wait time 0.44661879539489746 s
INFO 10-15 15:28:00 client.py:117] confirm_model_loaded: Mixtral-8x7B, 7c1ba581-d955-42db-a576-9ba34c906396
INFO 10-15 15:28:00 client.py:125] Model loaded
DEBUG 10-15 15:28:00 lpllm.py:1048] j: load cost 0.4481179714202881 s waiting cost 0.0014829635620117188 s
DEBUG 10-15 15:28:00 lpllm.py:924] 
DEBUG 10-15 15:28:00 lpllm.py:924] decoder loop j: 6 cur_layer_idx: 3 layer_attn_idx: 3 layer_mlp_idx: 2
DEBUG 10-15 15:28:00 lpllm.py:933] start load next layer cur_layer_idx: 4
DEBUG 10-15 15:28:00 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed6e0>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x004\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x98\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:28:00 client.py:72] load_into_gpu: Mixtral-8x7B, fc923019-5a31-4060-9fb2-0bc7bb189bc8
INFO 10-15 15:28:00 client.py:113] Model loaded: Mixtral-8x7B, fc923019-5a31-4060-9fb2-0bc7bb189bc8
DEBUG 10-15 15:28:00 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:00 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:00 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:00 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:00 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:00 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:00 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:00 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:00 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:00 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:00 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:00 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:00 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:00 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:00 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:00 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:00 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:00 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:00 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:00 lpllm.py:2265] GPU2CPU move cost 0.000594 seconds
DEBUG 10-15 15:28:00 lpmodule.py:340] layer_idx 3 attn layer_idx: 3
DEBUG 10-15 15:28:00 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:00 lpmodule.py:364] decoder_attn_batch update batch_dim 600-660
DEBUG 10-15 15:28:00 StaticCacheLen.py:84] static cache update layer_idx: 3, start_batch: 600, end_batch: 660
DEBUG 10-15 15:28:00 lpmodule.py:368] update for kv cache 600-660 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:00 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:00 lpmodule.py:374] update past key value cost 0.024371 seconds
DEBUG 10-15 15:28:00 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:00 lpmodule.py:227] layer idx 2
DEBUG 10-15 15:28:00 lpmodule.py:399] repeat qkv cost 0.028015 seconds
DEBUG 10-15 15:28:00 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:00 lpmodule.py:433] dot attn cost 0.033193 seconds
DEBUG 10-15 15:28:00 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:00 lpmodule.py:444] time cost move to cuda:1 0.023534536361694336 s
DEBUG 10-15 15:28:00 lpllm.py:2283] CPU attn cost 0.133108 seconds if batch True
DEBUG 10-15 15:28:00 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:00 lpllm.py:2294] CPU compute cost 0.134013 seconds
DEBUG 10-15 15:28:00 lpllm.py:2312] free cost 0.000101 seconds
DEBUG 10-15 15:28:00 lpllm.py:503] reset update experts
DEBUG 10-15 15:28:00 lpllm.py:1774] update state cost 2.2172927856445312e-05 s
DEBUG 10-15 15:28:00 lpllm.py:1743] restore layer func cost 0.0008766651153564453 s
DEBUG 10-15 15:28:00 lpllm.py:511] restore layer cost 0.0011086463928222656 s
DEBUG 10-15 15:28:00 lpllm.py:512] reset_next_layer_need: layer_attn_idx=3, layer_mlp_idx=3, j_loc=6
DEBUG 10-15 15:28:00 lpllm.py:1037] reset layer cost 0.0011854171752929688 s
DEBUG 10-15 15:28:00 lpllm.py:1038] have reset next layer layer_attn 3 layer_mlp 3 
DEBUG 10-15 15:28:00 lpllm.py:924] 
DEBUG 10-15 15:28:00 lpllm.py:924] decoder loop j: 7 cur_layer_idx: 3 layer_attn_idx: 3 layer_mlp_idx: 3
DEBUG 10-15 15:28:00 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:00 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:00 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:00 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:00 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:00 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:00 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:00 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:00 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:00 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:00 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:00 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:00 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:00 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:00 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:00 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:00 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:00 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:00 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:00 lpllm.py:2265] GPU2CPU move cost 0.000584 seconds
DEBUG 10-15 15:28:00 lpmodule.py:340] layer_idx 3 attn layer_idx: 3
DEBUG 10-15 15:28:00 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:00 lpmodule.py:364] decoder_attn_batch update batch_dim 660-720
DEBUG 10-15 15:28:00 StaticCacheLen.py:84] static cache update layer_idx: 3, start_batch: 660, end_batch: 720
DEBUG 10-15 15:28:00 lpmodule.py:368] update for kv cache 660-720 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:00 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:00 lpmodule.py:374] update past key value cost 0.022948 seconds
DEBUG 10-15 15:28:00 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:01 lpmodule.py:227] layer idx 3
DEBUG 10-15 15:28:01 lpmodule.py:399] repeat qkv cost 0.028412 seconds
DEBUG 10-15 15:28:01 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:01 lpmodule.py:433] dot attn cost 0.032521 seconds
DEBUG 10-15 15:28:01 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:01 lpmodule.py:444] time cost move to cuda:1 0.023342370986938477 s
DEBUG 10-15 15:28:01 lpllm.py:2283] CPU attn cost 0.131658 seconds if batch True
DEBUG 10-15 15:28:01 lpllm.py:2292] deal attn result cost 0.000017 seconds
DEBUG 10-15 15:28:01 lpllm.py:2294] CPU compute cost 0.132546 seconds
DEBUG 10-15 15:28:01 lpllm.py:2312] free cost 0.000072 seconds
DEBUG 10-15 15:28:01 lpllm.py:498] reset update attn
DEBUG 10-15 15:28:01 lpllm.py:1774] update state cost 3.910064697265625e-05 s
DEBUG 10-15 15:28:01 lpllm.py:1743] restore layer func cost 0.0004425048828125 s
DEBUG 10-15 15:28:01 lpllm.py:511] restore layer cost 0.0006926059722900391 s
DEBUG 10-15 15:28:01 lpllm.py:512] reset_next_layer_need: layer_attn_idx=4, layer_mlp_idx=3, j_loc=7
DEBUG 10-15 15:28:01 lpllm.py:1037] reset layer cost 0.0007646083831787109 s
DEBUG 10-15 15:28:01 lpllm.py:1038] have reset next layer layer_attn 4 layer_mlp 3 
DEBUG 10-15 15:28:01 lpllm.py:1044] j: 7 waiting the layer with layer_idx 4 before wait time 0.4509696960449219 s
INFO 10-15 15:28:01 client.py:117] confirm_model_loaded: Mixtral-8x7B, fc923019-5a31-4060-9fb2-0bc7bb189bc8
INFO 10-15 15:28:01 client.py:125] Model loaded
DEBUG 10-15 15:28:01 lpllm.py:1048] j: load cost 0.45285677909851074 s waiting cost 0.0018723011016845703 s
DEBUG 10-15 15:28:01 lpllm.py:924] 
DEBUG 10-15 15:28:01 lpllm.py:924] decoder loop j: 8 cur_layer_idx: 4 layer_attn_idx: 4 layer_mlp_idx: 3
DEBUG 10-15 15:28:01 lpllm.py:933] start load next layer cur_layer_idx: 5
DEBUG 10-15 15:28:01 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:28:01 client.py:72] load_into_gpu: Mixtral-8x7B, 74023679-58a8-4da1-8087-42ceaec3f1cc
INFO 10-15 15:28:01 client.py:113] Model loaded: Mixtral-8x7B, 74023679-58a8-4da1-8087-42ceaec3f1cc
DEBUG 10-15 15:28:01 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:01 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:01 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:01 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:01 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:01 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:01 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:01 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:01 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:01 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:01 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:01 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:01 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:01 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:01 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:01 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:01 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:01 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:01 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:01 lpllm.py:2265] GPU2CPU move cost 0.000524 seconds
DEBUG 10-15 15:28:01 lpmodule.py:340] layer_idx 4 attn layer_idx: 4
DEBUG 10-15 15:28:01 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:01 lpmodule.py:364] decoder_attn_batch update batch_dim 600-660
DEBUG 10-15 15:28:01 StaticCacheLen.py:84] static cache update layer_idx: 4, start_batch: 600, end_batch: 660
DEBUG 10-15 15:28:01 lpmodule.py:368] update for kv cache 600-660 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:01 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:01 lpmodule.py:374] update past key value cost 0.026030 seconds
DEBUG 10-15 15:28:01 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:01 lpmodule.py:227] layer idx 3
DEBUG 10-15 15:28:01 lpmodule.py:399] repeat qkv cost 0.028709 seconds
DEBUG 10-15 15:28:01 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:01 lpmodule.py:433] dot attn cost 0.032824 seconds
DEBUG 10-15 15:28:01 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:01 lpmodule.py:444] time cost move to cuda:1 0.023954153060913086 s
DEBUG 10-15 15:28:01 lpllm.py:2283] CPU attn cost 0.135715 seconds if batch True
DEBUG 10-15 15:28:01 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:28:01 lpllm.py:2294] CPU compute cost 0.136445 seconds
DEBUG 10-15 15:28:01 lpllm.py:2312] free cost 0.000072 seconds
DEBUG 10-15 15:28:01 lpllm.py:503] reset update experts
DEBUG 10-15 15:28:01 lpllm.py:1774] update state cost 2.2411346435546875e-05 s
DEBUG 10-15 15:28:01 lpllm.py:1743] restore layer func cost 0.0008528232574462891 s
DEBUG 10-15 15:28:01 lpllm.py:511] restore layer cost 0.0011012554168701172 s
DEBUG 10-15 15:28:01 lpllm.py:512] reset_next_layer_need: layer_attn_idx=4, layer_mlp_idx=4, j_loc=8
DEBUG 10-15 15:28:01 lpllm.py:1037] reset layer cost 0.0011754035949707031 s
DEBUG 10-15 15:28:01 lpllm.py:1038] have reset next layer layer_attn 4 layer_mlp 4 
DEBUG 10-15 15:28:01 lpllm.py:924] 
DEBUG 10-15 15:28:01 lpllm.py:924] decoder loop j: 9 cur_layer_idx: 4 layer_attn_idx: 4 layer_mlp_idx: 4
DEBUG 10-15 15:28:01 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:01 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:01 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:01 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:01 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:01 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:01 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:01 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:01 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:01 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:01 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:01 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:01 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:01 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:01 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:01 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:01 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:01 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:01 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:01 lpllm.py:2265] GPU2CPU move cost 0.000615 seconds
DEBUG 10-15 15:28:01 lpmodule.py:340] layer_idx 4 attn layer_idx: 4
DEBUG 10-15 15:28:01 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:01 lpmodule.py:364] decoder_attn_batch update batch_dim 660-720
DEBUG 10-15 15:28:01 StaticCacheLen.py:84] static cache update layer_idx: 4, start_batch: 660, end_batch: 720
DEBUG 10-15 15:28:01 lpmodule.py:368] update for kv cache 660-720 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:01 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:01 lpmodule.py:374] update past key value cost 0.025041 seconds
DEBUG 10-15 15:28:01 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:01 lpmodule.py:227] layer idx 4
DEBUG 10-15 15:28:01 lpmodule.py:399] repeat qkv cost 0.028138 seconds
DEBUG 10-15 15:28:01 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:01 lpmodule.py:433] dot attn cost 0.033169 seconds
DEBUG 10-15 15:28:01 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:01 lpmodule.py:444] time cost move to cuda:1 0.023477554321289062 s
DEBUG 10-15 15:28:01 lpllm.py:2283] CPU attn cost 0.134351 seconds if batch True
DEBUG 10-15 15:28:01 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:01 lpllm.py:2294] CPU compute cost 0.135249 seconds
DEBUG 10-15 15:28:01 lpllm.py:2312] free cost 0.000079 seconds
DEBUG 10-15 15:28:01 lpllm.py:498] reset update attn
DEBUG 10-15 15:28:01 lpllm.py:1774] update state cost 4.076957702636719e-05 s
DEBUG 10-15 15:28:01 lpllm.py:1743] restore layer func cost 0.0004191398620605469 s
DEBUG 10-15 15:28:01 lpllm.py:511] restore layer cost 0.0006680488586425781 s
DEBUG 10-15 15:28:01 lpllm.py:512] reset_next_layer_need: layer_attn_idx=5, layer_mlp_idx=4, j_loc=9
DEBUG 10-15 15:28:01 lpllm.py:1037] reset layer cost 0.0007388591766357422 s
DEBUG 10-15 15:28:01 lpllm.py:1038] have reset next layer layer_attn 5 layer_mlp 4 
DEBUG 10-15 15:28:01 lpllm.py:1044] j: 9 waiting the layer with layer_idx 5 before wait time 0.4519782066345215 s
INFO 10-15 15:28:01 client.py:117] confirm_model_loaded: Mixtral-8x7B, 74023679-58a8-4da1-8087-42ceaec3f1cc
INFO 10-15 15:28:01 client.py:125] Model loaded
DEBUG 10-15 15:28:01 lpllm.py:1048] j: load cost 0.4537627696990967 s waiting cost 0.001775979995727539 s
DEBUG 10-15 15:28:01 lpllm.py:924] 
DEBUG 10-15 15:28:01 lpllm.py:924] decoder loop j: 10 cur_layer_idx: 5 layer_attn_idx: 5 layer_mlp_idx: 4
DEBUG 10-15 15:28:01 lpllm.py:933] start load next layer cur_layer_idx: 6
DEBUG 10-15 15:28:01 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed650>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x003\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x96\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:28:01 client.py:72] load_into_gpu: Mixtral-8x7B, e96898cf-7113-411d-ace2-aa7848e0d3dd
INFO 10-15 15:28:01 client.py:113] Model loaded: Mixtral-8x7B, e96898cf-7113-411d-ace2-aa7848e0d3dd
DEBUG 10-15 15:28:01 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:01 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:01 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:01 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:01 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:01 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:01 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:01 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:01 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:01 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:01 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:01 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:01 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:01 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:01 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:01 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:01 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:01 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:01 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:01 lpllm.py:2265] GPU2CPU move cost 0.000568 seconds
DEBUG 10-15 15:28:01 lpmodule.py:340] layer_idx 5 attn layer_idx: 5
DEBUG 10-15 15:28:01 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:01 lpmodule.py:364] decoder_attn_batch update batch_dim 600-660
DEBUG 10-15 15:28:01 StaticCacheLen.py:84] static cache update layer_idx: 5, start_batch: 600, end_batch: 660
DEBUG 10-15 15:28:01 lpmodule.py:368] update for kv cache 600-660 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:01 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:01 lpmodule.py:374] update past key value cost 0.025131 seconds
DEBUG 10-15 15:28:01 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:01 lpmodule.py:227] layer idx 4
DEBUG 10-15 15:28:01 lpmodule.py:399] repeat qkv cost 0.028700 seconds
DEBUG 10-15 15:28:01 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:01 lpmodule.py:433] dot attn cost 0.032533 seconds
DEBUG 10-15 15:28:01 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:01 lpmodule.py:444] time cost move to cuda:1 0.023611068725585938 s
DEBUG 10-15 15:28:01 lpllm.py:2283] CPU attn cost 0.134236 seconds if batch True
DEBUG 10-15 15:28:01 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:01 lpllm.py:2294] CPU compute cost 0.135090 seconds
DEBUG 10-15 15:28:01 lpllm.py:2312] free cost 0.000074 seconds
DEBUG 10-15 15:28:01 lpllm.py:503] reset update experts
DEBUG 10-15 15:28:01 lpllm.py:1774] update state cost 3.504753112792969e-05 s
DEBUG 10-15 15:28:01 lpllm.py:1743] restore layer func cost 0.0008485317230224609 s
DEBUG 10-15 15:28:01 lpllm.py:511] restore layer cost 0.0010936260223388672 s
DEBUG 10-15 15:28:01 lpllm.py:512] reset_next_layer_need: layer_attn_idx=5, layer_mlp_idx=5, j_loc=10
DEBUG 10-15 15:28:01 lpllm.py:1037] reset layer cost 0.0011677742004394531 s
DEBUG 10-15 15:28:01 lpllm.py:1038] have reset next layer layer_attn 5 layer_mlp 5 
DEBUG 10-15 15:28:01 lpllm.py:924] 
DEBUG 10-15 15:28:01 lpllm.py:924] decoder loop j: 11 cur_layer_idx: 5 layer_attn_idx: 5 layer_mlp_idx: 5
DEBUG 10-15 15:28:01 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:01 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:01 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:01 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:01 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:01 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:01 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:01 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:01 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:01 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:01 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:01 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:01 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:01 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:01 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:01 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:01 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:01 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:01 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:01 lpllm.py:2265] GPU2CPU move cost 0.000568 seconds
DEBUG 10-15 15:28:01 lpmodule.py:340] layer_idx 5 attn layer_idx: 5
DEBUG 10-15 15:28:01 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:01 lpmodule.py:364] decoder_attn_batch update batch_dim 660-720
DEBUG 10-15 15:28:01 StaticCacheLen.py:84] static cache update layer_idx: 5, start_batch: 660, end_batch: 720
DEBUG 10-15 15:28:01 lpmodule.py:368] update for kv cache 660-720 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:01 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:01 lpmodule.py:374] update past key value cost 0.023138 seconds
DEBUG 10-15 15:28:01 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:01 lpmodule.py:227] layer idx 5
DEBUG 10-15 15:28:01 lpmodule.py:399] repeat qkv cost 0.029071 seconds
DEBUG 10-15 15:28:01 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:01 lpmodule.py:433] dot attn cost 0.033258 seconds
DEBUG 10-15 15:28:01 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:01 lpmodule.py:444] time cost move to cuda:1 0.02335524559020996 s
DEBUG 10-15 15:28:02 lpllm.py:2283] CPU attn cost 0.132509 seconds if batch True
DEBUG 10-15 15:28:02 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:02 lpllm.py:2294] CPU compute cost 0.133286 seconds
DEBUG 10-15 15:28:02 lpllm.py:2312] free cost 0.000072 seconds
DEBUG 10-15 15:28:02 lpllm.py:498] reset update attn
DEBUG 10-15 15:28:02 lpllm.py:1774] update state cost 4.172325134277344e-05 s
DEBUG 10-15 15:28:02 lpllm.py:1743] restore layer func cost 0.00040912628173828125 s
DEBUG 10-15 15:28:02 lpllm.py:511] restore layer cost 0.0006539821624755859 s
DEBUG 10-15 15:28:02 lpllm.py:512] reset_next_layer_need: layer_attn_idx=6, layer_mlp_idx=5, j_loc=11
DEBUG 10-15 15:28:02 lpllm.py:1037] reset layer cost 0.0007281303405761719 s
DEBUG 10-15 15:28:02 lpllm.py:1038] have reset next layer layer_attn 6 layer_mlp 5 
DEBUG 10-15 15:28:02 lpllm.py:1044] j: 11 waiting the layer with layer_idx 6 before wait time 0.4436304569244385 s
INFO 10-15 15:28:02 client.py:117] confirm_model_loaded: Mixtral-8x7B, e96898cf-7113-411d-ace2-aa7848e0d3dd
INFO 10-15 15:28:02 client.py:125] Model loaded
DEBUG 10-15 15:28:02 lpllm.py:1048] j: load cost 0.4452972412109375 s waiting cost 0.0016477108001708984 s
DEBUG 10-15 15:28:02 lpllm.py:924] 
DEBUG 10-15 15:28:02 lpllm.py:924] decoder loop j: 12 cur_layer_idx: 6 layer_attn_idx: 6 layer_mlp_idx: 5
DEBUG 10-15 15:28:02 lpllm.py:933] start load next layer cur_layer_idx: 7
DEBUG 10-15 15:28:02 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed6e0>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x004\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x98\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:28:02 client.py:72] load_into_gpu: Mixtral-8x7B, 5da96e6a-b1a8-428a-8dc0-a1f15a2dfdc8
INFO 10-15 15:28:02 client.py:113] Model loaded: Mixtral-8x7B, 5da96e6a-b1a8-428a-8dc0-a1f15a2dfdc8
DEBUG 10-15 15:28:02 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:02 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:02 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:02 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:02 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:02 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:02 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:02 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:02 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:02 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:02 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:02 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:02 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:02 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:02 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:02 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:02 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:02 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:02 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:02 lpllm.py:2265] GPU2CPU move cost 0.000618 seconds
DEBUG 10-15 15:28:02 lpmodule.py:340] layer_idx 6 attn layer_idx: 6
DEBUG 10-15 15:28:02 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:02 lpmodule.py:364] decoder_attn_batch update batch_dim 600-660
DEBUG 10-15 15:28:02 StaticCacheLen.py:84] static cache update layer_idx: 6, start_batch: 600, end_batch: 660
DEBUG 10-15 15:28:02 lpmodule.py:368] update for kv cache 600-660 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:02 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:02 lpmodule.py:374] update past key value cost 0.024864 seconds
DEBUG 10-15 15:28:02 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:02 lpmodule.py:227] layer idx 5
DEBUG 10-15 15:28:02 lpmodule.py:399] repeat qkv cost 0.029136 seconds
DEBUG 10-15 15:28:02 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:02 lpmodule.py:433] dot attn cost 0.033278 seconds
DEBUG 10-15 15:28:02 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:02 lpmodule.py:444] time cost move to cuda:1 0.023651599884033203 s
DEBUG 10-15 15:28:02 lpllm.py:2283] CPU attn cost 0.135055 seconds if batch True
DEBUG 10-15 15:28:02 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:28:02 lpllm.py:2294] CPU compute cost 0.135957 seconds
DEBUG 10-15 15:28:02 lpllm.py:2312] free cost 0.000077 seconds
DEBUG 10-15 15:28:02 lpllm.py:503] reset update experts
DEBUG 10-15 15:28:02 lpllm.py:1774] update state cost 3.814697265625e-05 s
DEBUG 10-15 15:28:02 lpllm.py:1743] restore layer func cost 0.0008509159088134766 s
DEBUG 10-15 15:28:02 lpllm.py:511] restore layer cost 0.0011131763458251953 s
DEBUG 10-15 15:28:02 lpllm.py:512] reset_next_layer_need: layer_attn_idx=6, layer_mlp_idx=6, j_loc=12
DEBUG 10-15 15:28:02 lpllm.py:1037] reset layer cost 0.0011887550354003906 s
DEBUG 10-15 15:28:02 lpllm.py:1038] have reset next layer layer_attn 6 layer_mlp 6 
DEBUG 10-15 15:28:02 lpllm.py:924] 
DEBUG 10-15 15:28:02 lpllm.py:924] decoder loop j: 13 cur_layer_idx: 6 layer_attn_idx: 6 layer_mlp_idx: 6
DEBUG 10-15 15:28:02 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:02 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:02 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:02 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:02 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:02 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:02 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:02 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:02 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:02 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:02 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:02 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:02 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:02 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:02 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:02 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:02 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:02 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:02 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:02 lpllm.py:2265] GPU2CPU move cost 0.000569 seconds
DEBUG 10-15 15:28:02 lpmodule.py:340] layer_idx 6 attn layer_idx: 6
DEBUG 10-15 15:28:02 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:02 lpmodule.py:364] decoder_attn_batch update batch_dim 660-720
DEBUG 10-15 15:28:02 StaticCacheLen.py:84] static cache update layer_idx: 6, start_batch: 660, end_batch: 720
DEBUG 10-15 15:28:02 lpmodule.py:368] update for kv cache 660-720 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:02 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:02 lpmodule.py:374] update past key value cost 0.022034 seconds
DEBUG 10-15 15:28:02 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:02 lpmodule.py:227] layer idx 6
DEBUG 10-15 15:28:02 lpmodule.py:399] repeat qkv cost 0.029245 seconds
DEBUG 10-15 15:28:02 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:02 lpmodule.py:433] dot attn cost 0.032187 seconds
DEBUG 10-15 15:28:02 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:02 lpmodule.py:444] time cost move to cuda:1 0.02579665184020996 s
DEBUG 10-15 15:28:02 lpllm.py:2283] CPU attn cost 0.133423 seconds if batch True
DEBUG 10-15 15:28:02 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:02 lpllm.py:2294] CPU compute cost 0.134283 seconds
DEBUG 10-15 15:28:02 lpllm.py:2312] free cost 0.000073 seconds
DEBUG 10-15 15:28:02 lpllm.py:498] reset update attn
DEBUG 10-15 15:28:02 lpllm.py:1774] update state cost 4.1961669921875e-05 s
DEBUG 10-15 15:28:02 lpllm.py:1743] restore layer func cost 0.00041222572326660156 s
DEBUG 10-15 15:28:02 lpllm.py:511] restore layer cost 0.0006804466247558594 s
DEBUG 10-15 15:28:02 lpllm.py:512] reset_next_layer_need: layer_attn_idx=7, layer_mlp_idx=6, j_loc=13
DEBUG 10-15 15:28:02 lpllm.py:1037] reset layer cost 0.0007538795471191406 s
DEBUG 10-15 15:28:02 lpllm.py:1038] have reset next layer layer_attn 7 layer_mlp 6 
DEBUG 10-15 15:28:02 lpllm.py:1044] j: 13 waiting the layer with layer_idx 7 before wait time 0.4451267719268799 s
INFO 10-15 15:28:02 client.py:117] confirm_model_loaded: Mixtral-8x7B, 5da96e6a-b1a8-428a-8dc0-a1f15a2dfdc8
INFO 10-15 15:28:02 client.py:125] Model loaded
DEBUG 10-15 15:28:02 lpllm.py:1048] j: load cost 0.44690728187561035 s waiting cost 0.0017642974853515625 s
DEBUG 10-15 15:28:02 lpllm.py:924] 
DEBUG 10-15 15:28:02 lpllm.py:924] decoder loop j: 14 cur_layer_idx: 7 layer_attn_idx: 7 layer_mlp_idx: 6
DEBUG 10-15 15:28:02 lpllm.py:933] start load next layer cur_layer_idx: 8
DEBUG 10-15 15:28:02 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:28:02 client.py:72] load_into_gpu: Mixtral-8x7B, fe6bd6ae-d745-4aab-aa1e-95f7d5235cc6
INFO 10-15 15:28:02 client.py:113] Model loaded: Mixtral-8x7B, fe6bd6ae-d745-4aab-aa1e-95f7d5235cc6
DEBUG 10-15 15:28:02 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:02 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:02 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:02 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:02 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:02 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:02 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:02 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:02 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:02 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:02 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:02 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:02 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:02 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:02 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:02 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:02 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:02 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:02 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:02 lpllm.py:2265] GPU2CPU move cost 0.000565 seconds
DEBUG 10-15 15:28:02 lpmodule.py:340] layer_idx 7 attn layer_idx: 7
DEBUG 10-15 15:28:02 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:02 lpmodule.py:364] decoder_attn_batch update batch_dim 600-660
DEBUG 10-15 15:28:02 StaticCacheLen.py:84] static cache update layer_idx: 7, start_batch: 600, end_batch: 660
DEBUG 10-15 15:28:02 lpmodule.py:368] update for kv cache 600-660 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:02 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:02 lpmodule.py:374] update past key value cost 0.027667 seconds
DEBUG 10-15 15:28:02 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:02 lpmodule.py:227] layer idx 6
DEBUG 10-15 15:28:02 lpmodule.py:399] repeat qkv cost 0.028783 seconds
DEBUG 10-15 15:28:02 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:02 lpmodule.py:433] dot attn cost 0.032961 seconds
DEBUG 10-15 15:28:02 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:02 lpmodule.py:444] time cost move to cuda:1 0.023841142654418945 s
DEBUG 10-15 15:28:02 lpllm.py:2283] CPU attn cost 0.136984 seconds if batch True
DEBUG 10-15 15:28:02 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:28:02 lpllm.py:2294] CPU compute cost 0.137830 seconds
DEBUG 10-15 15:28:02 lpllm.py:2312] free cost 0.000074 seconds
DEBUG 10-15 15:28:02 lpllm.py:503] reset update experts
DEBUG 10-15 15:28:02 lpllm.py:1774] update state cost 3.719329833984375e-05 s
DEBUG 10-15 15:28:02 lpllm.py:1743] restore layer func cost 0.0008339881896972656 s
DEBUG 10-15 15:28:02 lpllm.py:511] restore layer cost 0.001089334487915039 s
DEBUG 10-15 15:28:02 lpllm.py:512] reset_next_layer_need: layer_attn_idx=7, layer_mlp_idx=7, j_loc=14
DEBUG 10-15 15:28:02 lpllm.py:1037] reset layer cost 0.0011632442474365234 s
DEBUG 10-15 15:28:02 lpllm.py:1038] have reset next layer layer_attn 7 layer_mlp 7 
DEBUG 10-15 15:28:02 lpllm.py:924] 
DEBUG 10-15 15:28:02 lpllm.py:924] decoder loop j: 15 cur_layer_idx: 7 layer_attn_idx: 7 layer_mlp_idx: 7
DEBUG 10-15 15:28:02 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:02 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:02 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:02 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:02 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:02 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:02 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:02 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:02 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:02 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:02 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:02 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:02 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:02 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:02 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:02 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:02 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:02 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:02 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:02 lpllm.py:2265] GPU2CPU move cost 0.000645 seconds
DEBUG 10-15 15:28:02 lpmodule.py:340] layer_idx 7 attn layer_idx: 7
DEBUG 10-15 15:28:02 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:02 lpmodule.py:364] decoder_attn_batch update batch_dim 660-720
DEBUG 10-15 15:28:02 StaticCacheLen.py:84] static cache update layer_idx: 7, start_batch: 660, end_batch: 720
DEBUG 10-15 15:28:02 lpmodule.py:368] update for kv cache 660-720 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:02 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:02 lpmodule.py:374] update past key value cost 0.022759 seconds
DEBUG 10-15 15:28:02 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:02 lpmodule.py:227] layer idx 7
DEBUG 10-15 15:28:02 lpmodule.py:399] repeat qkv cost 0.028588 seconds
DEBUG 10-15 15:28:02 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:02 lpmodule.py:433] dot attn cost 0.033237 seconds
DEBUG 10-15 15:28:02 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:02 lpmodule.py:444] time cost move to cuda:1 0.02307438850402832 s
DEBUG 10-15 15:28:02 lpllm.py:2283] CPU attn cost 0.131560 seconds if batch True
DEBUG 10-15 15:28:02 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:28:02 lpllm.py:2294] CPU compute cost 0.132566 seconds
DEBUG 10-15 15:28:02 lpllm.py:2312] free cost 0.000079 seconds
DEBUG 10-15 15:28:02 lpllm.py:498] reset update attn
DEBUG 10-15 15:28:02 lpllm.py:1774] update state cost 4.1484832763671875e-05 s
DEBUG 10-15 15:28:02 lpllm.py:1743] restore layer func cost 0.00042510032653808594 s
DEBUG 10-15 15:28:02 lpllm.py:511] restore layer cost 0.0007205009460449219 s
DEBUG 10-15 15:28:02 lpllm.py:512] reset_next_layer_need: layer_attn_idx=8, layer_mlp_idx=7, j_loc=15
DEBUG 10-15 15:28:02 lpllm.py:1037] reset layer cost 0.0007936954498291016 s
DEBUG 10-15 15:28:02 lpllm.py:1038] have reset next layer layer_attn 8 layer_mlp 7 
DEBUG 10-15 15:28:02 lpllm.py:1044] j: 15 waiting the layer with layer_idx 8 before wait time 0.44626927375793457 s
INFO 10-15 15:28:02 client.py:117] confirm_model_loaded: Mixtral-8x7B, fe6bd6ae-d745-4aab-aa1e-95f7d5235cc6
INFO 10-15 15:28:02 client.py:125] Model loaded
DEBUG 10-15 15:28:02 lpllm.py:1048] j: load cost 0.44795870780944824 s waiting cost 0.001674652099609375 s
DEBUG 10-15 15:28:02 lpllm.py:924] 
DEBUG 10-15 15:28:02 lpllm.py:924] decoder loop j: 16 cur_layer_idx: 8 layer_attn_idx: 8 layer_mlp_idx: 7
DEBUG 10-15 15:28:02 lpllm.py:933] start load next layer cur_layer_idx: 9
DEBUG 10-15 15:28:02 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed650>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x003\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x96\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:28:02 client.py:72] load_into_gpu: Mixtral-8x7B, 2ea2b924-55d5-4835-b33f-f844f89984a0
INFO 10-15 15:28:02 client.py:113] Model loaded: Mixtral-8x7B, 2ea2b924-55d5-4835-b33f-f844f89984a0
DEBUG 10-15 15:28:02 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:02 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:02 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:02 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:02 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:02 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:02 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:02 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:02 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:02 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:02 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:02 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:02 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:02 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:02 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:02 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:02 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:02 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:02 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:02 lpllm.py:2265] GPU2CPU move cost 0.000551 seconds
DEBUG 10-15 15:28:02 lpmodule.py:340] layer_idx 8 attn layer_idx: 8
DEBUG 10-15 15:28:02 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:02 lpmodule.py:364] decoder_attn_batch update batch_dim 600-660
DEBUG 10-15 15:28:03 StaticCacheLen.py:84] static cache update layer_idx: 8, start_batch: 600, end_batch: 660
DEBUG 10-15 15:28:03 lpmodule.py:368] update for kv cache 600-660 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:03 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:03 lpmodule.py:374] update past key value cost 0.023870 seconds
DEBUG 10-15 15:28:03 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:03 lpmodule.py:227] layer idx 7
DEBUG 10-15 15:28:03 lpmodule.py:399] repeat qkv cost 0.028083 seconds
DEBUG 10-15 15:28:03 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:03 lpmodule.py:433] dot attn cost 0.033714 seconds
DEBUG 10-15 15:28:03 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:03 lpmodule.py:444] time cost move to cuda:1 0.022943735122680664 s
DEBUG 10-15 15:28:03 lpllm.py:2283] CPU attn cost 0.132305 seconds if batch True
DEBUG 10-15 15:28:03 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:28:03 lpllm.py:2294] CPU compute cost 0.133146 seconds
DEBUG 10-15 15:28:03 lpllm.py:2312] free cost 0.000071 seconds
DEBUG 10-15 15:28:03 lpllm.py:503] reset update experts
DEBUG 10-15 15:28:03 lpllm.py:1774] update state cost 2.0265579223632812e-05 s
DEBUG 10-15 15:28:03 lpllm.py:1743] restore layer func cost 0.0008471012115478516 s
DEBUG 10-15 15:28:03 lpllm.py:511] restore layer cost 0.001077413558959961 s
DEBUG 10-15 15:28:03 lpllm.py:512] reset_next_layer_need: layer_attn_idx=8, layer_mlp_idx=8, j_loc=16
DEBUG 10-15 15:28:03 lpllm.py:1037] reset layer cost 0.0011494159698486328 s
DEBUG 10-15 15:28:03 lpllm.py:1038] have reset next layer layer_attn 8 layer_mlp 8 
DEBUG 10-15 15:28:03 lpllm.py:924] 
DEBUG 10-15 15:28:03 lpllm.py:924] decoder loop j: 17 cur_layer_idx: 8 layer_attn_idx: 8 layer_mlp_idx: 8
DEBUG 10-15 15:28:03 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:03 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:03 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:03 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:03 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:03 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:03 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:03 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:03 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:03 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:03 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:03 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:03 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:03 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:03 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:03 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:03 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:03 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:03 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:03 lpllm.py:2265] GPU2CPU move cost 0.000621 seconds
DEBUG 10-15 15:28:03 lpmodule.py:340] layer_idx 8 attn layer_idx: 8
DEBUG 10-15 15:28:03 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:03 lpmodule.py:364] decoder_attn_batch update batch_dim 660-720
DEBUG 10-15 15:28:03 StaticCacheLen.py:84] static cache update layer_idx: 8, start_batch: 660, end_batch: 720
DEBUG 10-15 15:28:03 lpmodule.py:368] update for kv cache 660-720 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:03 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:03 lpmodule.py:374] update past key value cost 0.021540 seconds
DEBUG 10-15 15:28:03 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:03 lpmodule.py:227] layer idx 8
DEBUG 10-15 15:28:03 lpmodule.py:399] repeat qkv cost 0.027942 seconds
DEBUG 10-15 15:28:03 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:03 lpmodule.py:433] dot attn cost 0.034334 seconds
DEBUG 10-15 15:28:03 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:03 lpmodule.py:444] time cost move to cuda:1 0.023038864135742188 s
DEBUG 10-15 15:28:03 lpllm.py:2283] CPU attn cost 0.130765 seconds if batch True
DEBUG 10-15 15:28:03 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:03 lpllm.py:2294] CPU compute cost 0.131667 seconds
DEBUG 10-15 15:28:03 lpllm.py:2312] free cost 0.000072 seconds
DEBUG 10-15 15:28:03 lpllm.py:498] reset update attn
DEBUG 10-15 15:28:03 lpllm.py:1774] update state cost 2.7179718017578125e-05 s
DEBUG 10-15 15:28:03 lpllm.py:1743] restore layer func cost 0.00041174888610839844 s
DEBUG 10-15 15:28:03 lpllm.py:511] restore layer cost 0.0007011890411376953 s
DEBUG 10-15 15:28:03 lpllm.py:512] reset_next_layer_need: layer_attn_idx=9, layer_mlp_idx=8, j_loc=17
DEBUG 10-15 15:28:03 lpllm.py:1037] reset layer cost 0.0007760524749755859 s
DEBUG 10-15 15:28:03 lpllm.py:1038] have reset next layer layer_attn 9 layer_mlp 8 
DEBUG 10-15 15:28:03 lpllm.py:1044] j: 17 waiting the layer with layer_idx 9 before wait time 0.4407522678375244 s
INFO 10-15 15:28:03 client.py:117] confirm_model_loaded: Mixtral-8x7B, 2ea2b924-55d5-4835-b33f-f844f89984a0
INFO 10-15 15:28:03 client.py:125] Model loaded
DEBUG 10-15 15:28:03 lpllm.py:1048] j: load cost 0.4425046443939209 s waiting cost 0.0017364025115966797 s
DEBUG 10-15 15:28:03 lpllm.py:924] 
DEBUG 10-15 15:28:03 lpllm.py:924] decoder loop j: 18 cur_layer_idx: 9 layer_attn_idx: 9 layer_mlp_idx: 8
DEBUG 10-15 15:28:03 lpllm.py:933] start load next layer cur_layer_idx: 10
DEBUG 10-15 15:28:03 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed6e0>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x004\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x98\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:28:03 client.py:72] load_into_gpu: Mixtral-8x7B, c98fa3ad-124c-4973-90a0-f1327da3ca74
INFO 10-15 15:28:03 client.py:113] Model loaded: Mixtral-8x7B, c98fa3ad-124c-4973-90a0-f1327da3ca74
DEBUG 10-15 15:28:03 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:03 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:03 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:03 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:03 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:03 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:03 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:03 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:03 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:03 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:03 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:03 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:03 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:03 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:03 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:03 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:03 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:03 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:03 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:03 lpllm.py:2265] GPU2CPU move cost 0.000644 seconds
DEBUG 10-15 15:28:03 lpmodule.py:340] layer_idx 9 attn layer_idx: 9
DEBUG 10-15 15:28:03 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:03 lpmodule.py:364] decoder_attn_batch update batch_dim 600-660
DEBUG 10-15 15:28:03 StaticCacheLen.py:84] static cache update layer_idx: 9, start_batch: 600, end_batch: 660
DEBUG 10-15 15:28:03 lpmodule.py:368] update for kv cache 600-660 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:03 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:03 lpmodule.py:374] update past key value cost 0.025243 seconds
DEBUG 10-15 15:28:03 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:03 lpmodule.py:227] layer idx 8
DEBUG 10-15 15:28:03 lpmodule.py:399] repeat qkv cost 0.028801 seconds
DEBUG 10-15 15:28:03 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:03 lpmodule.py:433] dot attn cost 0.032765 seconds
DEBUG 10-15 15:28:03 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:03 lpmodule.py:444] time cost move to cuda:1 0.022684574127197266 s
DEBUG 10-15 15:28:03 lpllm.py:2283] CPU attn cost 0.133768 seconds if batch True
DEBUG 10-15 15:28:03 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:03 lpllm.py:2294] CPU compute cost 0.134712 seconds
DEBUG 10-15 15:28:03 lpllm.py:2312] free cost 0.000071 seconds
DEBUG 10-15 15:28:03 lpllm.py:503] reset update experts
DEBUG 10-15 15:28:03 lpllm.py:1774] update state cost 3.838539123535156e-05 s
DEBUG 10-15 15:28:03 lpllm.py:1743] restore layer func cost 0.0008535385131835938 s
DEBUG 10-15 15:28:03 lpllm.py:511] restore layer cost 0.0011105537414550781 s
DEBUG 10-15 15:28:03 lpllm.py:512] reset_next_layer_need: layer_attn_idx=9, layer_mlp_idx=9, j_loc=18
DEBUG 10-15 15:28:03 lpllm.py:1037] reset layer cost 0.0011844635009765625 s
DEBUG 10-15 15:28:03 lpllm.py:1038] have reset next layer layer_attn 9 layer_mlp 9 
DEBUG 10-15 15:28:03 lpllm.py:924] 
DEBUG 10-15 15:28:03 lpllm.py:924] decoder loop j: 19 cur_layer_idx: 9 layer_attn_idx: 9 layer_mlp_idx: 9
DEBUG 10-15 15:28:03 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:03 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:03 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:03 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:03 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:03 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:03 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:03 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:03 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:03 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:03 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:03 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:03 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:03 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:03 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:03 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:03 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:03 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:03 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:03 lpllm.py:2265] GPU2CPU move cost 0.000570 seconds
DEBUG 10-15 15:28:03 lpmodule.py:340] layer_idx 9 attn layer_idx: 9
DEBUG 10-15 15:28:03 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:03 lpmodule.py:364] decoder_attn_batch update batch_dim 660-720
DEBUG 10-15 15:28:03 StaticCacheLen.py:84] static cache update layer_idx: 9, start_batch: 660, end_batch: 720
DEBUG 10-15 15:28:03 lpmodule.py:368] update for kv cache 660-720 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:03 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:03 lpmodule.py:374] update past key value cost 0.022459 seconds
DEBUG 10-15 15:28:03 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:03 lpmodule.py:227] layer idx 9
DEBUG 10-15 15:28:03 lpmodule.py:399] repeat qkv cost 0.028954 seconds
DEBUG 10-15 15:28:03 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:03 lpmodule.py:433] dot attn cost 0.032758 seconds
DEBUG 10-15 15:28:03 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:03 lpmodule.py:444] time cost move to cuda:1 0.025435686111450195 s
DEBUG 10-15 15:28:03 lpllm.py:2283] CPU attn cost 0.133914 seconds if batch True
DEBUG 10-15 15:28:03 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:03 lpllm.py:2294] CPU compute cost 0.134763 seconds
DEBUG 10-15 15:28:03 lpllm.py:2312] free cost 0.000070 seconds
DEBUG 10-15 15:28:03 lpllm.py:498] reset update attn
DEBUG 10-15 15:28:03 lpllm.py:1774] update state cost 3.933906555175781e-05 s
DEBUG 10-15 15:28:03 lpllm.py:1743] restore layer func cost 0.0004172325134277344 s
DEBUG 10-15 15:28:03 lpllm.py:511] restore layer cost 0.0006783008575439453 s
DEBUG 10-15 15:28:03 lpllm.py:512] reset_next_layer_need: layer_attn_idx=10, layer_mlp_idx=9, j_loc=19
DEBUG 10-15 15:28:03 lpllm.py:1037] reset layer cost 0.0007500648498535156 s
DEBUG 10-15 15:28:03 lpllm.py:1038] have reset next layer layer_attn 10 layer_mlp 9 
DEBUG 10-15 15:28:03 lpllm.py:1044] j: 19 waiting the layer with layer_idx 10 before wait time 0.44245409965515137 s
INFO 10-15 15:28:03 client.py:117] confirm_model_loaded: Mixtral-8x7B, c98fa3ad-124c-4973-90a0-f1327da3ca74
INFO 10-15 15:28:03 client.py:125] Model loaded
DEBUG 10-15 15:28:03 lpllm.py:1048] j: load cost 0.44419145584106445 s waiting cost 0.0017218589782714844 s
DEBUG 10-15 15:28:03 lpllm.py:924] 
DEBUG 10-15 15:28:03 lpllm.py:924] decoder loop j: 20 cur_layer_idx: 10 layer_attn_idx: 10 layer_mlp_idx: 9
DEBUG 10-15 15:28:03 lpllm.py:933] start load next layer cur_layer_idx: 11
DEBUG 10-15 15:28:03 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:28:03 client.py:72] load_into_gpu: Mixtral-8x7B, b313884f-0b88-4f0c-85dc-b0c45a53ea42
INFO 10-15 15:28:03 client.py:113] Model loaded: Mixtral-8x7B, b313884f-0b88-4f0c-85dc-b0c45a53ea42
DEBUG 10-15 15:28:03 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:03 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:03 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:03 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:03 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:03 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:03 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:03 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:03 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:03 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:03 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:03 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:03 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:03 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:03 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:03 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:03 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:03 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:03 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:03 lpllm.py:2265] GPU2CPU move cost 0.000706 seconds
DEBUG 10-15 15:28:03 lpmodule.py:340] layer_idx 10 attn layer_idx: 10
DEBUG 10-15 15:28:03 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:03 lpmodule.py:364] decoder_attn_batch update batch_dim 600-660
DEBUG 10-15 15:28:03 StaticCacheLen.py:84] static cache update layer_idx: 10, start_batch: 600, end_batch: 660
DEBUG 10-15 15:28:03 lpmodule.py:368] update for kv cache 600-660 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:03 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:03 lpmodule.py:374] update past key value cost 0.022482 seconds
DEBUG 10-15 15:28:03 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:03 lpmodule.py:227] layer idx 9
DEBUG 10-15 15:28:03 lpmodule.py:399] repeat qkv cost 0.029035 seconds
DEBUG 10-15 15:28:03 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:03 lpmodule.py:433] dot attn cost 0.031733 seconds
DEBUG 10-15 15:28:03 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:03 lpmodule.py:444] time cost move to cuda:1 0.023039579391479492 s
DEBUG 10-15 15:28:04 lpllm.py:2283] CPU attn cost 0.131800 seconds if batch True
DEBUG 10-15 15:28:04 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:28:04 lpllm.py:2294] CPU compute cost 0.132794 seconds
DEBUG 10-15 15:28:04 lpllm.py:2312] free cost 0.000073 seconds
DEBUG 10-15 15:28:04 lpllm.py:503] reset update experts
DEBUG 10-15 15:28:04 lpllm.py:1774] update state cost 3.600120544433594e-05 s
DEBUG 10-15 15:28:04 lpllm.py:1743] restore layer func cost 0.0008523464202880859 s
DEBUG 10-15 15:28:04 lpllm.py:511] restore layer cost 0.0011229515075683594 s
DEBUG 10-15 15:28:04 lpllm.py:512] reset_next_layer_need: layer_attn_idx=10, layer_mlp_idx=10, j_loc=20
DEBUG 10-15 15:28:04 lpllm.py:1037] reset layer cost 0.001196146011352539 s
DEBUG 10-15 15:28:04 lpllm.py:1038] have reset next layer layer_attn 10 layer_mlp 10 
DEBUG 10-15 15:28:04 lpllm.py:924] 
DEBUG 10-15 15:28:04 lpllm.py:924] decoder loop j: 21 cur_layer_idx: 10 layer_attn_idx: 10 layer_mlp_idx: 10
DEBUG 10-15 15:28:04 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:04 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:04 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:04 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:04 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:04 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:04 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:04 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:04 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:04 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:04 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:04 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:04 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:04 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:04 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:04 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:04 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:04 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:04 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:04 lpllm.py:2265] GPU2CPU move cost 0.000425 seconds
DEBUG 10-15 15:28:04 lpmodule.py:340] layer_idx 10 attn layer_idx: 10
DEBUG 10-15 15:28:04 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:04 lpmodule.py:364] decoder_attn_batch update batch_dim 660-720
DEBUG 10-15 15:28:04 StaticCacheLen.py:84] static cache update layer_idx: 10, start_batch: 660, end_batch: 720
DEBUG 10-15 15:28:04 lpmodule.py:368] update for kv cache 660-720 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:04 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:04 lpmodule.py:374] update past key value cost 0.021123 seconds
DEBUG 10-15 15:28:04 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:04 lpmodule.py:227] layer idx 10
DEBUG 10-15 15:28:04 lpmodule.py:399] repeat qkv cost 0.028662 seconds
DEBUG 10-15 15:28:04 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:04 lpmodule.py:433] dot attn cost 0.032783 seconds
DEBUG 10-15 15:28:04 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:04 lpmodule.py:444] time cost move to cuda:1 0.022772789001464844 s
DEBUG 10-15 15:28:04 lpllm.py:2283] CPU attn cost 0.129214 seconds if batch True
DEBUG 10-15 15:28:04 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:04 lpllm.py:2294] CPU compute cost 0.129880 seconds
DEBUG 10-15 15:28:04 lpllm.py:2312] free cost 0.000072 seconds
DEBUG 10-15 15:28:04 lpllm.py:498] reset update attn
DEBUG 10-15 15:28:04 lpllm.py:1774] update state cost 4.2438507080078125e-05 s
DEBUG 10-15 15:28:04 lpllm.py:1743] restore layer func cost 0.0004124641418457031 s
DEBUG 10-15 15:28:04 lpllm.py:511] restore layer cost 0.0006890296936035156 s
DEBUG 10-15 15:28:04 lpllm.py:512] reset_next_layer_need: layer_attn_idx=11, layer_mlp_idx=10, j_loc=21
DEBUG 10-15 15:28:04 lpllm.py:1037] reset layer cost 0.0007610321044921875 s
DEBUG 10-15 15:28:04 lpllm.py:1038] have reset next layer layer_attn 11 layer_mlp 10 
DEBUG 10-15 15:28:04 lpllm.py:1044] j: 21 waiting the layer with layer_idx 11 before wait time 0.4486873149871826 s
INFO 10-15 15:28:04 client.py:117] confirm_model_loaded: Mixtral-8x7B, b313884f-0b88-4f0c-85dc-b0c45a53ea42
INFO 10-15 15:28:04 client.py:125] Model loaded
DEBUG 10-15 15:28:04 lpllm.py:1048] j: load cost 0.45043516159057617 s waiting cost 0.0017325878143310547 s
DEBUG 10-15 15:28:04 lpllm.py:924] 
DEBUG 10-15 15:28:04 lpllm.py:924] decoder loop j: 22 cur_layer_idx: 11 layer_attn_idx: 11 layer_mlp_idx: 10
DEBUG 10-15 15:28:04 lpllm.py:933] start load next layer cur_layer_idx: 12
DEBUG 10-15 15:28:04 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed650>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x003\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x96\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:28:04 client.py:72] load_into_gpu: Mixtral-8x7B, d2c35321-ac6b-4a78-926f-5983ac1f20e5
INFO 10-15 15:28:04 client.py:113] Model loaded: Mixtral-8x7B, d2c35321-ac6b-4a78-926f-5983ac1f20e5
DEBUG 10-15 15:28:04 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:04 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:04 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:04 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:04 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:04 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:04 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:04 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:04 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:04 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:04 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:04 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:04 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:04 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:04 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:04 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:04 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:04 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:04 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:04 lpllm.py:2265] GPU2CPU move cost 0.000603 seconds
DEBUG 10-15 15:28:04 lpmodule.py:340] layer_idx 11 attn layer_idx: 11
DEBUG 10-15 15:28:04 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:04 lpmodule.py:364] decoder_attn_batch update batch_dim 600-660
DEBUG 10-15 15:28:04 StaticCacheLen.py:84] static cache update layer_idx: 11, start_batch: 600, end_batch: 660
DEBUG 10-15 15:28:04 lpmodule.py:368] update for kv cache 600-660 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:04 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:04 lpmodule.py:374] update past key value cost 0.022234 seconds
DEBUG 10-15 15:28:04 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:04 lpmodule.py:227] layer idx 10
DEBUG 10-15 15:28:04 lpmodule.py:399] repeat qkv cost 0.028543 seconds
DEBUG 10-15 15:28:04 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:04 lpmodule.py:433] dot attn cost 0.032587 seconds
DEBUG 10-15 15:28:04 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:04 lpmodule.py:444] time cost move to cuda:1 0.02323007583618164 s
DEBUG 10-15 15:28:04 lpllm.py:2283] CPU attn cost 0.131985 seconds if batch True
DEBUG 10-15 15:28:04 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:28:04 lpllm.py:2294] CPU compute cost 0.132862 seconds
DEBUG 10-15 15:28:04 lpllm.py:2312] free cost 0.000073 seconds
DEBUG 10-15 15:28:04 lpllm.py:503] reset update experts
DEBUG 10-15 15:28:04 lpllm.py:1774] update state cost 2.193450927734375e-05 s
DEBUG 10-15 15:28:04 lpllm.py:1743] restore layer func cost 0.0008525848388671875 s
DEBUG 10-15 15:28:04 lpllm.py:511] restore layer cost 0.001094818115234375 s
DEBUG 10-15 15:28:04 lpllm.py:512] reset_next_layer_need: layer_attn_idx=11, layer_mlp_idx=11, j_loc=22
DEBUG 10-15 15:28:04 lpllm.py:1037] reset layer cost 0.0011682510375976562 s
DEBUG 10-15 15:28:04 lpllm.py:1038] have reset next layer layer_attn 11 layer_mlp 11 
DEBUG 10-15 15:28:04 lpllm.py:924] 
DEBUG 10-15 15:28:04 lpllm.py:924] decoder loop j: 23 cur_layer_idx: 11 layer_attn_idx: 11 layer_mlp_idx: 11
DEBUG 10-15 15:28:04 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:04 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:04 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:04 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:04 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:04 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:04 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:04 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:04 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:04 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:04 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:04 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:04 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:04 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:04 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:04 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:04 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:04 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:04 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:04 lpllm.py:2265] GPU2CPU move cost 0.000592 seconds
DEBUG 10-15 15:28:04 lpmodule.py:340] layer_idx 11 attn layer_idx: 11
DEBUG 10-15 15:28:04 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:04 lpmodule.py:364] decoder_attn_batch update batch_dim 660-720
DEBUG 10-15 15:28:04 StaticCacheLen.py:84] static cache update layer_idx: 11, start_batch: 660, end_batch: 720
DEBUG 10-15 15:28:04 lpmodule.py:368] update for kv cache 660-720 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:04 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:04 lpmodule.py:374] update past key value cost 0.021072 seconds
DEBUG 10-15 15:28:04 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:04 lpmodule.py:227] layer idx 11
DEBUG 10-15 15:28:04 lpmodule.py:399] repeat qkv cost 0.029068 seconds
DEBUG 10-15 15:28:04 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:04 lpmodule.py:433] dot attn cost 0.032660 seconds
DEBUG 10-15 15:28:04 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:04 lpmodule.py:444] time cost move to cuda:1 0.02310943603515625 s
DEBUG 10-15 15:28:04 lpllm.py:2283] CPU attn cost 0.130114 seconds if batch True
DEBUG 10-15 15:28:04 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:04 lpllm.py:2294] CPU compute cost 0.130981 seconds
DEBUG 10-15 15:28:04 lpllm.py:2312] free cost 0.000071 seconds
DEBUG 10-15 15:28:04 lpllm.py:498] reset update attn
DEBUG 10-15 15:28:04 lpllm.py:1774] update state cost 2.4557113647460938e-05 s
DEBUG 10-15 15:28:04 lpllm.py:1743] restore layer func cost 0.00041222572326660156 s
DEBUG 10-15 15:28:04 lpllm.py:511] restore layer cost 0.0006661415100097656 s
DEBUG 10-15 15:28:04 lpllm.py:512] reset_next_layer_need: layer_attn_idx=12, layer_mlp_idx=11, j_loc=23
DEBUG 10-15 15:28:04 lpllm.py:1037] reset layer cost 0.0007386207580566406 s
DEBUG 10-15 15:28:04 lpllm.py:1038] have reset next layer layer_attn 12 layer_mlp 11 
DEBUG 10-15 15:28:04 lpllm.py:1044] j: 23 waiting the layer with layer_idx 12 before wait time 0.4422025680541992 s
INFO 10-15 15:28:04 client.py:117] confirm_model_loaded: Mixtral-8x7B, d2c35321-ac6b-4a78-926f-5983ac1f20e5
INFO 10-15 15:28:04 client.py:125] Model loaded
DEBUG 10-15 15:28:04 lpllm.py:1048] j: load cost 0.44385552406311035 s waiting cost 0.0016376972198486328 s
DEBUG 10-15 15:28:04 lpllm.py:924] 
DEBUG 10-15 15:28:04 lpllm.py:924] decoder loop j: 24 cur_layer_idx: 12 layer_attn_idx: 12 layer_mlp_idx: 11
DEBUG 10-15 15:28:04 lpllm.py:933] start load next layer cur_layer_idx: 13
DEBUG 10-15 15:28:04 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed6e0>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x004\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x98\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:28:04 client.py:72] load_into_gpu: Mixtral-8x7B, 9e4693f2-334d-45a4-83f3-49d05d0b89f6
INFO 10-15 15:28:04 client.py:113] Model loaded: Mixtral-8x7B, 9e4693f2-334d-45a4-83f3-49d05d0b89f6
DEBUG 10-15 15:28:04 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:04 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:04 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:04 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:04 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:04 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:04 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:04 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:04 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:04 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:04 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:04 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:04 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:04 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:04 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:04 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:04 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:04 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:04 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:04 lpllm.py:2265] GPU2CPU move cost 0.000565 seconds
DEBUG 10-15 15:28:04 lpmodule.py:340] layer_idx 12 attn layer_idx: 12
DEBUG 10-15 15:28:04 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:04 lpmodule.py:364] decoder_attn_batch update batch_dim 600-660
DEBUG 10-15 15:28:04 StaticCacheLen.py:84] static cache update layer_idx: 12, start_batch: 600, end_batch: 660
DEBUG 10-15 15:28:04 lpmodule.py:368] update for kv cache 600-660 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:04 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:04 lpmodule.py:374] update past key value cost 0.023654 seconds
DEBUG 10-15 15:28:04 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:04 lpmodule.py:227] layer idx 11
DEBUG 10-15 15:28:04 lpmodule.py:399] repeat qkv cost 0.029082 seconds
DEBUG 10-15 15:28:04 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:04 lpmodule.py:433] dot attn cost 0.032383 seconds
DEBUG 10-15 15:28:04 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:04 lpmodule.py:444] time cost move to cuda:1 0.02280569076538086 s
DEBUG 10-15 15:28:04 lpllm.py:2283] CPU attn cost 0.131768 seconds if batch True
DEBUG 10-15 15:28:04 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:04 lpllm.py:2294] CPU compute cost 0.132549 seconds
DEBUG 10-15 15:28:04 lpllm.py:2312] free cost 0.000073 seconds
DEBUG 10-15 15:28:04 lpllm.py:503] reset update experts
DEBUG 10-15 15:28:04 lpllm.py:1774] update state cost 3.5762786865234375e-05 s
DEBUG 10-15 15:28:04 lpllm.py:1743] restore layer func cost 0.0008530616760253906 s
DEBUG 10-15 15:28:04 lpllm.py:511] restore layer cost 0.001096963882446289 s
DEBUG 10-15 15:28:04 lpllm.py:512] reset_next_layer_need: layer_attn_idx=12, layer_mlp_idx=12, j_loc=24
DEBUG 10-15 15:28:04 lpllm.py:1037] reset layer cost 0.0011706352233886719 s
DEBUG 10-15 15:28:04 lpllm.py:1038] have reset next layer layer_attn 12 layer_mlp 12 
DEBUG 10-15 15:28:04 lpllm.py:924] 
DEBUG 10-15 15:28:04 lpllm.py:924] decoder loop j: 25 cur_layer_idx: 12 layer_attn_idx: 12 layer_mlp_idx: 12
DEBUG 10-15 15:28:04 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:04 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:04 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:04 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:04 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:04 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:04 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:04 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:04 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:04 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:04 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:04 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:04 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:04 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:04 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:04 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:04 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:04 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:04 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:04 lpllm.py:2265] GPU2CPU move cost 0.000620 seconds
DEBUG 10-15 15:28:04 lpmodule.py:340] layer_idx 12 attn layer_idx: 12
DEBUG 10-15 15:28:04 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:04 lpmodule.py:364] decoder_attn_batch update batch_dim 660-720
DEBUG 10-15 15:28:05 StaticCacheLen.py:84] static cache update layer_idx: 12, start_batch: 660, end_batch: 720
DEBUG 10-15 15:28:05 lpmodule.py:368] update for kv cache 660-720 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:05 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:05 lpmodule.py:374] update past key value cost 0.023624 seconds
DEBUG 10-15 15:28:05 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:05 lpmodule.py:227] layer idx 12
DEBUG 10-15 15:28:05 lpmodule.py:399] repeat qkv cost 0.029321 seconds
DEBUG 10-15 15:28:05 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:05 lpmodule.py:433] dot attn cost 0.031877 seconds
DEBUG 10-15 15:28:05 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:05 lpmodule.py:444] time cost move to cuda:1 0.02364325523376465 s
DEBUG 10-15 15:28:05 lpllm.py:2283] CPU attn cost 0.132036 seconds if batch True
DEBUG 10-15 15:28:05 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:28:05 lpllm.py:2294] CPU compute cost 0.132929 seconds
DEBUG 10-15 15:28:05 lpllm.py:2312] free cost 0.000070 seconds
DEBUG 10-15 15:28:05 lpllm.py:498] reset update attn
DEBUG 10-15 15:28:05 lpllm.py:1774] update state cost 3.981590270996094e-05 s
DEBUG 10-15 15:28:05 lpllm.py:1743] restore layer func cost 0.0004038810729980469 s
DEBUG 10-15 15:28:05 lpllm.py:511] restore layer cost 0.0006678104400634766 s
DEBUG 10-15 15:28:05 lpllm.py:512] reset_next_layer_need: layer_attn_idx=13, layer_mlp_idx=12, j_loc=25
DEBUG 10-15 15:28:05 lpllm.py:1037] reset layer cost 0.0007386207580566406 s
DEBUG 10-15 15:28:05 lpllm.py:1038] have reset next layer layer_attn 13 layer_mlp 12 
DEBUG 10-15 15:28:05 lpllm.py:1044] j: 25 waiting the layer with layer_idx 13 before wait time 0.4415283203125 s
INFO 10-15 15:28:05 client.py:117] confirm_model_loaded: Mixtral-8x7B, 9e4693f2-334d-45a4-83f3-49d05d0b89f6
INFO 10-15 15:28:05 client.py:125] Model loaded
DEBUG 10-15 15:28:05 lpllm.py:1048] j: load cost 0.44327712059020996 s waiting cost 0.0017330646514892578 s
DEBUG 10-15 15:28:05 lpllm.py:924] 
DEBUG 10-15 15:28:05 lpllm.py:924] decoder loop j: 26 cur_layer_idx: 13 layer_attn_idx: 13 layer_mlp_idx: 12
DEBUG 10-15 15:28:05 lpllm.py:933] start load next layer cur_layer_idx: 14
DEBUG 10-15 15:28:05 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:28:05 client.py:72] load_into_gpu: Mixtral-8x7B, 4d2fbf77-fa7f-4512-a8da-ef08d48242eb
INFO 10-15 15:28:05 client.py:113] Model loaded: Mixtral-8x7B, 4d2fbf77-fa7f-4512-a8da-ef08d48242eb
DEBUG 10-15 15:28:05 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:05 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:05 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:05 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:05 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:05 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:05 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:05 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:05 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:05 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:05 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:05 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:05 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:05 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:05 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:05 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:05 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:05 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:05 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:05 lpllm.py:2265] GPU2CPU move cost 0.000599 seconds
DEBUG 10-15 15:28:05 lpmodule.py:340] layer_idx 13 attn layer_idx: 13
DEBUG 10-15 15:28:05 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:05 lpmodule.py:364] decoder_attn_batch update batch_dim 600-660
DEBUG 10-15 15:28:05 StaticCacheLen.py:84] static cache update layer_idx: 13, start_batch: 600, end_batch: 660
DEBUG 10-15 15:28:05 lpmodule.py:368] update for kv cache 600-660 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:05 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:05 lpmodule.py:374] update past key value cost 0.023340 seconds
DEBUG 10-15 15:28:05 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:05 lpmodule.py:227] layer idx 12
DEBUG 10-15 15:28:05 lpmodule.py:399] repeat qkv cost 0.029503 seconds
DEBUG 10-15 15:28:05 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:05 lpmodule.py:433] dot attn cost 0.032066 seconds
DEBUG 10-15 15:28:05 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:05 lpmodule.py:444] time cost move to cuda:1 0.023120403289794922 s
DEBUG 10-15 15:28:05 lpllm.py:2283] CPU attn cost 0.132198 seconds if batch True
DEBUG 10-15 15:28:05 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:28:05 lpllm.py:2294] CPU compute cost 0.133130 seconds
DEBUG 10-15 15:28:05 lpllm.py:2312] free cost 0.000082 seconds
DEBUG 10-15 15:28:05 lpllm.py:503] reset update experts
DEBUG 10-15 15:28:05 lpllm.py:1774] update state cost 3.814697265625e-05 s
DEBUG 10-15 15:28:05 lpllm.py:1743] restore layer func cost 0.0008530616760253906 s
DEBUG 10-15 15:28:05 lpllm.py:511] restore layer cost 0.0011072158813476562 s
DEBUG 10-15 15:28:05 lpllm.py:512] reset_next_layer_need: layer_attn_idx=13, layer_mlp_idx=13, j_loc=26
DEBUG 10-15 15:28:05 lpllm.py:1037] reset layer cost 0.0011794567108154297 s
DEBUG 10-15 15:28:05 lpllm.py:1038] have reset next layer layer_attn 13 layer_mlp 13 
DEBUG 10-15 15:28:05 lpllm.py:924] 
DEBUG 10-15 15:28:05 lpllm.py:924] decoder loop j: 27 cur_layer_idx: 13 layer_attn_idx: 13 layer_mlp_idx: 13
DEBUG 10-15 15:28:05 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:05 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:05 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:05 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:05 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:05 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:05 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:05 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:05 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:05 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:05 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:05 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:05 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:05 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:05 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:05 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:05 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:05 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:05 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:05 lpllm.py:2265] GPU2CPU move cost 0.000569 seconds
DEBUG 10-15 15:28:05 lpmodule.py:340] layer_idx 13 attn layer_idx: 13
DEBUG 10-15 15:28:05 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:05 lpmodule.py:364] decoder_attn_batch update batch_dim 660-720
DEBUG 10-15 15:28:05 StaticCacheLen.py:84] static cache update layer_idx: 13, start_batch: 660, end_batch: 720
DEBUG 10-15 15:28:05 lpmodule.py:368] update for kv cache 660-720 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:05 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:05 lpmodule.py:374] update past key value cost 0.022373 seconds
DEBUG 10-15 15:28:05 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:05 lpmodule.py:227] layer idx 13
DEBUG 10-15 15:28:05 lpmodule.py:399] repeat qkv cost 0.029375 seconds
DEBUG 10-15 15:28:05 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:05 lpmodule.py:433] dot attn cost 0.031716 seconds
DEBUG 10-15 15:28:05 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:05 lpmodule.py:444] time cost move to cuda:1 0.024915695190429688 s
DEBUG 10-15 15:28:05 lpllm.py:2283] CPU attn cost 0.132531 seconds if batch True
DEBUG 10-15 15:28:05 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:05 lpllm.py:2294] CPU compute cost 0.133343 seconds
DEBUG 10-15 15:28:05 lpllm.py:2312] free cost 0.000070 seconds
DEBUG 10-15 15:28:05 lpllm.py:498] reset update attn
DEBUG 10-15 15:28:05 lpllm.py:1774] update state cost 4.0531158447265625e-05 s
DEBUG 10-15 15:28:05 lpllm.py:1743] restore layer func cost 0.0004067420959472656 s
DEBUG 10-15 15:28:05 lpllm.py:511] restore layer cost 0.0006725788116455078 s
DEBUG 10-15 15:28:05 lpllm.py:512] reset_next_layer_need: layer_attn_idx=14, layer_mlp_idx=13, j_loc=27
DEBUG 10-15 15:28:05 lpllm.py:1037] reset layer cost 0.0007441043853759766 s
DEBUG 10-15 15:28:05 lpllm.py:1038] have reset next layer layer_attn 14 layer_mlp 13 
DEBUG 10-15 15:28:05 lpllm.py:1044] j: 27 waiting the layer with layer_idx 14 before wait time 0.44089365005493164 s
INFO 10-15 15:28:05 client.py:117] confirm_model_loaded: Mixtral-8x7B, 4d2fbf77-fa7f-4512-a8da-ef08d48242eb
INFO 10-15 15:28:05 client.py:125] Model loaded
DEBUG 10-15 15:28:05 lpllm.py:1048] j: load cost 0.44257593154907227 s waiting cost 0.0016672611236572266 s
DEBUG 10-15 15:28:05 lpllm.py:924] 
DEBUG 10-15 15:28:05 lpllm.py:924] decoder loop j: 28 cur_layer_idx: 14 layer_attn_idx: 14 layer_mlp_idx: 13
DEBUG 10-15 15:28:05 lpllm.py:933] start load next layer cur_layer_idx: 15
DEBUG 10-15 15:28:05 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed650>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x003\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x96\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:28:05 client.py:72] load_into_gpu: Mixtral-8x7B, 4f505ff8-4e23-4e0b-aee6-3f50d4c859b0
INFO 10-15 15:28:05 client.py:113] Model loaded: Mixtral-8x7B, 4f505ff8-4e23-4e0b-aee6-3f50d4c859b0
DEBUG 10-15 15:28:05 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:05 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:05 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:05 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:05 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:05 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:05 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:05 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:05 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:05 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:05 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:05 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:05 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:05 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:05 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:05 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:05 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:05 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:05 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:05 lpllm.py:2265] GPU2CPU move cost 0.000568 seconds
DEBUG 10-15 15:28:05 lpmodule.py:340] layer_idx 14 attn layer_idx: 14
DEBUG 10-15 15:28:05 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:05 lpmodule.py:364] decoder_attn_batch update batch_dim 600-660
DEBUG 10-15 15:28:05 StaticCacheLen.py:84] static cache update layer_idx: 14, start_batch: 600, end_batch: 660
DEBUG 10-15 15:28:05 lpmodule.py:368] update for kv cache 600-660 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:05 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:05 lpmodule.py:374] update past key value cost 0.022375 seconds
DEBUG 10-15 15:28:05 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:05 lpmodule.py:227] layer idx 13
DEBUG 10-15 15:28:05 lpmodule.py:399] repeat qkv cost 0.028165 seconds
DEBUG 10-15 15:28:05 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:05 lpmodule.py:433] dot attn cost 0.031391 seconds
DEBUG 10-15 15:28:05 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:05 lpmodule.py:444] time cost move to cuda:1 0.02465057373046875 s
DEBUG 10-15 15:28:05 lpllm.py:2283] CPU attn cost 0.130023 seconds if batch True
DEBUG 10-15 15:28:05 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:05 lpllm.py:2294] CPU compute cost 0.130873 seconds
DEBUG 10-15 15:28:05 lpllm.py:2312] free cost 0.000072 seconds
DEBUG 10-15 15:28:05 lpllm.py:503] reset update experts
DEBUG 10-15 15:28:05 lpllm.py:1774] update state cost 3.552436828613281e-05 s
DEBUG 10-15 15:28:05 lpllm.py:1743] restore layer func cost 0.0008370876312255859 s
DEBUG 10-15 15:28:05 lpllm.py:511] restore layer cost 0.001077413558959961 s
DEBUG 10-15 15:28:05 lpllm.py:512] reset_next_layer_need: layer_attn_idx=14, layer_mlp_idx=14, j_loc=28
DEBUG 10-15 15:28:05 lpllm.py:1037] reset layer cost 0.00115203857421875 s
DEBUG 10-15 15:28:05 lpllm.py:1038] have reset next layer layer_attn 14 layer_mlp 14 
DEBUG 10-15 15:28:05 lpllm.py:924] 
DEBUG 10-15 15:28:05 lpllm.py:924] decoder loop j: 29 cur_layer_idx: 14 layer_attn_idx: 14 layer_mlp_idx: 14
DEBUG 10-15 15:28:05 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:05 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:05 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:05 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:05 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:05 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:05 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:05 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:05 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:05 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:05 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:05 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:05 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:05 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:05 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:05 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:05 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:05 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:05 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:05 lpllm.py:2265] GPU2CPU move cost 0.000573 seconds
DEBUG 10-15 15:28:05 lpmodule.py:340] layer_idx 14 attn layer_idx: 14
DEBUG 10-15 15:28:05 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:05 lpmodule.py:364] decoder_attn_batch update batch_dim 660-720
DEBUG 10-15 15:28:05 StaticCacheLen.py:84] static cache update layer_idx: 14, start_batch: 660, end_batch: 720
DEBUG 10-15 15:28:05 lpmodule.py:368] update for kv cache 660-720 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:05 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:05 lpmodule.py:374] update past key value cost 0.023127 seconds
DEBUG 10-15 15:28:05 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:05 lpmodule.py:227] layer idx 14
DEBUG 10-15 15:28:05 lpmodule.py:399] repeat qkv cost 0.028817 seconds
DEBUG 10-15 15:28:05 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:05 lpmodule.py:433] dot attn cost 0.032267 seconds
DEBUG 10-15 15:28:05 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:05 lpmodule.py:444] time cost move to cuda:1 0.023343563079833984 s
DEBUG 10-15 15:28:06 lpllm.py:2283] CPU attn cost 0.131515 seconds if batch True
DEBUG 10-15 15:28:06 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:28:06 lpllm.py:2294] CPU compute cost 0.132362 seconds
DEBUG 10-15 15:28:06 lpllm.py:2312] free cost 0.000070 seconds
DEBUG 10-15 15:28:06 lpllm.py:498] reset update attn
DEBUG 10-15 15:28:06 lpllm.py:1774] update state cost 4.267692565917969e-05 s
DEBUG 10-15 15:28:06 lpllm.py:1743] restore layer func cost 0.0004150867462158203 s
DEBUG 10-15 15:28:06 lpllm.py:511] restore layer cost 0.0006709098815917969 s
DEBUG 10-15 15:28:06 lpllm.py:512] reset_next_layer_need: layer_attn_idx=15, layer_mlp_idx=14, j_loc=29
DEBUG 10-15 15:28:06 lpllm.py:1037] reset layer cost 0.0007405281066894531 s
DEBUG 10-15 15:28:06 lpllm.py:1038] have reset next layer layer_attn 15 layer_mlp 14 
DEBUG 10-15 15:28:06 lpllm.py:1044] j: 29 waiting the layer with layer_idx 15 before wait time 0.4396805763244629 s
INFO 10-15 15:28:06 client.py:117] confirm_model_loaded: Mixtral-8x7B, 4f505ff8-4e23-4e0b-aee6-3f50d4c859b0
INFO 10-15 15:28:06 client.py:125] Model loaded
DEBUG 10-15 15:28:06 lpllm.py:1048] j: load cost 0.44109487533569336 s waiting cost 0.0013990402221679688 s
DEBUG 10-15 15:28:06 lpllm.py:924] 
DEBUG 10-15 15:28:06 lpllm.py:924] decoder loop j: 30 cur_layer_idx: 15 layer_attn_idx: 15 layer_mlp_idx: 14
DEBUG 10-15 15:28:06 lpllm.py:933] start load next layer cur_layer_idx: 16
DEBUG 10-15 15:28:06 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed6e0>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x004\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x98\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:28:06 client.py:72] load_into_gpu: Mixtral-8x7B, cbd3237f-0a45-43dc-8ba7-ee3fbdfc213f
INFO 10-15 15:28:06 client.py:113] Model loaded: Mixtral-8x7B, cbd3237f-0a45-43dc-8ba7-ee3fbdfc213f
DEBUG 10-15 15:28:06 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:06 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:06 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:06 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:06 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:06 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:06 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:06 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:06 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:06 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:06 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:06 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:06 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:06 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:06 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:06 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:06 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:06 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:06 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:06 lpllm.py:2265] GPU2CPU move cost 0.000613 seconds
DEBUG 10-15 15:28:06 lpmodule.py:340] layer_idx 15 attn layer_idx: 15
DEBUG 10-15 15:28:06 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:06 lpmodule.py:364] decoder_attn_batch update batch_dim 600-660
DEBUG 10-15 15:28:06 StaticCacheLen.py:84] static cache update layer_idx: 15, start_batch: 600, end_batch: 660
DEBUG 10-15 15:28:06 lpmodule.py:368] update for kv cache 600-660 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:06 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:06 lpmodule.py:374] update past key value cost 0.024814 seconds
DEBUG 10-15 15:28:06 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:06 lpmodule.py:227] layer idx 14
DEBUG 10-15 15:28:06 lpmodule.py:399] repeat qkv cost 0.027937 seconds
DEBUG 10-15 15:28:06 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:06 lpmodule.py:433] dot attn cost 0.032548 seconds
DEBUG 10-15 15:28:06 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:06 lpmodule.py:444] time cost move to cuda:1 0.02330923080444336 s
DEBUG 10-15 15:28:06 lpllm.py:2283] CPU attn cost 0.132404 seconds if batch True
DEBUG 10-15 15:28:06 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:06 lpllm.py:2294] CPU compute cost 0.133309 seconds
DEBUG 10-15 15:28:06 lpllm.py:2312] free cost 0.000071 seconds
DEBUG 10-15 15:28:06 lpllm.py:503] reset update experts
DEBUG 10-15 15:28:06 lpllm.py:1774] update state cost 3.719329833984375e-05 s
DEBUG 10-15 15:28:06 lpllm.py:1743] restore layer func cost 0.0008394718170166016 s
DEBUG 10-15 15:28:06 lpllm.py:511] restore layer cost 0.0010833740234375 s
DEBUG 10-15 15:28:06 lpllm.py:512] reset_next_layer_need: layer_attn_idx=15, layer_mlp_idx=15, j_loc=30
DEBUG 10-15 15:28:06 lpllm.py:1037] reset layer cost 0.0011589527130126953 s
DEBUG 10-15 15:28:06 lpllm.py:1038] have reset next layer layer_attn 15 layer_mlp 15 
DEBUG 10-15 15:28:06 lpllm.py:924] 
DEBUG 10-15 15:28:06 lpllm.py:924] decoder loop j: 31 cur_layer_idx: 15 layer_attn_idx: 15 layer_mlp_idx: 15
DEBUG 10-15 15:28:06 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:06 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:06 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:06 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:06 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:06 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:06 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:06 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:06 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:06 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:06 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:06 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:06 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:06 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:06 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:06 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:06 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:06 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:06 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:06 lpllm.py:2265] GPU2CPU move cost 0.000585 seconds
DEBUG 10-15 15:28:06 lpmodule.py:340] layer_idx 15 attn layer_idx: 15
DEBUG 10-15 15:28:06 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:06 lpmodule.py:364] decoder_attn_batch update batch_dim 660-720
DEBUG 10-15 15:28:06 StaticCacheLen.py:84] static cache update layer_idx: 15, start_batch: 660, end_batch: 720
DEBUG 10-15 15:28:06 lpmodule.py:368] update for kv cache 660-720 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:06 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:06 lpmodule.py:374] update past key value cost 0.021835 seconds
DEBUG 10-15 15:28:06 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:06 lpmodule.py:227] layer idx 15
DEBUG 10-15 15:28:06 lpmodule.py:399] repeat qkv cost 0.030370 seconds
DEBUG 10-15 15:28:06 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:06 lpmodule.py:433] dot attn cost 0.032447 seconds
DEBUG 10-15 15:28:06 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:06 lpmodule.py:444] time cost move to cuda:1 0.02396368980407715 s
DEBUG 10-15 15:28:06 lpllm.py:2283] CPU attn cost 0.132823 seconds if batch True
DEBUG 10-15 15:28:06 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:06 lpllm.py:2294] CPU compute cost 0.133684 seconds
DEBUG 10-15 15:28:06 lpllm.py:2312] free cost 0.000094 seconds
DEBUG 10-15 15:28:06 lpllm.py:498] reset update attn
DEBUG 10-15 15:28:06 lpllm.py:1774] update state cost 4.172325134277344e-05 s
DEBUG 10-15 15:28:06 lpllm.py:1743] restore layer func cost 0.00041604042053222656 s
DEBUG 10-15 15:28:06 lpllm.py:511] restore layer cost 0.0006773471832275391 s
DEBUG 10-15 15:28:06 lpllm.py:512] reset_next_layer_need: layer_attn_idx=16, layer_mlp_idx=15, j_loc=31
DEBUG 10-15 15:28:06 lpllm.py:1037] reset layer cost 0.0007548332214355469 s
DEBUG 10-15 15:28:06 lpllm.py:1038] have reset next layer layer_attn 16 layer_mlp 15 
DEBUG 10-15 15:28:06 lpllm.py:1044] j: 31 waiting the layer with layer_idx 16 before wait time 0.4485628604888916 s
INFO 10-15 15:28:06 client.py:117] confirm_model_loaded: Mixtral-8x7B, cbd3237f-0a45-43dc-8ba7-ee3fbdfc213f
INFO 10-15 15:28:06 client.py:125] Model loaded
DEBUG 10-15 15:28:06 lpllm.py:1048] j: load cost 0.450150728225708 s waiting cost 0.0015721321105957031 s
DEBUG 10-15 15:28:06 lpllm.py:924] 
DEBUG 10-15 15:28:06 lpllm.py:924] decoder loop j: 32 cur_layer_idx: 16 layer_attn_idx: 16 layer_mlp_idx: 15
DEBUG 10-15 15:28:06 lpllm.py:933] start load next layer cur_layer_idx: 17
DEBUG 10-15 15:28:06 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:28:06 client.py:72] load_into_gpu: Mixtral-8x7B, 620269e3-52ff-410c-812a-ee6ea9cbcbe2
INFO 10-15 15:28:06 client.py:113] Model loaded: Mixtral-8x7B, 620269e3-52ff-410c-812a-ee6ea9cbcbe2
DEBUG 10-15 15:28:06 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:06 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:06 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:06 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:06 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:06 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:06 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:06 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:06 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:06 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:06 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:06 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:06 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:06 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:06 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:06 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:06 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:06 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:06 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:06 lpllm.py:2265] GPU2CPU move cost 0.000571 seconds
DEBUG 10-15 15:28:06 lpmodule.py:340] layer_idx 16 attn layer_idx: 16
DEBUG 10-15 15:28:06 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:06 lpmodule.py:364] decoder_attn_batch update batch_dim 600-660
DEBUG 10-15 15:28:06 StaticCacheLen.py:84] static cache update layer_idx: 16, start_batch: 600, end_batch: 660
DEBUG 10-15 15:28:06 lpmodule.py:368] update for kv cache 600-660 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:06 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:06 lpmodule.py:374] update past key value cost 0.023419 seconds
DEBUG 10-15 15:28:06 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:06 lpmodule.py:227] layer idx 15
DEBUG 10-15 15:28:06 lpmodule.py:399] repeat qkv cost 0.029716 seconds
DEBUG 10-15 15:28:06 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:06 lpmodule.py:433] dot attn cost 0.032305 seconds
DEBUG 10-15 15:28:06 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:06 lpmodule.py:444] time cost move to cuda:1 0.0262753963470459 s
DEBUG 10-15 15:28:06 lpllm.py:2283] CPU attn cost 0.135209 seconds if batch True
DEBUG 10-15 15:28:06 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:06 lpllm.py:2294] CPU compute cost 0.136093 seconds
DEBUG 10-15 15:28:06 lpllm.py:2312] free cost 0.000072 seconds
DEBUG 10-15 15:28:06 lpllm.py:503] reset update experts
DEBUG 10-15 15:28:06 lpllm.py:1774] update state cost 3.790855407714844e-05 s
DEBUG 10-15 15:28:06 lpllm.py:1743] restore layer func cost 0.0008704662322998047 s
DEBUG 10-15 15:28:06 lpllm.py:511] restore layer cost 0.0011394023895263672 s
DEBUG 10-15 15:28:06 lpllm.py:512] reset_next_layer_need: layer_attn_idx=16, layer_mlp_idx=16, j_loc=32
DEBUG 10-15 15:28:06 lpllm.py:1037] reset layer cost 0.0012154579162597656 s
DEBUG 10-15 15:28:06 lpllm.py:1038] have reset next layer layer_attn 16 layer_mlp 16 
DEBUG 10-15 15:28:06 lpllm.py:924] 
DEBUG 10-15 15:28:06 lpllm.py:924] decoder loop j: 33 cur_layer_idx: 16 layer_attn_idx: 16 layer_mlp_idx: 16
DEBUG 10-15 15:28:06 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:06 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:06 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:06 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:06 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:06 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:06 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:06 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:06 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:06 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:06 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:06 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:06 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:06 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:06 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:06 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:06 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:06 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:06 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:06 lpllm.py:2265] GPU2CPU move cost 0.000620 seconds
DEBUG 10-15 15:28:06 lpmodule.py:340] layer_idx 16 attn layer_idx: 16
DEBUG 10-15 15:28:06 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:06 lpmodule.py:364] decoder_attn_batch update batch_dim 660-720
DEBUG 10-15 15:28:06 StaticCacheLen.py:84] static cache update layer_idx: 16, start_batch: 660, end_batch: 720
DEBUG 10-15 15:28:06 lpmodule.py:368] update for kv cache 660-720 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:06 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:06 lpmodule.py:374] update past key value cost 0.022341 seconds
DEBUG 10-15 15:28:06 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:06 lpmodule.py:227] layer idx 16
DEBUG 10-15 15:28:06 lpmodule.py:399] repeat qkv cost 0.029144 seconds
DEBUG 10-15 15:28:06 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:06 lpmodule.py:433] dot attn cost 0.032655 seconds
DEBUG 10-15 15:28:06 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:06 lpmodule.py:444] time cost move to cuda:1 0.023529529571533203 s
DEBUG 10-15 15:28:06 lpllm.py:2283] CPU attn cost 0.133321 seconds if batch True
DEBUG 10-15 15:28:06 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:06 lpllm.py:2294] CPU compute cost 0.134212 seconds
DEBUG 10-15 15:28:06 lpllm.py:2312] free cost 0.000073 seconds
DEBUG 10-15 15:28:06 lpllm.py:498] reset update attn
DEBUG 10-15 15:28:06 lpllm.py:1774] update state cost 2.6226043701171875e-05 s
DEBUG 10-15 15:28:06 lpllm.py:1743] restore layer func cost 0.00041103363037109375 s
DEBUG 10-15 15:28:06 lpllm.py:511] restore layer cost 0.003662109375 s
DEBUG 10-15 15:28:06 lpllm.py:512] reset_next_layer_need: layer_attn_idx=17, layer_mlp_idx=16, j_loc=33
DEBUG 10-15 15:28:06 lpllm.py:1037] reset layer cost 0.0037355422973632812 s
DEBUG 10-15 15:28:06 lpllm.py:1038] have reset next layer layer_attn 17 layer_mlp 16 
DEBUG 10-15 15:28:06 lpllm.py:1044] j: 33 waiting the layer with layer_idx 17 before wait time 0.45447683334350586 s
INFO 10-15 15:28:06 client.py:117] confirm_model_loaded: Mixtral-8x7B, 620269e3-52ff-410c-812a-ee6ea9cbcbe2
INFO 10-15 15:28:06 client.py:125] Model loaded
DEBUG 10-15 15:28:06 lpllm.py:1048] j: load cost 0.45621490478515625 s waiting cost 0.0017223358154296875 s
DEBUG 10-15 15:28:06 lpllm.py:924] 
DEBUG 10-15 15:28:06 lpllm.py:924] decoder loop j: 34 cur_layer_idx: 17 layer_attn_idx: 17 layer_mlp_idx: 16
DEBUG 10-15 15:28:06 lpllm.py:933] start load next layer cur_layer_idx: 18
DEBUG 10-15 15:28:06 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed650>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x003\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x96\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:28:06 client.py:72] load_into_gpu: Mixtral-8x7B, 389b08cc-7286-47d5-a0a1-0b9765030a3a
INFO 10-15 15:28:06 client.py:113] Model loaded: Mixtral-8x7B, 389b08cc-7286-47d5-a0a1-0b9765030a3a
DEBUG 10-15 15:28:06 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:06 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:06 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:07 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:07 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:07 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:07 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:07 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:07 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:07 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:07 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:07 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:07 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:07 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:07 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:07 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:07 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:07 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:07 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:07 lpllm.py:2265] GPU2CPU move cost 0.000573 seconds
DEBUG 10-15 15:28:07 lpmodule.py:340] layer_idx 17 attn layer_idx: 17
DEBUG 10-15 15:28:07 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:07 lpmodule.py:364] decoder_attn_batch update batch_dim 600-660
DEBUG 10-15 15:28:07 StaticCacheLen.py:84] static cache update layer_idx: 17, start_batch: 600, end_batch: 660
DEBUG 10-15 15:28:07 lpmodule.py:368] update for kv cache 600-660 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:07 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:07 lpmodule.py:374] update past key value cost 0.022635 seconds
DEBUG 10-15 15:28:07 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:07 lpmodule.py:227] layer idx 16
DEBUG 10-15 15:28:07 lpmodule.py:399] repeat qkv cost 0.029840 seconds
DEBUG 10-15 15:28:07 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:07 lpmodule.py:433] dot attn cost 0.033154 seconds
DEBUG 10-15 15:28:07 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:07 lpmodule.py:444] time cost move to cuda:1 0.025979280471801758 s
DEBUG 10-15 15:28:07 lpllm.py:2283] CPU attn cost 0.135163 seconds if batch True
DEBUG 10-15 15:28:07 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:07 lpllm.py:2294] CPU compute cost 0.136045 seconds
DEBUG 10-15 15:28:07 lpllm.py:2312] free cost 0.000073 seconds
DEBUG 10-15 15:28:07 lpllm.py:503] reset update experts
DEBUG 10-15 15:28:07 lpllm.py:1774] update state cost 2.5033950805664062e-05 s
DEBUG 10-15 15:28:07 lpllm.py:1743] restore layer func cost 0.0008332729339599609 s
DEBUG 10-15 15:28:07 lpllm.py:511] restore layer cost 0.0010979175567626953 s
DEBUG 10-15 15:28:07 lpllm.py:512] reset_next_layer_need: layer_attn_idx=17, layer_mlp_idx=17, j_loc=34
DEBUG 10-15 15:28:07 lpllm.py:1037] reset layer cost 0.0011708736419677734 s
DEBUG 10-15 15:28:07 lpllm.py:1038] have reset next layer layer_attn 17 layer_mlp 17 
DEBUG 10-15 15:28:07 lpllm.py:924] 
DEBUG 10-15 15:28:07 lpllm.py:924] decoder loop j: 35 cur_layer_idx: 17 layer_attn_idx: 17 layer_mlp_idx: 17
DEBUG 10-15 15:28:07 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:07 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:07 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:07 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:07 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:07 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:07 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:07 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:07 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:07 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:07 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:07 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:07 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:07 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:07 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:07 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:07 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:07 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:07 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:07 lpllm.py:2265] GPU2CPU move cost 0.000608 seconds
DEBUG 10-15 15:28:07 lpmodule.py:340] layer_idx 17 attn layer_idx: 17
DEBUG 10-15 15:28:07 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:07 lpmodule.py:364] decoder_attn_batch update batch_dim 660-720
DEBUG 10-15 15:28:07 StaticCacheLen.py:84] static cache update layer_idx: 17, start_batch: 660, end_batch: 720
DEBUG 10-15 15:28:07 lpmodule.py:368] update for kv cache 660-720 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:07 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:07 lpmodule.py:374] update past key value cost 0.023852 seconds
DEBUG 10-15 15:28:07 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:07 lpmodule.py:227] layer idx 17
DEBUG 10-15 15:28:07 lpmodule.py:399] repeat qkv cost 0.029547 seconds
DEBUG 10-15 15:28:07 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:07 lpmodule.py:433] dot attn cost 0.031819 seconds
DEBUG 10-15 15:28:07 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:07 lpmodule.py:444] time cost move to cuda:1 0.022984743118286133 s
DEBUG 10-15 15:28:07 lpllm.py:2283] CPU attn cost 0.132085 seconds if batch True
DEBUG 10-15 15:28:07 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:07 lpllm.py:2294] CPU compute cost 0.132984 seconds
DEBUG 10-15 15:28:07 lpllm.py:2312] free cost 0.000073 seconds
DEBUG 10-15 15:28:07 lpllm.py:498] reset update attn
DEBUG 10-15 15:28:07 lpllm.py:1774] update state cost 2.5272369384765625e-05 s
DEBUG 10-15 15:28:07 lpllm.py:1743] restore layer func cost 0.0004088878631591797 s
DEBUG 10-15 15:28:07 lpllm.py:511] restore layer cost 0.0006518363952636719 s
DEBUG 10-15 15:28:07 lpllm.py:512] reset_next_layer_need: layer_attn_idx=18, layer_mlp_idx=17, j_loc=35
DEBUG 10-15 15:28:07 lpllm.py:1037] reset layer cost 0.0007407665252685547 s
DEBUG 10-15 15:28:07 lpllm.py:1038] have reset next layer layer_attn 18 layer_mlp 17 
DEBUG 10-15 15:28:07 lpllm.py:1044] j: 35 waiting the layer with layer_idx 18 before wait time 0.45198845863342285 s
INFO 10-15 15:28:07 client.py:117] confirm_model_loaded: Mixtral-8x7B, 389b08cc-7286-47d5-a0a1-0b9765030a3a
INFO 10-15 15:28:07 client.py:125] Model loaded
DEBUG 10-15 15:28:07 lpllm.py:1048] j: load cost 0.45365476608276367 s waiting cost 0.001649618148803711 s
DEBUG 10-15 15:28:07 lpllm.py:924] 
DEBUG 10-15 15:28:07 lpllm.py:924] decoder loop j: 36 cur_layer_idx: 18 layer_attn_idx: 18 layer_mlp_idx: 17
DEBUG 10-15 15:28:07 lpllm.py:933] start load next layer cur_layer_idx: 19
DEBUG 10-15 15:28:07 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed6e0>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x004\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x98\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:28:07 client.py:72] load_into_gpu: Mixtral-8x7B, 598d2eb3-fe6a-4e3f-91fd-c8cc5d2e4375
INFO 10-15 15:28:07 client.py:113] Model loaded: Mixtral-8x7B, 598d2eb3-fe6a-4e3f-91fd-c8cc5d2e4375
DEBUG 10-15 15:28:07 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:07 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:07 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:07 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:07 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:07 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:07 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:07 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:07 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:07 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:07 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:07 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:07 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:07 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:07 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:07 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:07 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:07 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:07 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:07 lpllm.py:2265] GPU2CPU move cost 0.000441 seconds
DEBUG 10-15 15:28:07 lpmodule.py:340] layer_idx 18 attn layer_idx: 18
DEBUG 10-15 15:28:07 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:07 lpmodule.py:364] decoder_attn_batch update batch_dim 600-660
DEBUG 10-15 15:28:07 StaticCacheLen.py:84] static cache update layer_idx: 18, start_batch: 600, end_batch: 660
DEBUG 10-15 15:28:07 lpmodule.py:368] update for kv cache 600-660 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:07 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:07 lpmodule.py:374] update past key value cost 0.024187 seconds
DEBUG 10-15 15:28:07 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:07 lpmodule.py:227] layer idx 17
DEBUG 10-15 15:28:07 lpmodule.py:399] repeat qkv cost 0.029728 seconds
DEBUG 10-15 15:28:07 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:07 lpmodule.py:433] dot attn cost 0.032708 seconds
DEBUG 10-15 15:28:07 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:07 lpmodule.py:444] time cost move to cuda:1 0.023095369338989258 s
DEBUG 10-15 15:28:07 lpllm.py:2283] CPU attn cost 0.133576 seconds if batch True
DEBUG 10-15 15:28:07 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:28:07 lpllm.py:2294] CPU compute cost 0.134255 seconds
DEBUG 10-15 15:28:07 lpllm.py:2312] free cost 0.000073 seconds
DEBUG 10-15 15:28:07 lpllm.py:503] reset update experts
DEBUG 10-15 15:28:07 lpllm.py:1774] update state cost 2.3603439331054688e-05 s
DEBUG 10-15 15:28:07 lpllm.py:1743] restore layer func cost 0.0008378028869628906 s
DEBUG 10-15 15:28:07 lpllm.py:511] restore layer cost 0.0010721683502197266 s
DEBUG 10-15 15:28:07 lpllm.py:512] reset_next_layer_need: layer_attn_idx=18, layer_mlp_idx=18, j_loc=36
DEBUG 10-15 15:28:07 lpllm.py:1037] reset layer cost 0.0011701583862304688 s
DEBUG 10-15 15:28:07 lpllm.py:1038] have reset next layer layer_attn 18 layer_mlp 18 
DEBUG 10-15 15:28:07 lpllm.py:924] 
DEBUG 10-15 15:28:07 lpllm.py:924] decoder loop j: 37 cur_layer_idx: 18 layer_attn_idx: 18 layer_mlp_idx: 18
DEBUG 10-15 15:28:07 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:07 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:07 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:07 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:07 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:07 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:07 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:07 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:07 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:07 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:07 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:07 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:07 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:07 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:07 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:07 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:07 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:07 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:07 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:07 lpllm.py:2265] GPU2CPU move cost 0.000593 seconds
DEBUG 10-15 15:28:07 lpmodule.py:340] layer_idx 18 attn layer_idx: 18
DEBUG 10-15 15:28:07 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:07 lpmodule.py:364] decoder_attn_batch update batch_dim 660-720
DEBUG 10-15 15:28:07 StaticCacheLen.py:84] static cache update layer_idx: 18, start_batch: 660, end_batch: 720
DEBUG 10-15 15:28:07 lpmodule.py:368] update for kv cache 660-720 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:07 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:07 lpmodule.py:374] update past key value cost 0.021454 seconds
DEBUG 10-15 15:28:07 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:07 lpmodule.py:227] layer idx 18
DEBUG 10-15 15:28:07 lpmodule.py:399] repeat qkv cost 0.028949 seconds
DEBUG 10-15 15:28:07 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:07 lpmodule.py:433] dot attn cost 0.031729 seconds
DEBUG 10-15 15:28:07 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:07 lpmodule.py:444] time cost move to cuda:1 0.023560762405395508 s
DEBUG 10-15 15:28:07 lpllm.py:2283] CPU attn cost 0.129158 seconds if batch True
DEBUG 10-15 15:28:07 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:07 lpllm.py:2294] CPU compute cost 0.130031 seconds
DEBUG 10-15 15:28:07 lpllm.py:2312] free cost 0.000073 seconds
DEBUG 10-15 15:28:07 lpllm.py:498] reset update attn
DEBUG 10-15 15:28:07 lpllm.py:1774] update state cost 2.384185791015625e-05 s
DEBUG 10-15 15:28:07 lpllm.py:1743] restore layer func cost 0.0004203319549560547 s
DEBUG 10-15 15:28:07 lpllm.py:511] restore layer cost 0.0006716251373291016 s
DEBUG 10-15 15:28:07 lpllm.py:512] reset_next_layer_need: layer_attn_idx=19, layer_mlp_idx=18, j_loc=37
DEBUG 10-15 15:28:07 lpllm.py:1037] reset layer cost 0.0007445812225341797 s
DEBUG 10-15 15:28:07 lpllm.py:1038] have reset next layer layer_attn 19 layer_mlp 18 
DEBUG 10-15 15:28:07 lpllm.py:1044] j: 37 waiting the layer with layer_idx 19 before wait time 0.44936108589172363 s
INFO 10-15 15:28:07 client.py:117] confirm_model_loaded: Mixtral-8x7B, 598d2eb3-fe6a-4e3f-91fd-c8cc5d2e4375
INFO 10-15 15:28:07 client.py:125] Model loaded
DEBUG 10-15 15:28:07 lpllm.py:1048] j: load cost 0.4510328769683838 s waiting cost 0.0016560554504394531 s
DEBUG 10-15 15:28:07 lpllm.py:924] 
DEBUG 10-15 15:28:07 lpllm.py:924] decoder loop j: 38 cur_layer_idx: 19 layer_attn_idx: 19 layer_mlp_idx: 18
DEBUG 10-15 15:28:07 lpllm.py:933] start load next layer cur_layer_idx: 20
DEBUG 10-15 15:28:07 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:28:07 client.py:72] load_into_gpu: Mixtral-8x7B, b491240d-72fa-4368-9753-f74c95f124f2
INFO 10-15 15:28:07 client.py:113] Model loaded: Mixtral-8x7B, b491240d-72fa-4368-9753-f74c95f124f2
DEBUG 10-15 15:28:07 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:07 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:07 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:07 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:07 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:07 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:07 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:07 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:07 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:07 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:07 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:07 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:07 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:07 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:07 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:07 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:07 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:07 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:07 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:07 lpllm.py:2265] GPU2CPU move cost 0.000611 seconds
DEBUG 10-15 15:28:07 lpmodule.py:340] layer_idx 19 attn layer_idx: 19
DEBUG 10-15 15:28:07 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:07 lpmodule.py:364] decoder_attn_batch update batch_dim 600-660
DEBUG 10-15 15:28:07 StaticCacheLen.py:84] static cache update layer_idx: 19, start_batch: 600, end_batch: 660
DEBUG 10-15 15:28:07 lpmodule.py:368] update for kv cache 600-660 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:07 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:07 lpmodule.py:374] update past key value cost 0.023113 seconds
DEBUG 10-15 15:28:07 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:07 lpmodule.py:227] layer idx 18
DEBUG 10-15 15:28:07 lpmodule.py:399] repeat qkv cost 0.028491 seconds
DEBUG 10-15 15:28:07 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:07 lpmodule.py:433] dot attn cost 0.033228 seconds
DEBUG 10-15 15:28:07 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:08 lpmodule.py:444] time cost move to cuda:1 0.023429393768310547 s
DEBUG 10-15 15:28:08 lpllm.py:2283] CPU attn cost 0.133546 seconds if batch True
DEBUG 10-15 15:28:08 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:08 lpllm.py:2294] CPU compute cost 0.134439 seconds
DEBUG 10-15 15:28:08 lpllm.py:2312] free cost 0.000071 seconds
DEBUG 10-15 15:28:08 lpllm.py:503] reset update experts
DEBUG 10-15 15:28:08 lpllm.py:1774] update state cost 2.1219253540039062e-05 s
DEBUG 10-15 15:28:08 lpllm.py:1743] restore layer func cost 0.0008382797241210938 s
DEBUG 10-15 15:28:08 lpllm.py:511] restore layer cost 0.0010688304901123047 s
DEBUG 10-15 15:28:08 lpllm.py:512] reset_next_layer_need: layer_attn_idx=19, layer_mlp_idx=19, j_loc=38
DEBUG 10-15 15:28:08 lpllm.py:1037] reset layer cost 0.0011637210845947266 s
DEBUG 10-15 15:28:08 lpllm.py:1038] have reset next layer layer_attn 19 layer_mlp 19 
DEBUG 10-15 15:28:08 lpllm.py:924] 
DEBUG 10-15 15:28:08 lpllm.py:924] decoder loop j: 39 cur_layer_idx: 19 layer_attn_idx: 19 layer_mlp_idx: 19
DEBUG 10-15 15:28:08 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:08 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:08 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:08 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:08 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:08 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:08 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:08 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:08 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:08 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:08 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:08 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:08 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:08 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:08 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:08 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:08 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:08 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:08 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:08 lpllm.py:2265] GPU2CPU move cost 0.000572 seconds
DEBUG 10-15 15:28:08 lpmodule.py:340] layer_idx 19 attn layer_idx: 19
DEBUG 10-15 15:28:08 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:08 lpmodule.py:364] decoder_attn_batch update batch_dim 660-720
DEBUG 10-15 15:28:08 StaticCacheLen.py:84] static cache update layer_idx: 19, start_batch: 660, end_batch: 720
DEBUG 10-15 15:28:08 lpmodule.py:368] update for kv cache 660-720 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:08 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:08 lpmodule.py:374] update past key value cost 0.022312 seconds
DEBUG 10-15 15:28:08 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:08 lpmodule.py:227] layer idx 19
DEBUG 10-15 15:28:08 lpmodule.py:399] repeat qkv cost 0.029377 seconds
DEBUG 10-15 15:28:08 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:08 lpmodule.py:433] dot attn cost 0.031638 seconds
DEBUG 10-15 15:28:08 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:08 lpmodule.py:444] time cost move to cuda:1 0.023383140563964844 s
DEBUG 10-15 15:28:08 lpllm.py:2283] CPU attn cost 0.129970 seconds if batch True
DEBUG 10-15 15:28:08 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:08 lpllm.py:2294] CPU compute cost 0.130836 seconds
DEBUG 10-15 15:28:08 lpllm.py:2312] free cost 0.000070 seconds
DEBUG 10-15 15:28:08 lpllm.py:498] reset update attn
DEBUG 10-15 15:28:08 lpllm.py:1774] update state cost 2.4557113647460938e-05 s
DEBUG 10-15 15:28:08 lpllm.py:1743] restore layer func cost 0.0004119873046875 s
DEBUG 10-15 15:28:08 lpllm.py:511] restore layer cost 0.0006673336029052734 s
DEBUG 10-15 15:28:08 lpllm.py:512] reset_next_layer_need: layer_attn_idx=20, layer_mlp_idx=19, j_loc=39
DEBUG 10-15 15:28:08 lpllm.py:1037] reset layer cost 0.0007395744323730469 s
DEBUG 10-15 15:28:08 lpllm.py:1038] have reset next layer layer_attn 20 layer_mlp 19 
DEBUG 10-15 15:28:08 lpllm.py:1044] j: 39 waiting the layer with layer_idx 20 before wait time 0.44290733337402344 s
INFO 10-15 15:28:08 client.py:117] confirm_model_loaded: Mixtral-8x7B, b491240d-72fa-4368-9753-f74c95f124f2
INFO 10-15 15:28:08 client.py:125] Model loaded
DEBUG 10-15 15:28:08 lpllm.py:1048] j: load cost 0.4443633556365967 s waiting cost 0.0014400482177734375 s
DEBUG 10-15 15:28:08 lpllm.py:924] 
DEBUG 10-15 15:28:08 lpllm.py:924] decoder loop j: 40 cur_layer_idx: 20 layer_attn_idx: 20 layer_mlp_idx: 19
DEBUG 10-15 15:28:08 lpllm.py:933] start load next layer cur_layer_idx: 21
DEBUG 10-15 15:28:08 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed650>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x003\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x96\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:28:08 client.py:72] load_into_gpu: Mixtral-8x7B, fa68e6df-a404-481e-a35c-990987b99a2d
INFO 10-15 15:28:08 client.py:113] Model loaded: Mixtral-8x7B, fa68e6df-a404-481e-a35c-990987b99a2d
DEBUG 10-15 15:28:08 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:08 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:08 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:08 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:08 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:08 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:08 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:08 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:08 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:08 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:08 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:08 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:08 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:08 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:08 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:08 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:08 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:08 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:08 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:08 lpllm.py:2265] GPU2CPU move cost 0.000610 seconds
DEBUG 10-15 15:28:08 lpmodule.py:340] layer_idx 20 attn layer_idx: 20
DEBUG 10-15 15:28:08 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:08 lpmodule.py:364] decoder_attn_batch update batch_dim 600-660
DEBUG 10-15 15:28:08 StaticCacheLen.py:84] static cache update layer_idx: 20, start_batch: 600, end_batch: 660
DEBUG 10-15 15:28:08 lpmodule.py:368] update for kv cache 600-660 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:08 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:08 lpmodule.py:374] update past key value cost 0.022604 seconds
DEBUG 10-15 15:28:08 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:08 lpmodule.py:227] layer idx 19
DEBUG 10-15 15:28:08 lpmodule.py:399] repeat qkv cost 0.029831 seconds
DEBUG 10-15 15:28:08 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:08 lpmodule.py:433] dot attn cost 0.031459 seconds
DEBUG 10-15 15:28:08 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:08 lpmodule.py:444] time cost move to cuda:1 0.02368903160095215 s
DEBUG 10-15 15:28:08 lpllm.py:2283] CPU attn cost 0.132132 seconds if batch True
DEBUG 10-15 15:28:08 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:28:08 lpllm.py:2294] CPU compute cost 0.133029 seconds
DEBUG 10-15 15:28:08 lpllm.py:2312] free cost 0.000071 seconds
DEBUG 10-15 15:28:08 lpllm.py:503] reset update experts
DEBUG 10-15 15:28:08 lpllm.py:1774] update state cost 2.5033950805664062e-05 s
DEBUG 10-15 15:28:08 lpllm.py:1743] restore layer func cost 0.0008416175842285156 s
DEBUG 10-15 15:28:08 lpllm.py:511] restore layer cost 0.0010857582092285156 s
DEBUG 10-15 15:28:08 lpllm.py:512] reset_next_layer_need: layer_attn_idx=20, layer_mlp_idx=20, j_loc=40
DEBUG 10-15 15:28:08 lpllm.py:1037] reset layer cost 0.0011782646179199219 s
DEBUG 10-15 15:28:08 lpllm.py:1038] have reset next layer layer_attn 20 layer_mlp 20 
DEBUG 10-15 15:28:08 lpllm.py:924] 
DEBUG 10-15 15:28:08 lpllm.py:924] decoder loop j: 41 cur_layer_idx: 20 layer_attn_idx: 20 layer_mlp_idx: 20
DEBUG 10-15 15:28:08 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:08 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:08 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:08 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:08 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:08 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:08 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:08 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:08 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:08 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:08 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:08 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:08 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:08 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:08 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:08 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:08 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:08 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:08 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:08 lpllm.py:2265] GPU2CPU move cost 0.000565 seconds
DEBUG 10-15 15:28:08 lpmodule.py:340] layer_idx 20 attn layer_idx: 20
DEBUG 10-15 15:28:08 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:08 lpmodule.py:364] decoder_attn_batch update batch_dim 660-720
DEBUG 10-15 15:28:08 StaticCacheLen.py:84] static cache update layer_idx: 20, start_batch: 660, end_batch: 720
DEBUG 10-15 15:28:08 lpmodule.py:368] update for kv cache 660-720 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:08 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:08 lpmodule.py:374] update past key value cost 0.020621 seconds
DEBUG 10-15 15:28:08 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:08 lpmodule.py:227] layer idx 20
DEBUG 10-15 15:28:08 lpmodule.py:399] repeat qkv cost 0.029211 seconds
DEBUG 10-15 15:28:08 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:08 lpmodule.py:433] dot attn cost 0.032573 seconds
DEBUG 10-15 15:28:08 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:08 lpmodule.py:444] time cost move to cuda:1 0.023164987564086914 s
DEBUG 10-15 15:28:08 lpllm.py:2283] CPU attn cost 0.129982 seconds if batch True
DEBUG 10-15 15:28:08 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:08 lpllm.py:2294] CPU compute cost 0.130819 seconds
DEBUG 10-15 15:28:08 lpllm.py:2312] free cost 0.000071 seconds
DEBUG 10-15 15:28:08 lpllm.py:498] reset update attn
DEBUG 10-15 15:28:08 lpllm.py:1774] update state cost 2.5272369384765625e-05 s
DEBUG 10-15 15:28:08 lpllm.py:1743] restore layer func cost 0.00040268898010253906 s
DEBUG 10-15 15:28:08 lpllm.py:511] restore layer cost 0.0006494522094726562 s
DEBUG 10-15 15:28:08 lpllm.py:512] reset_next_layer_need: layer_attn_idx=21, layer_mlp_idx=20, j_loc=41
DEBUG 10-15 15:28:08 lpllm.py:1037] reset layer cost 0.0007388591766357422 s
DEBUG 10-15 15:28:08 lpllm.py:1038] have reset next layer layer_attn 21 layer_mlp 20 
DEBUG 10-15 15:28:08 lpllm.py:1044] j: 41 waiting the layer with layer_idx 21 before wait time 0.4417459964752197 s
INFO 10-15 15:28:08 client.py:117] confirm_model_loaded: Mixtral-8x7B, fa68e6df-a404-481e-a35c-990987b99a2d
INFO 10-15 15:28:08 client.py:125] Model loaded
DEBUG 10-15 15:28:08 lpllm.py:1048] j: load cost 0.44344210624694824 s waiting cost 0.0016808509826660156 s
DEBUG 10-15 15:28:08 lpllm.py:924] 
DEBUG 10-15 15:28:08 lpllm.py:924] decoder loop j: 42 cur_layer_idx: 21 layer_attn_idx: 21 layer_mlp_idx: 20
DEBUG 10-15 15:28:08 lpllm.py:933] start load next layer cur_layer_idx: 22
DEBUG 10-15 15:28:08 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed6e0>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x004\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x98\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:28:08 client.py:72] load_into_gpu: Mixtral-8x7B, d11e73a9-d2fb-4e38-a65e-39958bd2a628
INFO 10-15 15:28:08 client.py:113] Model loaded: Mixtral-8x7B, d11e73a9-d2fb-4e38-a65e-39958bd2a628
DEBUG 10-15 15:28:08 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:08 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:08 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:08 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:08 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:08 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:08 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:08 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:08 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:08 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:08 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:08 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:08 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:08 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:08 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:08 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:08 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:08 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:08 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:08 lpllm.py:2265] GPU2CPU move cost 0.000572 seconds
DEBUG 10-15 15:28:08 lpmodule.py:340] layer_idx 21 attn layer_idx: 21
DEBUG 10-15 15:28:08 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:08 lpmodule.py:364] decoder_attn_batch update batch_dim 600-660
DEBUG 10-15 15:28:08 StaticCacheLen.py:84] static cache update layer_idx: 21, start_batch: 600, end_batch: 660
DEBUG 10-15 15:28:08 lpmodule.py:368] update for kv cache 600-660 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:08 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:08 lpmodule.py:374] update past key value cost 0.026671 seconds
DEBUG 10-15 15:28:08 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:08 lpmodule.py:227] layer idx 20
DEBUG 10-15 15:28:08 lpmodule.py:399] repeat qkv cost 0.028484 seconds
DEBUG 10-15 15:28:08 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:08 lpmodule.py:433] dot attn cost 0.032343 seconds
DEBUG 10-15 15:28:08 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:08 lpmodule.py:444] time cost move to cuda:1 0.023611783981323242 s
DEBUG 10-15 15:28:08 lpllm.py:2283] CPU attn cost 0.136190 seconds if batch True
DEBUG 10-15 15:28:08 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:08 lpllm.py:2294] CPU compute cost 0.137041 seconds
DEBUG 10-15 15:28:08 lpllm.py:2312] free cost 0.000069 seconds
DEBUG 10-15 15:28:08 lpllm.py:503] reset update experts
DEBUG 10-15 15:28:08 lpllm.py:1774] update state cost 2.3603439331054688e-05 s
DEBUG 10-15 15:28:09 lpllm.py:1743] restore layer func cost 0.0008335113525390625 s
DEBUG 10-15 15:28:09 lpllm.py:511] restore layer cost 0.0010805130004882812 s
DEBUG 10-15 15:28:09 lpllm.py:512] reset_next_layer_need: layer_attn_idx=21, layer_mlp_idx=21, j_loc=42
DEBUG 10-15 15:28:09 lpllm.py:1037] reset layer cost 0.0011703968048095703 s
DEBUG 10-15 15:28:09 lpllm.py:1038] have reset next layer layer_attn 21 layer_mlp 21 
DEBUG 10-15 15:28:09 lpllm.py:924] 
DEBUG 10-15 15:28:09 lpllm.py:924] decoder loop j: 43 cur_layer_idx: 21 layer_attn_idx: 21 layer_mlp_idx: 21
DEBUG 10-15 15:28:09 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:09 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:09 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:09 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:09 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:09 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:09 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:09 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:09 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:09 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:09 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:09 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:09 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:09 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:09 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:09 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:09 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:09 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:09 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:09 lpllm.py:2265] GPU2CPU move cost 0.000614 seconds
DEBUG 10-15 15:28:09 lpmodule.py:340] layer_idx 21 attn layer_idx: 21
DEBUG 10-15 15:28:09 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:09 lpmodule.py:364] decoder_attn_batch update batch_dim 660-720
DEBUG 10-15 15:28:09 StaticCacheLen.py:84] static cache update layer_idx: 21, start_batch: 660, end_batch: 720
DEBUG 10-15 15:28:09 lpmodule.py:368] update for kv cache 660-720 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:09 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:09 lpmodule.py:374] update past key value cost 0.023118 seconds
DEBUG 10-15 15:28:09 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:09 lpmodule.py:227] layer idx 21
DEBUG 10-15 15:28:09 lpmodule.py:399] repeat qkv cost 0.029585 seconds
DEBUG 10-15 15:28:09 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:09 lpmodule.py:433] dot attn cost 0.031742 seconds
DEBUG 10-15 15:28:09 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:09 lpmodule.py:444] time cost move to cuda:1 0.025773286819458008 s
DEBUG 10-15 15:28:09 lpllm.py:2283] CPU attn cost 0.134593 seconds if batch True
DEBUG 10-15 15:28:09 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:28:09 lpllm.py:2294] CPU compute cost 0.135476 seconds
DEBUG 10-15 15:28:09 lpllm.py:2312] free cost 0.000074 seconds
DEBUG 10-15 15:28:09 lpllm.py:498] reset update attn
DEBUG 10-15 15:28:09 lpllm.py:1774] update state cost 2.2649765014648438e-05 s
DEBUG 10-15 15:28:09 lpllm.py:1743] restore layer func cost 0.0004115104675292969 s
DEBUG 10-15 15:28:09 lpllm.py:511] restore layer cost 0.0006706714630126953 s
DEBUG 10-15 15:28:09 lpllm.py:512] reset_next_layer_need: layer_attn_idx=22, layer_mlp_idx=21, j_loc=43
DEBUG 10-15 15:28:09 lpllm.py:1037] reset layer cost 0.0007443428039550781 s
DEBUG 10-15 15:28:09 lpllm.py:1038] have reset next layer layer_attn 22 layer_mlp 21 
DEBUG 10-15 15:28:09 lpllm.py:1044] j: 43 waiting the layer with layer_idx 22 before wait time 0.44239139556884766 s
INFO 10-15 15:28:09 client.py:117] confirm_model_loaded: Mixtral-8x7B, d11e73a9-d2fb-4e38-a65e-39958bd2a628
INFO 10-15 15:28:09 client.py:125] Model loaded
DEBUG 10-15 15:28:09 lpllm.py:1048] j: load cost 0.444091796875 s waiting cost 0.0016837120056152344 s
DEBUG 10-15 15:28:09 lpllm.py:924] 
DEBUG 10-15 15:28:09 lpllm.py:924] decoder loop j: 44 cur_layer_idx: 22 layer_attn_idx: 22 layer_mlp_idx: 21
DEBUG 10-15 15:28:09 lpllm.py:933] start load next layer cur_layer_idx: 23
DEBUG 10-15 15:28:09 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:28:09 client.py:72] load_into_gpu: Mixtral-8x7B, b3e6e75b-85b4-4329-9e56-be83bc629f08
INFO 10-15 15:28:09 client.py:113] Model loaded: Mixtral-8x7B, b3e6e75b-85b4-4329-9e56-be83bc629f08
DEBUG 10-15 15:28:09 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:09 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:09 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:09 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:09 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:09 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:09 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:09 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:09 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:09 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:09 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:09 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:09 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:09 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:09 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:09 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:09 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:09 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:09 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:09 lpllm.py:2265] GPU2CPU move cost 0.000570 seconds
DEBUG 10-15 15:28:09 lpmodule.py:340] layer_idx 22 attn layer_idx: 22
DEBUG 10-15 15:28:09 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:09 lpmodule.py:364] decoder_attn_batch update batch_dim 600-660
DEBUG 10-15 15:28:09 StaticCacheLen.py:84] static cache update layer_idx: 22, start_batch: 600, end_batch: 660
DEBUG 10-15 15:28:09 lpmodule.py:368] update for kv cache 600-660 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:09 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:09 lpmodule.py:374] update past key value cost 0.026848 seconds
DEBUG 10-15 15:28:09 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:09 lpmodule.py:227] layer idx 21
DEBUG 10-15 15:28:09 lpmodule.py:399] repeat qkv cost 0.028569 seconds
DEBUG 10-15 15:28:09 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:09 lpmodule.py:433] dot attn cost 0.032383 seconds
DEBUG 10-15 15:28:09 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:09 lpmodule.py:444] time cost move to cuda:1 0.023001432418823242 s
DEBUG 10-15 15:28:09 lpllm.py:2283] CPU attn cost 0.134586 seconds if batch True
DEBUG 10-15 15:28:09 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:09 lpllm.py:2294] CPU compute cost 0.135442 seconds
DEBUG 10-15 15:28:09 lpllm.py:2312] free cost 0.000075 seconds
DEBUG 10-15 15:28:09 lpllm.py:503] reset update experts
DEBUG 10-15 15:28:09 lpllm.py:1774] update state cost 2.2172927856445312e-05 s
DEBUG 10-15 15:28:09 lpllm.py:1743] restore layer func cost 0.0008466243743896484 s
DEBUG 10-15 15:28:09 lpllm.py:511] restore layer cost 0.0011014938354492188 s
DEBUG 10-15 15:28:09 lpllm.py:512] reset_next_layer_need: layer_attn_idx=22, layer_mlp_idx=22, j_loc=44
DEBUG 10-15 15:28:09 lpllm.py:1037] reset layer cost 0.001176595687866211 s
DEBUG 10-15 15:28:09 lpllm.py:1038] have reset next layer layer_attn 22 layer_mlp 22 
DEBUG 10-15 15:28:09 lpllm.py:924] 
DEBUG 10-15 15:28:09 lpllm.py:924] decoder loop j: 45 cur_layer_idx: 22 layer_attn_idx: 22 layer_mlp_idx: 22
DEBUG 10-15 15:28:09 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:09 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:09 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:09 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:09 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:09 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:09 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:09 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:09 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:09 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:09 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:09 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:09 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:09 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:09 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:09 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:09 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:09 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:09 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:09 lpllm.py:2265] GPU2CPU move cost 0.000606 seconds
DEBUG 10-15 15:28:09 lpmodule.py:340] layer_idx 22 attn layer_idx: 22
DEBUG 10-15 15:28:09 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:09 lpmodule.py:364] decoder_attn_batch update batch_dim 660-720
DEBUG 10-15 15:28:09 StaticCacheLen.py:84] static cache update layer_idx: 22, start_batch: 660, end_batch: 720
DEBUG 10-15 15:28:09 lpmodule.py:368] update for kv cache 660-720 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:09 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:09 lpmodule.py:374] update past key value cost 0.023476 seconds
DEBUG 10-15 15:28:09 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:09 lpmodule.py:227] layer idx 22
DEBUG 10-15 15:28:09 lpmodule.py:399] repeat qkv cost 0.028603 seconds
DEBUG 10-15 15:28:09 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:09 lpmodule.py:433] dot attn cost 0.031861 seconds
DEBUG 10-15 15:28:09 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:09 lpmodule.py:444] time cost move to cuda:1 0.02358222007751465 s
DEBUG 10-15 15:28:09 lpllm.py:2283] CPU attn cost 0.131721 seconds if batch True
DEBUG 10-15 15:28:09 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:28:09 lpllm.py:2294] CPU compute cost 0.132600 seconds
DEBUG 10-15 15:28:09 lpllm.py:2312] free cost 0.000071 seconds
DEBUG 10-15 15:28:09 lpllm.py:498] reset update attn
DEBUG 10-15 15:28:09 lpllm.py:1774] update state cost 2.574920654296875e-05 s
DEBUG 10-15 15:28:09 lpllm.py:1743] restore layer func cost 0.00040912628173828125 s
DEBUG 10-15 15:28:09 lpllm.py:511] restore layer cost 0.0006539821624755859 s
DEBUG 10-15 15:28:09 lpllm.py:512] reset_next_layer_need: layer_attn_idx=23, layer_mlp_idx=22, j_loc=45
DEBUG 10-15 15:28:09 lpllm.py:1037] reset layer cost 0.0007266998291015625 s
DEBUG 10-15 15:28:09 lpllm.py:1038] have reset next layer layer_attn 23 layer_mlp 22 
DEBUG 10-15 15:28:09 lpllm.py:1044] j: 45 waiting the layer with layer_idx 23 before wait time 0.44811582565307617 s
INFO 10-15 15:28:09 client.py:117] confirm_model_loaded: Mixtral-8x7B, b3e6e75b-85b4-4329-9e56-be83bc629f08
INFO 10-15 15:28:09 client.py:125] Model loaded
DEBUG 10-15 15:28:09 lpllm.py:1048] j: load cost 0.4498329162597656 s waiting cost 0.00170135498046875 s
DEBUG 10-15 15:28:09 lpllm.py:924] 
DEBUG 10-15 15:28:09 lpllm.py:924] decoder loop j: 46 cur_layer_idx: 23 layer_attn_idx: 23 layer_mlp_idx: 22
DEBUG 10-15 15:28:09 lpllm.py:933] start load next layer cur_layer_idx: 24
DEBUG 10-15 15:28:09 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed650>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x003\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x96\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:28:09 client.py:72] load_into_gpu: Mixtral-8x7B, 9bc234d8-7c1a-41c8-8160-908a661cf5a6
INFO 10-15 15:28:09 client.py:113] Model loaded: Mixtral-8x7B, 9bc234d8-7c1a-41c8-8160-908a661cf5a6
DEBUG 10-15 15:28:09 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:09 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:09 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:09 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:09 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:09 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:09 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:09 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:09 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:09 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:09 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:09 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:09 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:09 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:09 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:09 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:09 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:09 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:09 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:09 lpllm.py:2265] GPU2CPU move cost 0.000586 seconds
DEBUG 10-15 15:28:09 lpmodule.py:340] layer_idx 23 attn layer_idx: 23
DEBUG 10-15 15:28:09 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:09 lpmodule.py:364] decoder_attn_batch update batch_dim 600-660
DEBUG 10-15 15:28:09 StaticCacheLen.py:84] static cache update layer_idx: 23, start_batch: 600, end_batch: 660
DEBUG 10-15 15:28:09 lpmodule.py:368] update for kv cache 600-660 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:09 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:09 lpmodule.py:374] update past key value cost 0.026050 seconds
DEBUG 10-15 15:28:09 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:09 lpmodule.py:227] layer idx 22
DEBUG 10-15 15:28:09 lpmodule.py:399] repeat qkv cost 0.029693 seconds
DEBUG 10-15 15:28:09 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:09 lpmodule.py:433] dot attn cost 0.030768 seconds
DEBUG 10-15 15:28:09 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:09 lpmodule.py:444] time cost move to cuda:1 0.023560285568237305 s
DEBUG 10-15 15:28:09 lpllm.py:2283] CPU attn cost 0.134142 seconds if batch True
DEBUG 10-15 15:28:09 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:28:09 lpllm.py:2294] CPU compute cost 0.135009 seconds
DEBUG 10-15 15:28:09 lpllm.py:2312] free cost 0.000073 seconds
DEBUG 10-15 15:28:09 lpllm.py:503] reset update experts
DEBUG 10-15 15:28:09 lpllm.py:1774] update state cost 2.09808349609375e-05 s
DEBUG 10-15 15:28:09 lpllm.py:1743] restore layer func cost 0.0008594989776611328 s
DEBUG 10-15 15:28:09 lpllm.py:511] restore layer cost 0.0011029243469238281 s
DEBUG 10-15 15:28:09 lpllm.py:512] reset_next_layer_need: layer_attn_idx=23, layer_mlp_idx=23, j_loc=46
DEBUG 10-15 15:28:09 lpllm.py:1037] reset layer cost 0.0011985301971435547 s
DEBUG 10-15 15:28:09 lpllm.py:1038] have reset next layer layer_attn 23 layer_mlp 23 
DEBUG 10-15 15:28:09 lpllm.py:924] 
DEBUG 10-15 15:28:09 lpllm.py:924] decoder loop j: 47 cur_layer_idx: 23 layer_attn_idx: 23 layer_mlp_idx: 23
DEBUG 10-15 15:28:09 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:09 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:09 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:09 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:09 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:09 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:09 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:09 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:09 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:09 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:09 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:09 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:09 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:09 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:09 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:09 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:09 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:09 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:09 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:09 lpllm.py:2265] GPU2CPU move cost 0.000532 seconds
DEBUG 10-15 15:28:09 lpmodule.py:340] layer_idx 23 attn layer_idx: 23
DEBUG 10-15 15:28:09 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:09 lpmodule.py:364] decoder_attn_batch update batch_dim 660-720
DEBUG 10-15 15:28:09 StaticCacheLen.py:84] static cache update layer_idx: 23, start_batch: 660, end_batch: 720
DEBUG 10-15 15:28:09 lpmodule.py:368] update for kv cache 660-720 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:09 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:09 lpmodule.py:374] update past key value cost 0.023560 seconds
DEBUG 10-15 15:28:09 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:09 lpmodule.py:227] layer idx 23
DEBUG 10-15 15:28:09 lpmodule.py:399] repeat qkv cost 0.027894 seconds
DEBUG 10-15 15:28:09 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:10 lpmodule.py:433] dot attn cost 0.031166 seconds
DEBUG 10-15 15:28:10 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:10 lpmodule.py:444] time cost move to cuda:1 0.02351856231689453 s
DEBUG 10-15 15:28:10 lpllm.py:2283] CPU attn cost 0.130416 seconds if batch True
DEBUG 10-15 15:28:10 lpllm.py:2292] deal attn result cost 0.000017 seconds
DEBUG 10-15 15:28:10 lpllm.py:2294] CPU compute cost 0.131265 seconds
DEBUG 10-15 15:28:10 lpllm.py:2312] free cost 0.000085 seconds
DEBUG 10-15 15:28:10 lpllm.py:498] reset update attn
DEBUG 10-15 15:28:10 lpllm.py:1774] update state cost 2.5510787963867188e-05 s
DEBUG 10-15 15:28:10 lpllm.py:1743] restore layer func cost 0.0004100799560546875 s
DEBUG 10-15 15:28:10 lpllm.py:511] restore layer cost 0.0006771087646484375 s
DEBUG 10-15 15:28:10 lpllm.py:512] reset_next_layer_need: layer_attn_idx=24, layer_mlp_idx=23, j_loc=47
DEBUG 10-15 15:28:10 lpllm.py:1037] reset layer cost 0.0007498264312744141 s
DEBUG 10-15 15:28:10 lpllm.py:1038] have reset next layer layer_attn 24 layer_mlp 23 
DEBUG 10-15 15:28:10 lpllm.py:1044] j: 47 waiting the layer with layer_idx 24 before wait time 0.44518589973449707 s
INFO 10-15 15:28:10 client.py:117] confirm_model_loaded: Mixtral-8x7B, 9bc234d8-7c1a-41c8-8160-908a661cf5a6
INFO 10-15 15:28:10 client.py:125] Model loaded
DEBUG 10-15 15:28:10 lpllm.py:1048] j: load cost 0.4468245506286621 s waiting cost 0.001622915267944336 s
DEBUG 10-15 15:28:10 lpllm.py:924] 
DEBUG 10-15 15:28:10 lpllm.py:924] decoder loop j: 48 cur_layer_idx: 24 layer_attn_idx: 24 layer_mlp_idx: 23
DEBUG 10-15 15:28:10 lpllm.py:933] start load next layer cur_layer_idx: 25
DEBUG 10-15 15:28:10 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed6e0>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x004\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x98\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:28:10 client.py:72] load_into_gpu: Mixtral-8x7B, a4cbd282-48a0-43a5-8ed6-814eb25673b3
INFO 10-15 15:28:10 client.py:113] Model loaded: Mixtral-8x7B, a4cbd282-48a0-43a5-8ed6-814eb25673b3
DEBUG 10-15 15:28:10 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:10 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:10 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:10 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:10 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:10 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:10 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:10 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:10 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:10 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:10 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:10 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:10 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:10 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:10 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:10 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:10 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:10 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:10 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:10 lpllm.py:2265] GPU2CPU move cost 0.000617 seconds
DEBUG 10-15 15:28:10 lpmodule.py:340] layer_idx 24 attn layer_idx: 24
DEBUG 10-15 15:28:10 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:10 lpmodule.py:364] decoder_attn_batch update batch_dim 600-660
DEBUG 10-15 15:28:10 StaticCacheLen.py:84] static cache update layer_idx: 24, start_batch: 600, end_batch: 660
DEBUG 10-15 15:28:10 lpmodule.py:368] update for kv cache 600-660 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:10 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:10 lpmodule.py:374] update past key value cost 0.026318 seconds
DEBUG 10-15 15:28:10 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:10 lpmodule.py:227] layer idx 23
DEBUG 10-15 15:28:10 lpmodule.py:399] repeat qkv cost 0.028721 seconds
DEBUG 10-15 15:28:10 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:10 lpmodule.py:433] dot attn cost 0.032206 seconds
DEBUG 10-15 15:28:10 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:10 lpmodule.py:444] time cost move to cuda:1 0.023412227630615234 s
DEBUG 10-15 15:28:10 lpllm.py:2283] CPU attn cost 0.136833 seconds if batch True
DEBUG 10-15 15:28:10 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:10 lpllm.py:2294] CPU compute cost 0.137746 seconds
DEBUG 10-15 15:28:10 lpllm.py:2312] free cost 0.000073 seconds
DEBUG 10-15 15:28:10 lpllm.py:503] reset update experts
DEBUG 10-15 15:28:10 lpllm.py:1774] update state cost 2.3126602172851562e-05 s
DEBUG 10-15 15:28:10 lpllm.py:1743] restore layer func cost 0.0008380413055419922 s
DEBUG 10-15 15:28:10 lpllm.py:511] restore layer cost 0.0010721683502197266 s
DEBUG 10-15 15:28:10 lpllm.py:512] reset_next_layer_need: layer_attn_idx=24, layer_mlp_idx=24, j_loc=48
DEBUG 10-15 15:28:10 lpllm.py:1037] reset layer cost 0.0011548995971679688 s
DEBUG 10-15 15:28:10 lpllm.py:1038] have reset next layer layer_attn 24 layer_mlp 24 
DEBUG 10-15 15:28:10 lpllm.py:924] 
DEBUG 10-15 15:28:10 lpllm.py:924] decoder loop j: 49 cur_layer_idx: 24 layer_attn_idx: 24 layer_mlp_idx: 24
DEBUG 10-15 15:28:10 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:10 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:10 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:10 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:10 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:10 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:10 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:10 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:10 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:10 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:10 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:10 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:10 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:10 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:10 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:10 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:10 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:10 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:10 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:10 lpllm.py:2265] GPU2CPU move cost 0.000572 seconds
DEBUG 10-15 15:28:10 lpmodule.py:340] layer_idx 24 attn layer_idx: 24
DEBUG 10-15 15:28:10 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:10 lpmodule.py:364] decoder_attn_batch update batch_dim 660-720
DEBUG 10-15 15:28:10 StaticCacheLen.py:84] static cache update layer_idx: 24, start_batch: 660, end_batch: 720
DEBUG 10-15 15:28:10 lpmodule.py:368] update for kv cache 660-720 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:10 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:10 lpmodule.py:374] update past key value cost 0.022931 seconds
DEBUG 10-15 15:28:10 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:10 lpmodule.py:227] layer idx 24
DEBUG 10-15 15:28:10 lpmodule.py:399] repeat qkv cost 0.028317 seconds
DEBUG 10-15 15:28:10 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:10 lpmodule.py:433] dot attn cost 0.031616 seconds
DEBUG 10-15 15:28:10 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:10 lpmodule.py:444] time cost move to cuda:1 0.022965192794799805 s
DEBUG 10-15 15:28:10 lpllm.py:2283] CPU attn cost 0.131480 seconds if batch True
DEBUG 10-15 15:28:10 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:10 lpllm.py:2294] CPU compute cost 0.132329 seconds
DEBUG 10-15 15:28:10 lpllm.py:2312] free cost 0.000070 seconds
DEBUG 10-15 15:28:10 lpllm.py:498] reset update attn
DEBUG 10-15 15:28:10 lpllm.py:1774] update state cost 2.4557113647460938e-05 s
DEBUG 10-15 15:28:10 lpllm.py:1743] restore layer func cost 0.00041294097900390625 s
DEBUG 10-15 15:28:10 lpllm.py:511] restore layer cost 0.0006415843963623047 s
DEBUG 10-15 15:28:10 lpllm.py:512] reset_next_layer_need: layer_attn_idx=25, layer_mlp_idx=24, j_loc=49
DEBUG 10-15 15:28:10 lpllm.py:1037] reset layer cost 0.0007314682006835938 s
DEBUG 10-15 15:28:10 lpllm.py:1038] have reset next layer layer_attn 25 layer_mlp 24 
DEBUG 10-15 15:28:10 lpllm.py:1044] j: 49 waiting the layer with layer_idx 25 before wait time 0.446441650390625 s
INFO 10-15 15:28:10 client.py:117] confirm_model_loaded: Mixtral-8x7B, a4cbd282-48a0-43a5-8ed6-814eb25673b3
INFO 10-15 15:28:10 client.py:125] Model loaded
DEBUG 10-15 15:28:10 lpllm.py:1048] j: load cost 0.44808053970336914 s waiting cost 0.001622915267944336 s
DEBUG 10-15 15:28:10 lpllm.py:924] 
DEBUG 10-15 15:28:10 lpllm.py:924] decoder loop j: 50 cur_layer_idx: 25 layer_attn_idx: 25 layer_mlp_idx: 24
DEBUG 10-15 15:28:10 lpllm.py:933] start load next layer cur_layer_idx: 26
DEBUG 10-15 15:28:10 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:28:10 client.py:72] load_into_gpu: Mixtral-8x7B, 91dc1972-8eb5-4a05-936b-6f581c33ba53
INFO 10-15 15:28:10 client.py:113] Model loaded: Mixtral-8x7B, 91dc1972-8eb5-4a05-936b-6f581c33ba53
DEBUG 10-15 15:28:10 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:10 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:10 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:10 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:10 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:10 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:10 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:10 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:10 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:10 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:10 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:10 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:10 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:10 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:10 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:10 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:10 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:10 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:10 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:10 lpllm.py:2265] GPU2CPU move cost 0.000362 seconds
DEBUG 10-15 15:28:10 lpmodule.py:340] layer_idx 25 attn layer_idx: 25
DEBUG 10-15 15:28:10 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:10 lpmodule.py:364] decoder_attn_batch update batch_dim 600-660
DEBUG 10-15 15:28:10 StaticCacheLen.py:84] static cache update layer_idx: 25, start_batch: 600, end_batch: 660
DEBUG 10-15 15:28:10 lpmodule.py:368] update for kv cache 600-660 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:10 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:10 lpmodule.py:374] update past key value cost 0.023697 seconds
DEBUG 10-15 15:28:10 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:10 lpmodule.py:227] layer idx 24
DEBUG 10-15 15:28:10 lpmodule.py:399] repeat qkv cost 0.028970 seconds
DEBUG 10-15 15:28:10 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:10 lpmodule.py:433] dot attn cost 0.031664 seconds
DEBUG 10-15 15:28:10 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:10 lpmodule.py:444] time cost move to cuda:1 0.023544788360595703 s
DEBUG 10-15 15:28:10 lpllm.py:2283] CPU attn cost 0.131948 seconds if batch True
DEBUG 10-15 15:28:10 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:10 lpllm.py:2294] CPU compute cost 0.132543 seconds
DEBUG 10-15 15:28:10 lpllm.py:2312] free cost 0.000072 seconds
DEBUG 10-15 15:28:10 lpllm.py:503] reset update experts
DEBUG 10-15 15:28:10 lpllm.py:1774] update state cost 2.1219253540039062e-05 s
DEBUG 10-15 15:28:10 lpllm.py:1743] restore layer func cost 0.0008647441864013672 s
DEBUG 10-15 15:28:10 lpllm.py:511] restore layer cost 0.001138448715209961 s
DEBUG 10-15 15:28:10 lpllm.py:512] reset_next_layer_need: layer_attn_idx=25, layer_mlp_idx=25, j_loc=50
DEBUG 10-15 15:28:10 lpllm.py:1037] reset layer cost 0.0012145042419433594 s
DEBUG 10-15 15:28:10 lpllm.py:1038] have reset next layer layer_attn 25 layer_mlp 25 
DEBUG 10-15 15:28:10 lpllm.py:924] 
DEBUG 10-15 15:28:10 lpllm.py:924] decoder loop j: 51 cur_layer_idx: 25 layer_attn_idx: 25 layer_mlp_idx: 25
DEBUG 10-15 15:28:10 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:10 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:10 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:10 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:10 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:10 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:10 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:10 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:10 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:10 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:10 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:10 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:10 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:10 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:10 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:10 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:10 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:10 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:10 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:10 lpllm.py:2265] GPU2CPU move cost 0.000623 seconds
DEBUG 10-15 15:28:10 lpmodule.py:340] layer_idx 25 attn layer_idx: 25
DEBUG 10-15 15:28:10 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:10 lpmodule.py:364] decoder_attn_batch update batch_dim 660-720
DEBUG 10-15 15:28:10 StaticCacheLen.py:84] static cache update layer_idx: 25, start_batch: 660, end_batch: 720
DEBUG 10-15 15:28:10 lpmodule.py:368] update for kv cache 660-720 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:10 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:10 lpmodule.py:374] update past key value cost 0.023310 seconds
DEBUG 10-15 15:28:10 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:10 lpmodule.py:227] layer idx 25
DEBUG 10-15 15:28:10 lpmodule.py:399] repeat qkv cost 0.028428 seconds
DEBUG 10-15 15:28:10 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:10 lpmodule.py:433] dot attn cost 0.032289 seconds
DEBUG 10-15 15:28:10 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:10 lpmodule.py:444] time cost move to cuda:1 0.02343153953552246 s
DEBUG 10-15 15:28:10 lpllm.py:2283] CPU attn cost 0.131873 seconds if batch True
DEBUG 10-15 15:28:10 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:10 lpllm.py:2294] CPU compute cost 0.132811 seconds
DEBUG 10-15 15:28:10 lpllm.py:2312] free cost 0.000075 seconds
DEBUG 10-15 15:28:11 lpllm.py:498] reset update attn
DEBUG 10-15 15:28:11 lpllm.py:1774] update state cost 2.288818359375e-05 s
DEBUG 10-15 15:28:11 lpllm.py:1743] restore layer func cost 0.00039958953857421875 s
DEBUG 10-15 15:28:11 lpllm.py:511] restore layer cost 0.0006494522094726562 s
DEBUG 10-15 15:28:11 lpllm.py:512] reset_next_layer_need: layer_attn_idx=26, layer_mlp_idx=25, j_loc=51
DEBUG 10-15 15:28:11 lpllm.py:1037] reset layer cost 0.0007212162017822266 s
DEBUG 10-15 15:28:11 lpllm.py:1038] have reset next layer layer_attn 26 layer_mlp 25 
DEBUG 10-15 15:28:11 lpllm.py:1044] j: 51 waiting the layer with layer_idx 26 before wait time 0.45053720474243164 s
INFO 10-15 15:28:11 client.py:117] confirm_model_loaded: Mixtral-8x7B, 91dc1972-8eb5-4a05-936b-6f581c33ba53
INFO 10-15 15:28:11 client.py:125] Model loaded
DEBUG 10-15 15:28:11 lpllm.py:1048] j: load cost 0.45218491554260254 s waiting cost 0.0016319751739501953 s
DEBUG 10-15 15:28:11 lpllm.py:924] 
DEBUG 10-15 15:28:11 lpllm.py:924] decoder loop j: 52 cur_layer_idx: 26 layer_attn_idx: 26 layer_mlp_idx: 25
DEBUG 10-15 15:28:11 lpllm.py:933] start load next layer cur_layer_idx: 27
DEBUG 10-15 15:28:11 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed650>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x003\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x96\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:28:11 client.py:72] load_into_gpu: Mixtral-8x7B, 0ebb85df-af4d-49a1-a413-0bd771c5e090
INFO 10-15 15:28:11 client.py:113] Model loaded: Mixtral-8x7B, 0ebb85df-af4d-49a1-a413-0bd771c5e090
DEBUG 10-15 15:28:11 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:11 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:11 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:11 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:11 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:11 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:11 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:11 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:11 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:11 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:11 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:11 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:11 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:11 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:11 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:11 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:11 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:11 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:11 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:11 lpllm.py:2265] GPU2CPU move cost 0.000344 seconds
DEBUG 10-15 15:28:11 lpmodule.py:340] layer_idx 26 attn layer_idx: 26
DEBUG 10-15 15:28:11 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:11 lpmodule.py:364] decoder_attn_batch update batch_dim 600-660
DEBUG 10-15 15:28:11 StaticCacheLen.py:84] static cache update layer_idx: 26, start_batch: 600, end_batch: 660
DEBUG 10-15 15:28:11 lpmodule.py:368] update for kv cache 600-660 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:11 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:11 lpmodule.py:374] update past key value cost 0.025366 seconds
DEBUG 10-15 15:28:11 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:11 lpmodule.py:227] layer idx 25
DEBUG 10-15 15:28:11 lpmodule.py:399] repeat qkv cost 0.028900 seconds
DEBUG 10-15 15:28:11 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:11 lpmodule.py:433] dot attn cost 0.032192 seconds
DEBUG 10-15 15:28:11 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:11 lpmodule.py:444] time cost move to cuda:1 0.022826433181762695 s
DEBUG 10-15 15:28:11 lpllm.py:2283] CPU attn cost 0.133320 seconds if batch True
DEBUG 10-15 15:28:11 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:11 lpllm.py:2294] CPU compute cost 0.133888 seconds
DEBUG 10-15 15:28:11 lpllm.py:2312] free cost 0.000077 seconds
DEBUG 10-15 15:28:11 lpllm.py:503] reset update experts
DEBUG 10-15 15:28:11 lpllm.py:1774] update state cost 2.288818359375e-05 s
DEBUG 10-15 15:28:11 lpllm.py:1743] restore layer func cost 0.0008389949798583984 s
DEBUG 10-15 15:28:11 lpllm.py:511] restore layer cost 0.0010747909545898438 s
DEBUG 10-15 15:28:11 lpllm.py:512] reset_next_layer_need: layer_attn_idx=26, layer_mlp_idx=26, j_loc=52
DEBUG 10-15 15:28:11 lpllm.py:1037] reset layer cost 0.0011682510375976562 s
DEBUG 10-15 15:28:11 lpllm.py:1038] have reset next layer layer_attn 26 layer_mlp 26 
DEBUG 10-15 15:28:11 lpllm.py:924] 
DEBUG 10-15 15:28:11 lpllm.py:924] decoder loop j: 53 cur_layer_idx: 26 layer_attn_idx: 26 layer_mlp_idx: 26
DEBUG 10-15 15:28:11 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:11 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:11 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:11 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:11 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:11 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:11 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:11 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:11 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:11 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:11 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:11 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:11 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:11 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:11 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:11 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:11 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:11 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:11 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:11 lpllm.py:2265] GPU2CPU move cost 0.000519 seconds
DEBUG 10-15 15:28:11 lpmodule.py:340] layer_idx 26 attn layer_idx: 26
DEBUG 10-15 15:28:11 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:11 lpmodule.py:364] decoder_attn_batch update batch_dim 660-720
DEBUG 10-15 15:28:11 StaticCacheLen.py:84] static cache update layer_idx: 26, start_batch: 660, end_batch: 720
DEBUG 10-15 15:28:11 lpmodule.py:368] update for kv cache 660-720 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:11 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:11 lpmodule.py:374] update past key value cost 0.022688 seconds
DEBUG 10-15 15:28:11 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:11 lpmodule.py:227] layer idx 26
DEBUG 10-15 15:28:11 lpmodule.py:399] repeat qkv cost 0.028008 seconds
DEBUG 10-15 15:28:11 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:11 lpmodule.py:433] dot attn cost 0.033206 seconds
DEBUG 10-15 15:28:11 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:11 lpmodule.py:444] time cost move to cuda:1 0.0231325626373291 s
DEBUG 10-15 15:28:11 lpllm.py:2283] CPU attn cost 0.132817 seconds if batch True
DEBUG 10-15 15:28:11 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:11 lpllm.py:2294] CPU compute cost 0.133627 seconds
DEBUG 10-15 15:28:11 lpllm.py:2312] free cost 0.000074 seconds
DEBUG 10-15 15:28:11 lpllm.py:498] reset update attn
DEBUG 10-15 15:28:11 lpllm.py:1774] update state cost 2.47955322265625e-05 s
DEBUG 10-15 15:28:11 lpllm.py:1743] restore layer func cost 0.0004010200500488281 s
DEBUG 10-15 15:28:11 lpllm.py:511] restore layer cost 0.0006291866302490234 s
DEBUG 10-15 15:28:11 lpllm.py:512] reset_next_layer_need: layer_attn_idx=27, layer_mlp_idx=26, j_loc=53
DEBUG 10-15 15:28:11 lpllm.py:1037] reset layer cost 0.0007178783416748047 s
DEBUG 10-15 15:28:11 lpllm.py:1038] have reset next layer layer_attn 27 layer_mlp 26 
DEBUG 10-15 15:28:11 lpllm.py:1044] j: 53 waiting the layer with layer_idx 27 before wait time 0.4442594051361084 s
INFO 10-15 15:28:11 client.py:117] confirm_model_loaded: Mixtral-8x7B, 0ebb85df-af4d-49a1-a413-0bd771c5e090
INFO 10-15 15:28:11 client.py:125] Model loaded
DEBUG 10-15 15:28:11 lpllm.py:1048] j: load cost 0.4458019733428955 s waiting cost 0.0015270709991455078 s
DEBUG 10-15 15:28:11 lpllm.py:924] 
DEBUG 10-15 15:28:11 lpllm.py:924] decoder loop j: 54 cur_layer_idx: 27 layer_attn_idx: 27 layer_mlp_idx: 26
DEBUG 10-15 15:28:11 lpllm.py:933] start load next layer cur_layer_idx: 28
DEBUG 10-15 15:28:11 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed6e0>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x004\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x98\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:28:11 client.py:72] load_into_gpu: Mixtral-8x7B, bc829779-b996-4945-98f8-932cf5444e46
INFO 10-15 15:28:11 client.py:113] Model loaded: Mixtral-8x7B, bc829779-b996-4945-98f8-932cf5444e46
DEBUG 10-15 15:28:11 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:11 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:11 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:11 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:11 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:11 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:11 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:11 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:11 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:11 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:11 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:11 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:11 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:11 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:11 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:11 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:11 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:11 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:11 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:11 lpllm.py:2265] GPU2CPU move cost 0.000364 seconds
DEBUG 10-15 15:28:11 lpmodule.py:340] layer_idx 27 attn layer_idx: 27
DEBUG 10-15 15:28:11 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:11 lpmodule.py:364] decoder_attn_batch update batch_dim 600-660
DEBUG 10-15 15:28:11 StaticCacheLen.py:84] static cache update layer_idx: 27, start_batch: 600, end_batch: 660
DEBUG 10-15 15:28:11 lpmodule.py:368] update for kv cache 600-660 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:11 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:11 lpmodule.py:374] update past key value cost 0.025158 seconds
DEBUG 10-15 15:28:11 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:11 lpmodule.py:227] layer idx 26
DEBUG 10-15 15:28:11 lpmodule.py:399] repeat qkv cost 0.028811 seconds
DEBUG 10-15 15:28:11 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:11 lpmodule.py:433] dot attn cost 0.032268 seconds
DEBUG 10-15 15:28:11 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:11 lpmodule.py:444] time cost move to cuda:1 0.023602962493896484 s
DEBUG 10-15 15:28:11 lpllm.py:2283] CPU attn cost 0.134229 seconds if batch True
DEBUG 10-15 15:28:11 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:11 lpllm.py:2294] CPU compute cost 0.134812 seconds
DEBUG 10-15 15:28:11 lpllm.py:2312] free cost 0.000073 seconds
DEBUG 10-15 15:28:11 lpllm.py:503] reset update experts
DEBUG 10-15 15:28:11 lpllm.py:1774] update state cost 2.2649765014648438e-05 s
DEBUG 10-15 15:28:11 lpllm.py:1743] restore layer func cost 0.0008454322814941406 s
DEBUG 10-15 15:28:11 lpllm.py:511] restore layer cost 0.0011131763458251953 s
DEBUG 10-15 15:28:11 lpllm.py:512] reset_next_layer_need: layer_attn_idx=27, layer_mlp_idx=27, j_loc=54
DEBUG 10-15 15:28:11 lpllm.py:1037] reset layer cost 0.0011887550354003906 s
DEBUG 10-15 15:28:11 lpllm.py:1038] have reset next layer layer_attn 27 layer_mlp 27 
DEBUG 10-15 15:28:11 lpllm.py:924] 
DEBUG 10-15 15:28:11 lpllm.py:924] decoder loop j: 55 cur_layer_idx: 27 layer_attn_idx: 27 layer_mlp_idx: 27
DEBUG 10-15 15:28:11 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:11 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:11 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:11 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:11 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:11 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:11 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:11 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:11 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:11 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:11 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:11 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:11 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:11 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:11 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:11 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:11 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:11 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:11 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:11 lpllm.py:2265] GPU2CPU move cost 0.000581 seconds
DEBUG 10-15 15:28:11 lpmodule.py:340] layer_idx 27 attn layer_idx: 27
DEBUG 10-15 15:28:11 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:11 lpmodule.py:364] decoder_attn_batch update batch_dim 660-720
DEBUG 10-15 15:28:11 StaticCacheLen.py:84] static cache update layer_idx: 27, start_batch: 660, end_batch: 720
DEBUG 10-15 15:28:11 lpmodule.py:368] update for kv cache 660-720 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:11 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:11 lpmodule.py:374] update past key value cost 0.023001 seconds
DEBUG 10-15 15:28:11 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:11 lpmodule.py:227] layer idx 27
DEBUG 10-15 15:28:11 lpmodule.py:399] repeat qkv cost 0.031299 seconds
DEBUG 10-15 15:28:11 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:11 lpmodule.py:433] dot attn cost 0.032025 seconds
DEBUG 10-15 15:28:11 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:11 lpmodule.py:444] time cost move to cuda:1 0.02301788330078125 s
DEBUG 10-15 15:28:11 lpllm.py:2283] CPU attn cost 0.133845 seconds if batch True
DEBUG 10-15 15:28:11 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:28:11 lpllm.py:2294] CPU compute cost 0.134740 seconds
DEBUG 10-15 15:28:11 lpllm.py:2312] free cost 0.000077 seconds
DEBUG 10-15 15:28:11 lpllm.py:498] reset update attn
DEBUG 10-15 15:28:11 lpllm.py:1774] update state cost 2.4557113647460938e-05 s
DEBUG 10-15 15:28:11 lpllm.py:1743] restore layer func cost 0.00040984153747558594 s
DEBUG 10-15 15:28:11 lpllm.py:511] restore layer cost 0.0006608963012695312 s
DEBUG 10-15 15:28:11 lpllm.py:512] reset_next_layer_need: layer_attn_idx=28, layer_mlp_idx=27, j_loc=55
DEBUG 10-15 15:28:11 lpllm.py:1037] reset layer cost 0.0007319450378417969 s
DEBUG 10-15 15:28:11 lpllm.py:1038] have reset next layer layer_attn 28 layer_mlp 27 
DEBUG 10-15 15:28:11 lpllm.py:1044] j: 55 waiting the layer with layer_idx 28 before wait time 0.444171667098999 s
INFO 10-15 15:28:11 client.py:117] confirm_model_loaded: Mixtral-8x7B, bc829779-b996-4945-98f8-932cf5444e46
INFO 10-15 15:28:11 client.py:125] Model loaded
DEBUG 10-15 15:28:11 lpllm.py:1048] j: load cost 0.4458310604095459 s waiting cost 0.0016431808471679688 s
DEBUG 10-15 15:28:11 lpllm.py:924] 
DEBUG 10-15 15:28:11 lpllm.py:924] decoder loop j: 56 cur_layer_idx: 28 layer_attn_idx: 28 layer_mlp_idx: 27
DEBUG 10-15 15:28:11 lpllm.py:933] start load next layer cur_layer_idx: 29
DEBUG 10-15 15:28:11 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:28:11 client.py:72] load_into_gpu: Mixtral-8x7B, 932d58c7-0d8c-405a-adce-e1f8efa6aa43
INFO 10-15 15:28:11 client.py:113] Model loaded: Mixtral-8x7B, 932d58c7-0d8c-405a-adce-e1f8efa6aa43
DEBUG 10-15 15:28:11 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:11 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:11 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:11 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:11 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:11 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:11 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:11 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:11 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:11 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:11 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:11 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:11 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:11 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:11 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:11 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:11 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:11 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:11 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:11 lpllm.py:2265] GPU2CPU move cost 0.000615 seconds
DEBUG 10-15 15:28:11 lpmodule.py:340] layer_idx 28 attn layer_idx: 28
DEBUG 10-15 15:28:11 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:11 lpmodule.py:364] decoder_attn_batch update batch_dim 600-660
DEBUG 10-15 15:28:11 StaticCacheLen.py:84] static cache update layer_idx: 28, start_batch: 600, end_batch: 660
DEBUG 10-15 15:28:11 lpmodule.py:368] update for kv cache 600-660 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:11 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:11 lpmodule.py:374] update past key value cost 0.025571 seconds
DEBUG 10-15 15:28:11 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:11 lpmodule.py:227] layer idx 27
DEBUG 10-15 15:28:11 lpmodule.py:399] repeat qkv cost 0.029065 seconds
DEBUG 10-15 15:28:11 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:12 lpmodule.py:433] dot attn cost 0.032556 seconds
DEBUG 10-15 15:28:12 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:12 lpmodule.py:444] time cost move to cuda:1 0.022935867309570312 s
DEBUG 10-15 15:28:12 lpllm.py:2283] CPU attn cost 0.134619 seconds if batch True
DEBUG 10-15 15:28:12 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:28:12 lpllm.py:2294] CPU compute cost 0.135532 seconds
DEBUG 10-15 15:28:12 lpllm.py:2312] free cost 0.000077 seconds
DEBUG 10-15 15:28:12 lpllm.py:503] reset update experts
DEBUG 10-15 15:28:12 lpllm.py:1774] update state cost 3.790855407714844e-05 s
DEBUG 10-15 15:28:12 lpllm.py:1743] restore layer func cost 0.0008502006530761719 s
DEBUG 10-15 15:28:12 lpllm.py:511] restore layer cost 0.001096963882446289 s
DEBUG 10-15 15:28:12 lpllm.py:512] reset_next_layer_need: layer_attn_idx=28, layer_mlp_idx=28, j_loc=56
DEBUG 10-15 15:28:12 lpllm.py:1037] reset layer cost 0.0011749267578125 s
DEBUG 10-15 15:28:12 lpllm.py:1038] have reset next layer layer_attn 28 layer_mlp 28 
DEBUG 10-15 15:28:12 lpllm.py:924] 
DEBUG 10-15 15:28:12 lpllm.py:924] decoder loop j: 57 cur_layer_idx: 28 layer_attn_idx: 28 layer_mlp_idx: 28
DEBUG 10-15 15:28:12 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:12 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:12 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:12 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:12 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:12 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:12 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:12 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:12 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:12 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:12 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:12 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:12 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:12 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:12 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:12 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:12 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:12 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:12 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:12 lpllm.py:2265] GPU2CPU move cost 0.000578 seconds
DEBUG 10-15 15:28:12 lpmodule.py:340] layer_idx 28 attn layer_idx: 28
DEBUG 10-15 15:28:12 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:12 lpmodule.py:364] decoder_attn_batch update batch_dim 660-720
DEBUG 10-15 15:28:12 StaticCacheLen.py:84] static cache update layer_idx: 28, start_batch: 660, end_batch: 720
DEBUG 10-15 15:28:12 lpmodule.py:368] update for kv cache 660-720 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:12 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:12 lpmodule.py:374] update past key value cost 0.021777 seconds
DEBUG 10-15 15:28:12 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:12 lpmodule.py:227] layer idx 28
DEBUG 10-15 15:28:12 lpmodule.py:399] repeat qkv cost 0.029337 seconds
DEBUG 10-15 15:28:12 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:12 lpmodule.py:433] dot attn cost 0.032372 seconds
DEBUG 10-15 15:28:12 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:12 lpmodule.py:444] time cost move to cuda:1 0.023145437240600586 s
DEBUG 10-15 15:28:12 lpllm.py:2283] CPU attn cost 0.131030 seconds if batch True
DEBUG 10-15 15:28:12 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:12 lpllm.py:2294] CPU compute cost 0.131893 seconds
DEBUG 10-15 15:28:12 lpllm.py:2312] free cost 0.000073 seconds
DEBUG 10-15 15:28:12 lpllm.py:498] reset update attn
DEBUG 10-15 15:28:12 lpllm.py:1774] update state cost 4.00543212890625e-05 s
DEBUG 10-15 15:28:12 lpllm.py:1743] restore layer func cost 0.0004124641418457031 s
DEBUG 10-15 15:28:12 lpllm.py:511] restore layer cost 0.0006606578826904297 s
DEBUG 10-15 15:28:12 lpllm.py:512] reset_next_layer_need: layer_attn_idx=29, layer_mlp_idx=28, j_loc=57
DEBUG 10-15 15:28:12 lpllm.py:1037] reset layer cost 0.0007345676422119141 s
DEBUG 10-15 15:28:12 lpllm.py:1038] have reset next layer layer_attn 29 layer_mlp 28 
DEBUG 10-15 15:28:12 lpllm.py:1044] j: 57 waiting the layer with layer_idx 29 before wait time 0.4510800838470459 s
INFO 10-15 15:28:12 client.py:117] confirm_model_loaded: Mixtral-8x7B, 932d58c7-0d8c-405a-adce-e1f8efa6aa43
INFO 10-15 15:28:12 client.py:125] Model loaded
DEBUG 10-15 15:28:12 lpllm.py:1048] j: load cost 0.4527597427368164 s waiting cost 0.0016644001007080078 s
DEBUG 10-15 15:28:12 lpllm.py:924] 
DEBUG 10-15 15:28:12 lpllm.py:924] decoder loop j: 58 cur_layer_idx: 29 layer_attn_idx: 29 layer_mlp_idx: 28
DEBUG 10-15 15:28:12 lpllm.py:933] start load next layer cur_layer_idx: 30
DEBUG 10-15 15:28:12 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed650>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x003\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x96\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:28:12 client.py:72] load_into_gpu: Mixtral-8x7B, a3afa054-5573-4fcd-ba23-8de0decf842a
INFO 10-15 15:28:12 client.py:113] Model loaded: Mixtral-8x7B, a3afa054-5573-4fcd-ba23-8de0decf842a
DEBUG 10-15 15:28:12 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:12 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:12 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:12 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:12 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:12 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:12 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:12 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:12 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:12 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:12 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:12 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:12 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:12 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:12 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:12 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:12 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:12 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:12 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:12 lpllm.py:2265] GPU2CPU move cost 0.000620 seconds
DEBUG 10-15 15:28:12 lpmodule.py:340] layer_idx 29 attn layer_idx: 29
DEBUG 10-15 15:28:12 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:12 lpmodule.py:364] decoder_attn_batch update batch_dim 600-660
DEBUG 10-15 15:28:12 StaticCacheLen.py:84] static cache update layer_idx: 29, start_batch: 600, end_batch: 660
DEBUG 10-15 15:28:12 lpmodule.py:368] update for kv cache 600-660 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:12 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:12 lpmodule.py:374] update past key value cost 0.025081 seconds
DEBUG 10-15 15:28:12 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:12 lpmodule.py:227] layer idx 28
DEBUG 10-15 15:28:12 lpmodule.py:399] repeat qkv cost 0.028501 seconds
DEBUG 10-15 15:28:12 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:12 lpmodule.py:433] dot attn cost 0.031767 seconds
DEBUG 10-15 15:28:12 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:12 lpmodule.py:444] time cost move to cuda:1 0.023214340209960938 s
DEBUG 10-15 15:28:12 lpllm.py:2283] CPU attn cost 0.133367 seconds if batch True
DEBUG 10-15 15:28:12 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:28:12 lpllm.py:2294] CPU compute cost 0.134271 seconds
DEBUG 10-15 15:28:12 lpllm.py:2312] free cost 0.000070 seconds
DEBUG 10-15 15:28:12 lpllm.py:503] reset update experts
DEBUG 10-15 15:28:12 lpllm.py:1774] update state cost 3.719329833984375e-05 s
DEBUG 10-15 15:28:12 lpllm.py:1743] restore layer func cost 0.0008497238159179688 s
DEBUG 10-15 15:28:12 lpllm.py:511] restore layer cost 0.0010991096496582031 s
DEBUG 10-15 15:28:12 lpllm.py:512] reset_next_layer_need: layer_attn_idx=29, layer_mlp_idx=29, j_loc=58
DEBUG 10-15 15:28:12 lpllm.py:1037] reset layer cost 0.0011751651763916016 s
DEBUG 10-15 15:28:12 lpllm.py:1038] have reset next layer layer_attn 29 layer_mlp 29 
DEBUG 10-15 15:28:12 lpllm.py:924] 
DEBUG 10-15 15:28:12 lpllm.py:924] decoder loop j: 59 cur_layer_idx: 29 layer_attn_idx: 29 layer_mlp_idx: 29
DEBUG 10-15 15:28:12 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:12 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:12 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:12 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:12 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:12 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:12 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:12 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:12 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:12 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:12 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:12 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:12 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:12 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:12 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:12 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:12 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:12 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:12 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:12 lpllm.py:2265] GPU2CPU move cost 0.000603 seconds
DEBUG 10-15 15:28:12 lpmodule.py:340] layer_idx 29 attn layer_idx: 29
DEBUG 10-15 15:28:12 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:12 lpmodule.py:364] decoder_attn_batch update batch_dim 660-720
DEBUG 10-15 15:28:12 StaticCacheLen.py:84] static cache update layer_idx: 29, start_batch: 660, end_batch: 720
DEBUG 10-15 15:28:12 lpmodule.py:368] update for kv cache 660-720 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:12 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:12 lpmodule.py:374] update past key value cost 0.023103 seconds
DEBUG 10-15 15:28:12 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:12 lpmodule.py:227] layer idx 29
DEBUG 10-15 15:28:12 lpmodule.py:399] repeat qkv cost 0.028791 seconds
DEBUG 10-15 15:28:12 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:12 lpmodule.py:433] dot attn cost 0.032070 seconds
DEBUG 10-15 15:28:12 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:12 lpmodule.py:444] time cost move to cuda:1 0.023823976516723633 s
DEBUG 10-15 15:28:12 lpllm.py:2283] CPU attn cost 0.132318 seconds if batch True
DEBUG 10-15 15:28:12 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:12 lpllm.py:2294] CPU compute cost 0.133218 seconds
DEBUG 10-15 15:28:12 lpllm.py:2312] free cost 0.000070 seconds
DEBUG 10-15 15:28:12 lpllm.py:498] reset update attn
DEBUG 10-15 15:28:12 lpllm.py:1774] update state cost 3.910064697265625e-05 s
DEBUG 10-15 15:28:12 lpllm.py:1743] restore layer func cost 0.0004124641418457031 s
DEBUG 10-15 15:28:12 lpllm.py:511] restore layer cost 0.0006604194641113281 s
DEBUG 10-15 15:28:12 lpllm.py:512] reset_next_layer_need: layer_attn_idx=30, layer_mlp_idx=29, j_loc=59
DEBUG 10-15 15:28:12 lpllm.py:1037] reset layer cost 0.0007343292236328125 s
DEBUG 10-15 15:28:12 lpllm.py:1038] have reset next layer layer_attn 30 layer_mlp 29 
DEBUG 10-15 15:28:12 lpllm.py:1044] j: 59 waiting the layer with layer_idx 30 before wait time 0.4528360366821289 s
INFO 10-15 15:28:12 client.py:117] confirm_model_loaded: Mixtral-8x7B, a3afa054-5573-4fcd-ba23-8de0decf842a
INFO 10-15 15:28:12 client.py:125] Model loaded
DEBUG 10-15 15:28:12 lpllm.py:1048] j: load cost 0.45423126220703125 s waiting cost 0.0013794898986816406 s
DEBUG 10-15 15:28:12 lpllm.py:924] 
DEBUG 10-15 15:28:12 lpllm.py:924] decoder loop j: 60 cur_layer_idx: 30 layer_attn_idx: 30 layer_mlp_idx: 29
DEBUG 10-15 15:28:12 lpllm.py:933] start load next layer cur_layer_idx: 31
DEBUG 10-15 15:28:12 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed6e0>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x004\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x98\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:28:12 client.py:72] load_into_gpu: Mixtral-8x7B, fd96d13a-99f4-4035-af25-adbc8dc69bd5
INFO 10-15 15:28:12 client.py:113] Model loaded: Mixtral-8x7B, fd96d13a-99f4-4035-af25-adbc8dc69bd5
DEBUG 10-15 15:28:12 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:12 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:12 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:12 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:12 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:12 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:12 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:12 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:12 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:12 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:12 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:12 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:12 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:12 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:12 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:12 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:12 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:12 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:12 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:12 lpllm.py:2265] GPU2CPU move cost 0.000566 seconds
DEBUG 10-15 15:28:12 lpmodule.py:340] layer_idx 30 attn layer_idx: 30
DEBUG 10-15 15:28:12 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:12 lpmodule.py:364] decoder_attn_batch update batch_dim 600-660
DEBUG 10-15 15:28:12 StaticCacheLen.py:84] static cache update layer_idx: 30, start_batch: 600, end_batch: 660
DEBUG 10-15 15:28:12 lpmodule.py:368] update for kv cache 600-660 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:12 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:12 lpmodule.py:374] update past key value cost 0.024737 seconds
DEBUG 10-15 15:28:12 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:12 lpmodule.py:227] layer idx 29
DEBUG 10-15 15:28:12 lpmodule.py:399] repeat qkv cost 0.028789 seconds
DEBUG 10-15 15:28:12 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:12 lpmodule.py:433] dot attn cost 0.031203 seconds
DEBUG 10-15 15:28:12 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:12 lpmodule.py:444] time cost move to cuda:1 0.024614334106445312 s
DEBUG 10-15 15:28:12 lpllm.py:2283] CPU attn cost 0.135277 seconds if batch True
DEBUG 10-15 15:28:12 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:12 lpllm.py:2294] CPU compute cost 0.136170 seconds
DEBUG 10-15 15:28:12 lpllm.py:2312] free cost 0.000097 seconds
DEBUG 10-15 15:28:13 lpllm.py:503] reset update experts
DEBUG 10-15 15:28:13 lpllm.py:1774] update state cost 2.3126602172851562e-05 s
DEBUG 10-15 15:28:13 lpllm.py:1743] restore layer func cost 0.0008409023284912109 s
DEBUG 10-15 15:28:13 lpllm.py:511] restore layer cost 0.00107574462890625 s
DEBUG 10-15 15:28:13 lpllm.py:512] reset_next_layer_need: layer_attn_idx=30, layer_mlp_idx=30, j_loc=60
DEBUG 10-15 15:28:13 lpllm.py:1037] reset layer cost 0.0011546611785888672 s
DEBUG 10-15 15:28:13 lpllm.py:1038] have reset next layer layer_attn 30 layer_mlp 30 
DEBUG 10-15 15:28:13 lpllm.py:924] 
DEBUG 10-15 15:28:13 lpllm.py:924] decoder loop j: 61 cur_layer_idx: 30 layer_attn_idx: 30 layer_mlp_idx: 30
DEBUG 10-15 15:28:13 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:13 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:13 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:13 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:13 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:13 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:13 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:13 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:13 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:13 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:13 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:13 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:13 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:13 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:13 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:13 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:13 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:13 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:13 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:13 lpllm.py:2265] GPU2CPU move cost 0.000618 seconds
DEBUG 10-15 15:28:13 lpmodule.py:340] layer_idx 30 attn layer_idx: 30
DEBUG 10-15 15:28:13 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:13 lpmodule.py:364] decoder_attn_batch update batch_dim 660-720
DEBUG 10-15 15:28:13 StaticCacheLen.py:84] static cache update layer_idx: 30, start_batch: 660, end_batch: 720
DEBUG 10-15 15:28:13 lpmodule.py:368] update for kv cache 660-720 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:13 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:13 lpmodule.py:374] update past key value cost 0.022232 seconds
DEBUG 10-15 15:28:13 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:13 lpmodule.py:227] layer idx 30
DEBUG 10-15 15:28:13 lpmodule.py:399] repeat qkv cost 0.029543 seconds
DEBUG 10-15 15:28:13 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:13 lpmodule.py:433] dot attn cost 0.032396 seconds
DEBUG 10-15 15:28:13 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:13 lpmodule.py:444] time cost move to cuda:1 0.023348093032836914 s
DEBUG 10-15 15:28:13 lpllm.py:2283] CPU attn cost 0.132192 seconds if batch True
DEBUG 10-15 15:28:13 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:13 lpllm.py:2294] CPU compute cost 0.133090 seconds
DEBUG 10-15 15:28:13 lpllm.py:2312] free cost 0.000072 seconds
DEBUG 10-15 15:28:13 lpllm.py:498] reset update attn
DEBUG 10-15 15:28:13 lpllm.py:1774] update state cost 4.076957702636719e-05 s
DEBUG 10-15 15:28:13 lpllm.py:1743] restore layer func cost 0.00040340423583984375 s
DEBUG 10-15 15:28:13 lpllm.py:511] restore layer cost 0.0006477832794189453 s
DEBUG 10-15 15:28:13 lpllm.py:512] reset_next_layer_need: layer_attn_idx=31, layer_mlp_idx=30, j_loc=61
DEBUG 10-15 15:28:13 lpllm.py:1037] reset layer cost 0.0007193088531494141 s
DEBUG 10-15 15:28:13 lpllm.py:1038] have reset next layer layer_attn 31 layer_mlp 30 
DEBUG 10-15 15:28:13 lpllm.py:1044] j: 61 waiting the layer with layer_idx 31 before wait time 0.4460108280181885 s
INFO 10-15 15:28:13 client.py:117] confirm_model_loaded: Mixtral-8x7B, fd96d13a-99f4-4035-af25-adbc8dc69bd5
INFO 10-15 15:28:13 client.py:125] Model loaded
DEBUG 10-15 15:28:13 lpllm.py:1048] j: load cost 0.44780492782592773 s waiting cost 0.0017790794372558594 s
DEBUG 10-15 15:28:13 lpllm.py:924] 
DEBUG 10-15 15:28:13 lpllm.py:924] decoder loop j: 62 cur_layer_idx: 31 layer_attn_idx: 31 layer_mlp_idx: 30
DEBUG 10-15 15:28:13 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:13 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:13 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:13 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:13 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:13 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:13 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:13 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:13 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:13 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:13 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:13 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:13 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:13 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:13 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:13 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:13 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:13 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:13 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:13 lpllm.py:2265] GPU2CPU move cost 0.000573 seconds
DEBUG 10-15 15:28:13 lpmodule.py:340] layer_idx 31 attn layer_idx: 31
DEBUG 10-15 15:28:13 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:13 lpmodule.py:364] decoder_attn_batch update batch_dim 600-660
DEBUG 10-15 15:28:13 StaticCacheLen.py:84] static cache update layer_idx: 31, start_batch: 600, end_batch: 660
DEBUG 10-15 15:28:13 lpmodule.py:368] update for kv cache 600-660 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:13 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:13 lpmodule.py:374] update past key value cost 0.024485 seconds
DEBUG 10-15 15:28:13 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:13 lpmodule.py:227] layer idx 30
DEBUG 10-15 15:28:13 lpmodule.py:399] repeat qkv cost 0.028128 seconds
DEBUG 10-15 15:28:13 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:13 lpmodule.py:433] dot attn cost 0.031833 seconds
DEBUG 10-15 15:28:13 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:13 lpmodule.py:444] time cost move to cuda:1 0.022977352142333984 s
DEBUG 10-15 15:28:13 lpllm.py:2283] CPU attn cost 0.131809 seconds if batch True
DEBUG 10-15 15:28:13 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:28:13 lpllm.py:2294] CPU compute cost 0.132652 seconds
DEBUG 10-15 15:28:13 lpllm.py:2312] free cost 0.000074 seconds
DEBUG 10-15 15:28:13 lpllm.py:503] reset update experts
DEBUG 10-15 15:28:13 lpllm.py:1774] update state cost 2.3126602172851562e-05 s
DEBUG 10-15 15:28:13 lpllm.py:1743] restore layer func cost 0.0008299350738525391 s
DEBUG 10-15 15:28:13 lpllm.py:511] restore layer cost 0.0010797977447509766 s
DEBUG 10-15 15:28:13 lpllm.py:512] reset_next_layer_need: layer_attn_idx=31, layer_mlp_idx=31, j_loc=62
DEBUG 10-15 15:28:13 lpllm.py:1037] reset layer cost 0.0011577606201171875 s
DEBUG 10-15 15:28:13 lpllm.py:1038] have reset next layer layer_attn 31 layer_mlp 31 
DEBUG 10-15 15:28:13 lpllm.py:924] 
DEBUG 10-15 15:28:13 lpllm.py:924] decoder loop j: 63 cur_layer_idx: 31 layer_attn_idx: 31 layer_mlp_idx: 31
DEBUG 10-15 15:28:13 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:13 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:13 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:13 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:13 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:13 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:13 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:13 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:13 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:13 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:13 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:13 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:13 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:13 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:13 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:13 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:13 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:13 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:13 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:13 lpllm.py:2265] GPU2CPU move cost 0.000636 seconds
DEBUG 10-15 15:28:13 lpmodule.py:340] layer_idx 31 attn layer_idx: 31
DEBUG 10-15 15:28:13 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:13 lpmodule.py:364] decoder_attn_batch update batch_dim 660-720
DEBUG 10-15 15:28:13 StaticCacheLen.py:84] static cache update layer_idx: 31, start_batch: 660, end_batch: 720
DEBUG 10-15 15:28:13 lpmodule.py:368] update for kv cache 660-720 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:13 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:13 lpmodule.py:374] update past key value cost 0.023989 seconds
DEBUG 10-15 15:28:13 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:13 lpmodule.py:227] layer idx 31
DEBUG 10-15 15:28:13 lpmodule.py:399] repeat qkv cost 0.029343 seconds
DEBUG 10-15 15:28:13 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:13 lpmodule.py:433] dot attn cost 0.031419 seconds
DEBUG 10-15 15:28:13 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:13 lpmodule.py:444] time cost move to cuda:1 0.02310943603515625 s
DEBUG 10-15 15:28:13 lpllm.py:2283] CPU attn cost 0.132570 seconds if batch True
DEBUG 10-15 15:28:13 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:28:13 lpllm.py:2294] CPU compute cost 0.133497 seconds
DEBUG 10-15 15:28:13 lpllm.py:2312] free cost 0.000072 seconds
DEBUG 10-15 15:28:13 lpllm.py:924] 
DEBUG 10-15 15:28:13 lpllm.py:924] decoder loop j: 64 cur_layer_idx: 32 layer_attn_idx: 32 layer_mlp_idx: 31
DEBUG 10-15 15:28:13 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:13 lpmodule.py:227] layer idx 31
DEBUG 10-15 15:28:13 lpllm.py:1085] last_mlp_output_chunk shape: torch.Size([60, 512, 4096]), mlp_output_chunk shape: torch.Size([60, 512, 4096])
DEBUG 10-15 15:28:13 lpllm.py:1086] last_mlp_output_chunk len 1, mlp_output_chunk len 1
DEBUG 10-15 15:28:13 lpllm.py:618] decoders batch for 5 cost 14.68284797668457 s
DEBUG 10-15 15:28:13 lpllm.py:848] hidden_states_chunks1 shape: torch.Size([60, 512, 4096]), hidden_states_chunks1 length: 1
DEBUG 10-15 15:28:13 lpllm.py:849] hidden_states_chunks2 shape: torch.Size([60, 512, 4096]), hidden_states_chunks2 length: 1
DEBUG 10-15 15:28:13 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:28:13 client.py:72] load_into_gpu: Mixtral-8x7B, 7c5490d2-f6f0-4533-9efb-159fc3d35c5f
INFO 10-15 15:28:13 client.py:113] Model loaded: Mixtral-8x7B, 7c5490d2-f6f0-4533-9efb-159fc3d35c5f
DEBUG 10-15 15:28:13 lpllm.py:1743] restore layer func cost 0.0009677410125732422 s
INFO 10-15 15:28:13 client.py:117] confirm_model_loaded: Mixtral-8x7B, 7c5490d2-f6f0-4533-9efb-159fc3d35c5f
INFO 10-15 15:28:14 client.py:125] Model loaded
DEBUG 10-15 15:28:14 lpllm.py:422] prepare layer cost 0.27239251136779785 s
DEBUG 10-15 15:28:14 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed650>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x003\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x96\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:28:14 client.py:72] load_into_gpu: Mixtral-8x7B, 43c3040a-dbf4-44ff-9253-da72b9871036
INFO 10-15 15:28:14 client.py:113] Model loaded: Mixtral-8x7B, 43c3040a-dbf4-44ff-9253-da72b9871036
DEBUG 10-15 15:28:14 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:14 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:14 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:14 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:14 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:14 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:14 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:14 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:14 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:14 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:14 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:14 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:14 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:14 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:14 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:14 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:14 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:14 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:14 lpllm.py:924] 
DEBUG 10-15 15:28:14 lpllm.py:924] decoder loop j: 1 cur_layer_idx: 0 layer_attn_idx: 0 layer_mlp_idx: 0
DEBUG 10-15 15:28:14 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:14 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:14 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:14 lpllm.py:2265] GPU2CPU move cost 0.000597 seconds
DEBUG 10-15 15:28:14 lpmodule.py:340] layer_idx 0 attn layer_idx: 0
DEBUG 10-15 15:28:14 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:14 lpmodule.py:364] decoder_attn_batch update batch_dim 720-780
DEBUG 10-15 15:28:14 StaticCacheLen.py:84] static cache update layer_idx: 0, start_batch: 720, end_batch: 780
DEBUG 10-15 15:28:14 lpmodule.py:368] update for kv cache 720-780 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:14 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:14 lpmodule.py:374] update past key value cost 0.024531 seconds
DEBUG 10-15 15:28:14 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:14 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:14 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:14 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:14 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:14 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:14 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:14 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:14 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:14 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:14 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:14 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:14 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:14 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:14 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:14 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:14 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:14 lpmodule.py:399] repeat qkv cost 0.031806 seconds
DEBUG 10-15 15:28:14 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:14 lpmodule.py:433] dot attn cost 0.031394 seconds
DEBUG 10-15 15:28:14 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:14 lpmodule.py:444] time cost move to cuda:1 0.02338266372680664 s
DEBUG 10-15 15:28:14 lpllm.py:2283] CPU attn cost 0.134951 seconds if batch True
DEBUG 10-15 15:28:14 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:14 lpllm.py:2294] CPU compute cost 0.135848 seconds
DEBUG 10-15 15:28:14 lpllm.py:2312] free cost 0.000073 seconds
DEBUG 10-15 15:28:14 lpllm.py:2265] GPU2CPU move cost 0.000556 seconds
DEBUG 10-15 15:28:14 lpmodule.py:340] layer_idx 0 attn layer_idx: 0
DEBUG 10-15 15:28:14 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:14 lpmodule.py:364] decoder_attn_batch update batch_dim 780-840
DEBUG 10-15 15:28:14 lpmodule.py:227] layer idx 0
DEBUG 10-15 15:28:14 StaticCacheLen.py:84] static cache update layer_idx: 0, start_batch: 780, end_batch: 840
DEBUG 10-15 15:28:14 lpmodule.py:368] update for kv cache 780-840 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:14 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:14 lpmodule.py:374] update past key value cost 0.022303 seconds
DEBUG 10-15 15:28:14 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:14 lpmodule.py:399] repeat qkv cost 0.029940 seconds
DEBUG 10-15 15:28:14 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:14 lpmodule.py:433] dot attn cost 0.030996 seconds
DEBUG 10-15 15:28:14 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:14 lpmodule.py:444] time cost move to cuda:1 0.023216962814331055 s
DEBUG 10-15 15:28:14 lpllm.py:2283] CPU attn cost 0.131963 seconds if batch True
DEBUG 10-15 15:28:14 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:14 lpllm.py:2294] CPU compute cost 0.132683 seconds
DEBUG 10-15 15:28:14 lpllm.py:2312] free cost 0.000072 seconds
DEBUG 10-15 15:28:14 lpllm.py:498] reset update attn
DEBUG 10-15 15:28:14 lpllm.py:1774] update state cost 2.47955322265625e-05 s
DEBUG 10-15 15:28:14 lpllm.py:1743] restore layer func cost 0.0004036426544189453 s
DEBUG 10-15 15:28:14 lpllm.py:511] restore layer cost 0.0006272792816162109 s
DEBUG 10-15 15:28:14 lpllm.py:512] reset_next_layer_need: layer_attn_idx=1, layer_mlp_idx=0, j_loc=1
DEBUG 10-15 15:28:14 lpllm.py:1037] reset layer cost 0.000701904296875 s
DEBUG 10-15 15:28:14 lpllm.py:1038] have reset next layer layer_attn 1 layer_mlp 0 
DEBUG 10-15 15:28:14 lpllm.py:1044] j: 1 waiting the layer with layer_idx 1 before wait time 0.3484508991241455 s
INFO 10-15 15:28:14 client.py:117] confirm_model_loaded: Mixtral-8x7B, 43c3040a-dbf4-44ff-9253-da72b9871036
INFO 10-15 15:28:14 client.py:125] Model loaded
DEBUG 10-15 15:28:14 lpllm.py:1048] j: load cost 0.3502964973449707 s waiting cost 0.0018298625946044922 s
DEBUG 10-15 15:28:14 lpllm.py:924] 
DEBUG 10-15 15:28:14 lpllm.py:924] decoder loop j: 2 cur_layer_idx: 1 layer_attn_idx: 1 layer_mlp_idx: 0
DEBUG 10-15 15:28:14 lpllm.py:933] start load next layer cur_layer_idx: 2
DEBUG 10-15 15:28:14 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed6e0>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x004\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x98\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:28:14 client.py:72] load_into_gpu: Mixtral-8x7B, 75108a52-4247-4359-838b-dc7ccf89244e
INFO 10-15 15:28:14 client.py:113] Model loaded: Mixtral-8x7B, 75108a52-4247-4359-838b-dc7ccf89244e
DEBUG 10-15 15:28:14 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:14 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:14 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:14 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:14 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:14 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:14 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:14 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:14 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:14 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:14 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:14 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:14 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:14 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:14 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:14 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:14 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:14 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:14 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:14 lpllm.py:2265] GPU2CPU move cost 0.000618 seconds
DEBUG 10-15 15:28:14 lpmodule.py:340] layer_idx 1 attn layer_idx: 1
DEBUG 10-15 15:28:14 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:14 lpmodule.py:364] decoder_attn_batch update batch_dim 720-780
DEBUG 10-15 15:28:14 StaticCacheLen.py:84] static cache update layer_idx: 1, start_batch: 720, end_batch: 780
DEBUG 10-15 15:28:14 lpmodule.py:368] update for kv cache 720-780 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:14 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:14 lpmodule.py:374] update past key value cost 0.022784 seconds
DEBUG 10-15 15:28:14 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:14 lpmodule.py:227] layer idx 0
DEBUG 10-15 15:28:14 lpmodule.py:399] repeat qkv cost 0.028092 seconds
DEBUG 10-15 15:28:14 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:14 lpmodule.py:433] dot attn cost 0.031384 seconds
DEBUG 10-15 15:28:14 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:14 lpmodule.py:444] time cost move to cuda:1 0.023665904998779297 s
DEBUG 10-15 15:28:14 lpllm.py:2283] CPU attn cost 0.131917 seconds if batch True
DEBUG 10-15 15:28:14 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:14 lpllm.py:2294] CPU compute cost 0.132822 seconds
DEBUG 10-15 15:28:14 lpllm.py:2312] free cost 0.000073 seconds
DEBUG 10-15 15:28:14 lpllm.py:503] reset update experts
DEBUG 10-15 15:28:14 lpllm.py:1774] update state cost 2.1696090698242188e-05 s
DEBUG 10-15 15:28:14 lpllm.py:1743] restore layer func cost 0.0008563995361328125 s
DEBUG 10-15 15:28:14 lpllm.py:511] restore layer cost 0.0011067390441894531 s
DEBUG 10-15 15:28:14 lpllm.py:512] reset_next_layer_need: layer_attn_idx=1, layer_mlp_idx=1, j_loc=2
DEBUG 10-15 15:28:14 lpllm.py:1037] reset layer cost 0.0011851787567138672 s
DEBUG 10-15 15:28:14 lpllm.py:1038] have reset next layer layer_attn 1 layer_mlp 1 
DEBUG 10-15 15:28:14 lpllm.py:924] 
DEBUG 10-15 15:28:14 lpllm.py:924] decoder loop j: 3 cur_layer_idx: 1 layer_attn_idx: 1 layer_mlp_idx: 1
DEBUG 10-15 15:28:14 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:14 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:14 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:14 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:14 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:14 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:14 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:14 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:14 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:14 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:14 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:14 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:14 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:14 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:14 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:14 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:14 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:14 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:14 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:14 lpllm.py:2265] GPU2CPU move cost 0.000576 seconds
DEBUG 10-15 15:28:14 lpmodule.py:340] layer_idx 1 attn layer_idx: 1
DEBUG 10-15 15:28:14 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:14 lpmodule.py:364] decoder_attn_batch update batch_dim 780-840
DEBUG 10-15 15:28:14 StaticCacheLen.py:84] static cache update layer_idx: 1, start_batch: 780, end_batch: 840
DEBUG 10-15 15:28:14 lpmodule.py:368] update for kv cache 780-840 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:14 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:14 lpmodule.py:374] update past key value cost 0.024395 seconds
DEBUG 10-15 15:28:14 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:14 lpmodule.py:227] layer idx 1
DEBUG 10-15 15:28:14 lpmodule.py:399] repeat qkv cost 0.028816 seconds
DEBUG 10-15 15:28:14 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:14 lpmodule.py:433] dot attn cost 0.030351 seconds
DEBUG 10-15 15:28:14 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:14 lpmodule.py:444] time cost move to cuda:1 0.024553775787353516 s
DEBUG 10-15 15:28:14 lpllm.py:2283] CPU attn cost 0.135254 seconds if batch True
DEBUG 10-15 15:28:14 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:14 lpllm.py:2294] CPU compute cost 0.136117 seconds
DEBUG 10-15 15:28:14 lpllm.py:2312] free cost 0.000073 seconds
DEBUG 10-15 15:28:14 lpllm.py:498] reset update attn
DEBUG 10-15 15:28:14 lpllm.py:1774] update state cost 2.47955322265625e-05 s
DEBUG 10-15 15:28:14 lpllm.py:1743] restore layer func cost 0.0003910064697265625 s
DEBUG 10-15 15:28:14 lpllm.py:511] restore layer cost 0.0006191730499267578 s
DEBUG 10-15 15:28:14 lpllm.py:512] reset_next_layer_need: layer_attn_idx=2, layer_mlp_idx=1, j_loc=3
DEBUG 10-15 15:28:14 lpllm.py:1037] reset layer cost 0.0007064342498779297 s
DEBUG 10-15 15:28:14 lpllm.py:1038] have reset next layer layer_attn 2 layer_mlp 1 
DEBUG 10-15 15:28:14 lpllm.py:1044] j: 3 waiting the layer with layer_idx 2 before wait time 0.4527096748352051 s
INFO 10-15 15:28:14 client.py:117] confirm_model_loaded: Mixtral-8x7B, 75108a52-4247-4359-838b-dc7ccf89244e
INFO 10-15 15:28:14 client.py:125] Model loaded
DEBUG 10-15 15:28:14 lpllm.py:1048] j: load cost 0.4545407295227051 s waiting cost 0.0018160343170166016 s
DEBUG 10-15 15:28:14 lpllm.py:924] 
DEBUG 10-15 15:28:14 lpllm.py:924] decoder loop j: 4 cur_layer_idx: 2 layer_attn_idx: 2 layer_mlp_idx: 1
DEBUG 10-15 15:28:14 lpllm.py:933] start load next layer cur_layer_idx: 3
DEBUG 10-15 15:28:14 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:28:14 client.py:72] load_into_gpu: Mixtral-8x7B, f91344b9-fe71-4150-a712-74530419c1f9
INFO 10-15 15:28:14 client.py:113] Model loaded: Mixtral-8x7B, f91344b9-fe71-4150-a712-74530419c1f9
DEBUG 10-15 15:28:14 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:14 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:14 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:14 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:14 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:14 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:14 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:14 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:14 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:14 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:14 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:14 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:14 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:14 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:14 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:14 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:14 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:14 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:14 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:14 lpllm.py:2265] GPU2CPU move cost 0.000621 seconds
DEBUG 10-15 15:28:14 lpmodule.py:340] layer_idx 2 attn layer_idx: 2
DEBUG 10-15 15:28:14 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:14 lpmodule.py:364] decoder_attn_batch update batch_dim 720-780
DEBUG 10-15 15:28:14 StaticCacheLen.py:84] static cache update layer_idx: 2, start_batch: 720, end_batch: 780
DEBUG 10-15 15:28:14 lpmodule.py:368] update for kv cache 720-780 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:14 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:14 lpmodule.py:374] update past key value cost 0.025011 seconds
DEBUG 10-15 15:28:14 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:14 lpmodule.py:227] layer idx 1
DEBUG 10-15 15:28:15 lpmodule.py:399] repeat qkv cost 0.027325 seconds
DEBUG 10-15 15:28:15 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:15 lpmodule.py:433] dot attn cost 0.031122 seconds
DEBUG 10-15 15:28:15 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:15 lpmodule.py:444] time cost move to cuda:1 0.024723529815673828 s
DEBUG 10-15 15:28:15 lpllm.py:2283] CPU attn cost 0.134921 seconds if batch True
DEBUG 10-15 15:28:15 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:15 lpllm.py:2294] CPU compute cost 0.135831 seconds
DEBUG 10-15 15:28:15 lpllm.py:2312] free cost 0.000072 seconds
DEBUG 10-15 15:28:15 lpllm.py:503] reset update experts
DEBUG 10-15 15:28:15 lpllm.py:1774] update state cost 2.193450927734375e-05 s
DEBUG 10-15 15:28:15 lpllm.py:1743] restore layer func cost 0.0008852481842041016 s
DEBUG 10-15 15:28:15 lpllm.py:511] restore layer cost 0.0011301040649414062 s
DEBUG 10-15 15:28:15 lpllm.py:512] reset_next_layer_need: layer_attn_idx=2, layer_mlp_idx=2, j_loc=4
DEBUG 10-15 15:28:15 lpllm.py:1037] reset layer cost 0.0012226104736328125 s
DEBUG 10-15 15:28:15 lpllm.py:1038] have reset next layer layer_attn 2 layer_mlp 2 
DEBUG 10-15 15:28:15 lpllm.py:924] 
DEBUG 10-15 15:28:15 lpllm.py:924] decoder loop j: 5 cur_layer_idx: 2 layer_attn_idx: 2 layer_mlp_idx: 2
DEBUG 10-15 15:28:15 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:15 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:15 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:15 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:15 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:15 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:15 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:15 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:15 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:15 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:15 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:15 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:15 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:15 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:15 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:15 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:15 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:15 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:15 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:15 lpllm.py:2265] GPU2CPU move cost 0.000575 seconds
DEBUG 10-15 15:28:15 lpmodule.py:340] layer_idx 2 attn layer_idx: 2
DEBUG 10-15 15:28:15 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:15 lpmodule.py:364] decoder_attn_batch update batch_dim 780-840
DEBUG 10-15 15:28:15 StaticCacheLen.py:84] static cache update layer_idx: 2, start_batch: 780, end_batch: 840
DEBUG 10-15 15:28:15 lpmodule.py:368] update for kv cache 780-840 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:15 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:15 lpmodule.py:374] update past key value cost 0.024318 seconds
DEBUG 10-15 15:28:15 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:15 lpmodule.py:227] layer idx 2
DEBUG 10-15 15:28:15 lpmodule.py:399] repeat qkv cost 0.028970 seconds
DEBUG 10-15 15:28:15 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:15 lpmodule.py:433] dot attn cost 0.030132 seconds
DEBUG 10-15 15:28:15 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:15 lpmodule.py:444] time cost move to cuda:1 0.02447366714477539 s
DEBUG 10-15 15:28:15 lpllm.py:2283] CPU attn cost 0.135495 seconds if batch True
DEBUG 10-15 15:28:15 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:28:15 lpllm.py:2294] CPU compute cost 0.136347 seconds
DEBUG 10-15 15:28:15 lpllm.py:2312] free cost 0.000073 seconds
DEBUG 10-15 15:28:15 lpllm.py:498] reset update attn
DEBUG 10-15 15:28:15 lpllm.py:1774] update state cost 2.384185791015625e-05 s
DEBUG 10-15 15:28:15 lpllm.py:1743] restore layer func cost 0.00040078163146972656 s
DEBUG 10-15 15:28:15 lpllm.py:511] restore layer cost 0.0006492137908935547 s
DEBUG 10-15 15:28:15 lpllm.py:512] reset_next_layer_need: layer_attn_idx=3, layer_mlp_idx=2, j_loc=5
DEBUG 10-15 15:28:15 lpllm.py:1037] reset layer cost 0.00072479248046875 s
DEBUG 10-15 15:28:15 lpllm.py:1038] have reset next layer layer_attn 3 layer_mlp 2 
DEBUG 10-15 15:28:15 lpllm.py:1044] j: 5 waiting the layer with layer_idx 3 before wait time 0.4529433250427246 s
INFO 10-15 15:28:15 client.py:117] confirm_model_loaded: Mixtral-8x7B, f91344b9-fe71-4150-a712-74530419c1f9
INFO 10-15 15:28:15 client.py:125] Model loaded
DEBUG 10-15 15:28:15 lpllm.py:1048] j: load cost 0.4547750949859619 s waiting cost 0.0018165111541748047 s
DEBUG 10-15 15:28:15 lpllm.py:924] 
DEBUG 10-15 15:28:15 lpllm.py:924] decoder loop j: 6 cur_layer_idx: 3 layer_attn_idx: 3 layer_mlp_idx: 2
DEBUG 10-15 15:28:15 lpllm.py:933] start load next layer cur_layer_idx: 4
DEBUG 10-15 15:28:15 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed650>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x003\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x96\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:28:15 client.py:72] load_into_gpu: Mixtral-8x7B, d0408bc0-0cbd-485a-b6cd-2cccc0689c55
INFO 10-15 15:28:15 client.py:113] Model loaded: Mixtral-8x7B, d0408bc0-0cbd-485a-b6cd-2cccc0689c55
DEBUG 10-15 15:28:15 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:15 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:15 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:15 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:15 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:15 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:15 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:15 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:15 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:15 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:15 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:15 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:15 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:15 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:15 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:15 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:15 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:15 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:15 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:15 lpllm.py:2265] GPU2CPU move cost 0.000597 seconds
DEBUG 10-15 15:28:15 lpmodule.py:340] layer_idx 3 attn layer_idx: 3
DEBUG 10-15 15:28:15 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:15 lpmodule.py:364] decoder_attn_batch update batch_dim 720-780
DEBUG 10-15 15:28:15 StaticCacheLen.py:84] static cache update layer_idx: 3, start_batch: 720, end_batch: 780
DEBUG 10-15 15:28:15 lpmodule.py:368] update for kv cache 720-780 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:15 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:15 lpmodule.py:374] update past key value cost 0.023518 seconds
DEBUG 10-15 15:28:15 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:15 lpmodule.py:227] layer idx 2
DEBUG 10-15 15:28:15 lpmodule.py:399] repeat qkv cost 0.030115 seconds
DEBUG 10-15 15:28:15 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:15 lpmodule.py:433] dot attn cost 0.032714 seconds
DEBUG 10-15 15:28:15 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:15 lpmodule.py:444] time cost move to cuda:1 0.03582262992858887 s
DEBUG 10-15 15:28:15 lpllm.py:2283] CPU attn cost 0.153941 seconds if batch True
DEBUG 10-15 15:28:15 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:15 lpllm.py:2294] CPU compute cost 0.154824 seconds
DEBUG 10-15 15:28:15 lpllm.py:2312] free cost 0.000073 seconds
DEBUG 10-15 15:28:15 lpllm.py:503] reset update experts
DEBUG 10-15 15:28:15 lpllm.py:1774] update state cost 2.0742416381835938e-05 s
DEBUG 10-15 15:28:15 lpllm.py:1743] restore layer func cost 0.0008447170257568359 s
DEBUG 10-15 15:28:15 lpllm.py:511] restore layer cost 0.0011055469512939453 s
DEBUG 10-15 15:28:15 lpllm.py:512] reset_next_layer_need: layer_attn_idx=3, layer_mlp_idx=3, j_loc=6
DEBUG 10-15 15:28:15 lpllm.py:1037] reset layer cost 0.0011837482452392578 s
DEBUG 10-15 15:28:15 lpllm.py:1038] have reset next layer layer_attn 3 layer_mlp 3 
DEBUG 10-15 15:28:15 lpllm.py:924] 
DEBUG 10-15 15:28:15 lpllm.py:924] decoder loop j: 7 cur_layer_idx: 3 layer_attn_idx: 3 layer_mlp_idx: 3
DEBUG 10-15 15:28:15 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:15 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:15 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:15 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:15 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:15 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:15 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:15 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:15 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:15 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:15 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:15 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:15 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:15 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:15 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:15 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:15 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:15 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:15 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:15 lpllm.py:2265] GPU2CPU move cost 0.000609 seconds
DEBUG 10-15 15:28:15 lpmodule.py:340] layer_idx 3 attn layer_idx: 3
DEBUG 10-15 15:28:15 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:15 lpmodule.py:364] decoder_attn_batch update batch_dim 780-840
DEBUG 10-15 15:28:15 StaticCacheLen.py:84] static cache update layer_idx: 3, start_batch: 780, end_batch: 840
DEBUG 10-15 15:28:15 lpmodule.py:368] update for kv cache 780-840 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:15 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:15 lpmodule.py:374] update past key value cost 0.026131 seconds
DEBUG 10-15 15:28:15 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:15 lpmodule.py:227] layer idx 3
DEBUG 10-15 15:28:15 lpmodule.py:399] repeat qkv cost 0.028392 seconds
DEBUG 10-15 15:28:15 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:15 lpmodule.py:433] dot attn cost 0.031889 seconds
DEBUG 10-15 15:28:15 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:15 lpmodule.py:444] time cost move to cuda:1 0.02479100227355957 s
DEBUG 10-15 15:28:15 lpllm.py:2283] CPU attn cost 0.139234 seconds if batch True
DEBUG 10-15 15:28:15 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:15 lpllm.py:2294] CPU compute cost 0.140135 seconds
DEBUG 10-15 15:28:15 lpllm.py:2312] free cost 0.000075 seconds
DEBUG 10-15 15:28:15 lpllm.py:498] reset update attn
DEBUG 10-15 15:28:15 lpllm.py:1774] update state cost 2.4557113647460938e-05 s
DEBUG 10-15 15:28:15 lpllm.py:1743] restore layer func cost 0.00040078163146972656 s
DEBUG 10-15 15:28:15 lpllm.py:511] restore layer cost 0.0006318092346191406 s
DEBUG 10-15 15:28:15 lpllm.py:512] reset_next_layer_need: layer_attn_idx=4, layer_mlp_idx=3, j_loc=7
DEBUG 10-15 15:28:15 lpllm.py:1037] reset layer cost 0.0007207393646240234 s
DEBUG 10-15 15:28:15 lpllm.py:1038] have reset next layer layer_attn 4 layer_mlp 3 
DEBUG 10-15 15:28:15 lpllm.py:1044] j: 7 waiting the layer with layer_idx 4 before wait time 0.4739036560058594 s
INFO 10-15 15:28:15 client.py:117] confirm_model_loaded: Mixtral-8x7B, d0408bc0-0cbd-485a-b6cd-2cccc0689c55
INFO 10-15 15:28:15 client.py:125] Model loaded
DEBUG 10-15 15:28:15 lpllm.py:1048] j: load cost 0.47576308250427246 s waiting cost 0.001844167709350586 s
DEBUG 10-15 15:28:15 lpllm.py:924] 
DEBUG 10-15 15:28:15 lpllm.py:924] decoder loop j: 8 cur_layer_idx: 4 layer_attn_idx: 4 layer_mlp_idx: 3
DEBUG 10-15 15:28:15 lpllm.py:933] start load next layer cur_layer_idx: 5
DEBUG 10-15 15:28:15 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed6e0>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x004\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x98\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:28:15 client.py:72] load_into_gpu: Mixtral-8x7B, f2d04415-3be9-4e4c-a169-4d22d8fc150a
INFO 10-15 15:28:15 client.py:113] Model loaded: Mixtral-8x7B, f2d04415-3be9-4e4c-a169-4d22d8fc150a
DEBUG 10-15 15:28:15 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:15 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:15 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:16 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:16 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:16 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:16 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:16 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:16 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:16 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:16 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:16 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:16 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:16 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:16 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:16 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:16 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:16 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:16 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:16 lpllm.py:2265] GPU2CPU move cost 0.000577 seconds
DEBUG 10-15 15:28:16 lpmodule.py:340] layer_idx 4 attn layer_idx: 4
DEBUG 10-15 15:28:16 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:16 lpmodule.py:364] decoder_attn_batch update batch_dim 720-780
DEBUG 10-15 15:28:16 StaticCacheLen.py:84] static cache update layer_idx: 4, start_batch: 720, end_batch: 780
DEBUG 10-15 15:28:16 lpmodule.py:368] update for kv cache 720-780 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:16 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:16 lpmodule.py:374] update past key value cost 0.024536 seconds
DEBUG 10-15 15:28:16 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:16 lpmodule.py:227] layer idx 3
DEBUG 10-15 15:28:16 lpmodule.py:399] repeat qkv cost 0.030229 seconds
DEBUG 10-15 15:28:16 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:16 lpmodule.py:433] dot attn cost 0.032363 seconds
DEBUG 10-15 15:28:16 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:16 lpmodule.py:444] time cost move to cuda:1 0.024941682815551758 s
DEBUG 10-15 15:28:16 lpllm.py:2283] CPU attn cost 0.140082 seconds if batch True
DEBUG 10-15 15:28:16 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:16 lpllm.py:2294] CPU compute cost 0.140947 seconds
DEBUG 10-15 15:28:16 lpllm.py:2312] free cost 0.000073 seconds
DEBUG 10-15 15:28:16 lpllm.py:503] reset update experts
DEBUG 10-15 15:28:16 lpllm.py:1774] update state cost 3.886222839355469e-05 s
DEBUG 10-15 15:28:16 lpllm.py:1743] restore layer func cost 0.0008420944213867188 s
DEBUG 10-15 15:28:16 lpllm.py:511] restore layer cost 0.0010921955108642578 s
DEBUG 10-15 15:28:16 lpllm.py:512] reset_next_layer_need: layer_attn_idx=4, layer_mlp_idx=4, j_loc=8
DEBUG 10-15 15:28:16 lpllm.py:1037] reset layer cost 0.0011675357818603516 s
DEBUG 10-15 15:28:16 lpllm.py:1038] have reset next layer layer_attn 4 layer_mlp 4 
DEBUG 10-15 15:28:16 lpllm.py:924] 
DEBUG 10-15 15:28:16 lpllm.py:924] decoder loop j: 9 cur_layer_idx: 4 layer_attn_idx: 4 layer_mlp_idx: 4
DEBUG 10-15 15:28:16 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:16 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:16 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:16 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:16 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:16 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:16 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:16 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:16 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:16 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:16 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:16 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:16 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:16 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:16 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:16 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:16 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:16 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:16 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:16 lpllm.py:2265] GPU2CPU move cost 0.000618 seconds
DEBUG 10-15 15:28:16 lpmodule.py:340] layer_idx 4 attn layer_idx: 4
DEBUG 10-15 15:28:16 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:16 lpmodule.py:364] decoder_attn_batch update batch_dim 780-840
DEBUG 10-15 15:28:16 StaticCacheLen.py:84] static cache update layer_idx: 4, start_batch: 780, end_batch: 840
DEBUG 10-15 15:28:16 lpmodule.py:368] update for kv cache 780-840 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:16 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:16 lpmodule.py:374] update past key value cost 0.022830 seconds
DEBUG 10-15 15:28:16 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:16 lpmodule.py:227] layer idx 4
DEBUG 10-15 15:28:16 lpmodule.py:399] repeat qkv cost 0.029111 seconds
DEBUG 10-15 15:28:16 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:16 lpmodule.py:433] dot attn cost 0.031075 seconds
DEBUG 10-15 15:28:16 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:16 lpmodule.py:444] time cost move to cuda:1 0.026012182235717773 s
DEBUG 10-15 15:28:16 lpllm.py:2283] CPU attn cost 0.138661 seconds if batch True
DEBUG 10-15 15:28:16 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:16 lpllm.py:2294] CPU compute cost 0.139554 seconds
DEBUG 10-15 15:28:16 lpllm.py:2312] free cost 0.000074 seconds
DEBUG 10-15 15:28:16 lpllm.py:498] reset update attn
DEBUG 10-15 15:28:16 lpllm.py:1774] update state cost 2.5272369384765625e-05 s
DEBUG 10-15 15:28:16 lpllm.py:1743] restore layer func cost 0.0004143714904785156 s
DEBUG 10-15 15:28:16 lpllm.py:511] restore layer cost 0.0006647109985351562 s
DEBUG 10-15 15:28:16 lpllm.py:512] reset_next_layer_need: layer_attn_idx=5, layer_mlp_idx=4, j_loc=9
DEBUG 10-15 15:28:16 lpllm.py:1037] reset layer cost 0.0007386207580566406 s
DEBUG 10-15 15:28:16 lpllm.py:1038] have reset next layer layer_attn 5 layer_mlp 4 
DEBUG 10-15 15:28:16 lpllm.py:1044] j: 9 waiting the layer with layer_idx 5 before wait time 0.5614879131317139 s
INFO 10-15 15:28:16 client.py:117] confirm_model_loaded: Mixtral-8x7B, f2d04415-3be9-4e4c-a169-4d22d8fc150a
INFO 10-15 15:28:16 client.py:125] Model loaded
DEBUG 10-15 15:28:16 lpllm.py:1048] j: load cost 0.5632944107055664 s waiting cost 0.0017905235290527344 s
DEBUG 10-15 15:28:16 lpllm.py:924] 
DEBUG 10-15 15:28:16 lpllm.py:924] decoder loop j: 10 cur_layer_idx: 5 layer_attn_idx: 5 layer_mlp_idx: 4
DEBUG 10-15 15:28:16 lpllm.py:933] start load next layer cur_layer_idx: 6
DEBUG 10-15 15:28:16 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:28:16 client.py:72] load_into_gpu: Mixtral-8x7B, 6e809d3f-333c-4d23-95b5-bbbb5581cb4b
INFO 10-15 15:28:16 client.py:113] Model loaded: Mixtral-8x7B, 6e809d3f-333c-4d23-95b5-bbbb5581cb4b
DEBUG 10-15 15:28:16 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:16 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:16 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:16 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:16 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:16 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:16 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:16 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:16 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:16 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:16 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:16 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:16 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:16 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:16 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:16 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:16 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:16 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:16 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:16 lpllm.py:2265] GPU2CPU move cost 0.000573 seconds
DEBUG 10-15 15:28:16 lpmodule.py:340] layer_idx 5 attn layer_idx: 5
DEBUG 10-15 15:28:16 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:16 lpmodule.py:364] decoder_attn_batch update batch_dim 720-780
DEBUG 10-15 15:28:16 StaticCacheLen.py:84] static cache update layer_idx: 5, start_batch: 720, end_batch: 780
DEBUG 10-15 15:28:16 lpmodule.py:368] update for kv cache 720-780 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:16 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:16 lpmodule.py:374] update past key value cost 0.023630 seconds
DEBUG 10-15 15:28:16 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:16 lpmodule.py:227] layer idx 4
DEBUG 10-15 15:28:16 lpmodule.py:399] repeat qkv cost 0.028037 seconds
DEBUG 10-15 15:28:16 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:16 lpmodule.py:433] dot attn cost 0.031192 seconds
DEBUG 10-15 15:28:16 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:16 lpmodule.py:444] time cost move to cuda:1 0.025068044662475586 s
DEBUG 10-15 15:28:16 lpllm.py:2283] CPU attn cost 0.139348 seconds if batch True
DEBUG 10-15 15:28:16 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:16 lpllm.py:2294] CPU compute cost 0.140227 seconds
DEBUG 10-15 15:28:16 lpllm.py:2312] free cost 0.000076 seconds
DEBUG 10-15 15:28:16 lpllm.py:503] reset update experts
DEBUG 10-15 15:28:16 lpllm.py:1774] update state cost 2.2411346435546875e-05 s
DEBUG 10-15 15:28:16 lpllm.py:1743] restore layer func cost 0.0008466243743896484 s
DEBUG 10-15 15:28:16 lpllm.py:511] restore layer cost 0.0010826587677001953 s
DEBUG 10-15 15:28:16 lpllm.py:512] reset_next_layer_need: layer_attn_idx=5, layer_mlp_idx=5, j_loc=10
DEBUG 10-15 15:28:16 lpllm.py:1037] reset layer cost 0.0011620521545410156 s
DEBUG 10-15 15:28:16 lpllm.py:1038] have reset next layer layer_attn 5 layer_mlp 5 
DEBUG 10-15 15:28:16 lpllm.py:924] 
DEBUG 10-15 15:28:16 lpllm.py:924] decoder loop j: 11 cur_layer_idx: 5 layer_attn_idx: 5 layer_mlp_idx: 5
DEBUG 10-15 15:28:16 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:16 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:16 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:16 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:16 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:16 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:16 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:16 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:16 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:16 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:16 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:16 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:16 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:16 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:16 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:16 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:16 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:16 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:16 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:16 lpllm.py:2265] GPU2CPU move cost 0.000592 seconds
DEBUG 10-15 15:28:16 lpmodule.py:340] layer_idx 5 attn layer_idx: 5
DEBUG 10-15 15:28:16 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:16 lpmodule.py:364] decoder_attn_batch update batch_dim 780-840
DEBUG 10-15 15:28:16 StaticCacheLen.py:84] static cache update layer_idx: 5, start_batch: 780, end_batch: 840
DEBUG 10-15 15:28:16 lpmodule.py:368] update for kv cache 780-840 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:16 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:16 lpmodule.py:374] update past key value cost 0.023654 seconds
DEBUG 10-15 15:28:16 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:16 lpmodule.py:227] layer idx 5
DEBUG 10-15 15:28:16 lpmodule.py:399] repeat qkv cost 0.029137 seconds
DEBUG 10-15 15:28:16 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:16 lpmodule.py:433] dot attn cost 0.032061 seconds
DEBUG 10-15 15:28:16 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:16 lpmodule.py:444] time cost move to cuda:1 0.024633407592773438 s
DEBUG 10-15 15:28:16 lpllm.py:2283] CPU attn cost 0.137688 seconds if batch True
DEBUG 10-15 15:28:16 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:28:16 lpllm.py:2294] CPU compute cost 0.138599 seconds
DEBUG 10-15 15:28:16 lpllm.py:2312] free cost 0.000072 seconds
DEBUG 10-15 15:28:16 lpllm.py:498] reset update attn
DEBUG 10-15 15:28:16 lpllm.py:1774] update state cost 2.2649765014648438e-05 s
DEBUG 10-15 15:28:16 lpllm.py:1743] restore layer func cost 0.000400543212890625 s
DEBUG 10-15 15:28:16 lpllm.py:511] restore layer cost 0.0006203651428222656 s
DEBUG 10-15 15:28:16 lpllm.py:512] reset_next_layer_need: layer_attn_idx=6, layer_mlp_idx=5, j_loc=11
DEBUG 10-15 15:28:16 lpllm.py:1037] reset layer cost 0.0006952285766601562 s
DEBUG 10-15 15:28:16 lpllm.py:1038] have reset next layer layer_attn 6 layer_mlp 5 
DEBUG 10-15 15:28:16 lpllm.py:1044] j: 11 waiting the layer with layer_idx 6 before wait time 0.44324231147766113 s
INFO 10-15 15:28:16 client.py:117] confirm_model_loaded: Mixtral-8x7B, 6e809d3f-333c-4d23-95b5-bbbb5581cb4b
INFO 10-15 15:28:16 client.py:125] Model loaded
DEBUG 10-15 15:28:16 lpllm.py:1048] j: load cost 0.445343017578125 s waiting cost 0.002084970474243164 s
DEBUG 10-15 15:28:16 lpllm.py:924] 
DEBUG 10-15 15:28:16 lpllm.py:924] decoder loop j: 12 cur_layer_idx: 6 layer_attn_idx: 6 layer_mlp_idx: 5
DEBUG 10-15 15:28:16 lpllm.py:933] start load next layer cur_layer_idx: 7
DEBUG 10-15 15:28:16 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed650>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x003\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x96\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:28:16 client.py:72] load_into_gpu: Mixtral-8x7B, f9378c17-2f23-4425-beab-a4517d21619b
INFO 10-15 15:28:16 client.py:113] Model loaded: Mixtral-8x7B, f9378c17-2f23-4425-beab-a4517d21619b
DEBUG 10-15 15:28:16 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:16 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:16 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:16 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:16 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:16 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:16 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:16 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:16 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:16 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:16 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:16 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:16 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:16 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:16 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:16 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:16 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:16 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:16 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:16 lpllm.py:2265] GPU2CPU move cost 0.000632 seconds
DEBUG 10-15 15:28:16 lpmodule.py:340] layer_idx 6 attn layer_idx: 6
DEBUG 10-15 15:28:16 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:16 lpmodule.py:364] decoder_attn_batch update batch_dim 720-780
DEBUG 10-15 15:28:16 StaticCacheLen.py:84] static cache update layer_idx: 6, start_batch: 720, end_batch: 780
DEBUG 10-15 15:28:16 lpmodule.py:368] update for kv cache 720-780 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:16 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:16 lpmodule.py:374] update past key value cost 0.022640 seconds
DEBUG 10-15 15:28:16 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:16 lpmodule.py:227] layer idx 5
DEBUG 10-15 15:28:16 lpmodule.py:399] repeat qkv cost 0.028507 seconds
DEBUG 10-15 15:28:16 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:16 lpmodule.py:433] dot attn cost 0.032372 seconds
DEBUG 10-15 15:28:16 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:17 lpmodule.py:444] time cost move to cuda:1 0.02521538734436035 s
DEBUG 10-15 15:28:17 lpllm.py:2283] CPU attn cost 0.136555 seconds if batch True
DEBUG 10-15 15:28:17 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:28:17 lpllm.py:2294] CPU compute cost 0.137475 seconds
DEBUG 10-15 15:28:17 lpllm.py:2312] free cost 0.000074 seconds
DEBUG 10-15 15:28:17 lpllm.py:503] reset update experts
DEBUG 10-15 15:28:17 lpllm.py:1774] update state cost 2.5510787963867188e-05 s
DEBUG 10-15 15:28:17 lpllm.py:1743] restore layer func cost 0.0008556842803955078 s
DEBUG 10-15 15:28:17 lpllm.py:511] restore layer cost 0.0010960102081298828 s
DEBUG 10-15 15:28:17 lpllm.py:512] reset_next_layer_need: layer_attn_idx=6, layer_mlp_idx=6, j_loc=12
DEBUG 10-15 15:28:17 lpllm.py:1037] reset layer cost 0.0011887550354003906 s
DEBUG 10-15 15:28:17 lpllm.py:1038] have reset next layer layer_attn 6 layer_mlp 6 
DEBUG 10-15 15:28:17 lpllm.py:924] 
DEBUG 10-15 15:28:17 lpllm.py:924] decoder loop j: 13 cur_layer_idx: 6 layer_attn_idx: 6 layer_mlp_idx: 6
DEBUG 10-15 15:28:17 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:17 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:17 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:17 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:17 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:17 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:17 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:17 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:17 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:17 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:17 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:17 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:17 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:17 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:17 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:17 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:17 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:17 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:17 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:17 lpllm.py:2265] GPU2CPU move cost 0.000570 seconds
DEBUG 10-15 15:28:17 lpmodule.py:340] layer_idx 6 attn layer_idx: 6
DEBUG 10-15 15:28:17 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:17 lpmodule.py:364] decoder_attn_batch update batch_dim 780-840
DEBUG 10-15 15:28:17 StaticCacheLen.py:84] static cache update layer_idx: 6, start_batch: 780, end_batch: 840
DEBUG 10-15 15:28:17 lpmodule.py:368] update for kv cache 780-840 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:17 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:17 lpmodule.py:374] update past key value cost 0.024391 seconds
DEBUG 10-15 15:28:17 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:17 lpmodule.py:227] layer idx 6
DEBUG 10-15 15:28:17 lpmodule.py:399] repeat qkv cost 0.029099 seconds
DEBUG 10-15 15:28:17 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:17 lpmodule.py:433] dot attn cost 0.032629 seconds
DEBUG 10-15 15:28:17 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:17 lpmodule.py:444] time cost move to cuda:1 0.024631023406982422 s
DEBUG 10-15 15:28:17 lpllm.py:2283] CPU attn cost 0.138891 seconds if batch True
DEBUG 10-15 15:28:17 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:17 lpllm.py:2294] CPU compute cost 0.139745 seconds
DEBUG 10-15 15:28:17 lpllm.py:2312] free cost 0.000072 seconds
DEBUG 10-15 15:28:17 lpllm.py:498] reset update attn
DEBUG 10-15 15:28:17 lpllm.py:1774] update state cost 2.5033950805664062e-05 s
DEBUG 10-15 15:28:17 lpllm.py:1743] restore layer func cost 0.00040459632873535156 s
DEBUG 10-15 15:28:17 lpllm.py:511] restore layer cost 0.0006518363952636719 s
DEBUG 10-15 15:28:17 lpllm.py:512] reset_next_layer_need: layer_attn_idx=7, layer_mlp_idx=6, j_loc=13
DEBUG 10-15 15:28:17 lpllm.py:1037] reset layer cost 0.0007398128509521484 s
DEBUG 10-15 15:28:17 lpllm.py:1038] have reset next layer layer_attn 7 layer_mlp 6 
DEBUG 10-15 15:28:17 lpllm.py:1044] j: 13 waiting the layer with layer_idx 7 before wait time 0.4427490234375 s
INFO 10-15 15:28:17 client.py:117] confirm_model_loaded: Mixtral-8x7B, f9378c17-2f23-4425-beab-a4517d21619b
INFO 10-15 15:28:17 client.py:125] Model loaded
DEBUG 10-15 15:28:17 lpllm.py:1048] j: load cost 0.4446239471435547 s waiting cost 0.0018587112426757812 s
DEBUG 10-15 15:28:17 lpllm.py:924] 
DEBUG 10-15 15:28:17 lpllm.py:924] decoder loop j: 14 cur_layer_idx: 7 layer_attn_idx: 7 layer_mlp_idx: 6
DEBUG 10-15 15:28:17 lpllm.py:933] start load next layer cur_layer_idx: 8
DEBUG 10-15 15:28:17 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed6e0>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x004\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x98\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:28:17 client.py:72] load_into_gpu: Mixtral-8x7B, 470c789f-c665-4fb7-b3f8-9b0ae7f4ae8f
INFO 10-15 15:28:17 client.py:113] Model loaded: Mixtral-8x7B, 470c789f-c665-4fb7-b3f8-9b0ae7f4ae8f
DEBUG 10-15 15:28:17 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:17 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:17 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:17 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:17 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:17 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:17 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:17 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:17 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:17 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:17 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:17 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:17 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:17 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:17 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:17 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:17 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:17 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:17 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:17 lpllm.py:2265] GPU2CPU move cost 0.000571 seconds
DEBUG 10-15 15:28:17 lpmodule.py:340] layer_idx 7 attn layer_idx: 7
DEBUG 10-15 15:28:17 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:17 lpmodule.py:364] decoder_attn_batch update batch_dim 720-780
DEBUG 10-15 15:28:17 StaticCacheLen.py:84] static cache update layer_idx: 7, start_batch: 720, end_batch: 780
DEBUG 10-15 15:28:17 lpmodule.py:368] update for kv cache 720-780 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:17 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:17 lpmodule.py:374] update past key value cost 0.021674 seconds
DEBUG 10-15 15:28:17 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:17 lpmodule.py:227] layer idx 6
DEBUG 10-15 15:28:17 lpmodule.py:399] repeat qkv cost 0.028811 seconds
DEBUG 10-15 15:28:17 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:17 lpmodule.py:433] dot attn cost 0.031815 seconds
DEBUG 10-15 15:28:17 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:17 lpmodule.py:444] time cost move to cuda:1 0.02480459213256836 s
DEBUG 10-15 15:28:17 lpllm.py:2283] CPU attn cost 0.135300 seconds if batch True
DEBUG 10-15 15:28:17 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:28:17 lpllm.py:2294] CPU compute cost 0.136161 seconds
DEBUG 10-15 15:28:17 lpllm.py:2312] free cost 0.000076 seconds
DEBUG 10-15 15:28:17 lpllm.py:503] reset update experts
DEBUG 10-15 15:28:17 lpllm.py:1774] update state cost 3.6716461181640625e-05 s
DEBUG 10-15 15:28:17 lpllm.py:1743] restore layer func cost 0.0008320808410644531 s
DEBUG 10-15 15:28:17 lpllm.py:511] restore layer cost 0.0011031627655029297 s
DEBUG 10-15 15:28:17 lpllm.py:512] reset_next_layer_need: layer_attn_idx=7, layer_mlp_idx=7, j_loc=14
DEBUG 10-15 15:28:17 lpllm.py:1037] reset layer cost 0.0011756420135498047 s
DEBUG 10-15 15:28:17 lpllm.py:1038] have reset next layer layer_attn 7 layer_mlp 7 
DEBUG 10-15 15:28:17 lpllm.py:924] 
DEBUG 10-15 15:28:17 lpllm.py:924] decoder loop j: 15 cur_layer_idx: 7 layer_attn_idx: 7 layer_mlp_idx: 7
DEBUG 10-15 15:28:17 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:17 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:17 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:17 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:17 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:17 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:17 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:17 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:17 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:17 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:17 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:17 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:17 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:17 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:17 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:17 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:17 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:17 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:17 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:17 lpllm.py:2265] GPU2CPU move cost 0.000633 seconds
DEBUG 10-15 15:28:17 lpmodule.py:340] layer_idx 7 attn layer_idx: 7
DEBUG 10-15 15:28:17 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:17 lpmodule.py:364] decoder_attn_batch update batch_dim 780-840
DEBUG 10-15 15:28:17 StaticCacheLen.py:84] static cache update layer_idx: 7, start_batch: 780, end_batch: 840
DEBUG 10-15 15:28:17 lpmodule.py:368] update for kv cache 780-840 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:17 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:17 lpmodule.py:374] update past key value cost 0.022561 seconds
DEBUG 10-15 15:28:17 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:17 lpmodule.py:227] layer idx 7
DEBUG 10-15 15:28:17 lpmodule.py:399] repeat qkv cost 0.028625 seconds
DEBUG 10-15 15:28:17 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:17 lpmodule.py:433] dot attn cost 0.032273 seconds
DEBUG 10-15 15:28:17 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:17 lpmodule.py:444] time cost move to cuda:1 0.0254213809967041 s
DEBUG 10-15 15:28:17 lpllm.py:2283] CPU attn cost 0.137199 seconds if batch True
DEBUG 10-15 15:28:17 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:17 lpllm.py:2294] CPU compute cost 0.138136 seconds
DEBUG 10-15 15:28:17 lpllm.py:2312] free cost 0.000078 seconds
DEBUG 10-15 15:28:17 lpllm.py:498] reset update attn
DEBUG 10-15 15:28:17 lpllm.py:1774] update state cost 4.029273986816406e-05 s
DEBUG 10-15 15:28:17 lpllm.py:1743] restore layer func cost 0.00039267539978027344 s
DEBUG 10-15 15:28:17 lpllm.py:511] restore layer cost 0.0006508827209472656 s
DEBUG 10-15 15:28:17 lpllm.py:512] reset_next_layer_need: layer_attn_idx=8, layer_mlp_idx=7, j_loc=15
DEBUG 10-15 15:28:17 lpllm.py:1037] reset layer cost 0.0007219314575195312 s
DEBUG 10-15 15:28:17 lpllm.py:1038] have reset next layer layer_attn 8 layer_mlp 7 
DEBUG 10-15 15:28:17 lpllm.py:1044] j: 15 waiting the layer with layer_idx 8 before wait time 0.442521333694458 s
INFO 10-15 15:28:17 client.py:117] confirm_model_loaded: Mixtral-8x7B, 470c789f-c665-4fb7-b3f8-9b0ae7f4ae8f
INFO 10-15 15:28:17 client.py:125] Model loaded
DEBUG 10-15 15:28:17 lpllm.py:1048] j: load cost 0.4443502426147461 s waiting cost 0.0018131732940673828 s
DEBUG 10-15 15:28:17 lpllm.py:924] 
DEBUG 10-15 15:28:17 lpllm.py:924] decoder loop j: 16 cur_layer_idx: 8 layer_attn_idx: 8 layer_mlp_idx: 7
DEBUG 10-15 15:28:17 lpllm.py:933] start load next layer cur_layer_idx: 9
DEBUG 10-15 15:28:17 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:28:17 client.py:72] load_into_gpu: Mixtral-8x7B, 6d8d64fd-93f5-4cea-8588-e8394783c3c8
INFO 10-15 15:28:17 client.py:113] Model loaded: Mixtral-8x7B, 6d8d64fd-93f5-4cea-8588-e8394783c3c8
DEBUG 10-15 15:28:17 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:17 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:17 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:17 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:17 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:17 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:17 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:17 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:17 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:17 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:17 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:17 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:17 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:17 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:17 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:17 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:17 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:17 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:17 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:17 lpllm.py:2265] GPU2CPU move cost 0.000576 seconds
DEBUG 10-15 15:28:17 lpmodule.py:340] layer_idx 8 attn layer_idx: 8
DEBUG 10-15 15:28:17 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:17 lpmodule.py:364] decoder_attn_batch update batch_dim 720-780
DEBUG 10-15 15:28:17 StaticCacheLen.py:84] static cache update layer_idx: 8, start_batch: 720, end_batch: 780
DEBUG 10-15 15:28:17 lpmodule.py:368] update for kv cache 720-780 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:17 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:17 lpmodule.py:374] update past key value cost 0.022769 seconds
DEBUG 10-15 15:28:17 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:17 lpmodule.py:227] layer idx 7
DEBUG 10-15 15:28:17 lpmodule.py:399] repeat qkv cost 0.028378 seconds
DEBUG 10-15 15:28:17 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:17 lpmodule.py:433] dot attn cost 0.031398 seconds
DEBUG 10-15 15:28:17 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:17 lpmodule.py:444] time cost move to cuda:1 0.024820566177368164 s
DEBUG 10-15 15:28:17 lpllm.py:2283] CPU attn cost 0.136117 seconds if batch True
DEBUG 10-15 15:28:17 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:17 lpllm.py:2294] CPU compute cost 0.136979 seconds
DEBUG 10-15 15:28:17 lpllm.py:2312] free cost 0.000074 seconds
DEBUG 10-15 15:28:17 lpllm.py:503] reset update experts
DEBUG 10-15 15:28:17 lpllm.py:1774] update state cost 3.933906555175781e-05 s
DEBUG 10-15 15:28:17 lpllm.py:1743] restore layer func cost 0.0008561611175537109 s
DEBUG 10-15 15:28:17 lpllm.py:511] restore layer cost 0.0011227130889892578 s
DEBUG 10-15 15:28:17 lpllm.py:512] reset_next_layer_need: layer_attn_idx=8, layer_mlp_idx=8, j_loc=16
DEBUG 10-15 15:28:17 lpllm.py:1037] reset layer cost 0.0011947154998779297 s
DEBUG 10-15 15:28:17 lpllm.py:1038] have reset next layer layer_attn 8 layer_mlp 8 
DEBUG 10-15 15:28:17 lpllm.py:924] 
DEBUG 10-15 15:28:17 lpllm.py:924] decoder loop j: 17 cur_layer_idx: 8 layer_attn_idx: 8 layer_mlp_idx: 8
DEBUG 10-15 15:28:17 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:17 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:17 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:18 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:18 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:18 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:18 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:18 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:18 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:18 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:18 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:18 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:18 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:18 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:18 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:18 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:18 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:18 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:18 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:18 lpllm.py:2265] GPU2CPU move cost 0.000616 seconds
DEBUG 10-15 15:28:18 lpmodule.py:340] layer_idx 8 attn layer_idx: 8
DEBUG 10-15 15:28:18 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:18 lpmodule.py:364] decoder_attn_batch update batch_dim 780-840
DEBUG 10-15 15:28:18 StaticCacheLen.py:84] static cache update layer_idx: 8, start_batch: 780, end_batch: 840
DEBUG 10-15 15:28:18 lpmodule.py:368] update for kv cache 780-840 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:18 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:18 lpmodule.py:374] update past key value cost 0.025979 seconds
DEBUG 10-15 15:28:18 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:18 lpmodule.py:227] layer idx 8
DEBUG 10-15 15:28:18 lpmodule.py:399] repeat qkv cost 0.031271 seconds
DEBUG 10-15 15:28:18 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:18 lpmodule.py:433] dot attn cost 0.031956 seconds
DEBUG 10-15 15:28:18 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:18 lpmodule.py:444] time cost move to cuda:1 0.02462005615234375 s
DEBUG 10-15 15:28:18 lpllm.py:2283] CPU attn cost 0.142424 seconds if batch True
DEBUG 10-15 15:28:18 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:18 lpllm.py:2294] CPU compute cost 0.143336 seconds
DEBUG 10-15 15:28:18 lpllm.py:2312] free cost 0.000074 seconds
DEBUG 10-15 15:28:18 lpllm.py:498] reset update attn
DEBUG 10-15 15:28:18 lpllm.py:1774] update state cost 2.7179718017578125e-05 s
DEBUG 10-15 15:28:18 lpllm.py:1743] restore layer func cost 0.00039839744567871094 s
DEBUG 10-15 15:28:18 lpllm.py:511] restore layer cost 0.0006358623504638672 s
DEBUG 10-15 15:28:18 lpllm.py:512] reset_next_layer_need: layer_attn_idx=9, layer_mlp_idx=8, j_loc=17
DEBUG 10-15 15:28:18 lpllm.py:1037] reset layer cost 0.0007088184356689453 s
DEBUG 10-15 15:28:18 lpllm.py:1038] have reset next layer layer_attn 9 layer_mlp 8 
DEBUG 10-15 15:28:18 lpllm.py:1044] j: 17 waiting the layer with layer_idx 9 before wait time 0.44226741790771484 s
INFO 10-15 15:28:18 client.py:117] confirm_model_loaded: Mixtral-8x7B, 6d8d64fd-93f5-4cea-8588-e8394783c3c8
INFO 10-15 15:28:18 client.py:125] Model loaded
DEBUG 10-15 15:28:18 lpllm.py:1048] j: load cost 0.443953275680542 s waiting cost 0.00167083740234375 s
DEBUG 10-15 15:28:18 lpllm.py:924] 
DEBUG 10-15 15:28:18 lpllm.py:924] decoder loop j: 18 cur_layer_idx: 9 layer_attn_idx: 9 layer_mlp_idx: 8
DEBUG 10-15 15:28:18 lpllm.py:933] start load next layer cur_layer_idx: 10
DEBUG 10-15 15:28:18 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed650>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x003\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x96\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:28:18 client.py:72] load_into_gpu: Mixtral-8x7B, 1c9cd3c2-982e-45b9-91f2-fc133154ddae
INFO 10-15 15:28:18 client.py:113] Model loaded: Mixtral-8x7B, 1c9cd3c2-982e-45b9-91f2-fc133154ddae
DEBUG 10-15 15:28:18 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:18 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:18 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:18 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:18 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:18 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:18 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:18 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:18 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:18 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:18 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:18 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:18 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:18 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:18 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:18 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:18 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:18 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:18 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:18 lpllm.py:2265] GPU2CPU move cost 0.000580 seconds
DEBUG 10-15 15:28:18 lpmodule.py:340] layer_idx 9 attn layer_idx: 9
DEBUG 10-15 15:28:18 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:18 lpmodule.py:364] decoder_attn_batch update batch_dim 720-780
DEBUG 10-15 15:28:18 StaticCacheLen.py:84] static cache update layer_idx: 9, start_batch: 720, end_batch: 780
DEBUG 10-15 15:28:18 lpmodule.py:368] update for kv cache 720-780 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:18 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:18 lpmodule.py:374] update past key value cost 0.025476 seconds
DEBUG 10-15 15:28:18 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:18 lpmodule.py:227] layer idx 8
DEBUG 10-15 15:28:18 lpmodule.py:399] repeat qkv cost 0.028962 seconds
DEBUG 10-15 15:28:18 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:18 lpmodule.py:433] dot attn cost 0.031370 seconds
DEBUG 10-15 15:28:18 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:18 lpmodule.py:444] time cost move to cuda:1 0.024561405181884766 s
DEBUG 10-15 15:28:18 lpllm.py:2283] CPU attn cost 0.139892 seconds if batch True
DEBUG 10-15 15:28:18 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:28:18 lpllm.py:2294] CPU compute cost 0.140771 seconds
DEBUG 10-15 15:28:18 lpllm.py:2312] free cost 0.000074 seconds
DEBUG 10-15 15:28:18 lpllm.py:503] reset update experts
DEBUG 10-15 15:28:18 lpllm.py:1774] update state cost 2.3603439331054688e-05 s
DEBUG 10-15 15:28:18 lpllm.py:1743] restore layer func cost 0.0008411407470703125 s
DEBUG 10-15 15:28:18 lpllm.py:511] restore layer cost 0.001102447509765625 s
DEBUG 10-15 15:28:18 lpllm.py:512] reset_next_layer_need: layer_attn_idx=9, layer_mlp_idx=9, j_loc=18
DEBUG 10-15 15:28:18 lpllm.py:1037] reset layer cost 0.0011739730834960938 s
DEBUG 10-15 15:28:18 lpllm.py:1038] have reset next layer layer_attn 9 layer_mlp 9 
DEBUG 10-15 15:28:18 lpllm.py:924] 
DEBUG 10-15 15:28:18 lpllm.py:924] decoder loop j: 19 cur_layer_idx: 9 layer_attn_idx: 9 layer_mlp_idx: 9
DEBUG 10-15 15:28:18 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:18 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:18 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:18 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:18 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:18 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:18 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:18 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:18 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:18 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:18 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:18 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:18 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:18 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:18 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:18 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:18 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:18 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:18 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:18 lpllm.py:2265] GPU2CPU move cost 0.000582 seconds
DEBUG 10-15 15:28:18 lpmodule.py:340] layer_idx 9 attn layer_idx: 9
DEBUG 10-15 15:28:18 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:18 lpmodule.py:364] decoder_attn_batch update batch_dim 780-840
DEBUG 10-15 15:28:18 StaticCacheLen.py:84] static cache update layer_idx: 9, start_batch: 780, end_batch: 840
DEBUG 10-15 15:28:18 lpmodule.py:368] update for kv cache 780-840 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:18 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:18 lpmodule.py:374] update past key value cost 0.027940 seconds
DEBUG 10-15 15:28:18 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:18 lpmodule.py:227] layer idx 9
DEBUG 10-15 15:28:18 lpmodule.py:399] repeat qkv cost 0.029941 seconds
DEBUG 10-15 15:28:18 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:18 lpmodule.py:433] dot attn cost 0.030897 seconds
DEBUG 10-15 15:28:18 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:18 lpmodule.py:444] time cost move to cuda:1 0.02471327781677246 s
DEBUG 10-15 15:28:18 lpllm.py:2283] CPU attn cost 0.141795 seconds if batch True
DEBUG 10-15 15:28:18 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:28:18 lpllm.py:2294] CPU compute cost 0.142681 seconds
DEBUG 10-15 15:28:18 lpllm.py:2312] free cost 0.000073 seconds
DEBUG 10-15 15:28:18 lpllm.py:498] reset update attn
DEBUG 10-15 15:28:18 lpllm.py:1774] update state cost 2.3603439331054688e-05 s
DEBUG 10-15 15:28:18 lpllm.py:1743] restore layer func cost 0.0003924369812011719 s
DEBUG 10-15 15:28:18 lpllm.py:511] restore layer cost 0.0006256103515625 s
DEBUG 10-15 15:28:18 lpllm.py:512] reset_next_layer_need: layer_attn_idx=10, layer_mlp_idx=9, j_loc=19
DEBUG 10-15 15:28:18 lpllm.py:1037] reset layer cost 0.0007138252258300781 s
DEBUG 10-15 15:28:18 lpllm.py:1038] have reset next layer layer_attn 10 layer_mlp 9 
DEBUG 10-15 15:28:18 lpllm.py:1044] j: 19 waiting the layer with layer_idx 10 before wait time 0.4552173614501953 s
INFO 10-15 15:28:18 client.py:117] confirm_model_loaded: Mixtral-8x7B, 1c9cd3c2-982e-45b9-91f2-fc133154ddae
INFO 10-15 15:28:18 client.py:125] Model loaded
DEBUG 10-15 15:28:18 lpllm.py:1048] j: load cost 0.4569728374481201 s waiting cost 0.0017404556274414062 s
DEBUG 10-15 15:28:18 lpllm.py:924] 
DEBUG 10-15 15:28:18 lpllm.py:924] decoder loop j: 20 cur_layer_idx: 10 layer_attn_idx: 10 layer_mlp_idx: 9
DEBUG 10-15 15:28:18 lpllm.py:933] start load next layer cur_layer_idx: 11
DEBUG 10-15 15:28:18 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed6e0>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x004\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x98\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:28:18 client.py:72] load_into_gpu: Mixtral-8x7B, 0d6d05ec-887d-4f4c-8882-297e83b883a7
INFO 10-15 15:28:18 client.py:113] Model loaded: Mixtral-8x7B, 0d6d05ec-887d-4f4c-8882-297e83b883a7
DEBUG 10-15 15:28:18 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:18 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:18 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:18 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:18 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:18 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:18 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:18 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:18 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:18 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:18 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:18 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:18 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:18 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:18 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:18 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:18 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:18 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:18 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:18 lpllm.py:2265] GPU2CPU move cost 0.000613 seconds
DEBUG 10-15 15:28:18 lpmodule.py:340] layer_idx 10 attn layer_idx: 10
DEBUG 10-15 15:28:18 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:18 lpmodule.py:364] decoder_attn_batch update batch_dim 720-780
DEBUG 10-15 15:28:18 StaticCacheLen.py:84] static cache update layer_idx: 10, start_batch: 720, end_batch: 780
DEBUG 10-15 15:28:18 lpmodule.py:368] update for kv cache 720-780 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:18 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:18 lpmodule.py:374] update past key value cost 0.025263 seconds
DEBUG 10-15 15:28:18 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:18 lpmodule.py:227] layer idx 9
DEBUG 10-15 15:28:18 lpmodule.py:399] repeat qkv cost 0.029464 seconds
DEBUG 10-15 15:28:18 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:18 lpmodule.py:433] dot attn cost 0.031039 seconds
DEBUG 10-15 15:28:18 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:18 lpmodule.py:444] time cost move to cuda:1 0.024820804595947266 s
DEBUG 10-15 15:28:18 lpllm.py:2283] CPU attn cost 0.140520 seconds if batch True
DEBUG 10-15 15:28:18 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:18 lpllm.py:2294] CPU compute cost 0.141415 seconds
DEBUG 10-15 15:28:18 lpllm.py:2312] free cost 0.000072 seconds
DEBUG 10-15 15:28:18 lpllm.py:503] reset update experts
DEBUG 10-15 15:28:18 lpllm.py:1774] update state cost 2.384185791015625e-05 s
DEBUG 10-15 15:28:18 lpllm.py:1743] restore layer func cost 0.0008213520050048828 s
DEBUG 10-15 15:28:18 lpllm.py:511] restore layer cost 0.001058816909790039 s
DEBUG 10-15 15:28:18 lpllm.py:512] reset_next_layer_need: layer_attn_idx=10, layer_mlp_idx=10, j_loc=20
DEBUG 10-15 15:28:18 lpllm.py:1037] reset layer cost 0.0011441707611083984 s
DEBUG 10-15 15:28:18 lpllm.py:1038] have reset next layer layer_attn 10 layer_mlp 10 
DEBUG 10-15 15:28:18 lpllm.py:924] 
DEBUG 10-15 15:28:18 lpllm.py:924] decoder loop j: 21 cur_layer_idx: 10 layer_attn_idx: 10 layer_mlp_idx: 10
DEBUG 10-15 15:28:18 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:18 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:18 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:18 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:18 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:18 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:18 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:18 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:18 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:18 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:18 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:18 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:18 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:18 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:18 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:18 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:18 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:18 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:18 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:18 lpllm.py:2265] GPU2CPU move cost 0.000572 seconds
DEBUG 10-15 15:28:18 lpmodule.py:340] layer_idx 10 attn layer_idx: 10
DEBUG 10-15 15:28:18 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:18 lpmodule.py:364] decoder_attn_batch update batch_dim 780-840
DEBUG 10-15 15:28:18 StaticCacheLen.py:84] static cache update layer_idx: 10, start_batch: 780, end_batch: 840
DEBUG 10-15 15:28:18 lpmodule.py:368] update for kv cache 780-840 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:18 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:18 lpmodule.py:374] update past key value cost 0.025787 seconds
DEBUG 10-15 15:28:18 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:18 lpmodule.py:227] layer idx 10
DEBUG 10-15 15:28:18 lpmodule.py:399] repeat qkv cost 0.030166 seconds
DEBUG 10-15 15:28:18 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:19 lpmodule.py:433] dot attn cost 0.032123 seconds
DEBUG 10-15 15:28:19 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:19 lpmodule.py:444] time cost move to cuda:1 0.028463363647460938 s
DEBUG 10-15 15:28:19 lpllm.py:2283] CPU attn cost 0.144744 seconds if batch True
DEBUG 10-15 15:28:19 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:28:19 lpllm.py:2294] CPU compute cost 0.145598 seconds
DEBUG 10-15 15:28:19 lpllm.py:2312] free cost 0.000073 seconds
DEBUG 10-15 15:28:19 lpllm.py:498] reset update attn
DEBUG 10-15 15:28:19 lpllm.py:1774] update state cost 2.5033950805664062e-05 s
DEBUG 10-15 15:28:19 lpllm.py:1743] restore layer func cost 0.0003998279571533203 s
DEBUG 10-15 15:28:19 lpllm.py:511] restore layer cost 0.0006372928619384766 s
DEBUG 10-15 15:28:19 lpllm.py:512] reset_next_layer_need: layer_attn_idx=11, layer_mlp_idx=10, j_loc=21
DEBUG 10-15 15:28:19 lpllm.py:1037] reset layer cost 0.00072479248046875 s
DEBUG 10-15 15:28:19 lpllm.py:1038] have reset next layer layer_attn 11 layer_mlp 10 
DEBUG 10-15 15:28:19 lpllm.py:1044] j: 21 waiting the layer with layer_idx 11 before wait time 0.4522581100463867 s
INFO 10-15 15:28:19 client.py:117] confirm_model_loaded: Mixtral-8x7B, 0d6d05ec-887d-4f4c-8882-297e83b883a7
INFO 10-15 15:28:19 client.py:125] Model loaded
DEBUG 10-15 15:28:19 lpllm.py:1048] j: load cost 0.4539518356323242 s waiting cost 0.0016775131225585938 s
DEBUG 10-15 15:28:19 lpllm.py:924] 
DEBUG 10-15 15:28:19 lpllm.py:924] decoder loop j: 22 cur_layer_idx: 11 layer_attn_idx: 11 layer_mlp_idx: 10
DEBUG 10-15 15:28:19 lpllm.py:933] start load next layer cur_layer_idx: 12
DEBUG 10-15 15:28:19 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:28:19 client.py:72] load_into_gpu: Mixtral-8x7B, 86780173-f0a9-4a99-a81d-46e18dbea012
INFO 10-15 15:28:19 client.py:113] Model loaded: Mixtral-8x7B, 86780173-f0a9-4a99-a81d-46e18dbea012
DEBUG 10-15 15:28:19 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:19 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:19 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:19 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:19 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:19 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:19 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:19 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:19 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:19 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:19 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:19 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:19 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:19 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:19 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:19 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:19 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:19 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:19 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:19 lpllm.py:2265] GPU2CPU move cost 0.000608 seconds
DEBUG 10-15 15:28:19 lpmodule.py:340] layer_idx 11 attn layer_idx: 11
DEBUG 10-15 15:28:19 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:19 lpmodule.py:364] decoder_attn_batch update batch_dim 720-780
DEBUG 10-15 15:28:19 StaticCacheLen.py:84] static cache update layer_idx: 11, start_batch: 720, end_batch: 780
DEBUG 10-15 15:28:19 lpmodule.py:368] update for kv cache 720-780 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:19 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:19 lpmodule.py:374] update past key value cost 0.025761 seconds
DEBUG 10-15 15:28:19 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:19 lpmodule.py:227] layer idx 10
DEBUG 10-15 15:28:19 lpmodule.py:399] repeat qkv cost 0.029411 seconds
DEBUG 10-15 15:28:19 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:19 lpmodule.py:433] dot attn cost 0.032174 seconds
DEBUG 10-15 15:28:19 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:19 lpmodule.py:444] time cost move to cuda:1 0.024860858917236328 s
DEBUG 10-15 15:28:19 lpllm.py:2283] CPU attn cost 0.141280 seconds if batch True
DEBUG 10-15 15:28:19 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:19 lpllm.py:2294] CPU compute cost 0.142174 seconds
DEBUG 10-15 15:28:19 lpllm.py:2312] free cost 0.000075 seconds
DEBUG 10-15 15:28:19 lpllm.py:503] reset update experts
DEBUG 10-15 15:28:19 lpllm.py:1774] update state cost 2.2411346435546875e-05 s
DEBUG 10-15 15:28:19 lpllm.py:1743] restore layer func cost 0.000843048095703125 s
DEBUG 10-15 15:28:19 lpllm.py:511] restore layer cost 0.0010983943939208984 s
DEBUG 10-15 15:28:19 lpllm.py:512] reset_next_layer_need: layer_attn_idx=11, layer_mlp_idx=11, j_loc=22
DEBUG 10-15 15:28:19 lpllm.py:1037] reset layer cost 0.0011713504791259766 s
DEBUG 10-15 15:28:19 lpllm.py:1038] have reset next layer layer_attn 11 layer_mlp 11 
DEBUG 10-15 15:28:19 lpllm.py:924] 
DEBUG 10-15 15:28:19 lpllm.py:924] decoder loop j: 23 cur_layer_idx: 11 layer_attn_idx: 11 layer_mlp_idx: 11
DEBUG 10-15 15:28:19 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:19 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:19 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:19 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:19 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:19 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:19 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:19 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:19 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:19 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:19 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:19 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:19 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:19 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:19 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:19 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:19 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:19 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:19 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:19 lpllm.py:2265] GPU2CPU move cost 0.000597 seconds
DEBUG 10-15 15:28:19 lpmodule.py:340] layer_idx 11 attn layer_idx: 11
DEBUG 10-15 15:28:19 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:19 lpmodule.py:364] decoder_attn_batch update batch_dim 780-840
DEBUG 10-15 15:28:19 StaticCacheLen.py:84] static cache update layer_idx: 11, start_batch: 780, end_batch: 840
DEBUG 10-15 15:28:19 lpmodule.py:368] update for kv cache 780-840 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:19 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:19 lpmodule.py:374] update past key value cost 0.024326 seconds
DEBUG 10-15 15:28:19 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:19 lpmodule.py:227] layer idx 11
DEBUG 10-15 15:28:19 lpmodule.py:399] repeat qkv cost 0.029187 seconds
DEBUG 10-15 15:28:19 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:19 lpmodule.py:433] dot attn cost 0.032029 seconds
DEBUG 10-15 15:28:19 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:19 lpmodule.py:444] time cost move to cuda:1 0.05505180358886719 s
DEBUG 10-15 15:28:19 lpllm.py:2283] CPU attn cost 0.175848 seconds if batch True
DEBUG 10-15 15:28:19 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:19 lpllm.py:2294] CPU compute cost 0.176746 seconds
DEBUG 10-15 15:28:19 lpllm.py:2312] free cost 0.000075 seconds
DEBUG 10-15 15:28:19 lpllm.py:498] reset update attn
DEBUG 10-15 15:28:19 lpllm.py:1774] update state cost 2.3126602172851562e-05 s
DEBUG 10-15 15:28:19 lpllm.py:1743] restore layer func cost 0.0003914833068847656 s
DEBUG 10-15 15:28:19 lpllm.py:511] restore layer cost 0.0006365776062011719 s
DEBUG 10-15 15:28:19 lpllm.py:512] reset_next_layer_need: layer_attn_idx=12, layer_mlp_idx=11, j_loc=23
DEBUG 10-15 15:28:19 lpllm.py:1037] reset layer cost 0.0007307529449462891 s
DEBUG 10-15 15:28:19 lpllm.py:1038] have reset next layer layer_attn 12 layer_mlp 11 
DEBUG 10-15 15:28:19 lpllm.py:1044] j: 23 waiting the layer with layer_idx 12 before wait time 0.47078585624694824 s
INFO 10-15 15:28:19 client.py:117] confirm_model_loaded: Mixtral-8x7B, 86780173-f0a9-4a99-a81d-46e18dbea012
INFO 10-15 15:28:19 client.py:125] Model loaded
DEBUG 10-15 15:28:19 lpllm.py:1048] j: load cost 0.47257518768310547 s waiting cost 0.0017728805541992188 s
DEBUG 10-15 15:28:19 lpllm.py:924] 
DEBUG 10-15 15:28:19 lpllm.py:924] decoder loop j: 24 cur_layer_idx: 12 layer_attn_idx: 12 layer_mlp_idx: 11
DEBUG 10-15 15:28:19 lpllm.py:933] start load next layer cur_layer_idx: 13
DEBUG 10-15 15:28:19 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed650>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x003\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x96\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:28:19 client.py:72] load_into_gpu: Mixtral-8x7B, 7742ca8c-72e5-4a77-8793-a9e1bc8c3af0
INFO 10-15 15:28:19 client.py:113] Model loaded: Mixtral-8x7B, 7742ca8c-72e5-4a77-8793-a9e1bc8c3af0
DEBUG 10-15 15:28:19 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:19 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:19 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:19 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:19 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:19 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:19 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:19 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:19 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:19 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:19 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:19 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:19 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:19 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:19 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:19 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:19 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:19 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:19 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:19 lpllm.py:2265] GPU2CPU move cost 0.000610 seconds
DEBUG 10-15 15:28:19 lpmodule.py:340] layer_idx 12 attn layer_idx: 12
DEBUG 10-15 15:28:19 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:19 lpmodule.py:364] decoder_attn_batch update batch_dim 720-780
DEBUG 10-15 15:28:19 StaticCacheLen.py:84] static cache update layer_idx: 12, start_batch: 720, end_batch: 780
DEBUG 10-15 15:28:19 lpmodule.py:368] update for kv cache 720-780 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:19 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:19 lpmodule.py:374] update past key value cost 0.025399 seconds
DEBUG 10-15 15:28:19 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:19 lpmodule.py:227] layer idx 11
DEBUG 10-15 15:28:19 lpmodule.py:399] repeat qkv cost 0.029511 seconds
DEBUG 10-15 15:28:19 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:19 lpmodule.py:433] dot attn cost 0.031755 seconds
DEBUG 10-15 15:28:19 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:19 lpmodule.py:444] time cost move to cuda:1 0.024724483489990234 s
DEBUG 10-15 15:28:19 lpllm.py:2283] CPU attn cost 0.141504 seconds if batch True
DEBUG 10-15 15:28:19 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:19 lpllm.py:2294] CPU compute cost 0.142392 seconds
DEBUG 10-15 15:28:19 lpllm.py:2312] free cost 0.000072 seconds
DEBUG 10-15 15:28:19 lpllm.py:503] reset update experts
DEBUG 10-15 15:28:19 lpllm.py:1774] update state cost 2.9325485229492188e-05 s
DEBUG 10-15 15:28:19 lpllm.py:1743] restore layer func cost 0.0008382797241210938 s
DEBUG 10-15 15:28:19 lpllm.py:511] restore layer cost 0.0010883808135986328 s
DEBUG 10-15 15:28:19 lpllm.py:512] reset_next_layer_need: layer_attn_idx=12, layer_mlp_idx=12, j_loc=24
DEBUG 10-15 15:28:19 lpllm.py:1037] reset layer cost 0.0011751651763916016 s
DEBUG 10-15 15:28:19 lpllm.py:1038] have reset next layer layer_attn 12 layer_mlp 12 
DEBUG 10-15 15:28:19 lpllm.py:924] 
DEBUG 10-15 15:28:19 lpllm.py:924] decoder loop j: 25 cur_layer_idx: 12 layer_attn_idx: 12 layer_mlp_idx: 12
DEBUG 10-15 15:28:19 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:19 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:19 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:19 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:19 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:19 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:19 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:19 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:19 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:19 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:19 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:19 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:19 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:19 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:19 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:19 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:19 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:19 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:19 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:19 lpllm.py:2265] GPU2CPU move cost 0.000626 seconds
DEBUG 10-15 15:28:19 lpmodule.py:340] layer_idx 12 attn layer_idx: 12
DEBUG 10-15 15:28:19 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:19 lpmodule.py:364] decoder_attn_batch update batch_dim 780-840
DEBUG 10-15 15:28:19 StaticCacheLen.py:84] static cache update layer_idx: 12, start_batch: 780, end_batch: 840
DEBUG 10-15 15:28:19 lpmodule.py:368] update for kv cache 780-840 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:19 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:19 lpmodule.py:374] update past key value cost 0.025529 seconds
DEBUG 10-15 15:28:19 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:19 lpmodule.py:227] layer idx 12
DEBUG 10-15 15:28:19 lpmodule.py:399] repeat qkv cost 0.029280 seconds
DEBUG 10-15 15:28:19 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:19 lpmodule.py:433] dot attn cost 0.032277 seconds
DEBUG 10-15 15:28:19 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:19 lpmodule.py:444] time cost move to cuda:1 0.02521038055419922 s
DEBUG 10-15 15:28:19 lpllm.py:2283] CPU attn cost 0.144221 seconds if batch True
DEBUG 10-15 15:28:19 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:19 lpllm.py:2294] CPU compute cost 0.145138 seconds
DEBUG 10-15 15:28:19 lpllm.py:2312] free cost 0.000075 seconds
DEBUG 10-15 15:28:20 lpllm.py:498] reset update attn
DEBUG 10-15 15:28:20 lpllm.py:1774] update state cost 2.7894973754882812e-05 s
DEBUG 10-15 15:28:20 lpllm.py:1743] restore layer func cost 0.0003991127014160156 s
DEBUG 10-15 15:28:20 lpllm.py:511] restore layer cost 0.0006396770477294922 s
DEBUG 10-15 15:28:20 lpllm.py:512] reset_next_layer_need: layer_attn_idx=13, layer_mlp_idx=12, j_loc=25
DEBUG 10-15 15:28:20 lpllm.py:1037] reset layer cost 0.0007231235504150391 s
DEBUG 10-15 15:28:20 lpllm.py:1038] have reset next layer layer_attn 13 layer_mlp 12 
DEBUG 10-15 15:28:20 lpllm.py:1044] j: 25 waiting the layer with layer_idx 13 before wait time 0.45117855072021484 s
INFO 10-15 15:28:20 client.py:117] confirm_model_loaded: Mixtral-8x7B, 7742ca8c-72e5-4a77-8793-a9e1bc8c3af0
INFO 10-15 15:28:20 client.py:125] Model loaded
DEBUG 10-15 15:28:20 lpllm.py:1048] j: load cost 0.45296287536621094 s waiting cost 0.0017697811126708984 s
DEBUG 10-15 15:28:20 lpllm.py:924] 
DEBUG 10-15 15:28:20 lpllm.py:924] decoder loop j: 26 cur_layer_idx: 13 layer_attn_idx: 13 layer_mlp_idx: 12
DEBUG 10-15 15:28:20 lpllm.py:933] start load next layer cur_layer_idx: 14
DEBUG 10-15 15:28:20 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed6e0>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x004\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x98\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:28:20 client.py:72] load_into_gpu: Mixtral-8x7B, 28ac6bdc-0403-425d-bbdd-8c1c3fd9e758
INFO 10-15 15:28:20 client.py:113] Model loaded: Mixtral-8x7B, 28ac6bdc-0403-425d-bbdd-8c1c3fd9e758
DEBUG 10-15 15:28:20 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:20 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:20 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:20 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:20 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:20 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:20 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:20 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:20 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:20 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:20 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:20 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:20 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:20 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:20 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:20 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:20 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:20 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:20 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:20 lpllm.py:2265] GPU2CPU move cost 0.000583 seconds
DEBUG 10-15 15:28:20 lpmodule.py:340] layer_idx 13 attn layer_idx: 13
DEBUG 10-15 15:28:20 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:20 lpmodule.py:364] decoder_attn_batch update batch_dim 720-780
DEBUG 10-15 15:28:20 StaticCacheLen.py:84] static cache update layer_idx: 13, start_batch: 720, end_batch: 780
DEBUG 10-15 15:28:20 lpmodule.py:368] update for kv cache 720-780 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:20 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:20 lpmodule.py:374] update past key value cost 0.024637 seconds
DEBUG 10-15 15:28:20 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:20 lpmodule.py:227] layer idx 12
DEBUG 10-15 15:28:20 lpmodule.py:399] repeat qkv cost 0.028457 seconds
DEBUG 10-15 15:28:20 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:20 lpmodule.py:433] dot attn cost 0.031333 seconds
DEBUG 10-15 15:28:20 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:20 lpmodule.py:444] time cost move to cuda:1 0.025044918060302734 s
DEBUG 10-15 15:28:20 lpllm.py:2283] CPU attn cost 0.138052 seconds if batch True
DEBUG 10-15 15:28:20 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:20 lpllm.py:2294] CPU compute cost 0.138930 seconds
DEBUG 10-15 15:28:20 lpllm.py:2312] free cost 0.000075 seconds
DEBUG 10-15 15:28:20 lpllm.py:503] reset update experts
DEBUG 10-15 15:28:20 lpllm.py:1774] update state cost 3.9577484130859375e-05 s
DEBUG 10-15 15:28:20 lpllm.py:1743] restore layer func cost 0.0008254051208496094 s
DEBUG 10-15 15:28:20 lpllm.py:511] restore layer cost 0.0010781288146972656 s
DEBUG 10-15 15:28:20 lpllm.py:512] reset_next_layer_need: layer_attn_idx=13, layer_mlp_idx=13, j_loc=26
DEBUG 10-15 15:28:20 lpllm.py:1037] reset layer cost 0.0011529922485351562 s
DEBUG 10-15 15:28:20 lpllm.py:1038] have reset next layer layer_attn 13 layer_mlp 13 
DEBUG 10-15 15:28:20 lpllm.py:924] 
DEBUG 10-15 15:28:20 lpllm.py:924] decoder loop j: 27 cur_layer_idx: 13 layer_attn_idx: 13 layer_mlp_idx: 13
DEBUG 10-15 15:28:20 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:20 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:20 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:20 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:20 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:20 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:20 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:20 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:20 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:20 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:20 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:20 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:20 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:20 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:20 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:20 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:20 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:20 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:20 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:20 lpllm.py:2265] GPU2CPU move cost 0.000614 seconds
DEBUG 10-15 15:28:20 lpmodule.py:340] layer_idx 13 attn layer_idx: 13
DEBUG 10-15 15:28:20 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:20 lpmodule.py:364] decoder_attn_batch update batch_dim 780-840
DEBUG 10-15 15:28:20 StaticCacheLen.py:84] static cache update layer_idx: 13, start_batch: 780, end_batch: 840
DEBUG 10-15 15:28:20 lpmodule.py:368] update for kv cache 780-840 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:20 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:20 lpmodule.py:374] update past key value cost 0.025528 seconds
DEBUG 10-15 15:28:20 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:20 lpmodule.py:227] layer idx 13
DEBUG 10-15 15:28:20 lpmodule.py:399] repeat qkv cost 0.028006 seconds
DEBUG 10-15 15:28:20 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:20 lpmodule.py:433] dot attn cost 0.031388 seconds
DEBUG 10-15 15:28:20 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:20 lpmodule.py:444] time cost move to cuda:1 0.030935287475585938 s
DEBUG 10-15 15:28:20 lpllm.py:2283] CPU attn cost 0.144468 seconds if batch True
DEBUG 10-15 15:28:20 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:28:20 lpllm.py:2294] CPU compute cost 0.145371 seconds
DEBUG 10-15 15:28:20 lpllm.py:2312] free cost 0.000071 seconds
DEBUG 10-15 15:28:20 lpllm.py:498] reset update attn
DEBUG 10-15 15:28:20 lpllm.py:1774] update state cost 2.5272369384765625e-05 s
DEBUG 10-15 15:28:20 lpllm.py:1743] restore layer func cost 0.0003898143768310547 s
DEBUG 10-15 15:28:20 lpllm.py:511] restore layer cost 0.0006346702575683594 s
DEBUG 10-15 15:28:20 lpllm.py:512] reset_next_layer_need: layer_attn_idx=14, layer_mlp_idx=13, j_loc=27
DEBUG 10-15 15:28:20 lpllm.py:1037] reset layer cost 0.0007061958312988281 s
DEBUG 10-15 15:28:20 lpllm.py:1038] have reset next layer layer_attn 14 layer_mlp 13 
DEBUG 10-15 15:28:20 lpllm.py:1044] j: 27 waiting the layer with layer_idx 14 before wait time 0.562706708908081 s
INFO 10-15 15:28:20 client.py:117] confirm_model_loaded: Mixtral-8x7B, 28ac6bdc-0403-425d-bbdd-8c1c3fd9e758
INFO 10-15 15:28:20 client.py:125] Model loaded
DEBUG 10-15 15:28:20 lpllm.py:1048] j: load cost 0.5644588470458984 s waiting cost 0.0017361640930175781 s
DEBUG 10-15 15:28:20 lpllm.py:924] 
DEBUG 10-15 15:28:20 lpllm.py:924] decoder loop j: 28 cur_layer_idx: 14 layer_attn_idx: 14 layer_mlp_idx: 13
DEBUG 10-15 15:28:20 lpllm.py:933] start load next layer cur_layer_idx: 15
DEBUG 10-15 15:28:20 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:28:20 client.py:72] load_into_gpu: Mixtral-8x7B, 6a90c2b6-3ff5-493e-901a-c154f1e361b0
INFO 10-15 15:28:20 client.py:113] Model loaded: Mixtral-8x7B, 6a90c2b6-3ff5-493e-901a-c154f1e361b0
DEBUG 10-15 15:28:20 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:20 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:20 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:20 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:20 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:20 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:20 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:20 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:20 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:20 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:20 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:20 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:20 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:20 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:20 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:20 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:20 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:20 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:20 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:20 lpllm.py:2265] GPU2CPU move cost 0.000572 seconds
DEBUG 10-15 15:28:20 lpmodule.py:340] layer_idx 14 attn layer_idx: 14
DEBUG 10-15 15:28:20 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:20 lpmodule.py:364] decoder_attn_batch update batch_dim 720-780
DEBUG 10-15 15:28:20 StaticCacheLen.py:84] static cache update layer_idx: 14, start_batch: 720, end_batch: 780
DEBUG 10-15 15:28:20 lpmodule.py:368] update for kv cache 720-780 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:20 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:20 lpmodule.py:374] update past key value cost 0.023361 seconds
DEBUG 10-15 15:28:20 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:20 lpmodule.py:227] layer idx 13
DEBUG 10-15 15:28:20 lpmodule.py:399] repeat qkv cost 0.027946 seconds
DEBUG 10-15 15:28:20 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:20 lpmodule.py:433] dot attn cost 0.032785 seconds
DEBUG 10-15 15:28:20 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:20 lpmodule.py:444] time cost move to cuda:1 0.025176525115966797 s
DEBUG 10-15 15:28:20 lpllm.py:2283] CPU attn cost 0.137758 seconds if batch True
DEBUG 10-15 15:28:20 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:28:20 lpllm.py:2294] CPU compute cost 0.138626 seconds
DEBUG 10-15 15:28:20 lpllm.py:2312] free cost 0.000093 seconds
DEBUG 10-15 15:28:20 lpllm.py:503] reset update experts
DEBUG 10-15 15:28:20 lpllm.py:1774] update state cost 2.3126602172851562e-05 s
DEBUG 10-15 15:28:20 lpllm.py:1743] restore layer func cost 0.0008447170257568359 s
DEBUG 10-15 15:28:20 lpllm.py:511] restore layer cost 0.0011022090911865234 s
DEBUG 10-15 15:28:20 lpllm.py:512] reset_next_layer_need: layer_attn_idx=14, layer_mlp_idx=14, j_loc=28
DEBUG 10-15 15:28:20 lpllm.py:1037] reset layer cost 0.0011744499206542969 s
DEBUG 10-15 15:28:20 lpllm.py:1038] have reset next layer layer_attn 14 layer_mlp 14 
DEBUG 10-15 15:28:20 lpllm.py:924] 
DEBUG 10-15 15:28:20 lpllm.py:924] decoder loop j: 29 cur_layer_idx: 14 layer_attn_idx: 14 layer_mlp_idx: 14
DEBUG 10-15 15:28:20 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:20 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:20 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:20 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:20 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:20 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:20 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:20 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:20 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:20 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:20 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:20 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:20 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:20 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:20 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:20 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:20 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:20 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:20 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:20 lpllm.py:2265] GPU2CPU move cost 0.000312 seconds
DEBUG 10-15 15:28:20 lpmodule.py:340] layer_idx 14 attn layer_idx: 14
DEBUG 10-15 15:28:20 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:20 lpmodule.py:364] decoder_attn_batch update batch_dim 780-840
DEBUG 10-15 15:28:20 StaticCacheLen.py:84] static cache update layer_idx: 14, start_batch: 780, end_batch: 840
DEBUG 10-15 15:28:20 lpmodule.py:368] update for kv cache 780-840 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:20 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:20 lpmodule.py:374] update past key value cost 0.025968 seconds
DEBUG 10-15 15:28:20 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:20 lpmodule.py:227] layer idx 14
DEBUG 10-15 15:28:20 lpmodule.py:399] repeat qkv cost 0.028400 seconds
DEBUG 10-15 15:28:20 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:20 lpmodule.py:433] dot attn cost 0.031997 seconds
DEBUG 10-15 15:28:20 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:20 lpmodule.py:444] time cost move to cuda:1 0.02463841438293457 s
DEBUG 10-15 15:28:20 lpllm.py:2283] CPU attn cost 0.139559 seconds if batch True
DEBUG 10-15 15:28:20 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:20 lpllm.py:2294] CPU compute cost 0.140093 seconds
DEBUG 10-15 15:28:20 lpllm.py:2312] free cost 0.000073 seconds
DEBUG 10-15 15:28:21 lpllm.py:498] reset update attn
DEBUG 10-15 15:28:21 lpllm.py:1774] update state cost 3.910064697265625e-05 s
DEBUG 10-15 15:28:21 lpllm.py:1743] restore layer func cost 0.00039505958557128906 s
DEBUG 10-15 15:28:21 lpllm.py:511] restore layer cost 0.0006339550018310547 s
DEBUG 10-15 15:28:21 lpllm.py:512] reset_next_layer_need: layer_attn_idx=15, layer_mlp_idx=14, j_loc=29
DEBUG 10-15 15:28:21 lpllm.py:1037] reset layer cost 0.0007174015045166016 s
DEBUG 10-15 15:28:21 lpllm.py:1038] have reset next layer layer_attn 15 layer_mlp 14 
DEBUG 10-15 15:28:21 lpllm.py:1044] j: 29 waiting the layer with layer_idx 15 before wait time 0.44791412353515625 s
INFO 10-15 15:28:21 client.py:117] confirm_model_loaded: Mixtral-8x7B, 6a90c2b6-3ff5-493e-901a-c154f1e361b0
INFO 10-15 15:28:21 client.py:125] Model loaded
DEBUG 10-15 15:28:21 lpllm.py:1048] j: load cost 0.4492316246032715 s waiting cost 0.0013012886047363281 s
DEBUG 10-15 15:28:21 lpllm.py:924] 
DEBUG 10-15 15:28:21 lpllm.py:924] decoder loop j: 30 cur_layer_idx: 15 layer_attn_idx: 15 layer_mlp_idx: 14
DEBUG 10-15 15:28:21 lpllm.py:933] start load next layer cur_layer_idx: 16
DEBUG 10-15 15:28:21 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed650>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x003\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x96\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:28:21 client.py:72] load_into_gpu: Mixtral-8x7B, d35c8d90-3c69-4721-9997-5b3590e236fc
INFO 10-15 15:28:21 client.py:113] Model loaded: Mixtral-8x7B, d35c8d90-3c69-4721-9997-5b3590e236fc
DEBUG 10-15 15:28:21 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:21 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:21 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:21 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:21 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:21 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:21 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:21 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:21 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:21 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:21 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:21 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:21 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:21 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:21 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:21 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:21 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:21 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:21 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:21 lpllm.py:2265] GPU2CPU move cost 0.000483 seconds
DEBUG 10-15 15:28:21 lpmodule.py:340] layer_idx 15 attn layer_idx: 15
DEBUG 10-15 15:28:21 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:21 lpmodule.py:364] decoder_attn_batch update batch_dim 720-780
DEBUG 10-15 15:28:21 StaticCacheLen.py:84] static cache update layer_idx: 15, start_batch: 720, end_batch: 780
DEBUG 10-15 15:28:21 lpmodule.py:368] update for kv cache 720-780 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:21 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:21 lpmodule.py:374] update past key value cost 0.021700 seconds
DEBUG 10-15 15:28:21 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:21 lpmodule.py:227] layer idx 14
DEBUG 10-15 15:28:21 lpmodule.py:399] repeat qkv cost 0.028616 seconds
DEBUG 10-15 15:28:21 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:21 lpmodule.py:433] dot attn cost 0.032351 seconds
DEBUG 10-15 15:28:21 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:21 lpmodule.py:444] time cost move to cuda:1 0.025597095489501953 s
DEBUG 10-15 15:28:21 lpllm.py:2283] CPU attn cost 0.136936 seconds if batch True
DEBUG 10-15 15:28:21 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:21 lpllm.py:2294] CPU compute cost 0.137669 seconds
DEBUG 10-15 15:28:21 lpllm.py:2312] free cost 0.000076 seconds
DEBUG 10-15 15:28:21 lpllm.py:503] reset update experts
DEBUG 10-15 15:28:21 lpllm.py:1774] update state cost 2.1696090698242188e-05 s
DEBUG 10-15 15:28:21 lpllm.py:1743] restore layer func cost 0.0008606910705566406 s
DEBUG 10-15 15:28:21 lpllm.py:511] restore layer cost 0.0011243820190429688 s
DEBUG 10-15 15:28:21 lpllm.py:512] reset_next_layer_need: layer_attn_idx=15, layer_mlp_idx=15, j_loc=30
DEBUG 10-15 15:28:21 lpllm.py:1037] reset layer cost 0.0011975765228271484 s
DEBUG 10-15 15:28:21 lpllm.py:1038] have reset next layer layer_attn 15 layer_mlp 15 
DEBUG 10-15 15:28:21 lpllm.py:924] 
DEBUG 10-15 15:28:21 lpllm.py:924] decoder loop j: 31 cur_layer_idx: 15 layer_attn_idx: 15 layer_mlp_idx: 15
DEBUG 10-15 15:28:21 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:21 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:21 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:21 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:21 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:21 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:21 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:21 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:21 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:21 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:21 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:21 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:21 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:21 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:21 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:21 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:21 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:21 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:21 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:21 lpllm.py:2265] GPU2CPU move cost 0.000598 seconds
DEBUG 10-15 15:28:21 lpmodule.py:340] layer_idx 15 attn layer_idx: 15
DEBUG 10-15 15:28:21 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:21 lpmodule.py:364] decoder_attn_batch update batch_dim 780-840
DEBUG 10-15 15:28:21 StaticCacheLen.py:84] static cache update layer_idx: 15, start_batch: 780, end_batch: 840
DEBUG 10-15 15:28:21 lpmodule.py:368] update for kv cache 780-840 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:21 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:21 lpmodule.py:374] update past key value cost 0.026479 seconds
DEBUG 10-15 15:28:21 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:21 lpmodule.py:227] layer idx 15
DEBUG 10-15 15:28:21 lpmodule.py:399] repeat qkv cost 0.028631 seconds
DEBUG 10-15 15:28:21 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:21 lpmodule.py:433] dot attn cost 0.032425 seconds
DEBUG 10-15 15:28:21 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:21 lpmodule.py:444] time cost move to cuda:1 0.02484440803527832 s
DEBUG 10-15 15:28:21 lpllm.py:2283] CPU attn cost 0.141835 seconds if batch True
DEBUG 10-15 15:28:21 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:21 lpllm.py:2294] CPU compute cost 0.142792 seconds
DEBUG 10-15 15:28:21 lpllm.py:2312] free cost 0.000090 seconds
DEBUG 10-15 15:28:21 lpllm.py:498] reset update attn
DEBUG 10-15 15:28:21 lpllm.py:1774] update state cost 2.4080276489257812e-05 s
DEBUG 10-15 15:28:21 lpllm.py:1743] restore layer func cost 0.0003955364227294922 s
DEBUG 10-15 15:28:21 lpllm.py:511] restore layer cost 0.0006265640258789062 s
DEBUG 10-15 15:28:21 lpllm.py:512] reset_next_layer_need: layer_attn_idx=16, layer_mlp_idx=15, j_loc=31
DEBUG 10-15 15:28:21 lpllm.py:1037] reset layer cost 0.0007164478302001953 s
DEBUG 10-15 15:28:21 lpllm.py:1038] have reset next layer layer_attn 16 layer_mlp 15 
DEBUG 10-15 15:28:21 lpllm.py:1044] j: 31 waiting the layer with layer_idx 16 before wait time 0.44472312927246094 s
INFO 10-15 15:28:21 client.py:117] confirm_model_loaded: Mixtral-8x7B, d35c8d90-3c69-4721-9997-5b3590e236fc
INFO 10-15 15:28:21 client.py:125] Model loaded
DEBUG 10-15 15:28:21 lpllm.py:1048] j: load cost 0.44602108001708984 s waiting cost 0.00128173828125 s
DEBUG 10-15 15:28:21 lpllm.py:924] 
DEBUG 10-15 15:28:21 lpllm.py:924] decoder loop j: 32 cur_layer_idx: 16 layer_attn_idx: 16 layer_mlp_idx: 15
DEBUG 10-15 15:28:21 lpllm.py:933] start load next layer cur_layer_idx: 17
DEBUG 10-15 15:28:21 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed6e0>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x004\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x98\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:28:21 client.py:72] load_into_gpu: Mixtral-8x7B, ab6011fe-e41e-4b94-9170-19c05f09bb1f
INFO 10-15 15:28:21 client.py:113] Model loaded: Mixtral-8x7B, ab6011fe-e41e-4b94-9170-19c05f09bb1f
DEBUG 10-15 15:28:21 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:21 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:21 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:21 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:21 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:21 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:21 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:21 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:21 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:21 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:21 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:21 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:21 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:21 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:21 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:21 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:21 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:21 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:21 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:21 lpllm.py:2265] GPU2CPU move cost 0.000592 seconds
DEBUG 10-15 15:28:21 lpmodule.py:340] layer_idx 16 attn layer_idx: 16
DEBUG 10-15 15:28:21 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:21 lpmodule.py:364] decoder_attn_batch update batch_dim 720-780
DEBUG 10-15 15:28:21 StaticCacheLen.py:84] static cache update layer_idx: 16, start_batch: 720, end_batch: 780
DEBUG 10-15 15:28:21 lpmodule.py:368] update for kv cache 720-780 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:21 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:21 lpmodule.py:374] update past key value cost 0.022499 seconds
DEBUG 10-15 15:28:21 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:21 lpmodule.py:227] layer idx 15
DEBUG 10-15 15:28:21 lpmodule.py:399] repeat qkv cost 0.028043 seconds
DEBUG 10-15 15:28:21 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:21 lpmodule.py:433] dot attn cost 0.032646 seconds
DEBUG 10-15 15:28:21 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:21 lpmodule.py:444] time cost move to cuda:1 0.02488851547241211 s
DEBUG 10-15 15:28:21 lpllm.py:2283] CPU attn cost 0.137465 seconds if batch True
DEBUG 10-15 15:28:21 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:21 lpllm.py:2294] CPU compute cost 0.138355 seconds
DEBUG 10-15 15:28:21 lpllm.py:2312] free cost 0.000075 seconds
DEBUG 10-15 15:28:21 lpllm.py:503] reset update experts
DEBUG 10-15 15:28:21 lpllm.py:1774] update state cost 2.2172927856445312e-05 s
DEBUG 10-15 15:28:21 lpllm.py:1743] restore layer func cost 0.0008323192596435547 s
DEBUG 10-15 15:28:21 lpllm.py:511] restore layer cost 0.001088857650756836 s
DEBUG 10-15 15:28:21 lpllm.py:512] reset_next_layer_need: layer_attn_idx=16, layer_mlp_idx=16, j_loc=32
DEBUG 10-15 15:28:21 lpllm.py:1037] reset layer cost 0.001163482666015625 s
DEBUG 10-15 15:28:21 lpllm.py:1038] have reset next layer layer_attn 16 layer_mlp 16 
DEBUG 10-15 15:28:21 lpllm.py:924] 
DEBUG 10-15 15:28:21 lpllm.py:924] decoder loop j: 33 cur_layer_idx: 16 layer_attn_idx: 16 layer_mlp_idx: 16
DEBUG 10-15 15:28:21 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:21 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:21 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:21 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:21 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:21 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:21 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:21 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:21 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:21 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:21 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:21 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:21 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:21 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:21 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:21 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:21 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:21 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:21 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:21 lpllm.py:2265] GPU2CPU move cost 0.000576 seconds
DEBUG 10-15 15:28:21 lpmodule.py:340] layer_idx 16 attn layer_idx: 16
DEBUG 10-15 15:28:21 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:21 lpmodule.py:364] decoder_attn_batch update batch_dim 780-840
DEBUG 10-15 15:28:21 StaticCacheLen.py:84] static cache update layer_idx: 16, start_batch: 780, end_batch: 840
DEBUG 10-15 15:28:21 lpmodule.py:368] update for kv cache 780-840 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:21 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:21 lpmodule.py:374] update past key value cost 0.024751 seconds
DEBUG 10-15 15:28:21 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:21 lpmodule.py:227] layer idx 16
DEBUG 10-15 15:28:21 lpmodule.py:399] repeat qkv cost 0.029004 seconds
DEBUG 10-15 15:28:21 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:21 lpmodule.py:433] dot attn cost 0.031936 seconds
DEBUG 10-15 15:28:21 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:21 lpmodule.py:444] time cost move to cuda:1 0.025094270706176758 s
DEBUG 10-15 15:28:21 lpllm.py:2283] CPU attn cost 0.139571 seconds if batch True
DEBUG 10-15 15:28:21 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:21 lpllm.py:2294] CPU compute cost 0.140450 seconds
DEBUG 10-15 15:28:21 lpllm.py:2312] free cost 0.000075 seconds
DEBUG 10-15 15:28:21 lpllm.py:498] reset update attn
DEBUG 10-15 15:28:21 lpllm.py:1774] update state cost 4.029273986816406e-05 s
DEBUG 10-15 15:28:21 lpllm.py:1743] restore layer func cost 0.00040268898010253906 s
DEBUG 10-15 15:28:21 lpllm.py:511] restore layer cost 0.0006508827209472656 s
DEBUG 10-15 15:28:21 lpllm.py:512] reset_next_layer_need: layer_attn_idx=17, layer_mlp_idx=16, j_loc=33
DEBUG 10-15 15:28:21 lpllm.py:1037] reset layer cost 0.0007374286651611328 s
DEBUG 10-15 15:28:21 lpllm.py:1038] have reset next layer layer_attn 17 layer_mlp 16 
DEBUG 10-15 15:28:21 lpllm.py:1044] j: 33 waiting the layer with layer_idx 17 before wait time 0.4420962333679199 s
INFO 10-15 15:28:21 client.py:117] confirm_model_loaded: Mixtral-8x7B, ab6011fe-e41e-4b94-9170-19c05f09bb1f
INFO 10-15 15:28:21 client.py:125] Model loaded
DEBUG 10-15 15:28:21 lpllm.py:1048] j: load cost 0.4437994956970215 s waiting cost 0.001687765121459961 s
DEBUG 10-15 15:28:21 lpllm.py:924] 
DEBUG 10-15 15:28:21 lpllm.py:924] decoder loop j: 34 cur_layer_idx: 17 layer_attn_idx: 17 layer_mlp_idx: 16
DEBUG 10-15 15:28:21 lpllm.py:933] start load next layer cur_layer_idx: 18
DEBUG 10-15 15:28:21 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:28:21 client.py:72] load_into_gpu: Mixtral-8x7B, 9f4852cb-f37b-43cf-8571-cbcea53436c1
INFO 10-15 15:28:21 client.py:113] Model loaded: Mixtral-8x7B, 9f4852cb-f37b-43cf-8571-cbcea53436c1
DEBUG 10-15 15:28:21 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:21 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:21 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:21 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:21 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:21 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:21 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:21 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:21 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:21 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:21 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:21 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:21 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:21 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:21 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:21 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:21 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:21 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:21 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:21 lpllm.py:2265] GPU2CPU move cost 0.000593 seconds
DEBUG 10-15 15:28:21 lpmodule.py:340] layer_idx 17 attn layer_idx: 17
DEBUG 10-15 15:28:21 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:21 lpmodule.py:364] decoder_attn_batch update batch_dim 720-780
DEBUG 10-15 15:28:21 StaticCacheLen.py:84] static cache update layer_idx: 17, start_batch: 720, end_batch: 780
DEBUG 10-15 15:28:21 lpmodule.py:368] update for kv cache 720-780 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:21 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:21 lpmodule.py:374] update past key value cost 0.025321 seconds
DEBUG 10-15 15:28:21 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:22 lpmodule.py:227] layer idx 16
DEBUG 10-15 15:28:22 lpmodule.py:399] repeat qkv cost 0.028439 seconds
DEBUG 10-15 15:28:22 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:22 lpmodule.py:433] dot attn cost 0.032353 seconds
DEBUG 10-15 15:28:22 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:22 lpmodule.py:444] time cost move to cuda:1 0.025676965713500977 s
DEBUG 10-15 15:28:22 lpllm.py:2283] CPU attn cost 0.140662 seconds if batch True
DEBUG 10-15 15:28:22 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:28:22 lpllm.py:2294] CPU compute cost 0.141549 seconds
DEBUG 10-15 15:28:22 lpllm.py:2312] free cost 0.000084 seconds
DEBUG 10-15 15:28:22 lpllm.py:503] reset update experts
DEBUG 10-15 15:28:22 lpllm.py:1774] update state cost 3.790855407714844e-05 s
DEBUG 10-15 15:28:22 lpllm.py:1743] restore layer func cost 0.0008592605590820312 s
DEBUG 10-15 15:28:22 lpllm.py:511] restore layer cost 0.0011212825775146484 s
DEBUG 10-15 15:28:22 lpllm.py:512] reset_next_layer_need: layer_attn_idx=17, layer_mlp_idx=17, j_loc=34
DEBUG 10-15 15:28:22 lpllm.py:1037] reset layer cost 0.0011949539184570312 s
DEBUG 10-15 15:28:22 lpllm.py:1038] have reset next layer layer_attn 17 layer_mlp 17 
DEBUG 10-15 15:28:22 lpllm.py:924] 
DEBUG 10-15 15:28:22 lpllm.py:924] decoder loop j: 35 cur_layer_idx: 17 layer_attn_idx: 17 layer_mlp_idx: 17
DEBUG 10-15 15:28:22 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:22 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:22 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:22 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:22 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:22 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:22 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:22 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:22 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:22 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:22 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:22 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:22 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:22 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:22 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:22 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:22 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:22 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:22 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:22 lpllm.py:2265] GPU2CPU move cost 0.000637 seconds
DEBUG 10-15 15:28:22 lpmodule.py:340] layer_idx 17 attn layer_idx: 17
DEBUG 10-15 15:28:22 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:22 lpmodule.py:364] decoder_attn_batch update batch_dim 780-840
DEBUG 10-15 15:28:22 StaticCacheLen.py:84] static cache update layer_idx: 17, start_batch: 780, end_batch: 840
DEBUG 10-15 15:28:22 lpmodule.py:368] update for kv cache 780-840 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:22 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:22 lpmodule.py:374] update past key value cost 0.027600 seconds
DEBUG 10-15 15:28:22 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:22 lpmodule.py:227] layer idx 17
DEBUG 10-15 15:28:22 lpmodule.py:399] repeat qkv cost 0.028122 seconds
DEBUG 10-15 15:28:22 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:22 lpmodule.py:433] dot attn cost 0.032347 seconds
DEBUG 10-15 15:28:22 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:22 lpmodule.py:444] time cost move to cuda:1 0.02521967887878418 s
DEBUG 10-15 15:28:22 lpllm.py:2283] CPU attn cost 0.142771 seconds if batch True
DEBUG 10-15 15:28:22 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:28:22 lpllm.py:2294] CPU compute cost 0.143702 seconds
DEBUG 10-15 15:28:22 lpllm.py:2312] free cost 0.000076 seconds
DEBUG 10-15 15:28:22 lpllm.py:498] reset update attn
DEBUG 10-15 15:28:22 lpllm.py:1774] update state cost 3.9577484130859375e-05 s
DEBUG 10-15 15:28:22 lpllm.py:1743] restore layer func cost 0.00040531158447265625 s
DEBUG 10-15 15:28:22 lpllm.py:511] restore layer cost 0.0006694793701171875 s
DEBUG 10-15 15:28:22 lpllm.py:512] reset_next_layer_need: layer_attn_idx=18, layer_mlp_idx=17, j_loc=35
DEBUG 10-15 15:28:22 lpllm.py:1037] reset layer cost 0.0007410049438476562 s
DEBUG 10-15 15:28:22 lpllm.py:1038] have reset next layer layer_attn 18 layer_mlp 17 
DEBUG 10-15 15:28:22 lpllm.py:1044] j: 35 waiting the layer with layer_idx 18 before wait time 0.45211219787597656 s
INFO 10-15 15:28:22 client.py:117] confirm_model_loaded: Mixtral-8x7B, 9f4852cb-f37b-43cf-8571-cbcea53436c1
INFO 10-15 15:28:22 client.py:125] Model loaded
DEBUG 10-15 15:28:22 lpllm.py:1048] j: load cost 0.45377206802368164 s waiting cost 0.0016427040100097656 s
DEBUG 10-15 15:28:22 lpllm.py:924] 
DEBUG 10-15 15:28:22 lpllm.py:924] decoder loop j: 36 cur_layer_idx: 18 layer_attn_idx: 18 layer_mlp_idx: 17
DEBUG 10-15 15:28:22 lpllm.py:933] start load next layer cur_layer_idx: 19
DEBUG 10-15 15:28:22 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed650>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x003\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x96\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:28:22 client.py:72] load_into_gpu: Mixtral-8x7B, c0e3249c-4a5c-4544-aa5b-5fbcfafc4906
INFO 10-15 15:28:22 client.py:113] Model loaded: Mixtral-8x7B, c0e3249c-4a5c-4544-aa5b-5fbcfafc4906
DEBUG 10-15 15:28:22 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:22 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:22 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:22 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:22 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:22 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:22 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:22 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:22 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:22 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:22 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:22 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:22 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:22 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:22 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:22 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:22 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:22 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:22 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:22 lpllm.py:2265] GPU2CPU move cost 0.000580 seconds
DEBUG 10-15 15:28:22 lpmodule.py:340] layer_idx 18 attn layer_idx: 18
DEBUG 10-15 15:28:22 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:22 lpmodule.py:364] decoder_attn_batch update batch_dim 720-780
DEBUG 10-15 15:28:22 StaticCacheLen.py:84] static cache update layer_idx: 18, start_batch: 720, end_batch: 780
DEBUG 10-15 15:28:22 lpmodule.py:368] update for kv cache 720-780 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:22 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:22 lpmodule.py:374] update past key value cost 0.024245 seconds
DEBUG 10-15 15:28:22 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:22 lpmodule.py:227] layer idx 17
DEBUG 10-15 15:28:22 lpmodule.py:399] repeat qkv cost 0.030827 seconds
DEBUG 10-15 15:28:22 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:22 lpmodule.py:433] dot attn cost 0.031418 seconds
DEBUG 10-15 15:28:22 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:22 lpmodule.py:444] time cost move to cuda:1 0.025107860565185547 s
DEBUG 10-15 15:28:22 lpllm.py:2283] CPU attn cost 0.140050 seconds if batch True
DEBUG 10-15 15:28:22 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:22 lpllm.py:2294] CPU compute cost 0.140915 seconds
DEBUG 10-15 15:28:22 lpllm.py:2312] free cost 0.000093 seconds
DEBUG 10-15 15:28:22 lpllm.py:503] reset update experts
DEBUG 10-15 15:28:22 lpllm.py:1774] update state cost 2.47955322265625e-05 s
DEBUG 10-15 15:28:22 lpllm.py:1743] restore layer func cost 0.0008337497711181641 s
DEBUG 10-15 15:28:22 lpllm.py:511] restore layer cost 0.0010721683502197266 s
DEBUG 10-15 15:28:22 lpllm.py:512] reset_next_layer_need: layer_attn_idx=18, layer_mlp_idx=18, j_loc=36
DEBUG 10-15 15:28:22 lpllm.py:1037] reset layer cost 0.0011456012725830078 s
DEBUG 10-15 15:28:22 lpllm.py:1038] have reset next layer layer_attn 18 layer_mlp 18 
DEBUG 10-15 15:28:22 lpllm.py:924] 
DEBUG 10-15 15:28:22 lpllm.py:924] decoder loop j: 37 cur_layer_idx: 18 layer_attn_idx: 18 layer_mlp_idx: 18
DEBUG 10-15 15:28:22 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:22 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:22 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:22 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:22 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:22 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:22 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:22 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:22 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:22 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:22 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:22 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:22 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:22 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:22 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:22 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:22 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:22 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:22 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:22 lpllm.py:2265] GPU2CPU move cost 0.000582 seconds
DEBUG 10-15 15:28:22 lpmodule.py:340] layer_idx 18 attn layer_idx: 18
DEBUG 10-15 15:28:22 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:22 lpmodule.py:364] decoder_attn_batch update batch_dim 780-840
DEBUG 10-15 15:28:22 StaticCacheLen.py:84] static cache update layer_idx: 18, start_batch: 780, end_batch: 840
DEBUG 10-15 15:28:22 lpmodule.py:368] update for kv cache 780-840 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:22 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:22 lpmodule.py:374] update past key value cost 0.024687 seconds
DEBUG 10-15 15:28:22 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:22 lpmodule.py:227] layer idx 18
DEBUG 10-15 15:28:22 lpmodule.py:399] repeat qkv cost 0.029227 seconds
DEBUG 10-15 15:28:22 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:22 lpmodule.py:433] dot attn cost 0.031970 seconds
DEBUG 10-15 15:28:22 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:22 lpmodule.py:444] time cost move to cuda:1 0.025235652923583984 s
DEBUG 10-15 15:28:22 lpllm.py:2283] CPU attn cost 0.139869 seconds if batch True
DEBUG 10-15 15:28:22 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:22 lpllm.py:2294] CPU compute cost 0.140740 seconds
DEBUG 10-15 15:28:22 lpllm.py:2312] free cost 0.000076 seconds
DEBUG 10-15 15:28:22 lpllm.py:498] reset update attn
DEBUG 10-15 15:28:22 lpllm.py:1774] update state cost 2.4318695068359375e-05 s
DEBUG 10-15 15:28:22 lpllm.py:1743] restore layer func cost 0.0004010200500488281 s
DEBUG 10-15 15:28:22 lpllm.py:511] restore layer cost 0.0006339550018310547 s
DEBUG 10-15 15:28:22 lpllm.py:512] reset_next_layer_need: layer_attn_idx=19, layer_mlp_idx=18, j_loc=37
DEBUG 10-15 15:28:22 lpllm.py:1037] reset layer cost 0.0007050037384033203 s
DEBUG 10-15 15:28:22 lpllm.py:1038] have reset next layer layer_attn 19 layer_mlp 18 
DEBUG 10-15 15:28:22 lpllm.py:1044] j: 37 waiting the layer with layer_idx 19 before wait time 0.45641469955444336 s
INFO 10-15 15:28:22 client.py:117] confirm_model_loaded: Mixtral-8x7B, c0e3249c-4a5c-4544-aa5b-5fbcfafc4906
INFO 10-15 15:28:22 client.py:125] Model loaded
DEBUG 10-15 15:28:22 lpllm.py:1048] j: load cost 0.45773816108703613 s waiting cost 0.001308441162109375 s
DEBUG 10-15 15:28:22 lpllm.py:924] 
DEBUG 10-15 15:28:22 lpllm.py:924] decoder loop j: 38 cur_layer_idx: 19 layer_attn_idx: 19 layer_mlp_idx: 18
DEBUG 10-15 15:28:22 lpllm.py:933] start load next layer cur_layer_idx: 20
DEBUG 10-15 15:28:22 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed6e0>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x004\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x98\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:28:22 client.py:72] load_into_gpu: Mixtral-8x7B, 6e3308a1-5b67-48e6-b9c0-89ec4b410ca6
INFO 10-15 15:28:22 client.py:113] Model loaded: Mixtral-8x7B, 6e3308a1-5b67-48e6-b9c0-89ec4b410ca6
DEBUG 10-15 15:28:22 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:22 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:22 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:22 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:22 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:22 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:22 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:22 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:22 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:22 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:22 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:22 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:22 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:22 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:22 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:22 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:22 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:22 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:22 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:22 lpllm.py:2265] GPU2CPU move cost 0.000634 seconds
DEBUG 10-15 15:28:22 lpmodule.py:340] layer_idx 19 attn layer_idx: 19
DEBUG 10-15 15:28:22 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:22 lpmodule.py:364] decoder_attn_batch update batch_dim 720-780
DEBUG 10-15 15:28:22 StaticCacheLen.py:84] static cache update layer_idx: 19, start_batch: 720, end_batch: 780
DEBUG 10-15 15:28:22 lpmodule.py:368] update for kv cache 720-780 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:22 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:22 lpmodule.py:374] update past key value cost 0.023710 seconds
DEBUG 10-15 15:28:22 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:22 lpmodule.py:227] layer idx 18
DEBUG 10-15 15:28:22 lpmodule.py:399] repeat qkv cost 0.029204 seconds
DEBUG 10-15 15:28:22 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:22 lpmodule.py:433] dot attn cost 0.031614 seconds
DEBUG 10-15 15:28:22 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:23 lpmodule.py:444] time cost move to cuda:1 0.02757430076599121 s
DEBUG 10-15 15:28:23 lpllm.py:2283] CPU attn cost 0.141361 seconds if batch True
DEBUG 10-15 15:28:23 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:28:23 lpllm.py:2294] CPU compute cost 0.142284 seconds
DEBUG 10-15 15:28:23 lpllm.py:2312] free cost 0.000072 seconds
DEBUG 10-15 15:28:23 lpllm.py:503] reset update experts
DEBUG 10-15 15:28:23 lpllm.py:1774] update state cost 2.193450927734375e-05 s
DEBUG 10-15 15:28:23 lpllm.py:1743] restore layer func cost 0.0008411407470703125 s
DEBUG 10-15 15:28:23 lpllm.py:511] restore layer cost 0.0010943412780761719 s
DEBUG 10-15 15:28:23 lpllm.py:512] reset_next_layer_need: layer_attn_idx=19, layer_mlp_idx=19, j_loc=38
DEBUG 10-15 15:28:23 lpllm.py:1037] reset layer cost 0.0011682510375976562 s
DEBUG 10-15 15:28:23 lpllm.py:1038] have reset next layer layer_attn 19 layer_mlp 19 
DEBUG 10-15 15:28:23 lpllm.py:924] 
DEBUG 10-15 15:28:23 lpllm.py:924] decoder loop j: 39 cur_layer_idx: 19 layer_attn_idx: 19 layer_mlp_idx: 19
DEBUG 10-15 15:28:23 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:23 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:23 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:23 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:23 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:23 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:23 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:23 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:23 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:23 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:23 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:23 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:23 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:23 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:23 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:23 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:23 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:23 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:23 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:23 lpllm.py:2265] GPU2CPU move cost 0.000572 seconds
DEBUG 10-15 15:28:23 lpmodule.py:340] layer_idx 19 attn layer_idx: 19
DEBUG 10-15 15:28:23 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:23 lpmodule.py:364] decoder_attn_batch update batch_dim 780-840
DEBUG 10-15 15:28:23 StaticCacheLen.py:84] static cache update layer_idx: 19, start_batch: 780, end_batch: 840
DEBUG 10-15 15:28:23 lpmodule.py:368] update for kv cache 780-840 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:23 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:23 lpmodule.py:374] update past key value cost 0.027822 seconds
DEBUG 10-15 15:28:23 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:23 lpmodule.py:227] layer idx 19
DEBUG 10-15 15:28:23 lpmodule.py:399] repeat qkv cost 0.028508 seconds
DEBUG 10-15 15:28:23 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:23 lpmodule.py:433] dot attn cost 0.032519 seconds
DEBUG 10-15 15:28:23 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:23 lpmodule.py:444] time cost move to cuda:1 0.02536749839782715 s
DEBUG 10-15 15:28:23 lpllm.py:2283] CPU attn cost 0.142839 seconds if batch True
DEBUG 10-15 15:28:23 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:23 lpllm.py:2294] CPU compute cost 0.143696 seconds
DEBUG 10-15 15:28:23 lpllm.py:2312] free cost 0.000072 seconds
DEBUG 10-15 15:28:23 lpllm.py:498] reset update attn
DEBUG 10-15 15:28:23 lpllm.py:1774] update state cost 2.384185791015625e-05 s
DEBUG 10-15 15:28:23 lpllm.py:1743] restore layer func cost 0.0003914833068847656 s
DEBUG 10-15 15:28:23 lpllm.py:511] restore layer cost 0.0006272792816162109 s
DEBUG 10-15 15:28:23 lpllm.py:512] reset_next_layer_need: layer_attn_idx=20, layer_mlp_idx=19, j_loc=39
DEBUG 10-15 15:28:23 lpllm.py:1037] reset layer cost 0.0006992816925048828 s
DEBUG 10-15 15:28:23 lpllm.py:1038] have reset next layer layer_attn 20 layer_mlp 19 
DEBUG 10-15 15:28:23 lpllm.py:1044] j: 39 waiting the layer with layer_idx 20 before wait time 0.45897412300109863 s
INFO 10-15 15:28:23 client.py:117] confirm_model_loaded: Mixtral-8x7B, 6e3308a1-5b67-48e6-b9c0-89ec4b410ca6
INFO 10-15 15:28:23 client.py:125] Model loaded
DEBUG 10-15 15:28:23 lpllm.py:1048] j: load cost 0.4603598117828369 s waiting cost 0.0013692378997802734 s
DEBUG 10-15 15:28:23 lpllm.py:924] 
DEBUG 10-15 15:28:23 lpllm.py:924] decoder loop j: 40 cur_layer_idx: 20 layer_attn_idx: 20 layer_mlp_idx: 19
DEBUG 10-15 15:28:23 lpllm.py:933] start load next layer cur_layer_idx: 21
DEBUG 10-15 15:28:23 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:28:23 client.py:72] load_into_gpu: Mixtral-8x7B, 2711457b-926d-445f-8f29-f6243bc8cd8a
INFO 10-15 15:28:23 client.py:113] Model loaded: Mixtral-8x7B, 2711457b-926d-445f-8f29-f6243bc8cd8a
DEBUG 10-15 15:28:23 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:23 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:23 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:23 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:23 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:23 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:23 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:23 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:23 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:23 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:23 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:23 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:23 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:23 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:23 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:23 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:23 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:23 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:23 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:23 lpllm.py:2265] GPU2CPU move cost 0.000614 seconds
DEBUG 10-15 15:28:23 lpmodule.py:340] layer_idx 20 attn layer_idx: 20
DEBUG 10-15 15:28:23 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:23 lpmodule.py:364] decoder_attn_batch update batch_dim 720-780
DEBUG 10-15 15:28:23 StaticCacheLen.py:84] static cache update layer_idx: 20, start_batch: 720, end_batch: 780
DEBUG 10-15 15:28:23 lpmodule.py:368] update for kv cache 720-780 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:23 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:23 lpmodule.py:374] update past key value cost 0.023747 seconds
DEBUG 10-15 15:28:23 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:23 lpmodule.py:227] layer idx 19
DEBUG 10-15 15:28:23 lpmodule.py:399] repeat qkv cost 0.029341 seconds
DEBUG 10-15 15:28:23 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:23 lpmodule.py:433] dot attn cost 0.032628 seconds
DEBUG 10-15 15:28:23 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:23 lpmodule.py:444] time cost move to cuda:1 0.025217771530151367 s
DEBUG 10-15 15:28:23 lpllm.py:2283] CPU attn cost 0.139365 seconds if batch True
DEBUG 10-15 15:28:23 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:23 lpllm.py:2294] CPU compute cost 0.140290 seconds
DEBUG 10-15 15:28:23 lpllm.py:2312] free cost 0.000077 seconds
DEBUG 10-15 15:28:23 lpllm.py:503] reset update experts
DEBUG 10-15 15:28:23 lpllm.py:1774] update state cost 2.8133392333984375e-05 s
DEBUG 10-15 15:28:23 lpllm.py:1743] restore layer func cost 0.0011098384857177734 s
DEBUG 10-15 15:28:23 lpllm.py:511] restore layer cost 0.0014424324035644531 s
DEBUG 10-15 15:28:23 lpllm.py:512] reset_next_layer_need: layer_attn_idx=20, layer_mlp_idx=20, j_loc=40
DEBUG 10-15 15:28:23 lpllm.py:1037] reset layer cost 0.0015380382537841797 s
DEBUG 10-15 15:28:23 lpllm.py:1038] have reset next layer layer_attn 20 layer_mlp 20 
DEBUG 10-15 15:28:23 lpllm.py:924] 
DEBUG 10-15 15:28:23 lpllm.py:924] decoder loop j: 41 cur_layer_idx: 20 layer_attn_idx: 20 layer_mlp_idx: 20
DEBUG 10-15 15:28:23 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:23 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:23 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:23 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:23 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:23 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:23 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:23 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:23 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:23 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:23 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:23 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:23 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:23 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:23 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:23 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:23 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:23 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:23 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:23 lpllm.py:2265] GPU2CPU move cost 0.000640 seconds
DEBUG 10-15 15:28:23 lpmodule.py:340] layer_idx 20 attn layer_idx: 20
DEBUG 10-15 15:28:23 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:23 lpmodule.py:364] decoder_attn_batch update batch_dim 780-840
DEBUG 10-15 15:28:23 StaticCacheLen.py:84] static cache update layer_idx: 20, start_batch: 780, end_batch: 840
DEBUG 10-15 15:28:23 lpmodule.py:368] update for kv cache 780-840 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:23 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:23 lpmodule.py:374] update past key value cost 0.028681 seconds
DEBUG 10-15 15:28:23 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:23 lpmodule.py:227] layer idx 20
DEBUG 10-15 15:28:23 lpmodule.py:399] repeat qkv cost 0.028712 seconds
DEBUG 10-15 15:28:23 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:23 lpmodule.py:433] dot attn cost 0.032107 seconds
DEBUG 10-15 15:28:23 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:23 lpmodule.py:444] time cost move to cuda:1 0.025093793869018555 s
DEBUG 10-15 15:28:23 lpllm.py:2283] CPU attn cost 0.144240 seconds if batch True
DEBUG 10-15 15:28:23 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:23 lpllm.py:2294] CPU compute cost 0.145195 seconds
DEBUG 10-15 15:28:23 lpllm.py:2312] free cost 0.000073 seconds
DEBUG 10-15 15:28:23 lpllm.py:498] reset update attn
DEBUG 10-15 15:28:23 lpllm.py:1774] update state cost 2.5510787963867188e-05 s
DEBUG 10-15 15:28:23 lpllm.py:1743] restore layer func cost 0.00039386749267578125 s
DEBUG 10-15 15:28:23 lpllm.py:511] restore layer cost 0.0006277561187744141 s
DEBUG 10-15 15:28:23 lpllm.py:512] reset_next_layer_need: layer_attn_idx=21, layer_mlp_idx=20, j_loc=41
DEBUG 10-15 15:28:23 lpllm.py:1037] reset layer cost 0.0007288455963134766 s
DEBUG 10-15 15:28:23 lpllm.py:1038] have reset next layer layer_attn 21 layer_mlp 20 
DEBUG 10-15 15:28:23 lpllm.py:1044] j: 41 waiting the layer with layer_idx 21 before wait time 0.44858527183532715 s
INFO 10-15 15:28:23 client.py:117] confirm_model_loaded: Mixtral-8x7B, 2711457b-926d-445f-8f29-f6243bc8cd8a
INFO 10-15 15:28:23 client.py:125] Model loaded
DEBUG 10-15 15:28:23 lpllm.py:1048] j: load cost 0.4500422477722168 s waiting cost 0.00144195556640625 s
DEBUG 10-15 15:28:23 lpllm.py:924] 
DEBUG 10-15 15:28:23 lpllm.py:924] decoder loop j: 42 cur_layer_idx: 21 layer_attn_idx: 21 layer_mlp_idx: 20
DEBUG 10-15 15:28:23 lpllm.py:933] start load next layer cur_layer_idx: 22
DEBUG 10-15 15:28:23 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed650>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x003\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x96\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:28:23 client.py:72] load_into_gpu: Mixtral-8x7B, ff7c93c5-0dd5-4949-bc26-fa9ebe63e144
INFO 10-15 15:28:23 client.py:113] Model loaded: Mixtral-8x7B, ff7c93c5-0dd5-4949-bc26-fa9ebe63e144
DEBUG 10-15 15:28:23 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:23 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:23 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:23 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:23 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:23 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:23 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:23 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:23 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:23 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:23 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:23 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:23 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:23 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:23 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:23 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:23 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:23 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:23 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:23 lpllm.py:2265] GPU2CPU move cost 0.000606 seconds
DEBUG 10-15 15:28:23 lpmodule.py:340] layer_idx 21 attn layer_idx: 21
DEBUG 10-15 15:28:23 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:23 lpmodule.py:364] decoder_attn_batch update batch_dim 720-780
DEBUG 10-15 15:28:23 StaticCacheLen.py:84] static cache update layer_idx: 21, start_batch: 720, end_batch: 780
DEBUG 10-15 15:28:23 lpmodule.py:368] update for kv cache 720-780 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:23 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:23 lpmodule.py:374] update past key value cost 0.022810 seconds
DEBUG 10-15 15:28:23 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:23 lpmodule.py:227] layer idx 20
DEBUG 10-15 15:28:23 lpmodule.py:399] repeat qkv cost 0.028082 seconds
DEBUG 10-15 15:28:23 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:23 lpmodule.py:433] dot attn cost 0.034709 seconds
DEBUG 10-15 15:28:23 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:23 lpmodule.py:444] time cost move to cuda:1 0.026308059692382812 s
DEBUG 10-15 15:28:23 lpllm.py:2283] CPU attn cost 0.140858 seconds if batch True
DEBUG 10-15 15:28:23 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:23 lpllm.py:2294] CPU compute cost 0.141754 seconds
DEBUG 10-15 15:28:23 lpllm.py:2312] free cost 0.000073 seconds
DEBUG 10-15 15:28:23 lpllm.py:503] reset update experts
DEBUG 10-15 15:28:23 lpllm.py:1774] update state cost 2.2649765014648438e-05 s
DEBUG 10-15 15:28:23 lpllm.py:1743] restore layer func cost 0.0008378028869628906 s
DEBUG 10-15 15:28:23 lpllm.py:511] restore layer cost 0.0010826587677001953 s
DEBUG 10-15 15:28:23 lpllm.py:512] reset_next_layer_need: layer_attn_idx=21, layer_mlp_idx=21, j_loc=42
DEBUG 10-15 15:28:23 lpllm.py:1037] reset layer cost 0.0011703968048095703 s
DEBUG 10-15 15:28:23 lpllm.py:1038] have reset next layer layer_attn 21 layer_mlp 21 
DEBUG 10-15 15:28:23 lpllm.py:924] 
DEBUG 10-15 15:28:23 lpllm.py:924] decoder loop j: 43 cur_layer_idx: 21 layer_attn_idx: 21 layer_mlp_idx: 21
DEBUG 10-15 15:28:23 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:23 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:23 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:24 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:24 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:24 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:24 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:24 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:24 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:24 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:24 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:24 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:24 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:24 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:24 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:24 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:24 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:24 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:24 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:24 lpllm.py:2265] GPU2CPU move cost 0.000662 seconds
DEBUG 10-15 15:28:24 lpmodule.py:340] layer_idx 21 attn layer_idx: 21
DEBUG 10-15 15:28:24 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:24 lpmodule.py:364] decoder_attn_batch update batch_dim 780-840
DEBUG 10-15 15:28:24 StaticCacheLen.py:84] static cache update layer_idx: 21, start_batch: 780, end_batch: 840
DEBUG 10-15 15:28:24 lpmodule.py:368] update for kv cache 780-840 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:24 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:24 lpmodule.py:374] update past key value cost 0.027204 seconds
DEBUG 10-15 15:28:24 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:24 lpmodule.py:227] layer idx 21
DEBUG 10-15 15:28:24 lpmodule.py:399] repeat qkv cost 0.027978 seconds
DEBUG 10-15 15:28:24 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:24 lpmodule.py:433] dot attn cost 0.034551 seconds
DEBUG 10-15 15:28:24 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:24 lpmodule.py:444] time cost move to cuda:1 0.025210857391357422 s
DEBUG 10-15 15:28:24 lpllm.py:2283] CPU attn cost 0.144476 seconds if batch True
DEBUG 10-15 15:28:24 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:28:24 lpllm.py:2294] CPU compute cost 0.145488 seconds
DEBUG 10-15 15:28:24 lpllm.py:2312] free cost 0.000104 seconds
DEBUG 10-15 15:28:24 lpllm.py:498] reset update attn
DEBUG 10-15 15:28:24 lpllm.py:1774] update state cost 4.1484832763671875e-05 s
DEBUG 10-15 15:28:24 lpllm.py:1743] restore layer func cost 0.0004134178161621094 s
DEBUG 10-15 15:28:24 lpllm.py:511] restore layer cost 0.0006885528564453125 s
DEBUG 10-15 15:28:24 lpllm.py:512] reset_next_layer_need: layer_attn_idx=22, layer_mlp_idx=21, j_loc=43
DEBUG 10-15 15:28:24 lpllm.py:1037] reset layer cost 0.0007698535919189453 s
DEBUG 10-15 15:28:24 lpllm.py:1038] have reset next layer layer_attn 22 layer_mlp 21 
DEBUG 10-15 15:28:24 lpllm.py:1044] j: 43 waiting the layer with layer_idx 22 before wait time 0.4472200870513916 s
INFO 10-15 15:28:24 client.py:117] confirm_model_loaded: Mixtral-8x7B, ff7c93c5-0dd5-4949-bc26-fa9ebe63e144
INFO 10-15 15:28:24 client.py:125] Model loaded
DEBUG 10-15 15:28:24 lpllm.py:1048] j: load cost 0.4488799571990967 s waiting cost 0.001644134521484375 s
DEBUG 10-15 15:28:24 lpllm.py:924] 
DEBUG 10-15 15:28:24 lpllm.py:924] decoder loop j: 44 cur_layer_idx: 22 layer_attn_idx: 22 layer_mlp_idx: 21
DEBUG 10-15 15:28:24 lpllm.py:933] start load next layer cur_layer_idx: 23
DEBUG 10-15 15:28:24 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed6e0>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x004\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x98\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:28:24 client.py:72] load_into_gpu: Mixtral-8x7B, 99326706-b541-479d-8422-fce4f1a4cb5f
INFO 10-15 15:28:24 client.py:113] Model loaded: Mixtral-8x7B, 99326706-b541-479d-8422-fce4f1a4cb5f
DEBUG 10-15 15:28:24 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:24 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:24 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:24 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:24 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:24 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:24 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:24 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:24 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:24 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:24 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:24 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:24 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:24 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:24 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:24 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:24 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:24 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:24 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:24 lpllm.py:2265] GPU2CPU move cost 0.000608 seconds
DEBUG 10-15 15:28:24 lpmodule.py:340] layer_idx 22 attn layer_idx: 22
DEBUG 10-15 15:28:24 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:24 lpmodule.py:364] decoder_attn_batch update batch_dim 720-780
DEBUG 10-15 15:28:24 StaticCacheLen.py:84] static cache update layer_idx: 22, start_batch: 720, end_batch: 780
DEBUG 10-15 15:28:24 lpmodule.py:368] update for kv cache 720-780 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:24 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:24 lpmodule.py:374] update past key value cost 0.023495 seconds
DEBUG 10-15 15:28:24 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:24 lpmodule.py:227] layer idx 21
DEBUG 10-15 15:28:24 lpmodule.py:399] repeat qkv cost 0.029873 seconds
DEBUG 10-15 15:28:24 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:24 lpmodule.py:433] dot attn cost 0.031056 seconds
DEBUG 10-15 15:28:24 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:24 lpmodule.py:444] time cost move to cuda:1 0.027819395065307617 s
DEBUG 10-15 15:28:24 lpllm.py:2283] CPU attn cost 0.141172 seconds if batch True
DEBUG 10-15 15:28:24 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:28:24 lpllm.py:2294] CPU compute cost 0.142083 seconds
DEBUG 10-15 15:28:24 lpllm.py:2312] free cost 0.000107 seconds
DEBUG 10-15 15:28:24 lpllm.py:503] reset update experts
DEBUG 10-15 15:28:24 lpllm.py:1774] update state cost 4.792213439941406e-05 s
DEBUG 10-15 15:28:24 lpllm.py:1743] restore layer func cost 0.0010287761688232422 s
DEBUG 10-15 15:28:24 lpllm.py:511] restore layer cost 0.0017790794372558594 s
DEBUG 10-15 15:28:24 lpllm.py:512] reset_next_layer_need: layer_attn_idx=22, layer_mlp_idx=22, j_loc=44
DEBUG 10-15 15:28:24 lpllm.py:1037] reset layer cost 0.0019800662994384766 s
DEBUG 10-15 15:28:24 lpllm.py:1038] have reset next layer layer_attn 22 layer_mlp 22 
DEBUG 10-15 15:28:24 lpllm.py:924] 
DEBUG 10-15 15:28:24 lpllm.py:924] decoder loop j: 45 cur_layer_idx: 22 layer_attn_idx: 22 layer_mlp_idx: 22
DEBUG 10-15 15:28:24 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:24 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:24 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:24 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:24 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:24 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:24 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:24 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:24 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:24 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:24 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:24 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:24 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:24 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:24 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:24 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:24 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:24 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:24 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:24 lpllm.py:2265] GPU2CPU move cost 0.000661 seconds
DEBUG 10-15 15:28:24 lpmodule.py:340] layer_idx 22 attn layer_idx: 22
DEBUG 10-15 15:28:24 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:24 lpmodule.py:364] decoder_attn_batch update batch_dim 780-840
DEBUG 10-15 15:28:24 StaticCacheLen.py:84] static cache update layer_idx: 22, start_batch: 780, end_batch: 840
DEBUG 10-15 15:28:24 lpmodule.py:368] update for kv cache 780-840 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:24 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:24 lpmodule.py:374] update past key value cost 0.026228 seconds
DEBUG 10-15 15:28:24 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:24 lpmodule.py:227] layer idx 22
DEBUG 10-15 15:28:24 lpmodule.py:399] repeat qkv cost 0.028349 seconds
DEBUG 10-15 15:28:24 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:24 lpmodule.py:433] dot attn cost 0.033047 seconds
DEBUG 10-15 15:28:24 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:24 lpmodule.py:444] time cost move to cuda:1 0.025205373764038086 s
DEBUG 10-15 15:28:24 lpllm.py:2283] CPU attn cost 0.141726 seconds if batch True
DEBUG 10-15 15:28:24 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:24 lpllm.py:2294] CPU compute cost 0.142697 seconds
DEBUG 10-15 15:28:24 lpllm.py:2312] free cost 0.000079 seconds
DEBUG 10-15 15:28:24 lpllm.py:498] reset update attn
DEBUG 10-15 15:28:24 lpllm.py:1774] update state cost 4.601478576660156e-05 s
DEBUG 10-15 15:28:24 lpllm.py:1743] restore layer func cost 0.00041985511779785156 s
DEBUG 10-15 15:28:24 lpllm.py:511] restore layer cost 0.0006809234619140625 s
DEBUG 10-15 15:28:24 lpllm.py:512] reset_next_layer_need: layer_attn_idx=23, layer_mlp_idx=22, j_loc=45
DEBUG 10-15 15:28:24 lpllm.py:1037] reset layer cost 0.0007674694061279297 s
DEBUG 10-15 15:28:24 lpllm.py:1038] have reset next layer layer_attn 23 layer_mlp 22 
DEBUG 10-15 15:28:24 lpllm.py:1044] j: 45 waiting the layer with layer_idx 23 before wait time 0.4606032371520996 s
INFO 10-15 15:28:24 client.py:117] confirm_model_loaded: Mixtral-8x7B, 99326706-b541-479d-8422-fce4f1a4cb5f
INFO 10-15 15:28:24 client.py:125] Model loaded
DEBUG 10-15 15:28:24 lpllm.py:1048] j: load cost 0.4624905586242676 s waiting cost 0.0018715858459472656 s
DEBUG 10-15 15:28:24 lpllm.py:924] 
DEBUG 10-15 15:28:24 lpllm.py:924] decoder loop j: 46 cur_layer_idx: 23 layer_attn_idx: 23 layer_mlp_idx: 22
DEBUG 10-15 15:28:24 lpllm.py:933] start load next layer cur_layer_idx: 24
DEBUG 10-15 15:28:24 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:28:24 client.py:72] load_into_gpu: Mixtral-8x7B, 9a7aabce-b935-48bc-9b83-4565ae2b3d76
INFO 10-15 15:28:24 client.py:113] Model loaded: Mixtral-8x7B, 9a7aabce-b935-48bc-9b83-4565ae2b3d76
DEBUG 10-15 15:28:24 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:24 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:24 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:24 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:24 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:24 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:24 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:24 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:24 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:24 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:24 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:24 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:24 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:24 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:24 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:24 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:24 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:24 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:24 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:24 lpllm.py:2265] GPU2CPU move cost 0.000572 seconds
DEBUG 10-15 15:28:24 lpmodule.py:340] layer_idx 23 attn layer_idx: 23
DEBUG 10-15 15:28:24 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:24 lpmodule.py:364] decoder_attn_batch update batch_dim 720-780
DEBUG 10-15 15:28:24 StaticCacheLen.py:84] static cache update layer_idx: 23, start_batch: 720, end_batch: 780
DEBUG 10-15 15:28:24 lpmodule.py:368] update for kv cache 720-780 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:24 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:24 lpmodule.py:374] update past key value cost 0.022618 seconds
DEBUG 10-15 15:28:24 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:24 lpmodule.py:227] layer idx 22
DEBUG 10-15 15:28:24 lpmodule.py:399] repeat qkv cost 0.027720 seconds
DEBUG 10-15 15:28:24 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:24 lpmodule.py:433] dot attn cost 0.032840 seconds
DEBUG 10-15 15:28:24 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:24 lpmodule.py:444] time cost move to cuda:1 0.02530193328857422 s
DEBUG 10-15 15:28:24 lpllm.py:2283] CPU attn cost 0.136922 seconds if batch True
DEBUG 10-15 15:28:24 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:24 lpllm.py:2294] CPU compute cost 0.137774 seconds
DEBUG 10-15 15:28:24 lpllm.py:2312] free cost 0.000080 seconds
DEBUG 10-15 15:28:24 lpllm.py:503] reset update experts
DEBUG 10-15 15:28:24 lpllm.py:1774] update state cost 2.4318695068359375e-05 s
DEBUG 10-15 15:28:24 lpllm.py:1743] restore layer func cost 0.0008838176727294922 s
DEBUG 10-15 15:28:24 lpllm.py:511] restore layer cost 0.0011341571807861328 s
DEBUG 10-15 15:28:24 lpllm.py:512] reset_next_layer_need: layer_attn_idx=23, layer_mlp_idx=23, j_loc=46
DEBUG 10-15 15:28:24 lpllm.py:1037] reset layer cost 0.0012297630310058594 s
DEBUG 10-15 15:28:24 lpllm.py:1038] have reset next layer layer_attn 23 layer_mlp 23 
DEBUG 10-15 15:28:24 lpllm.py:924] 
DEBUG 10-15 15:28:24 lpllm.py:924] decoder loop j: 47 cur_layer_idx: 23 layer_attn_idx: 23 layer_mlp_idx: 23
DEBUG 10-15 15:28:24 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:24 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:24 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:24 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:24 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:24 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:24 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:24 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:24 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:24 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:24 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:24 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:24 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:24 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:24 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:24 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:24 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:24 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:24 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:24 lpllm.py:2265] GPU2CPU move cost 0.000643 seconds
DEBUG 10-15 15:28:24 lpmodule.py:340] layer_idx 23 attn layer_idx: 23
DEBUG 10-15 15:28:24 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:24 lpmodule.py:364] decoder_attn_batch update batch_dim 780-840
DEBUG 10-15 15:28:24 StaticCacheLen.py:84] static cache update layer_idx: 23, start_batch: 780, end_batch: 840
DEBUG 10-15 15:28:24 lpmodule.py:368] update for kv cache 780-840 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:24 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:24 lpmodule.py:374] update past key value cost 0.023008 seconds
DEBUG 10-15 15:28:24 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:24 lpmodule.py:227] layer idx 23
DEBUG 10-15 15:28:24 lpmodule.py:399] repeat qkv cost 0.029203 seconds
DEBUG 10-15 15:28:24 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:25 lpmodule.py:433] dot attn cost 0.033695 seconds
DEBUG 10-15 15:28:25 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:25 lpmodule.py:444] time cost move to cuda:1 0.02514481544494629 s
DEBUG 10-15 15:28:25 lpllm.py:2283] CPU attn cost 0.142126 seconds if batch True
DEBUG 10-15 15:28:25 lpllm.py:2292] deal attn result cost 0.000003 seconds
DEBUG 10-15 15:28:25 lpllm.py:2294] CPU compute cost 0.143067 seconds
DEBUG 10-15 15:28:25 lpllm.py:2312] free cost 0.000074 seconds
DEBUG 10-15 15:28:25 lpllm.py:498] reset update attn
DEBUG 10-15 15:28:25 lpllm.py:1774] update state cost 2.4318695068359375e-05 s
DEBUG 10-15 15:28:25 lpllm.py:1743] restore layer func cost 0.0004177093505859375 s
DEBUG 10-15 15:28:25 lpllm.py:511] restore layer cost 0.0006532669067382812 s
DEBUG 10-15 15:28:25 lpllm.py:512] reset_next_layer_need: layer_attn_idx=24, layer_mlp_idx=23, j_loc=47
DEBUG 10-15 15:28:25 lpllm.py:1037] reset layer cost 0.0007407665252685547 s
DEBUG 10-15 15:28:25 lpllm.py:1038] have reset next layer layer_attn 24 layer_mlp 23 
DEBUG 10-15 15:28:25 lpllm.py:1044] j: 47 waiting the layer with layer_idx 24 before wait time 0.44788074493408203 s
INFO 10-15 15:28:25 client.py:117] confirm_model_loaded: Mixtral-8x7B, 9a7aabce-b935-48bc-9b83-4565ae2b3d76
INFO 10-15 15:28:25 client.py:125] Model loaded
DEBUG 10-15 15:28:25 lpllm.py:1048] j: load cost 0.44953227043151855 s waiting cost 0.0016331672668457031 s
DEBUG 10-15 15:28:25 lpllm.py:924] 
DEBUG 10-15 15:28:25 lpllm.py:924] decoder loop j: 48 cur_layer_idx: 24 layer_attn_idx: 24 layer_mlp_idx: 23
DEBUG 10-15 15:28:25 lpllm.py:933] start load next layer cur_layer_idx: 25
DEBUG 10-15 15:28:25 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed650>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x003\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x96\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:28:25 client.py:72] load_into_gpu: Mixtral-8x7B, f1717b21-160d-4f03-832c-b513603c882e
INFO 10-15 15:28:25 client.py:113] Model loaded: Mixtral-8x7B, f1717b21-160d-4f03-832c-b513603c882e
DEBUG 10-15 15:28:25 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:25 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:25 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:25 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:25 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:25 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:25 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:25 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:25 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:25 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:25 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:25 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:25 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:25 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:25 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:25 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:25 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:25 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:25 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:25 lpllm.py:2265] GPU2CPU move cost 0.000609 seconds
DEBUG 10-15 15:28:25 lpmodule.py:340] layer_idx 24 attn layer_idx: 24
DEBUG 10-15 15:28:25 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:25 lpmodule.py:364] decoder_attn_batch update batch_dim 720-780
DEBUG 10-15 15:28:25 StaticCacheLen.py:84] static cache update layer_idx: 24, start_batch: 720, end_batch: 780
DEBUG 10-15 15:28:25 lpmodule.py:368] update for kv cache 720-780 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:25 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:25 lpmodule.py:374] update past key value cost 0.021551 seconds
DEBUG 10-15 15:28:25 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:25 lpmodule.py:227] layer idx 23
DEBUG 10-15 15:28:25 lpmodule.py:399] repeat qkv cost 0.027650 seconds
DEBUG 10-15 15:28:25 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:25 lpmodule.py:433] dot attn cost 0.035724 seconds
DEBUG 10-15 15:28:25 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:25 lpmodule.py:444] time cost move to cuda:1 0.027289152145385742 s
DEBUG 10-15 15:28:25 lpllm.py:2283] CPU attn cost 0.140564 seconds if batch True
DEBUG 10-15 15:28:25 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:28:25 lpllm.py:2294] CPU compute cost 0.141465 seconds
DEBUG 10-15 15:28:25 lpllm.py:2312] free cost 0.000070 seconds
DEBUG 10-15 15:28:25 lpllm.py:503] reset update experts
DEBUG 10-15 15:28:25 lpllm.py:1774] update state cost 3.0279159545898438e-05 s
DEBUG 10-15 15:28:25 lpllm.py:1743] restore layer func cost 0.0009617805480957031 s
DEBUG 10-15 15:28:25 lpllm.py:511] restore layer cost 0.0012135505676269531 s
DEBUG 10-15 15:28:25 lpllm.py:512] reset_next_layer_need: layer_attn_idx=24, layer_mlp_idx=24, j_loc=48
DEBUG 10-15 15:28:25 lpllm.py:1037] reset layer cost 0.0012989044189453125 s
DEBUG 10-15 15:28:25 lpllm.py:1038] have reset next layer layer_attn 24 layer_mlp 24 
DEBUG 10-15 15:28:25 lpllm.py:924] 
DEBUG 10-15 15:28:25 lpllm.py:924] decoder loop j: 49 cur_layer_idx: 24 layer_attn_idx: 24 layer_mlp_idx: 24
DEBUG 10-15 15:28:25 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:25 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:25 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:25 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:25 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:25 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:25 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:25 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:25 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:25 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:25 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:25 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:25 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:25 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:25 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:25 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:25 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:25 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:25 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:25 lpllm.py:2265] GPU2CPU move cost 0.000607 seconds
DEBUG 10-15 15:28:25 lpmodule.py:340] layer_idx 24 attn layer_idx: 24
DEBUG 10-15 15:28:25 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:25 lpmodule.py:364] decoder_attn_batch update batch_dim 780-840
DEBUG 10-15 15:28:25 StaticCacheLen.py:84] static cache update layer_idx: 24, start_batch: 780, end_batch: 840
DEBUG 10-15 15:28:25 lpmodule.py:368] update for kv cache 780-840 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:25 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:25 lpmodule.py:374] update past key value cost 0.022746 seconds
DEBUG 10-15 15:28:25 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:25 lpmodule.py:227] layer idx 24
DEBUG 10-15 15:28:25 lpmodule.py:399] repeat qkv cost 0.028171 seconds
DEBUG 10-15 15:28:25 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:25 lpmodule.py:433] dot attn cost 0.032977 seconds
DEBUG 10-15 15:28:25 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:25 lpmodule.py:444] time cost move to cuda:1 0.025913476943969727 s
DEBUG 10-15 15:28:25 lpllm.py:2283] CPU attn cost 0.138680 seconds if batch True
DEBUG 10-15 15:28:25 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:25 lpllm.py:2294] CPU compute cost 0.139570 seconds
DEBUG 10-15 15:28:25 lpllm.py:2312] free cost 0.000074 seconds
DEBUG 10-15 15:28:25 lpllm.py:498] reset update attn
DEBUG 10-15 15:28:25 lpllm.py:1774] update state cost 2.7894973754882812e-05 s
DEBUG 10-15 15:28:25 lpllm.py:1743] restore layer func cost 0.00042057037353515625 s
DEBUG 10-15 15:28:25 lpllm.py:511] restore layer cost 0.0006628036499023438 s
DEBUG 10-15 15:28:25 lpllm.py:512] reset_next_layer_need: layer_attn_idx=25, layer_mlp_idx=24, j_loc=49
DEBUG 10-15 15:28:25 lpllm.py:1037] reset layer cost 0.0007491111755371094 s
DEBUG 10-15 15:28:25 lpllm.py:1038] have reset next layer layer_attn 25 layer_mlp 24 
DEBUG 10-15 15:28:25 lpllm.py:1044] j: 49 waiting the layer with layer_idx 25 before wait time 0.45186424255371094 s
INFO 10-15 15:28:25 client.py:117] confirm_model_loaded: Mixtral-8x7B, f1717b21-160d-4f03-832c-b513603c882e
INFO 10-15 15:28:25 client.py:125] Model loaded
DEBUG 10-15 15:28:25 lpllm.py:1048] j: load cost 0.45341014862060547 s waiting cost 0.0015294551849365234 s
DEBUG 10-15 15:28:25 lpllm.py:924] 
DEBUG 10-15 15:28:25 lpllm.py:924] decoder loop j: 50 cur_layer_idx: 25 layer_attn_idx: 25 layer_mlp_idx: 24
DEBUG 10-15 15:28:25 lpllm.py:933] start load next layer cur_layer_idx: 26
DEBUG 10-15 15:28:25 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed6e0>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x004\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x98\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:28:25 client.py:72] load_into_gpu: Mixtral-8x7B, ae7d329c-957a-4405-9143-2b8682585702
INFO 10-15 15:28:25 client.py:113] Model loaded: Mixtral-8x7B, ae7d329c-957a-4405-9143-2b8682585702
DEBUG 10-15 15:28:25 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:25 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:25 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:25 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:25 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:25 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:25 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:25 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:25 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:25 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:25 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:25 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:25 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:25 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:25 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:25 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:25 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:25 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:25 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:25 lpllm.py:2265] GPU2CPU move cost 0.000606 seconds
DEBUG 10-15 15:28:25 lpmodule.py:340] layer_idx 25 attn layer_idx: 25
DEBUG 10-15 15:28:25 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:25 lpmodule.py:364] decoder_attn_batch update batch_dim 720-780
DEBUG 10-15 15:28:25 StaticCacheLen.py:84] static cache update layer_idx: 25, start_batch: 720, end_batch: 780
DEBUG 10-15 15:28:25 lpmodule.py:368] update for kv cache 720-780 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:25 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:25 lpmodule.py:374] update past key value cost 0.021097 seconds
DEBUG 10-15 15:28:25 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:25 lpmodule.py:227] layer idx 24
DEBUG 10-15 15:28:25 lpmodule.py:399] repeat qkv cost 0.027853 seconds
DEBUG 10-15 15:28:25 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:25 lpmodule.py:433] dot attn cost 0.039222 seconds
DEBUG 10-15 15:28:25 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:25 lpmodule.py:444] time cost move to cuda:1 0.026857376098632812 s
DEBUG 10-15 15:28:25 lpllm.py:2283] CPU attn cost 0.143984 seconds if batch True
DEBUG 10-15 15:28:25 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:25 lpllm.py:2294] CPU compute cost 0.144875 seconds
DEBUG 10-15 15:28:25 lpllm.py:2312] free cost 0.000075 seconds
DEBUG 10-15 15:28:25 lpllm.py:503] reset update experts
DEBUG 10-15 15:28:25 lpllm.py:1774] update state cost 2.0742416381835938e-05 s
DEBUG 10-15 15:28:25 lpllm.py:1743] restore layer func cost 0.0008876323699951172 s
DEBUG 10-15 15:28:25 lpllm.py:511] restore layer cost 0.0011458396911621094 s
DEBUG 10-15 15:28:25 lpllm.py:512] reset_next_layer_need: layer_attn_idx=25, layer_mlp_idx=25, j_loc=50
DEBUG 10-15 15:28:25 lpllm.py:1037] reset layer cost 0.001216888427734375 s
DEBUG 10-15 15:28:25 lpllm.py:1038] have reset next layer layer_attn 25 layer_mlp 25 
DEBUG 10-15 15:28:25 lpllm.py:924] 
DEBUG 10-15 15:28:25 lpllm.py:924] decoder loop j: 51 cur_layer_idx: 25 layer_attn_idx: 25 layer_mlp_idx: 25
DEBUG 10-15 15:28:25 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:25 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:25 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:25 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:25 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:25 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:25 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:25 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:25 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:25 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:25 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:25 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:25 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:25 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:25 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:25 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:25 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:25 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:25 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:25 lpllm.py:2265] GPU2CPU move cost 0.000596 seconds
DEBUG 10-15 15:28:25 lpmodule.py:340] layer_idx 25 attn layer_idx: 25
DEBUG 10-15 15:28:25 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:25 lpmodule.py:364] decoder_attn_batch update batch_dim 780-840
DEBUG 10-15 15:28:25 StaticCacheLen.py:84] static cache update layer_idx: 25, start_batch: 780, end_batch: 840
DEBUG 10-15 15:28:25 lpmodule.py:368] update for kv cache 780-840 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:25 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:25 lpmodule.py:374] update past key value cost 0.022150 seconds
DEBUG 10-15 15:28:25 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:25 lpmodule.py:227] layer idx 25
DEBUG 10-15 15:28:25 lpmodule.py:399] repeat qkv cost 0.028169 seconds
DEBUG 10-15 15:28:25 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:25 lpmodule.py:433] dot attn cost 0.039362 seconds
DEBUG 10-15 15:28:25 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:25 lpmodule.py:444] time cost move to cuda:1 0.024959802627563477 s
DEBUG 10-15 15:28:25 lpllm.py:2283] CPU attn cost 0.143451 seconds if batch True
DEBUG 10-15 15:28:25 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:25 lpllm.py:2294] CPU compute cost 0.144348 seconds
DEBUG 10-15 15:28:25 lpllm.py:2312] free cost 0.000075 seconds
DEBUG 10-15 15:28:26 lpllm.py:498] reset update attn
DEBUG 10-15 15:28:26 lpllm.py:1774] update state cost 2.3126602172851562e-05 s
DEBUG 10-15 15:28:26 lpllm.py:1743] restore layer func cost 0.0004353523254394531 s
DEBUG 10-15 15:28:26 lpllm.py:511] restore layer cost 0.0006804466247558594 s
DEBUG 10-15 15:28:26 lpllm.py:512] reset_next_layer_need: layer_attn_idx=26, layer_mlp_idx=25, j_loc=51
DEBUG 10-15 15:28:26 lpllm.py:1037] reset layer cost 0.0007665157318115234 s
DEBUG 10-15 15:28:26 lpllm.py:1038] have reset next layer layer_attn 26 layer_mlp 25 
DEBUG 10-15 15:28:26 lpllm.py:1044] j: 51 waiting the layer with layer_idx 26 before wait time 0.4443173408508301 s
INFO 10-15 15:28:26 client.py:117] confirm_model_loaded: Mixtral-8x7B, ae7d329c-957a-4405-9143-2b8682585702
INFO 10-15 15:28:26 client.py:125] Model loaded
DEBUG 10-15 15:28:26 lpllm.py:1048] j: load cost 0.446028470993042 s waiting cost 0.0016968250274658203 s
DEBUG 10-15 15:28:26 lpllm.py:924] 
DEBUG 10-15 15:28:26 lpllm.py:924] decoder loop j: 52 cur_layer_idx: 26 layer_attn_idx: 26 layer_mlp_idx: 25
DEBUG 10-15 15:28:26 lpllm.py:933] start load next layer cur_layer_idx: 27
DEBUG 10-15 15:28:26 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:28:26 client.py:72] load_into_gpu: Mixtral-8x7B, 1fa3f906-d8d4-4759-9ad0-21c494fbee90
INFO 10-15 15:28:26 client.py:113] Model loaded: Mixtral-8x7B, 1fa3f906-d8d4-4759-9ad0-21c494fbee90
DEBUG 10-15 15:28:26 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:26 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:26 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:26 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:26 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:26 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:26 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:26 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:26 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:26 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:26 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:26 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:26 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:26 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:26 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:26 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:26 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:26 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:26 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:26 lpllm.py:2265] GPU2CPU move cost 0.000649 seconds
DEBUG 10-15 15:28:26 lpmodule.py:340] layer_idx 26 attn layer_idx: 26
DEBUG 10-15 15:28:26 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:26 lpmodule.py:364] decoder_attn_batch update batch_dim 720-780
DEBUG 10-15 15:28:26 StaticCacheLen.py:84] static cache update layer_idx: 26, start_batch: 720, end_batch: 780
DEBUG 10-15 15:28:26 lpmodule.py:368] update for kv cache 720-780 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:26 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:26 lpmodule.py:374] update past key value cost 0.022189 seconds
DEBUG 10-15 15:28:26 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:26 lpmodule.py:227] layer idx 25
DEBUG 10-15 15:28:26 lpmodule.py:399] repeat qkv cost 0.027484 seconds
DEBUG 10-15 15:28:26 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:26 lpmodule.py:433] dot attn cost 0.036694 seconds
DEBUG 10-15 15:28:26 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:26 lpmodule.py:444] time cost move to cuda:1 0.027199983596801758 s
DEBUG 10-15 15:28:26 lpllm.py:2283] CPU attn cost 0.142154 seconds if batch True
DEBUG 10-15 15:28:26 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:26 lpllm.py:2294] CPU compute cost 0.143079 seconds
DEBUG 10-15 15:28:26 lpllm.py:2312] free cost 0.000073 seconds
DEBUG 10-15 15:28:26 lpllm.py:503] reset update experts
DEBUG 10-15 15:28:26 lpllm.py:1774] update state cost 3.647804260253906e-05 s
DEBUG 10-15 15:28:26 lpllm.py:1743] restore layer func cost 0.0008778572082519531 s
DEBUG 10-15 15:28:26 lpllm.py:511] restore layer cost 0.0011448860168457031 s
DEBUG 10-15 15:28:26 lpllm.py:512] reset_next_layer_need: layer_attn_idx=26, layer_mlp_idx=26, j_loc=52
DEBUG 10-15 15:28:26 lpllm.py:1037] reset layer cost 0.001214742660522461 s
DEBUG 10-15 15:28:26 lpllm.py:1038] have reset next layer layer_attn 26 layer_mlp 26 
DEBUG 10-15 15:28:26 lpllm.py:924] 
DEBUG 10-15 15:28:26 lpllm.py:924] decoder loop j: 53 cur_layer_idx: 26 layer_attn_idx: 26 layer_mlp_idx: 26
DEBUG 10-15 15:28:26 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:26 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:26 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:26 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:26 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:26 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:26 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:26 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:26 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:26 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:26 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:26 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:26 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:26 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:26 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:26 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:26 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:26 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:26 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:26 lpllm.py:2265] GPU2CPU move cost 0.000574 seconds
DEBUG 10-15 15:28:26 lpmodule.py:340] layer_idx 26 attn layer_idx: 26
DEBUG 10-15 15:28:26 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:26 lpmodule.py:364] decoder_attn_batch update batch_dim 780-840
DEBUG 10-15 15:28:26 StaticCacheLen.py:84] static cache update layer_idx: 26, start_batch: 780, end_batch: 840
DEBUG 10-15 15:28:26 lpmodule.py:368] update for kv cache 780-840 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:26 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:26 lpmodule.py:374] update past key value cost 0.025009 seconds
DEBUG 10-15 15:28:26 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:26 lpmodule.py:227] layer idx 26
DEBUG 10-15 15:28:26 lpmodule.py:399] repeat qkv cost 0.028293 seconds
DEBUG 10-15 15:28:26 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:26 lpmodule.py:433] dot attn cost 0.034067 seconds
DEBUG 10-15 15:28:26 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:26 lpmodule.py:444] time cost move to cuda:1 0.025084257125854492 s
DEBUG 10-15 15:28:26 lpllm.py:2283] CPU attn cost 0.140893 seconds if batch True
DEBUG 10-15 15:28:26 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:26 lpllm.py:2294] CPU compute cost 0.141707 seconds
DEBUG 10-15 15:28:26 lpllm.py:2312] free cost 0.000076 seconds
DEBUG 10-15 15:28:26 lpllm.py:498] reset update attn
DEBUG 10-15 15:28:26 lpllm.py:1774] update state cost 2.2411346435546875e-05 s
DEBUG 10-15 15:28:26 lpllm.py:1743] restore layer func cost 0.00039887428283691406 s
DEBUG 10-15 15:28:26 lpllm.py:511] restore layer cost 0.0006306171417236328 s
DEBUG 10-15 15:28:26 lpllm.py:512] reset_next_layer_need: layer_attn_idx=27, layer_mlp_idx=26, j_loc=53
DEBUG 10-15 15:28:26 lpllm.py:1037] reset layer cost 0.0007040500640869141 s
DEBUG 10-15 15:28:26 lpllm.py:1038] have reset next layer layer_attn 27 layer_mlp 26 
DEBUG 10-15 15:28:26 lpllm.py:1044] j: 53 waiting the layer with layer_idx 27 before wait time 0.4504728317260742 s
INFO 10-15 15:28:26 client.py:117] confirm_model_loaded: Mixtral-8x7B, 1fa3f906-d8d4-4759-9ad0-21c494fbee90
INFO 10-15 15:28:26 client.py:125] Model loaded
DEBUG 10-15 15:28:26 lpllm.py:1048] j: load cost 0.4521219730377197 s waiting cost 0.0016317367553710938 s
DEBUG 10-15 15:28:26 lpllm.py:924] 
DEBUG 10-15 15:28:26 lpllm.py:924] decoder loop j: 54 cur_layer_idx: 27 layer_attn_idx: 27 layer_mlp_idx: 26
DEBUG 10-15 15:28:26 lpllm.py:933] start load next layer cur_layer_idx: 28
DEBUG 10-15 15:28:26 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed650>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x003\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x96\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:28:26 client.py:72] load_into_gpu: Mixtral-8x7B, 56e55fca-48d3-4c05-8362-d60cdc388c29
INFO 10-15 15:28:26 client.py:113] Model loaded: Mixtral-8x7B, 56e55fca-48d3-4c05-8362-d60cdc388c29
DEBUG 10-15 15:28:26 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:26 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:26 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:26 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:26 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:26 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:26 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:26 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:26 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:26 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:26 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:26 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:26 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:26 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:26 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:26 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:26 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:26 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:26 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:26 lpllm.py:2265] GPU2CPU move cost 0.000619 seconds
DEBUG 10-15 15:28:26 lpmodule.py:340] layer_idx 27 attn layer_idx: 27
DEBUG 10-15 15:28:26 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:26 lpmodule.py:364] decoder_attn_batch update batch_dim 720-780
DEBUG 10-15 15:28:26 StaticCacheLen.py:84] static cache update layer_idx: 27, start_batch: 720, end_batch: 780
DEBUG 10-15 15:28:26 lpmodule.py:368] update for kv cache 720-780 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:26 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:26 lpmodule.py:374] update past key value cost 0.023576 seconds
DEBUG 10-15 15:28:26 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:26 lpmodule.py:227] layer idx 26
DEBUG 10-15 15:28:26 lpmodule.py:399] repeat qkv cost 0.027669 seconds
DEBUG 10-15 15:28:26 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:26 lpmodule.py:433] dot attn cost 0.034870 seconds
DEBUG 10-15 15:28:26 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:26 lpmodule.py:444] time cost move to cuda:1 0.02530646324157715 s
DEBUG 10-15 15:28:26 lpllm.py:2283] CPU attn cost 0.139657 seconds if batch True
DEBUG 10-15 15:28:26 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:26 lpllm.py:2294] CPU compute cost 0.140583 seconds
DEBUG 10-15 15:28:26 lpllm.py:2312] free cost 0.000072 seconds
DEBUG 10-15 15:28:26 lpllm.py:503] reset update experts
DEBUG 10-15 15:28:26 lpllm.py:1774] update state cost 2.09808349609375e-05 s
DEBUG 10-15 15:28:26 lpllm.py:1743] restore layer func cost 0.0008912086486816406 s
DEBUG 10-15 15:28:26 lpllm.py:511] restore layer cost 0.0011353492736816406 s
DEBUG 10-15 15:28:26 lpllm.py:512] reset_next_layer_need: layer_attn_idx=27, layer_mlp_idx=27, j_loc=54
DEBUG 10-15 15:28:26 lpllm.py:1037] reset layer cost 0.0012083053588867188 s
DEBUG 10-15 15:28:26 lpllm.py:1038] have reset next layer layer_attn 27 layer_mlp 27 
DEBUG 10-15 15:28:26 lpllm.py:924] 
DEBUG 10-15 15:28:26 lpllm.py:924] decoder loop j: 55 cur_layer_idx: 27 layer_attn_idx: 27 layer_mlp_idx: 27
DEBUG 10-15 15:28:26 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:26 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:26 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:26 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:26 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:26 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:26 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:26 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:26 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:26 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:26 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:26 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:26 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:26 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:26 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:26 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:26 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:26 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:26 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:26 lpllm.py:2265] GPU2CPU move cost 0.000581 seconds
DEBUG 10-15 15:28:26 lpmodule.py:340] layer_idx 27 attn layer_idx: 27
DEBUG 10-15 15:28:26 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:26 lpmodule.py:364] decoder_attn_batch update batch_dim 780-840
DEBUG 10-15 15:28:26 StaticCacheLen.py:84] static cache update layer_idx: 27, start_batch: 780, end_batch: 840
DEBUG 10-15 15:28:26 lpmodule.py:368] update for kv cache 780-840 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:26 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:26 lpmodule.py:374] update past key value cost 0.024133 seconds
DEBUG 10-15 15:28:26 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:26 lpmodule.py:227] layer idx 27
DEBUG 10-15 15:28:26 lpmodule.py:399] repeat qkv cost 0.027961 seconds
DEBUG 10-15 15:28:26 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:26 lpmodule.py:433] dot attn cost 0.035295 seconds
DEBUG 10-15 15:28:26 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:26 lpmodule.py:444] time cost move to cuda:1 0.025038480758666992 s
DEBUG 10-15 15:28:26 lpllm.py:2283] CPU attn cost 0.148754 seconds if batch True
DEBUG 10-15 15:28:26 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:26 lpllm.py:2294] CPU compute cost 0.149629 seconds
DEBUG 10-15 15:28:26 lpllm.py:2312] free cost 0.000075 seconds
DEBUG 10-15 15:28:26 lpllm.py:498] reset update attn
DEBUG 10-15 15:28:26 lpllm.py:1774] update state cost 3.933906555175781e-05 s
DEBUG 10-15 15:28:26 lpllm.py:1743] restore layer func cost 0.00041294097900390625 s
DEBUG 10-15 15:28:26 lpllm.py:511] restore layer cost 0.0006654262542724609 s
DEBUG 10-15 15:28:26 lpllm.py:512] reset_next_layer_need: layer_attn_idx=28, layer_mlp_idx=27, j_loc=55
DEBUG 10-15 15:28:26 lpllm.py:1037] reset layer cost 0.0007364749908447266 s
DEBUG 10-15 15:28:26 lpllm.py:1038] have reset next layer layer_attn 28 layer_mlp 27 
DEBUG 10-15 15:28:26 lpllm.py:1044] j: 55 waiting the layer with layer_idx 28 before wait time 0.4472675323486328 s
INFO 10-15 15:28:26 client.py:117] confirm_model_loaded: Mixtral-8x7B, 56e55fca-48d3-4c05-8362-d60cdc388c29
INFO 10-15 15:28:26 client.py:125] Model loaded
DEBUG 10-15 15:28:26 lpllm.py:1048] j: load cost 0.44869017601013184 s waiting cost 0.0014071464538574219 s
DEBUG 10-15 15:28:26 lpllm.py:924] 
DEBUG 10-15 15:28:26 lpllm.py:924] decoder loop j: 56 cur_layer_idx: 28 layer_attn_idx: 28 layer_mlp_idx: 27
DEBUG 10-15 15:28:26 lpllm.py:933] start load next layer cur_layer_idx: 29
DEBUG 10-15 15:28:26 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed6e0>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x004\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x98\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:28:26 client.py:72] load_into_gpu: Mixtral-8x7B, c1825a6d-7e5d-4d1c-8be9-3dab00eebfd2
INFO 10-15 15:28:26 client.py:113] Model loaded: Mixtral-8x7B, c1825a6d-7e5d-4d1c-8be9-3dab00eebfd2
DEBUG 10-15 15:28:26 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:26 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:26 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:26 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:26 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:26 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:26 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:26 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:26 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:26 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:26 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:26 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:27 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:27 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:27 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:27 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:27 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:27 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:27 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:27 lpllm.py:2265] GPU2CPU move cost 0.000645 seconds
DEBUG 10-15 15:28:27 lpmodule.py:340] layer_idx 28 attn layer_idx: 28
DEBUG 10-15 15:28:27 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:27 lpmodule.py:364] decoder_attn_batch update batch_dim 720-780
DEBUG 10-15 15:28:27 StaticCacheLen.py:84] static cache update layer_idx: 28, start_batch: 720, end_batch: 780
DEBUG 10-15 15:28:27 lpmodule.py:368] update for kv cache 720-780 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:27 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:27 lpmodule.py:374] update past key value cost 0.023085 seconds
DEBUG 10-15 15:28:27 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:27 lpmodule.py:227] layer idx 27
DEBUG 10-15 15:28:27 lpmodule.py:399] repeat qkv cost 0.028896 seconds
DEBUG 10-15 15:28:27 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:27 lpmodule.py:433] dot attn cost 0.034096 seconds
DEBUG 10-15 15:28:27 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:27 lpmodule.py:444] time cost move to cuda:1 0.02477288246154785 s
DEBUG 10-15 15:28:27 lpllm.py:2283] CPU attn cost 0.138690 seconds if batch True
DEBUG 10-15 15:28:27 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:27 lpllm.py:2294] CPU compute cost 0.139606 seconds
DEBUG 10-15 15:28:27 lpllm.py:2312] free cost 0.000071 seconds
DEBUG 10-15 15:28:27 lpllm.py:503] reset update experts
DEBUG 10-15 15:28:27 lpllm.py:1774] update state cost 2.2411346435546875e-05 s
DEBUG 10-15 15:28:27 lpllm.py:1743] restore layer func cost 0.0008881092071533203 s
DEBUG 10-15 15:28:27 lpllm.py:511] restore layer cost 0.0011429786682128906 s
DEBUG 10-15 15:28:27 lpllm.py:512] reset_next_layer_need: layer_attn_idx=28, layer_mlp_idx=28, j_loc=56
DEBUG 10-15 15:28:27 lpllm.py:1037] reset layer cost 0.0012159347534179688 s
DEBUG 10-15 15:28:27 lpllm.py:1038] have reset next layer layer_attn 28 layer_mlp 28 
DEBUG 10-15 15:28:27 lpllm.py:924] 
DEBUG 10-15 15:28:27 lpllm.py:924] decoder loop j: 57 cur_layer_idx: 28 layer_attn_idx: 28 layer_mlp_idx: 28
DEBUG 10-15 15:28:27 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:27 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:27 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:27 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:27 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:27 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:27 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:27 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:27 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:27 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:27 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:27 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:27 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:27 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:27 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:27 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:27 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:27 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:27 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:27 lpllm.py:2265] GPU2CPU move cost 0.000606 seconds
DEBUG 10-15 15:28:27 lpmodule.py:340] layer_idx 28 attn layer_idx: 28
DEBUG 10-15 15:28:27 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:27 lpmodule.py:364] decoder_attn_batch update batch_dim 780-840
DEBUG 10-15 15:28:27 StaticCacheLen.py:84] static cache update layer_idx: 28, start_batch: 780, end_batch: 840
DEBUG 10-15 15:28:27 lpmodule.py:368] update for kv cache 780-840 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:27 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:27 lpmodule.py:374] update past key value cost 0.024011 seconds
DEBUG 10-15 15:28:27 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:27 lpmodule.py:227] layer idx 28
DEBUG 10-15 15:28:27 lpmodule.py:399] repeat qkv cost 0.028529 seconds
DEBUG 10-15 15:28:27 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:27 lpmodule.py:433] dot attn cost 0.033964 seconds
DEBUG 10-15 15:28:27 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:27 lpmodule.py:444] time cost move to cuda:1 0.02481555938720703 s
DEBUG 10-15 15:28:27 lpllm.py:2283] CPU attn cost 0.138978 seconds if batch True
DEBUG 10-15 15:28:27 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:27 lpllm.py:2294] CPU compute cost 0.139941 seconds
DEBUG 10-15 15:28:27 lpllm.py:2312] free cost 0.000085 seconds
DEBUG 10-15 15:28:27 lpllm.py:498] reset update attn
DEBUG 10-15 15:28:27 lpllm.py:1774] update state cost 2.4318695068359375e-05 s
DEBUG 10-15 15:28:27 lpllm.py:1743] restore layer func cost 0.00041747093200683594 s
DEBUG 10-15 15:28:27 lpllm.py:511] restore layer cost 0.0006539821624755859 s
DEBUG 10-15 15:28:27 lpllm.py:512] reset_next_layer_need: layer_attn_idx=29, layer_mlp_idx=28, j_loc=57
DEBUG 10-15 15:28:27 lpllm.py:1037] reset layer cost 0.0007243156433105469 s
DEBUG 10-15 15:28:27 lpllm.py:1038] have reset next layer layer_attn 29 layer_mlp 28 
DEBUG 10-15 15:28:27 lpllm.py:1044] j: 57 waiting the layer with layer_idx 29 before wait time 0.49034929275512695 s
INFO 10-15 15:28:27 client.py:117] confirm_model_loaded: Mixtral-8x7B, c1825a6d-7e5d-4d1c-8be9-3dab00eebfd2
INFO 10-15 15:28:27 client.py:125] Model loaded
DEBUG 10-15 15:28:27 lpllm.py:1048] j: load cost 0.4920463562011719 s waiting cost 0.001682281494140625 s
DEBUG 10-15 15:28:27 lpllm.py:924] 
DEBUG 10-15 15:28:27 lpllm.py:924] decoder loop j: 58 cur_layer_idx: 29 layer_attn_idx: 29 layer_mlp_idx: 28
DEBUG 10-15 15:28:27 lpllm.py:933] start load next layer cur_layer_idx: 30
DEBUG 10-15 15:28:27 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:28:27 client.py:72] load_into_gpu: Mixtral-8x7B, af104912-45c0-489f-9a12-dd73c65f0efa
INFO 10-15 15:28:27 client.py:113] Model loaded: Mixtral-8x7B, af104912-45c0-489f-9a12-dd73c65f0efa
DEBUG 10-15 15:28:27 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:27 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:27 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:27 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:27 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:27 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:27 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:27 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:27 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:27 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:27 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:27 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:27 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:27 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:27 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:27 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:27 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:27 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:27 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:27 lpllm.py:2265] GPU2CPU move cost 0.000450 seconds
DEBUG 10-15 15:28:27 lpmodule.py:340] layer_idx 29 attn layer_idx: 29
DEBUG 10-15 15:28:27 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:27 lpmodule.py:364] decoder_attn_batch update batch_dim 720-780
DEBUG 10-15 15:28:27 StaticCacheLen.py:84] static cache update layer_idx: 29, start_batch: 720, end_batch: 780
DEBUG 10-15 15:28:27 lpmodule.py:368] update for kv cache 720-780 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:27 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:27 lpmodule.py:374] update past key value cost 0.024393 seconds
DEBUG 10-15 15:28:27 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:27 lpmodule.py:227] layer idx 28
DEBUG 10-15 15:28:27 lpmodule.py:399] repeat qkv cost 0.028800 seconds
DEBUG 10-15 15:28:27 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:27 lpmodule.py:433] dot attn cost 0.036534 seconds
DEBUG 10-15 15:28:27 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:27 lpmodule.py:444] time cost move to cuda:1 0.025540828704833984 s
DEBUG 10-15 15:28:27 lpllm.py:2283] CPU attn cost 0.142574 seconds if batch True
DEBUG 10-15 15:28:27 lpllm.py:2292] deal attn result cost 0.000003 seconds
DEBUG 10-15 15:28:27 lpllm.py:2294] CPU compute cost 0.143289 seconds
DEBUG 10-15 15:28:27 lpllm.py:2312] free cost 0.000074 seconds
DEBUG 10-15 15:28:27 lpllm.py:503] reset update experts
DEBUG 10-15 15:28:27 lpllm.py:1774] update state cost 2.288818359375e-05 s
DEBUG 10-15 15:28:27 lpllm.py:1743] restore layer func cost 0.0008919239044189453 s
DEBUG 10-15 15:28:27 lpllm.py:511] restore layer cost 0.0011456012725830078 s
DEBUG 10-15 15:28:27 lpllm.py:512] reset_next_layer_need: layer_attn_idx=29, layer_mlp_idx=29, j_loc=58
DEBUG 10-15 15:28:27 lpllm.py:1037] reset layer cost 0.0012199878692626953 s
DEBUG 10-15 15:28:27 lpllm.py:1038] have reset next layer layer_attn 29 layer_mlp 29 
DEBUG 10-15 15:28:27 lpllm.py:924] 
DEBUG 10-15 15:28:27 lpllm.py:924] decoder loop j: 59 cur_layer_idx: 29 layer_attn_idx: 29 layer_mlp_idx: 29
DEBUG 10-15 15:28:27 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:27 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:27 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:27 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:27 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:27 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:27 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:27 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:27 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:27 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:27 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:27 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:27 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:27 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:27 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:27 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:27 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:27 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:27 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:27 lpllm.py:2265] GPU2CPU move cost 0.000622 seconds
DEBUG 10-15 15:28:27 lpmodule.py:340] layer_idx 29 attn layer_idx: 29
DEBUG 10-15 15:28:27 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:27 lpmodule.py:364] decoder_attn_batch update batch_dim 780-840
DEBUG 10-15 15:28:27 StaticCacheLen.py:84] static cache update layer_idx: 29, start_batch: 780, end_batch: 840
DEBUG 10-15 15:28:27 lpmodule.py:368] update for kv cache 780-840 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:27 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:27 lpmodule.py:374] update past key value cost 0.022909 seconds
DEBUG 10-15 15:28:27 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:27 lpmodule.py:227] layer idx 29
DEBUG 10-15 15:28:27 lpmodule.py:399] repeat qkv cost 0.029085 seconds
DEBUG 10-15 15:28:27 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:27 lpmodule.py:433] dot attn cost 0.031640 seconds
DEBUG 10-15 15:28:27 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:27 lpmodule.py:444] time cost move to cuda:1 0.025989770889282227 s
DEBUG 10-15 15:28:27 lpllm.py:2283] CPU attn cost 0.137521 seconds if batch True
DEBUG 10-15 15:28:27 lpllm.py:2292] deal attn result cost 0.000003 seconds
DEBUG 10-15 15:28:27 lpllm.py:2294] CPU compute cost 0.138444 seconds
DEBUG 10-15 15:28:27 lpllm.py:2312] free cost 0.000108 seconds
DEBUG 10-15 15:28:27 lpllm.py:498] reset update attn
DEBUG 10-15 15:28:27 lpllm.py:1774] update state cost 3.552436828613281e-05 s
DEBUG 10-15 15:28:27 lpllm.py:1743] restore layer func cost 0.0004169940948486328 s
DEBUG 10-15 15:28:27 lpllm.py:511] restore layer cost 0.0006666183471679688 s
DEBUG 10-15 15:28:27 lpllm.py:512] reset_next_layer_need: layer_attn_idx=30, layer_mlp_idx=29, j_loc=59
DEBUG 10-15 15:28:27 lpllm.py:1037] reset layer cost 0.0007364749908447266 s
DEBUG 10-15 15:28:27 lpllm.py:1038] have reset next layer layer_attn 30 layer_mlp 29 
DEBUG 10-15 15:28:27 lpllm.py:1044] j: 59 waiting the layer with layer_idx 30 before wait time 0.4620063304901123 s
INFO 10-15 15:28:27 client.py:117] confirm_model_loaded: Mixtral-8x7B, af104912-45c0-489f-9a12-dd73c65f0efa
INFO 10-15 15:28:27 client.py:125] Model loaded
DEBUG 10-15 15:28:27 lpllm.py:1048] j: load cost 0.4636828899383545 s waiting cost 0.0016605854034423828 s
DEBUG 10-15 15:28:27 lpllm.py:924] 
DEBUG 10-15 15:28:27 lpllm.py:924] decoder loop j: 60 cur_layer_idx: 30 layer_attn_idx: 30 layer_mlp_idx: 29
DEBUG 10-15 15:28:27 lpllm.py:933] start load next layer cur_layer_idx: 31
DEBUG 10-15 15:28:27 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed650>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x003\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x96\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:28:27 client.py:72] load_into_gpu: Mixtral-8x7B, 0fbb090f-009a-424d-aed0-5d7ce5c52188
INFO 10-15 15:28:27 client.py:113] Model loaded: Mixtral-8x7B, 0fbb090f-009a-424d-aed0-5d7ce5c52188
DEBUG 10-15 15:28:27 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:27 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:27 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:27 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:27 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:27 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:27 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:27 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:27 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:27 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:27 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:27 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:27 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:27 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:27 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:27 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:27 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:27 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:27 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:27 lpllm.py:2265] GPU2CPU move cost 0.000571 seconds
DEBUG 10-15 15:28:27 lpmodule.py:340] layer_idx 30 attn layer_idx: 30
DEBUG 10-15 15:28:27 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:27 lpmodule.py:364] decoder_attn_batch update batch_dim 720-780
DEBUG 10-15 15:28:27 StaticCacheLen.py:84] static cache update layer_idx: 30, start_batch: 720, end_batch: 780
DEBUG 10-15 15:28:27 lpmodule.py:368] update for kv cache 720-780 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:27 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:27 lpmodule.py:374] update past key value cost 0.022940 seconds
DEBUG 10-15 15:28:27 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:27 lpmodule.py:227] layer idx 29
DEBUG 10-15 15:28:27 lpmodule.py:399] repeat qkv cost 0.029533 seconds
DEBUG 10-15 15:28:27 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:27 lpmodule.py:433] dot attn cost 0.032450 seconds
DEBUG 10-15 15:28:27 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:28 lpmodule.py:444] time cost move to cuda:1 0.024651765823364258 s
DEBUG 10-15 15:28:28 lpllm.py:2283] CPU attn cost 0.136737 seconds if batch True
DEBUG 10-15 15:28:28 lpllm.py:2292] deal attn result cost 0.000003 seconds
DEBUG 10-15 15:28:28 lpllm.py:2294] CPU compute cost 0.137591 seconds
DEBUG 10-15 15:28:28 lpllm.py:2312] free cost 0.000074 seconds
DEBUG 10-15 15:28:28 lpllm.py:503] reset update experts
DEBUG 10-15 15:28:28 lpllm.py:1774] update state cost 3.719329833984375e-05 s
DEBUG 10-15 15:28:28 lpllm.py:1743] restore layer func cost 0.0008392333984375 s
DEBUG 10-15 15:28:28 lpllm.py:511] restore layer cost 0.0010988712310791016 s
DEBUG 10-15 15:28:28 lpllm.py:512] reset_next_layer_need: layer_attn_idx=30, layer_mlp_idx=30, j_loc=60
DEBUG 10-15 15:28:28 lpllm.py:1037] reset layer cost 0.001171112060546875 s
DEBUG 10-15 15:28:28 lpllm.py:1038] have reset next layer layer_attn 30 layer_mlp 30 
DEBUG 10-15 15:28:28 lpllm.py:924] 
DEBUG 10-15 15:28:28 lpllm.py:924] decoder loop j: 61 cur_layer_idx: 30 layer_attn_idx: 30 layer_mlp_idx: 30
DEBUG 10-15 15:28:28 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:28 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:28 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:28 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:28 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:28 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:28 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:28 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:28 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:28 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:28 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:28 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:28 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:28 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:28 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:28 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:28 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:28 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:28 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:28 lpllm.py:2265] GPU2CPU move cost 0.000614 seconds
DEBUG 10-15 15:28:28 lpmodule.py:340] layer_idx 30 attn layer_idx: 30
DEBUG 10-15 15:28:28 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:28 lpmodule.py:364] decoder_attn_batch update batch_dim 780-840
DEBUG 10-15 15:28:28 StaticCacheLen.py:84] static cache update layer_idx: 30, start_batch: 780, end_batch: 840
DEBUG 10-15 15:28:28 lpmodule.py:368] update for kv cache 780-840 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:28 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:28 lpmodule.py:374] update past key value cost 0.023100 seconds
DEBUG 10-15 15:28:28 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:28 lpmodule.py:227] layer idx 30
DEBUG 10-15 15:28:28 lpmodule.py:399] repeat qkv cost 0.028624 seconds
DEBUG 10-15 15:28:28 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:28 lpmodule.py:433] dot attn cost 0.038233 seconds
DEBUG 10-15 15:28:28 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:28 lpmodule.py:444] time cost move to cuda:1 0.02593517303466797 s
DEBUG 10-15 15:28:28 lpllm.py:2283] CPU attn cost 0.143520 seconds if batch True
DEBUG 10-15 15:28:28 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:28 lpllm.py:2294] CPU compute cost 0.144417 seconds
DEBUG 10-15 15:28:28 lpllm.py:2312] free cost 0.000073 seconds
DEBUG 10-15 15:28:28 lpllm.py:498] reset update attn
DEBUG 10-15 15:28:28 lpllm.py:1774] update state cost 2.288818359375e-05 s
DEBUG 10-15 15:28:28 lpllm.py:1743] restore layer func cost 0.0004024505615234375 s
DEBUG 10-15 15:28:28 lpllm.py:511] restore layer cost 0.0006508827209472656 s
DEBUG 10-15 15:28:28 lpllm.py:512] reset_next_layer_need: layer_attn_idx=31, layer_mlp_idx=30, j_loc=61
DEBUG 10-15 15:28:28 lpllm.py:1037] reset layer cost 0.0007221698760986328 s
DEBUG 10-15 15:28:28 lpllm.py:1038] have reset next layer layer_attn 31 layer_mlp 30 
DEBUG 10-15 15:28:28 lpllm.py:1044] j: 61 waiting the layer with layer_idx 31 before wait time 0.45470356941223145 s
INFO 10-15 15:28:28 client.py:117] confirm_model_loaded: Mixtral-8x7B, 0fbb090f-009a-424d-aed0-5d7ce5c52188
INFO 10-15 15:28:28 client.py:125] Model loaded
DEBUG 10-15 15:28:28 lpllm.py:1048] j: load cost 0.456376314163208 s waiting cost 0.00165557861328125 s
DEBUG 10-15 15:28:28 lpllm.py:924] 
DEBUG 10-15 15:28:28 lpllm.py:924] decoder loop j: 62 cur_layer_idx: 31 layer_attn_idx: 31 layer_mlp_idx: 30
DEBUG 10-15 15:28:28 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:28 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:28 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:28 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:28 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:28 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:28 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:28 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:28 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:28 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:28 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:28 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:28 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:28 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:28 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:28 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:28 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:28 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:28 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:28 lpllm.py:2265] GPU2CPU move cost 0.000578 seconds
DEBUG 10-15 15:28:28 lpmodule.py:340] layer_idx 31 attn layer_idx: 31
DEBUG 10-15 15:28:28 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:28 lpmodule.py:364] decoder_attn_batch update batch_dim 720-780
DEBUG 10-15 15:28:28 StaticCacheLen.py:84] static cache update layer_idx: 31, start_batch: 720, end_batch: 780
DEBUG 10-15 15:28:28 lpmodule.py:368] update for kv cache 720-780 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:28 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:28 lpmodule.py:374] update past key value cost 0.024264 seconds
DEBUG 10-15 15:28:28 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:28 lpmodule.py:227] layer idx 30
DEBUG 10-15 15:28:28 lpmodule.py:399] repeat qkv cost 0.032290 seconds
DEBUG 10-15 15:28:28 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:28 lpmodule.py:433] dot attn cost 0.036721 seconds
DEBUG 10-15 15:28:28 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:28 lpmodule.py:444] time cost move to cuda:1 0.02688908576965332 s
DEBUG 10-15 15:28:28 lpllm.py:2283] CPU attn cost 0.148049 seconds if batch True
DEBUG 10-15 15:28:28 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:28 lpllm.py:2294] CPU compute cost 0.148924 seconds
DEBUG 10-15 15:28:28 lpllm.py:2312] free cost 0.000075 seconds
DEBUG 10-15 15:28:28 lpllm.py:503] reset update experts
DEBUG 10-15 15:28:28 lpllm.py:1774] update state cost 2.1696090698242188e-05 s
DEBUG 10-15 15:28:28 lpllm.py:1743] restore layer func cost 0.0008692741394042969 s
DEBUG 10-15 15:28:28 lpllm.py:511] restore layer cost 0.001116037368774414 s
DEBUG 10-15 15:28:28 lpllm.py:512] reset_next_layer_need: layer_attn_idx=31, layer_mlp_idx=31, j_loc=62
DEBUG 10-15 15:28:28 lpllm.py:1037] reset layer cost 0.0012068748474121094 s
DEBUG 10-15 15:28:28 lpllm.py:1038] have reset next layer layer_attn 31 layer_mlp 31 
DEBUG 10-15 15:28:28 lpllm.py:924] 
DEBUG 10-15 15:28:28 lpllm.py:924] decoder loop j: 63 cur_layer_idx: 31 layer_attn_idx: 31 layer_mlp_idx: 31
DEBUG 10-15 15:28:28 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:28 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:28 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:28 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:28 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:28 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:28 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:28 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:28 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:28 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:28 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:28 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:28 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:28 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:28 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:28 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:28 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:28 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:28 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:28 lpllm.py:2265] GPU2CPU move cost 0.000616 seconds
DEBUG 10-15 15:28:28 lpmodule.py:340] layer_idx 31 attn layer_idx: 31
DEBUG 10-15 15:28:28 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:28 lpmodule.py:364] decoder_attn_batch update batch_dim 780-840
DEBUG 10-15 15:28:28 StaticCacheLen.py:84] static cache update layer_idx: 31, start_batch: 780, end_batch: 840
DEBUG 10-15 15:28:28 lpmodule.py:368] update for kv cache 780-840 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:28 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:28 lpmodule.py:374] update past key value cost 0.025834 seconds
DEBUG 10-15 15:28:28 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:28 lpmodule.py:227] layer idx 31
DEBUG 10-15 15:28:28 lpmodule.py:399] repeat qkv cost 0.029347 seconds
DEBUG 10-15 15:28:28 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:28 lpmodule.py:433] dot attn cost 0.034391 seconds
DEBUG 10-15 15:28:28 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:28 lpmodule.py:444] time cost move to cuda:1 0.02458477020263672 s
DEBUG 10-15 15:28:28 lpllm.py:2283] CPU attn cost 0.143908 seconds if batch True
DEBUG 10-15 15:28:28 lpllm.py:2292] deal attn result cost 0.000003 seconds
DEBUG 10-15 15:28:28 lpllm.py:2294] CPU compute cost 0.144811 seconds
DEBUG 10-15 15:28:28 lpllm.py:2312] free cost 0.000079 seconds
DEBUG 10-15 15:28:28 lpllm.py:924] 
DEBUG 10-15 15:28:28 lpllm.py:924] decoder loop j: 64 cur_layer_idx: 32 layer_attn_idx: 32 layer_mlp_idx: 31
DEBUG 10-15 15:28:28 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:28 lpmodule.py:227] layer idx 31
DEBUG 10-15 15:28:28 lpllm.py:1085] last_mlp_output_chunk shape: torch.Size([60, 512, 4096]), mlp_output_chunk shape: torch.Size([60, 512, 4096])
DEBUG 10-15 15:28:28 lpllm.py:1086] last_mlp_output_chunk len 1, mlp_output_chunk len 1
DEBUG 10-15 15:28:28 lpllm.py:618] decoders batch for 6 cost 15.08567190170288 s
DEBUG 10-15 15:28:28 lpllm.py:848] hidden_states_chunks1 shape: torch.Size([60, 512, 4096]), hidden_states_chunks1 length: 1
DEBUG 10-15 15:28:28 lpllm.py:849] hidden_states_chunks2 shape: torch.Size([60, 512, 4096]), hidden_states_chunks2 length: 1
DEBUG 10-15 15:28:28 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed6e0>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x004\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x98\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:28:28 client.py:72] load_into_gpu: Mixtral-8x7B, 37d64d01-4ef1-4bf7-ab06-de2a9129d5f4
INFO 10-15 15:28:28 client.py:113] Model loaded: Mixtral-8x7B, 37d64d01-4ef1-4bf7-ab06-de2a9129d5f4
DEBUG 10-15 15:28:28 lpllm.py:1743] restore layer func cost 0.0010292530059814453 s
INFO 10-15 15:28:28 client.py:117] confirm_model_loaded: Mixtral-8x7B, 37d64d01-4ef1-4bf7-ab06-de2a9129d5f4
INFO 10-15 15:28:29 client.py:125] Model loaded
DEBUG 10-15 15:28:29 lpllm.py:422] prepare layer cost 0.2696552276611328 s
DEBUG 10-15 15:28:29 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:28:29 client.py:72] load_into_gpu: Mixtral-8x7B, 01929ee0-015b-4e47-a26d-a191663140fe
INFO 10-15 15:28:29 client.py:113] Model loaded: Mixtral-8x7B, 01929ee0-015b-4e47-a26d-a191663140fe
DEBUG 10-15 15:28:29 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:29 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:29 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:29 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:29 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:29 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:29 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:29 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:29 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:29 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:29 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:29 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:29 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:29 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:29 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:29 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:29 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:29 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:29 lpllm.py:924] 
DEBUG 10-15 15:28:29 lpllm.py:924] decoder loop j: 1 cur_layer_idx: 0 layer_attn_idx: 0 layer_mlp_idx: 0
DEBUG 10-15 15:28:29 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:29 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:29 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:29 lpllm.py:2265] GPU2CPU move cost 0.000577 seconds
DEBUG 10-15 15:28:29 lpmodule.py:340] layer_idx 0 attn layer_idx: 0
DEBUG 10-15 15:28:29 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:29 lpmodule.py:364] decoder_attn_batch update batch_dim 840-900
DEBUG 10-15 15:28:29 StaticCacheLen.py:84] static cache update layer_idx: 0, start_batch: 840, end_batch: 900
DEBUG 10-15 15:28:29 lpmodule.py:368] update for kv cache 840-900 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:29 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:29 lpmodule.py:374] update past key value cost 0.022874 seconds
DEBUG 10-15 15:28:29 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:29 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:29 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:29 lpmodule.py:399] repeat qkv cost 0.028996 seconds
DEBUG 10-15 15:28:29 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:29 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:29 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:29 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:29 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:29 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:29 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:29 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:29 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:29 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:29 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:29 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:29 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:29 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:29 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:29 lpmodule.py:433] dot attn cost 0.035760 seconds
DEBUG 10-15 15:28:29 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:29 lpmodule.py:444] time cost move to cuda:1 0.0245819091796875 s
DEBUG 10-15 15:28:29 lpllm.py:2283] CPU attn cost 0.139810 seconds if batch True
DEBUG 10-15 15:28:29 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:29 lpllm.py:2294] CPU compute cost 0.140673 seconds
DEBUG 10-15 15:28:29 lpllm.py:2312] free cost 0.000075 seconds
DEBUG 10-15 15:28:29 lpllm.py:2265] GPU2CPU move cost 0.000245 seconds
DEBUG 10-15 15:28:29 lpmodule.py:340] layer_idx 0 attn layer_idx: 0
DEBUG 10-15 15:28:29 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:29 lpmodule.py:364] decoder_attn_batch update batch_dim 900-960
DEBUG 10-15 15:28:29 lpmodule.py:227] layer idx 0
DEBUG 10-15 15:28:29 StaticCacheLen.py:84] static cache update layer_idx: 0, start_batch: 900, end_batch: 960
DEBUG 10-15 15:28:29 StaticCacheLen.py:90] static cache update layer_idx: 0, update seq_length to 512
DEBUG 10-15 15:28:29 lpmodule.py:368] update for kv cache 900-960 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:29 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:29 lpmodule.py:374] update past key value cost 0.021448 seconds
DEBUG 10-15 15:28:29 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:29 lpmodule.py:399] repeat qkv cost 0.029192 seconds
DEBUG 10-15 15:28:29 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:29 lpmodule.py:433] dot attn cost 0.037317 seconds
DEBUG 10-15 15:28:29 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:29 lpmodule.py:444] time cost move to cuda:1 0.025053977966308594 s
DEBUG 10-15 15:28:29 lpllm.py:2283] CPU attn cost 0.140791 seconds if batch True
DEBUG 10-15 15:28:29 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:29 lpllm.py:2294] CPU compute cost 0.141212 seconds
DEBUG 10-15 15:28:29 lpllm.py:2312] free cost 0.000072 seconds
DEBUG 10-15 15:28:29 lpllm.py:498] reset update attn
DEBUG 10-15 15:28:29 lpllm.py:1774] update state cost 2.193450927734375e-05 s
DEBUG 10-15 15:28:29 lpllm.py:1743] restore layer func cost 0.0004189014434814453 s
DEBUG 10-15 15:28:29 lpllm.py:511] restore layer cost 0.0006737709045410156 s
DEBUG 10-15 15:28:29 lpllm.py:512] reset_next_layer_need: layer_attn_idx=1, layer_mlp_idx=0, j_loc=1
DEBUG 10-15 15:28:29 lpllm.py:1037] reset layer cost 0.000743865966796875 s
DEBUG 10-15 15:28:29 lpllm.py:1038] have reset next layer layer_attn 1 layer_mlp 0 
DEBUG 10-15 15:28:29 lpllm.py:1044] j: 1 waiting the layer with layer_idx 1 before wait time 0.3530895709991455 s
INFO 10-15 15:28:29 client.py:117] confirm_model_loaded: Mixtral-8x7B, 01929ee0-015b-4e47-a26d-a191663140fe
INFO 10-15 15:28:29 client.py:125] Model loaded
DEBUG 10-15 15:28:29 lpllm.py:1048] j: load cost 0.3547701835632324 s waiting cost 0.0016660690307617188 s
DEBUG 10-15 15:28:29 lpllm.py:924] 
DEBUG 10-15 15:28:29 lpllm.py:924] decoder loop j: 2 cur_layer_idx: 1 layer_attn_idx: 1 layer_mlp_idx: 0
DEBUG 10-15 15:28:29 lpllm.py:933] start load next layer cur_layer_idx: 2
DEBUG 10-15 15:28:29 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed650>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x003\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x96\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:28:29 client.py:72] load_into_gpu: Mixtral-8x7B, f5dfc043-1d35-43f8-aa66-f0572ec553fa
INFO 10-15 15:28:29 client.py:113] Model loaded: Mixtral-8x7B, f5dfc043-1d35-43f8-aa66-f0572ec553fa
DEBUG 10-15 15:28:29 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:29 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:29 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:29 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:29 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:29 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:29 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:29 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:29 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:29 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:29 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:29 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:29 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:29 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:29 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:29 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:29 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:29 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:29 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:29 lpllm.py:2265] GPU2CPU move cost 0.000639 seconds
DEBUG 10-15 15:28:29 lpmodule.py:340] layer_idx 1 attn layer_idx: 1
DEBUG 10-15 15:28:29 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:29 lpmodule.py:364] decoder_attn_batch update batch_dim 840-900
DEBUG 10-15 15:28:29 StaticCacheLen.py:84] static cache update layer_idx: 1, start_batch: 840, end_batch: 900
DEBUG 10-15 15:28:29 lpmodule.py:368] update for kv cache 840-900 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:29 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:29 lpmodule.py:374] update past key value cost 0.020782 seconds
DEBUG 10-15 15:28:29 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:29 lpmodule.py:227] layer idx 0
DEBUG 10-15 15:28:29 lpmodule.py:399] repeat qkv cost 0.028112 seconds
DEBUG 10-15 15:28:29 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:29 lpmodule.py:433] dot attn cost 0.032765 seconds
DEBUG 10-15 15:28:29 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:29 lpmodule.py:444] time cost move to cuda:1 0.025298595428466797 s
DEBUG 10-15 15:28:29 lpllm.py:2283] CPU attn cost 0.134779 seconds if batch True
DEBUG 10-15 15:28:29 lpllm.py:2292] deal attn result cost 0.000001 seconds
DEBUG 10-15 15:28:29 lpllm.py:2294] CPU compute cost 0.135701 seconds
DEBUG 10-15 15:28:29 lpllm.py:2312] free cost 0.000071 seconds
DEBUG 10-15 15:28:29 lpllm.py:503] reset update experts
DEBUG 10-15 15:28:29 lpllm.py:1774] update state cost 2.2411346435546875e-05 s
DEBUG 10-15 15:28:29 lpllm.py:1743] restore layer func cost 0.0008704662322998047 s
DEBUG 10-15 15:28:29 lpllm.py:511] restore layer cost 0.0011277198791503906 s
DEBUG 10-15 15:28:29 lpllm.py:512] reset_next_layer_need: layer_attn_idx=1, layer_mlp_idx=1, j_loc=2
DEBUG 10-15 15:28:29 lpllm.py:1037] reset layer cost 0.0011997222900390625 s
DEBUG 10-15 15:28:29 lpllm.py:1038] have reset next layer layer_attn 1 layer_mlp 1 
DEBUG 10-15 15:28:29 lpllm.py:924] 
DEBUG 10-15 15:28:29 lpllm.py:924] decoder loop j: 3 cur_layer_idx: 1 layer_attn_idx: 1 layer_mlp_idx: 1
DEBUG 10-15 15:28:29 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:29 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:29 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:29 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:29 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:29 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:29 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:29 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:29 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:29 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:29 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:29 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:29 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:29 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:29 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:29 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:29 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:29 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:29 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:29 lpllm.py:2265] GPU2CPU move cost 0.000592 seconds
DEBUG 10-15 15:28:29 lpmodule.py:340] layer_idx 1 attn layer_idx: 1
DEBUG 10-15 15:28:29 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:29 lpmodule.py:364] decoder_attn_batch update batch_dim 900-960
DEBUG 10-15 15:28:29 StaticCacheLen.py:84] static cache update layer_idx: 1, start_batch: 900, end_batch: 960
DEBUG 10-15 15:28:29 StaticCacheLen.py:90] static cache update layer_idx: 1, update seq_length to 512
DEBUG 10-15 15:28:29 lpmodule.py:368] update for kv cache 900-960 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:29 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:29 lpmodule.py:374] update past key value cost 0.022983 seconds
DEBUG 10-15 15:28:29 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:29 lpmodule.py:227] layer idx 1
DEBUG 10-15 15:28:29 lpmodule.py:399] repeat qkv cost 0.027516 seconds
DEBUG 10-15 15:28:29 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:29 lpmodule.py:433] dot attn cost 0.032871 seconds
DEBUG 10-15 15:28:29 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:29 lpmodule.py:444] time cost move to cuda:1 0.025774478912353516 s
DEBUG 10-15 15:28:29 lpllm.py:2283] CPU attn cost 0.137823 seconds if batch True
DEBUG 10-15 15:28:29 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:29 lpllm.py:2294] CPU compute cost 0.138705 seconds
DEBUG 10-15 15:28:29 lpllm.py:2312] free cost 0.000074 seconds
DEBUG 10-15 15:28:30 lpllm.py:498] reset update attn
DEBUG 10-15 15:28:30 lpllm.py:1774] update state cost 2.3126602172851562e-05 s
DEBUG 10-15 15:28:30 lpllm.py:1743] restore layer func cost 0.00040531158447265625 s
DEBUG 10-15 15:28:30 lpllm.py:511] restore layer cost 0.000652313232421875 s
DEBUG 10-15 15:28:30 lpllm.py:512] reset_next_layer_need: layer_attn_idx=2, layer_mlp_idx=1, j_loc=3
DEBUG 10-15 15:28:30 lpllm.py:1037] reset layer cost 0.0007297992706298828 s
DEBUG 10-15 15:28:30 lpllm.py:1038] have reset next layer layer_attn 2 layer_mlp 1 
DEBUG 10-15 15:28:30 lpllm.py:1044] j: 3 waiting the layer with layer_idx 2 before wait time 0.45966649055480957 s
INFO 10-15 15:28:30 client.py:117] confirm_model_loaded: Mixtral-8x7B, f5dfc043-1d35-43f8-aa66-f0572ec553fa
INFO 10-15 15:28:30 client.py:125] Model loaded
DEBUG 10-15 15:28:30 lpllm.py:1048] j: load cost 0.46116161346435547 s waiting cost 0.0014805793762207031 s
DEBUG 10-15 15:28:30 lpllm.py:924] 
DEBUG 10-15 15:28:30 lpllm.py:924] decoder loop j: 4 cur_layer_idx: 2 layer_attn_idx: 2 layer_mlp_idx: 1
DEBUG 10-15 15:28:30 lpllm.py:933] start load next layer cur_layer_idx: 3
DEBUG 10-15 15:28:30 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed6e0>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x004\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x98\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:28:30 client.py:72] load_into_gpu: Mixtral-8x7B, 3694002b-31c2-4a58-a78a-92923d8d0288
INFO 10-15 15:28:30 client.py:113] Model loaded: Mixtral-8x7B, 3694002b-31c2-4a58-a78a-92923d8d0288
DEBUG 10-15 15:28:30 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:30 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:30 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:30 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:30 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:30 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:30 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:30 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:30 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:30 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:30 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:30 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:30 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:30 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:30 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:30 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:30 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:30 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:30 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:30 lpllm.py:2265] GPU2CPU move cost 0.000614 seconds
DEBUG 10-15 15:28:30 lpmodule.py:340] layer_idx 2 attn layer_idx: 2
DEBUG 10-15 15:28:30 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:30 lpmodule.py:364] decoder_attn_batch update batch_dim 840-900
DEBUG 10-15 15:28:30 StaticCacheLen.py:84] static cache update layer_idx: 2, start_batch: 840, end_batch: 900
DEBUG 10-15 15:28:30 lpmodule.py:368] update for kv cache 840-900 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:30 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:30 lpmodule.py:374] update past key value cost 0.022689 seconds
DEBUG 10-15 15:28:30 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:30 lpmodule.py:227] layer idx 1
DEBUG 10-15 15:28:30 lpmodule.py:399] repeat qkv cost 0.027584 seconds
DEBUG 10-15 15:28:30 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:30 lpmodule.py:433] dot attn cost 0.035380 seconds
DEBUG 10-15 15:28:30 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:30 lpmodule.py:444] time cost move to cuda:1 0.02737736701965332 s
DEBUG 10-15 15:28:30 lpllm.py:2283] CPU attn cost 0.141148 seconds if batch True
DEBUG 10-15 15:28:30 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:30 lpllm.py:2294] CPU compute cost 0.142047 seconds
DEBUG 10-15 15:28:30 lpllm.py:2312] free cost 0.000072 seconds
DEBUG 10-15 15:28:30 lpllm.py:503] reset update experts
DEBUG 10-15 15:28:30 lpllm.py:1774] update state cost 2.09808349609375e-05 s
DEBUG 10-15 15:28:30 lpllm.py:1743] restore layer func cost 0.0008728504180908203 s
DEBUG 10-15 15:28:30 lpllm.py:511] restore layer cost 0.0011324882507324219 s
DEBUG 10-15 15:28:30 lpllm.py:512] reset_next_layer_need: layer_attn_idx=2, layer_mlp_idx=2, j_loc=4
DEBUG 10-15 15:28:30 lpllm.py:1037] reset layer cost 0.001203775405883789 s
DEBUG 10-15 15:28:30 lpllm.py:1038] have reset next layer layer_attn 2 layer_mlp 2 
DEBUG 10-15 15:28:30 lpllm.py:924] 
DEBUG 10-15 15:28:30 lpllm.py:924] decoder loop j: 5 cur_layer_idx: 2 layer_attn_idx: 2 layer_mlp_idx: 2
DEBUG 10-15 15:28:30 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:30 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:30 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:30 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:30 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:30 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:30 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:30 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:30 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:30 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:30 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:30 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:30 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:30 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:30 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:30 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:30 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:30 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:30 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:30 lpllm.py:2265] GPU2CPU move cost 0.000570 seconds
DEBUG 10-15 15:28:30 lpmodule.py:340] layer_idx 2 attn layer_idx: 2
DEBUG 10-15 15:28:30 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:30 lpmodule.py:364] decoder_attn_batch update batch_dim 900-960
DEBUG 10-15 15:28:30 StaticCacheLen.py:84] static cache update layer_idx: 2, start_batch: 900, end_batch: 960
DEBUG 10-15 15:28:30 StaticCacheLen.py:90] static cache update layer_idx: 2, update seq_length to 512
DEBUG 10-15 15:28:30 lpmodule.py:368] update for kv cache 900-960 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:30 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:30 lpmodule.py:374] update past key value cost 0.023018 seconds
DEBUG 10-15 15:28:30 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:30 lpmodule.py:227] layer idx 2
DEBUG 10-15 15:28:30 lpmodule.py:399] repeat qkv cost 0.028273 seconds
DEBUG 10-15 15:28:30 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:30 lpmodule.py:433] dot attn cost 0.034754 seconds
DEBUG 10-15 15:28:30 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-15 15:28:30 lpmodule.py:444] time cost move to cuda:1 0.0251157283782959 s
DEBUG 10-15 15:28:30 lpllm.py:2283] CPU attn cost 0.139161 seconds if batch True
DEBUG 10-15 15:28:30 lpllm.py:2292] deal attn result cost 0.000002 seconds
DEBUG 10-15 15:28:30 lpllm.py:2294] CPU compute cost 0.140079 seconds
DEBUG 10-15 15:28:30 lpllm.py:2312] free cost 0.000082 seconds
DEBUG 10-15 15:28:30 lpllm.py:498] reset update attn
DEBUG 10-15 15:28:30 lpllm.py:1774] update state cost 2.3126602172851562e-05 s
DEBUG 10-15 15:28:30 lpllm.py:1743] restore layer func cost 0.0004100799560546875 s
DEBUG 10-15 15:28:30 lpllm.py:511] restore layer cost 0.0006492137908935547 s
DEBUG 10-15 15:28:30 lpllm.py:512] reset_next_layer_need: layer_attn_idx=3, layer_mlp_idx=2, j_loc=5
DEBUG 10-15 15:28:30 lpllm.py:1037] reset layer cost 0.0007352828979492188 s
DEBUG 10-15 15:28:30 lpllm.py:1038] have reset next layer layer_attn 3 layer_mlp 2 
DEBUG 10-15 15:28:30 lpllm.py:1044] j: 5 waiting the layer with layer_idx 3 before wait time 0.4502253532409668 s
INFO 10-15 15:28:30 client.py:117] confirm_model_loaded: Mixtral-8x7B, 3694002b-31c2-4a58-a78a-92923d8d0288
INFO 10-15 15:28:30 client.py:125] Model loaded
DEBUG 10-15 15:28:30 lpllm.py:1048] j: load cost 0.4519479274749756 s waiting cost 0.0017066001892089844 s
DEBUG 10-15 15:28:30 lpllm.py:924] 
DEBUG 10-15 15:28:30 lpllm.py:924] decoder loop j: 6 cur_layer_idx: 3 layer_attn_idx: 3 layer_mlp_idx: 2
DEBUG 10-15 15:28:30 lpllm.py:933] start load next layer cur_layer_idx: 4
DEBUG 10-15 15:28:30 lpllm.py:1804] cuda memory handles: {1: <capsule object NULL at 0x7f85a81ed590>} {1: b'\xd0\xd0\x01\x08\x00\x00\x00\x00\x96\xca\x13\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\xed\x00\x00\x00\xff\x00\x002\x00\x00\x00\x00\x00\x00\x00\xa5?\xd8\xc1\x94\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-15 15:28:30 client.py:72] load_into_gpu: Mixtral-8x7B, 4a98b639-5d4d-4c22-a1c3-ce6789a6ca0b
INFO 10-15 15:28:30 client.py:113] Model loaded: Mixtral-8x7B, 4a98b639-5d4d-4c22-a1c3-ce6789a6ca0b
DEBUG 10-15 15:28:30 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:30 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:30 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:30 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:30 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:30 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:30 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:30 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:30 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:30 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:30 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:30 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:30 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:30 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:30 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:30 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:30 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:30 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:30 lpllm.py:2084] waiting out_queue
DEBUG 10-15 15:28:30 lpllm.py:2265] GPU2CPU move cost 0.000666 seconds
DEBUG 10-15 15:28:30 lpmodule.py:340] layer_idx 3 attn layer_idx: 3
DEBUG 10-15 15:28:30 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-15 15:28:30 lpmodule.py:364] decoder_attn_batch update batch_dim 840-900
DEBUG 10-15 15:28:30 StaticCacheLen.py:84] static cache update layer_idx: 3, start_batch: 840, end_batch: 900
DEBUG 10-15 15:28:30 lpmodule.py:368] update for kv cache 840-900 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-15 15:28:30 lpmodule.py:373] key_states device cpu
DEBUG 10-15 15:28:30 lpmodule.py:374] update past key value cost 0.022450 seconds
DEBUG 10-15 15:28:30 lpmodule.py:389] before repeat qkv key shape torch.Size([60, 8, 512, 128]), value shape torch.Size([60, 8, 512, 128])
DEBUG 10-15 15:28:30 lpmodule.py:227] layer idx 2
DEBUG 10-15 15:28:30 lpmodule.py:399] repeat qkv cost 0.028962 seconds
DEBUG 10-15 15:28:30 lpmodule.py:400] q shape torch.Size([60, 32, 512, 128]), k shape torch.Size([60, 32, 512, 128]), v shape torch.Size([60, 32, 512, 128])
DEBUG 10-15 15:28:30 lpmodule.py:433] dot attn cost 0.034498 seconds
DEBUG 10-15 15:28:30 lpmodule.py:437] layer self attn q_proj device cuda:1
ERROR 10-15 15:28:30 lpllm.py:2218] Error in GPU2CPU thread: CUDA out of memory. Tried to allocate 240.00 MiB. GPU 1 has a total capacity of 23.64 GiB of which 197.56 MiB is free. Process 43681 has 1.08 GiB memory in use. Process 93724 has 384.00 MiB memory in use. Process 2514488 has 21.67 GiB memory in use. Of the allocated memory 11.40 GiB is allocated by PyTorch, and 1.17 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
DEBUG 10-15 15:28:30 lpllm.py:503] reset update experts
DEBUG 10-15 15:28:30 lpllm.py:1774] update state cost 2.0742416381835938e-05 s
DEBUG 10-15 15:28:30 lpllm.py:1743] restore layer func cost 0.00084686279296875 s
DEBUG 10-15 15:28:30 lpllm.py:511] restore layer cost 0.0011096000671386719 s
DEBUG 10-15 15:28:30 lpllm.py:512] reset_next_layer_need: layer_attn_idx=3, layer_mlp_idx=3, j_loc=6
DEBUG 10-15 15:28:30 lpllm.py:1037] reset layer cost 0.0011868476867675781 s
DEBUG 10-15 15:28:30 lpllm.py:1038] have reset next layer layer_attn 3 layer_mlp 3 
DEBUG 10-15 15:28:30 lpllm.py:924] 
DEBUG 10-15 15:28:30 lpllm.py:924] decoder loop j: 7 cur_layer_idx: 3 layer_attn_idx: 3 layer_mlp_idx: 3
DEBUG 10-15 15:28:30 lpmodule.py:452] hidden_states.shape torch.Size([60, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-15 15:28:30 lpmodule.py:456] input_layernorm num_params: 1
DEBUG 10-15 15:28:30 lpmodule.py:458] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-15 15:28:30 lpmodule.py:499] kv_seq_len 512 past_key_value 0
DEBUG 10-15 15:28:30 lpmodule.py:505] inv_freq device cpu
DEBUG 10-15 15:28:30 lpmodule.py:506] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:30 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:30 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:30 lpmodule.py:506]         ...,
DEBUG 10-15 15:28:30 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:30 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-15 15:28:30 lpmodule.py:506]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-15 15:28:30 lpmodule.py:512] query_states device cuda:1
DEBUG 10-15 15:28:30 lpmodule.py:513] key_states device cuda:1
DEBUG 10-15 15:28:30 lpmodule.py:514] value_states device cuda:1
DEBUG 10-15 15:28:30 lpmodule.py:515] position_ids device cuda:1
DEBUG 10-15 15:28:30 lpmodule.py:516] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:30 lpmodule.py:517] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-15 15:28:30 lpllm.py:2084] waiting out_queue
WARNING 10-15 15:33:08 lpllm.py:353] Received signal 2, initiating shutdown...
