DEBUG 10-14 21:17:13 lpllm.py:267] other_tensor_data_index: {'model.embed_tokens.weight': (0, 262144000), 'model.norm.weight': (93143433216, 8192), 'lm_head.weight': (93143441408, 262144000)}
DEBUG 10-14 21:17:13 lpllm.py:275] cuda memory needed for all qkv related 2684878848, for single layer 2902540288
DEBUG 10-14 21:17:13 lpllm.py:282] cuda memory needed for all model 524296192
DEBUG 10-14 21:17:14 client.py:72] load_into_gpu: Mixtral-8x7B, d310cdba-0422-45a3-8a05-7ad4617b1576
INFO 10-14 21:17:14 client.py:113] Model loaded: Mixtral-8x7B, d310cdba-0422-45a3-8a05-7ad4617b1576
INFO 10-14 21:17:14 client.py:117] confirm_model_loaded: Mixtral-8x7B, d310cdba-0422-45a3-8a05-7ad4617b1576
INFO 10-14 21:17:14 client.py:125] Model loaded
here pin
INFO 10-14 21:17:14 pinpool.py:31] Initializing PinnedMemoryPool with 8GB total, allocating in 2048MB chunks...
DEBUG 10-14 21:17:15 pinpool.py:43] Allocated chunk 1: 1073741824 elements (2048.0 MB)
DEBUG 10-14 21:17:16 pinpool.py:43] Allocated chunk 2: 1073741824 elements (2048.0 MB)
DEBUG 10-14 21:17:17 pinpool.py:43] Allocated chunk 3: 1073741824 elements (2048.0 MB)
DEBUG 10-14 21:17:18 pinpool.py:43] Allocated chunk 4: 1073741824 elements (2048.0 MB)
INFO 10-14 21:17:18 pinpool.py:55] Successfully allocated 4 chunks, total 4294967296 elements (8192.0 MB) in 3.877s
DEBUG 10-14 21:17:18 lpllm.py:2203] GPU2CPU thread started, id 1910824
INFO 10-14 21:17:18 lpllm.py:2360] CPU compute thread started, id 1910825
DEBUG 10-14 21:17:18 lpllm.py:2480] CPU2GPU thread started, id 1910827
torch.Size([20, 512])
init kv cache time cost 0.25160765647888184 s
当前程序内存占用: 11670.38 MB
DEBUG 10-14 21:17:18 lpllm.py:600] input_ids shape torch.Size([20, 512]) orig_shape for decoders one chunk: torch.Size([20, 512, 4096])
DEBUG 10-14 21:17:18 lpllm.py:601] chunk_batch_size: 10 hidden_states_chunk shape and value: torch.Size([10, 512, 4096])
DEBUG 10-14 21:17:18 lpllm.py:602] attention_mask shape and value: None
DEBUG 10-14 21:17:18 lpllm.py:845] hidden_states_chunks1 shape: torch.Size([10, 512, 4096]), hidden_states_chunks1 length: 1
DEBUG 10-14 21:17:18 lpllm.py:846] hidden_states_chunks2 shape: torch.Size([10, 512, 4096]), hidden_states_chunks2 length: 1
DEBUG 10-14 21:17:18 lpllm.py:1801] cuda memory handles: {1: <capsule object NULL at 0x7fdf3c6d9ec0>} {1: b'\xe0\xaa29\x00\x00\x00\x00\xe8&\x1d\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\x86\x00\x00\x00\xff\x00\x00.\x00\x00\x00\x00\x00\x00\x00)\xd8\xd5\xc1\xa0\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-14 21:17:18 client.py:72] load_into_gpu: Mixtral-8x7B, 305f7c67-cb44-4e8e-8c3f-8cdc3d6d389d
INFO 10-14 21:17:18 client.py:113] Model loaded: Mixtral-8x7B, 305f7c67-cb44-4e8e-8c3f-8cdc3d6d389d
DEBUG 10-14 21:17:18 lpllm.py:1740] restore layer func cost 0.0014221668243408203 s
INFO 10-14 21:17:18 client.py:117] confirm_model_loaded: Mixtral-8x7B, 305f7c67-cb44-4e8e-8c3f-8cdc3d6d389d
INFO 10-14 21:17:19 client.py:125] Model loaded
DEBUG 10-14 21:17:19 lpllm.py:422] prepare layer cost 0.2892332077026367 s
DEBUG 10-14 21:17:19 lpllm.py:1801] cuda memory handles: {1: <capsule object NULL at 0x7fdf3c6d9f80>} {1: b'\xe0\xaa29\x00\x00\x00\x00\xe8&\x1d\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\x86\x00\x00\x00\xff\x00\x00/\x00\x00\x00\x00\x00\x00\x00)\xd8\xd5\xc1\xa2\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-14 21:17:19 client.py:72] load_into_gpu: Mixtral-8x7B, b17112ea-c3d0-41bb-a7e2-b19a923e579c
INFO 10-14 21:17:19 client.py:113] Model loaded: Mixtral-8x7B, b17112ea-c3d0-41bb-a7e2-b19a923e579c
DEBUG 10-14 21:17:19 lpmodule.py:451] hidden_states.shape torch.Size([10, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:19 lpmodule.py:455] input_layernorm num_params: 1
DEBUG 10-14 21:17:19 lpmodule.py:457] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-14 21:17:19 lpmodule.py:498] kv_seq_len 512 past_key_value 0
DEBUG 10-14 21:17:19 lpmodule.py:504] inv_freq device cpu
DEBUG 10-14 21:17:19 lpmodule.py:505] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:19 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:19 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:19 lpmodule.py:505]         ...,
DEBUG 10-14 21:17:19 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:19 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:19 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-14 21:17:19 lpmodule.py:511] query_states device cuda:1
DEBUG 10-14 21:17:19 lpmodule.py:512] key_states device cuda:1
DEBUG 10-14 21:17:19 lpmodule.py:513] value_states device cuda:1
DEBUG 10-14 21:17:19 lpmodule.py:514] position_ids device cuda:1
DEBUG 10-14 21:17:19 lpmodule.py:515] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:19 lpmodule.py:516] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:19 lpllm.py:921] 
DEBUG 10-14 21:17:19 lpllm.py:921] decoder loop j: 1 cur_layer_idx: 0 layer_attn_idx: 0 layer_mlp_idx: 0
DEBUG 10-14 21:17:19 lpmodule.py:451] hidden_states.shape torch.Size([10, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:19 lpmodule.py:455] input_layernorm num_params: 1
DEBUG 10-14 21:17:19 lpmodule.py:457] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-14 21:17:19 lpllm.py:2262] GPU2CPU move cost 0.001421 seconds
DEBUG 10-14 21:17:19 lpmodule.py:340] layer_idx 0 attn layer_idx: 0
DEBUG 10-14 21:17:19 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-14 21:17:19 lpmodule.py:364] decoder_attn_batch update batch_dim 0-10
DEBUG 10-14 21:17:19 lpmodule.py:498] kv_seq_len 512 past_key_value 0
DEBUG 10-14 21:17:19 lpmodule.py:504] inv_freq device cpu
DEBUG 10-14 21:17:19 lpmodule.py:505] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:19 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:19 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:19 lpmodule.py:505]         ...,
DEBUG 10-14 21:17:19 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:19 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:19 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-14 21:17:19 lpmodule.py:511] query_states device cuda:1
DEBUG 10-14 21:17:19 lpmodule.py:512] key_states device cuda:1
DEBUG 10-14 21:17:19 lpmodule.py:513] value_states device cuda:1
DEBUG 10-14 21:17:19 lpmodule.py:514] position_ids device cuda:1
DEBUG 10-14 21:17:19 lpmodule.py:515] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:19 lpmodule.py:516] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:19 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:19 StaticCacheLen.py:84] static cache update layer_idx: 0, start_batch: 0, end_batch: 10
DEBUG 10-14 21:17:19 lpmodule.py:368] update for kv cache 0-10 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-14 21:17:19 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:19 lpmodule.py:374] update past key value cost 0.017174 seconds
DEBUG 10-14 21:17:19 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 512, 128]), value shape torch.Size([10, 8, 512, 128])
DEBUG 10-14 21:17:19 lpmodule.py:399] repeat qkv cost 0.012899 seconds
DEBUG 10-14 21:17:19 lpmodule.py:400] q shape torch.Size([10, 32, 512, 128]), k shape torch.Size([10, 32, 512, 128]), v shape torch.Size([10, 32, 512, 128])
DEBUG 10-14 21:17:19 lpmodule.py:433] dot attn cost 0.097964 seconds
DEBUG 10-14 21:17:19 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.013686895370483398 s
DEBUG 10-14 21:17:19 lpllm.py:2280] CPU attn cost 0.148401 seconds if batch True
DEBUG 10-14 21:17:19 lpllm.py:2289] deal attn result cost 0.000003 seconds
DEBUG 10-14 21:17:19 lpllm.py:2291] CPU compute cost 0.150380 seconds
DEBUG 10-14 21:17:19 lpllm.py:2309] free cost 0.000331 seconds
DEBUG 10-14 21:17:19 lpllm.py:2262] GPU2CPU move cost 0.000714 seconds
DEBUG 10-14 21:17:19 lpmodule.py:340] layer_idx 0 attn layer_idx: 0
DEBUG 10-14 21:17:19 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-14 21:17:19 lpmodule.py:364] decoder_attn_batch update batch_dim 10-20
DEBUG 10-14 21:17:19 StaticCacheLen.py:84] static cache update layer_idx: 0, start_batch: 10, end_batch: 20
DEBUG 10-14 21:17:19 StaticCacheLen.py:90] static cache update layer_idx: 0, update seq_length to 512
DEBUG 10-14 21:17:19 lpmodule.py:368] update for kv cache 10-20 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-14 21:17:19 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:19 lpmodule.py:374] update past key value cost 0.003162 seconds
DEBUG 10-14 21:17:19 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 512, 128]), value shape torch.Size([10, 8, 512, 128])
DEBUG 10-14 21:17:19 lpmodule.py:227] layer idx 0
DEBUG 10-14 21:17:19 lpmodule.py:399] repeat qkv cost 0.015378 seconds
DEBUG 10-14 21:17:19 lpmodule.py:400] q shape torch.Size([10, 32, 512, 128]), k shape torch.Size([10, 32, 512, 128]), v shape torch.Size([10, 32, 512, 128])
DEBUG 10-14 21:17:19 lpmodule.py:433] dot attn cost 0.070984 seconds
DEBUG 10-14 21:17:19 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.05062365531921387 s
DEBUG 10-14 21:17:19 lpllm.py:2280] CPU attn cost 0.151944 seconds if batch True
DEBUG 10-14 21:17:19 lpllm.py:2289] deal attn result cost 0.000003 seconds
DEBUG 10-14 21:17:19 lpllm.py:2291] CPU compute cost 0.153090 seconds
DEBUG 10-14 21:17:19 lpllm.py:2309] free cost 0.000170 seconds
DEBUG 10-14 21:17:19 lpllm.py:498] reset update attn
DEBUG 10-14 21:17:19 lpllm.py:1771] update state cost 2.3365020751953125e-05 s
DEBUG 10-14 21:17:19 lpllm.py:1740] restore layer func cost 0.0005104541778564453 s
DEBUG 10-14 21:17:19 lpllm.py:511] restore layer cost 0.0007698535919189453 s
DEBUG 10-14 21:17:19 lpllm.py:512] reset_next_layer_need: layer_attn_idx=1, layer_mlp_idx=0, j_loc=1
DEBUG 10-14 21:17:19 lpllm.py:1034] reset layer cost 0.0008656978607177734 s
DEBUG 10-14 21:17:19 lpllm.py:1035] have reset next layer layer_attn 1 layer_mlp 0 
DEBUG 10-14 21:17:19 lpllm.py:1041] j: 1 waiting the layer with layer_idx 1 before wait time 0.7374129295349121 s
INFO 10-14 21:17:19 client.py:117] confirm_model_loaded: Mixtral-8x7B, b17112ea-c3d0-41bb-a7e2-b19a923e579c
INFO 10-14 21:17:19 client.py:125] Model loaded
DEBUG 10-14 21:17:19 lpllm.py:1045] j: load cost 0.7389321327209473 s waiting cost 0.001497030258178711 s
DEBUG 10-14 21:17:19 lpllm.py:921] 
DEBUG 10-14 21:17:19 lpllm.py:921] decoder loop j: 2 cur_layer_idx: 1 layer_attn_idx: 1 layer_mlp_idx: 0
DEBUG 10-14 21:17:19 lpllm.py:930] start load next layer cur_layer_idx: 2
DEBUG 10-14 21:17:19 lpllm.py:1801] cuda memory handles: {1: <capsule object NULL at 0x7fdf3c6da010>} {1: b'\xe0\xaa29\x00\x00\x00\x00\xe8&\x1d\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\x86\x00\x00\x00\xff\x00\x000\x00\x00\x00\x00\x00\x00\x00)\xd8\xd5\xc1\xa4\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-14 21:17:19 client.py:72] load_into_gpu: Mixtral-8x7B, f35c05e5-f1f6-4941-8cf3-38a18a02d964
INFO 10-14 21:17:19 client.py:113] Model loaded: Mixtral-8x7B, f35c05e5-f1f6-4941-8cf3-38a18a02d964
DEBUG 10-14 21:17:19 lpmodule.py:451] hidden_states.shape torch.Size([10, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:19 lpmodule.py:455] input_layernorm num_params: 1
DEBUG 10-14 21:17:19 lpmodule.py:457] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-14 21:17:19 lpmodule.py:498] kv_seq_len 512 past_key_value 0
DEBUG 10-14 21:17:19 lpmodule.py:504] inv_freq device cpu
DEBUG 10-14 21:17:19 lpmodule.py:505] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:19 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:19 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:19 lpmodule.py:505]         ...,
DEBUG 10-14 21:17:19 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:19 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:19 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-14 21:17:19 lpmodule.py:511] query_states device cuda:1
DEBUG 10-14 21:17:19 lpmodule.py:512] key_states device cuda:1
DEBUG 10-14 21:17:19 lpmodule.py:513] value_states device cuda:1
DEBUG 10-14 21:17:19 lpmodule.py:514] position_ids device cuda:1
DEBUG 10-14 21:17:19 lpmodule.py:515] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:19 lpmodule.py:516] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:19 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:19 lpllm.py:2262] GPU2CPU move cost 0.000632 seconds
DEBUG 10-14 21:17:19 lpmodule.py:340] layer_idx 1 attn layer_idx: 1
DEBUG 10-14 21:17:19 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-14 21:17:19 lpmodule.py:364] decoder_attn_batch update batch_dim 0-10
DEBUG 10-14 21:17:19 StaticCacheLen.py:84] static cache update layer_idx: 1, start_batch: 0, end_batch: 10
DEBUG 10-14 21:17:19 lpmodule.py:368] update for kv cache 0-10 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-14 21:17:19 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:19 lpmodule.py:374] update past key value cost 0.003706 seconds
DEBUG 10-14 21:17:19 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 512, 128]), value shape torch.Size([10, 8, 512, 128])
DEBUG 10-14 21:17:19 lpmodule.py:227] layer idx 0
DEBUG 10-14 21:17:20 lpmodule.py:399] repeat qkv cost 0.013163 seconds
DEBUG 10-14 21:17:20 lpmodule.py:400] q shape torch.Size([10, 32, 512, 128]), k shape torch.Size([10, 32, 512, 128]), v shape torch.Size([10, 32, 512, 128])
DEBUG 10-14 21:17:20 lpmodule.py:433] dot attn cost 0.066667 seconds
DEBUG 10-14 21:17:20 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.025180339813232422 s
DEBUG 10-14 21:17:20 lpllm.py:2280] CPU attn cost 0.117515 seconds if batch True
DEBUG 10-14 21:17:20 lpllm.py:2289] deal attn result cost 0.000001 seconds
DEBUG 10-14 21:17:20 lpllm.py:2291] CPU compute cost 0.118577 seconds
DEBUG 10-14 21:17:20 lpllm.py:2309] free cost 0.000112 seconds
DEBUG 10-14 21:17:20 lpllm.py:503] reset update experts
DEBUG 10-14 21:17:20 lpllm.py:1771] update state cost 2.5272369384765625e-05 s
DEBUG 10-14 21:17:20 lpllm.py:1740] restore layer func cost 0.001439809799194336 s
DEBUG 10-14 21:17:20 lpllm.py:511] restore layer cost 0.0017316341400146484 s
DEBUG 10-14 21:17:20 lpllm.py:512] reset_next_layer_need: layer_attn_idx=1, layer_mlp_idx=1, j_loc=2
DEBUG 10-14 21:17:20 lpllm.py:1034] reset layer cost 0.00185394287109375 s
DEBUG 10-14 21:17:20 lpllm.py:1035] have reset next layer layer_attn 1 layer_mlp 1 
DEBUG 10-14 21:17:20 lpllm.py:921] 
DEBUG 10-14 21:17:20 lpllm.py:921] decoder loop j: 3 cur_layer_idx: 1 layer_attn_idx: 1 layer_mlp_idx: 1
DEBUG 10-14 21:17:20 lpmodule.py:451] hidden_states.shape torch.Size([10, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:20 lpmodule.py:455] input_layernorm num_params: 1
DEBUG 10-14 21:17:20 lpmodule.py:457] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-14 21:17:20 lpmodule.py:498] kv_seq_len 512 past_key_value 0
DEBUG 10-14 21:17:20 lpmodule.py:504] inv_freq device cpu
DEBUG 10-14 21:17:20 lpmodule.py:505] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:20 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:20 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:20 lpmodule.py:505]         ...,
DEBUG 10-14 21:17:20 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:20 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:20 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-14 21:17:20 lpmodule.py:511] query_states device cuda:1
DEBUG 10-14 21:17:20 lpmodule.py:512] key_states device cuda:1
DEBUG 10-14 21:17:20 lpmodule.py:513] value_states device cuda:1
DEBUG 10-14 21:17:20 lpmodule.py:514] position_ids device cuda:1
DEBUG 10-14 21:17:20 lpmodule.py:515] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:20 lpmodule.py:516] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:20 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:20 lpllm.py:2262] GPU2CPU move cost 0.000442 seconds
DEBUG 10-14 21:17:20 lpmodule.py:340] layer_idx 1 attn layer_idx: 1
DEBUG 10-14 21:17:20 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-14 21:17:20 lpmodule.py:364] decoder_attn_batch update batch_dim 10-20
DEBUG 10-14 21:17:20 StaticCacheLen.py:84] static cache update layer_idx: 1, start_batch: 10, end_batch: 20
DEBUG 10-14 21:17:20 StaticCacheLen.py:90] static cache update layer_idx: 1, update seq_length to 512
DEBUG 10-14 21:17:20 lpmodule.py:368] update for kv cache 10-20 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-14 21:17:20 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:20 lpmodule.py:374] update past key value cost 0.006546 seconds
DEBUG 10-14 21:17:20 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 512, 128]), value shape torch.Size([10, 8, 512, 128])
DEBUG 10-14 21:17:20 lpmodule.py:227] layer idx 1
DEBUG 10-14 21:17:20 lpmodule.py:399] repeat qkv cost 0.014400 seconds
DEBUG 10-14 21:17:20 lpmodule.py:400] q shape torch.Size([10, 32, 512, 128]), k shape torch.Size([10, 32, 512, 128]), v shape torch.Size([10, 32, 512, 128])
DEBUG 10-14 21:17:20 lpllm.py:498] reset update attn
DEBUG 10-14 21:17:20 lpllm.py:1771] update state cost 2.9325485229492188e-05 s
DEBUG 10-14 21:17:20 lpllm.py:1740] restore layer func cost 0.00064849853515625 s
DEBUG 10-14 21:17:20 lpllm.py:511] restore layer cost 0.0010073184967041016 s
DEBUG 10-14 21:17:20 lpllm.py:512] reset_next_layer_need: layer_attn_idx=2, layer_mlp_idx=1, j_loc=3
DEBUG 10-14 21:17:20 lpllm.py:1034] reset layer cost 0.0011303424835205078 s
DEBUG 10-14 21:17:20 lpllm.py:1035] have reset next layer layer_attn 2 layer_mlp 1 
DEBUG 10-14 21:17:20 lpllm.py:1041] j: 3 waiting the layer with layer_idx 2 before wait time 0.2303142547607422 s
INFO 10-14 21:17:20 client.py:117] confirm_model_loaded: Mixtral-8x7B, f35c05e5-f1f6-4941-8cf3-38a18a02d964
DEBUG 10-14 21:17:20 lpmodule.py:433] dot attn cost 0.075349 seconds
DEBUG 10-14 21:17:20 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.028455018997192383 s
DEBUG 10-14 21:17:20 lpllm.py:2280] CPU attn cost 0.136987 seconds if batch True
DEBUG 10-14 21:17:20 lpllm.py:2289] deal attn result cost 0.000002 seconds
DEBUG 10-14 21:17:20 lpllm.py:2291] CPU compute cost 0.137810 seconds
DEBUG 10-14 21:17:20 lpllm.py:2309] free cost 0.000193 seconds
INFO 10-14 21:17:20 client.py:125] Model loaded
DEBUG 10-14 21:17:20 lpllm.py:1045] j: load cost 0.3223285675048828 s waiting cost 0.0919950008392334 s
DEBUG 10-14 21:17:20 lpllm.py:921] 
DEBUG 10-14 21:17:20 lpllm.py:921] decoder loop j: 4 cur_layer_idx: 2 layer_attn_idx: 2 layer_mlp_idx: 1
DEBUG 10-14 21:17:20 lpllm.py:930] start load next layer cur_layer_idx: 3
DEBUG 10-14 21:17:20 lpllm.py:1801] cuda memory handles: {1: <capsule object NULL at 0x7fdf3c6d9ec0>} {1: b'\xe0\xaa29\x00\x00\x00\x00\xe8&\x1d\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\x86\x00\x00\x00\xff\x00\x00.\x00\x00\x00\x00\x00\x00\x00)\xd8\xd5\xc1\xa0\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-14 21:17:20 client.py:72] load_into_gpu: Mixtral-8x7B, cc2c26d9-190e-4843-8bdb-18510684b056
INFO 10-14 21:17:20 client.py:113] Model loaded: Mixtral-8x7B, cc2c26d9-190e-4843-8bdb-18510684b056
DEBUG 10-14 21:17:20 lpmodule.py:451] hidden_states.shape torch.Size([10, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:20 lpmodule.py:455] input_layernorm num_params: 1
DEBUG 10-14 21:17:20 lpmodule.py:457] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-14 21:17:20 lpmodule.py:498] kv_seq_len 512 past_key_value 0
DEBUG 10-14 21:17:20 lpmodule.py:504] inv_freq device cpu
DEBUG 10-14 21:17:20 lpmodule.py:505] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:20 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:20 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:20 lpmodule.py:505]         ...,
DEBUG 10-14 21:17:20 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:20 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:20 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-14 21:17:20 lpmodule.py:511] query_states device cuda:1
DEBUG 10-14 21:17:20 lpmodule.py:512] key_states device cuda:1
DEBUG 10-14 21:17:20 lpmodule.py:513] value_states device cuda:1
DEBUG 10-14 21:17:20 lpmodule.py:514] position_ids device cuda:1
DEBUG 10-14 21:17:20 lpmodule.py:515] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:20 lpmodule.py:516] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:20 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:20 lpllm.py:2262] GPU2CPU move cost 0.000897 seconds
DEBUG 10-14 21:17:20 lpmodule.py:340] layer_idx 2 attn layer_idx: 2
DEBUG 10-14 21:17:20 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-14 21:17:20 lpmodule.py:364] decoder_attn_batch update batch_dim 0-10
DEBUG 10-14 21:17:20 StaticCacheLen.py:84] static cache update layer_idx: 2, start_batch: 0, end_batch: 10
DEBUG 10-14 21:17:20 lpmodule.py:368] update for kv cache 0-10 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-14 21:17:20 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:20 lpmodule.py:374] update past key value cost 0.004138 seconds
DEBUG 10-14 21:17:20 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 512, 128]), value shape torch.Size([10, 8, 512, 128])
DEBUG 10-14 21:17:20 lpmodule.py:227] layer idx 1
DEBUG 10-14 21:17:20 lpmodule.py:399] repeat qkv cost 0.013508 seconds
DEBUG 10-14 21:17:20 lpmodule.py:400] q shape torch.Size([10, 32, 512, 128]), k shape torch.Size([10, 32, 512, 128]), v shape torch.Size([10, 32, 512, 128])
DEBUG 10-14 21:17:20 lpllm.py:503] reset update experts
DEBUG 10-14 21:17:20 lpllm.py:1771] update state cost 3.910064697265625e-05 s
DEBUG 10-14 21:17:20 lpllm.py:1740] restore layer func cost 0.003246307373046875 s
DEBUG 10-14 21:17:20 lpllm.py:511] restore layer cost 0.003983020782470703 s
DEBUG 10-14 21:17:20 lpllm.py:512] reset_next_layer_need: layer_attn_idx=2, layer_mlp_idx=2, j_loc=4
DEBUG 10-14 21:17:20 lpllm.py:1034] reset layer cost 0.0042493343353271484 s
DEBUG 10-14 21:17:20 lpllm.py:1035] have reset next layer layer_attn 2 layer_mlp 2 
DEBUG 10-14 21:17:20 lpllm.py:921] 
DEBUG 10-14 21:17:20 lpllm.py:921] decoder loop j: 5 cur_layer_idx: 2 layer_attn_idx: 2 layer_mlp_idx: 2
DEBUG 10-14 21:17:20 lpmodule.py:451] hidden_states.shape torch.Size([10, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:20 lpmodule.py:455] input_layernorm num_params: 1
DEBUG 10-14 21:17:20 lpmodule.py:457] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-14 21:17:20 lpmodule.py:498] kv_seq_len 512 past_key_value 0
DEBUG 10-14 21:17:20 lpmodule.py:504] inv_freq device cpu
DEBUG 10-14 21:17:20 lpmodule.py:505] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:20 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:20 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:20 lpmodule.py:505]         ...,
DEBUG 10-14 21:17:20 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:20 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:20 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-14 21:17:20 lpmodule.py:511] query_states device cuda:1
DEBUG 10-14 21:17:20 lpmodule.py:512] key_states device cuda:1
DEBUG 10-14 21:17:20 lpmodule.py:513] value_states device cuda:1
DEBUG 10-14 21:17:20 lpmodule.py:514] position_ids device cuda:1
DEBUG 10-14 21:17:20 lpmodule.py:515] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:20 lpmodule.py:516] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:20 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:20 lpmodule.py:433] dot attn cost 0.067016 seconds
DEBUG 10-14 21:17:20 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.015225410461425781 s
DEBUG 10-14 21:17:20 lpllm.py:2280] CPU attn cost 0.107949 seconds if batch True
DEBUG 10-14 21:17:20 lpllm.py:2289] deal attn result cost 0.000002 seconds
DEBUG 10-14 21:17:20 lpllm.py:2291] CPU compute cost 0.109275 seconds
DEBUG 10-14 21:17:20 lpllm.py:2309] free cost 0.000143 seconds
DEBUG 10-14 21:17:20 lpllm.py:2262] GPU2CPU move cost 0.000447 seconds
DEBUG 10-14 21:17:20 lpmodule.py:340] layer_idx 2 attn layer_idx: 2
DEBUG 10-14 21:17:20 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-14 21:17:20 lpmodule.py:364] decoder_attn_batch update batch_dim 10-20
DEBUG 10-14 21:17:20 StaticCacheLen.py:84] static cache update layer_idx: 2, start_batch: 10, end_batch: 20
DEBUG 10-14 21:17:20 StaticCacheLen.py:90] static cache update layer_idx: 2, update seq_length to 512
DEBUG 10-14 21:17:20 lpmodule.py:368] update for kv cache 10-20 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-14 21:17:20 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:20 lpmodule.py:374] update past key value cost 0.002768 seconds
DEBUG 10-14 21:17:20 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 512, 128]), value shape torch.Size([10, 8, 512, 128])
DEBUG 10-14 21:17:20 lpmodule.py:227] layer idx 2
DEBUG 10-14 21:17:20 lpmodule.py:399] repeat qkv cost 0.011591 seconds
DEBUG 10-14 21:17:20 lpmodule.py:400] q shape torch.Size([10, 32, 512, 128]), k shape torch.Size([10, 32, 512, 128]), v shape torch.Size([10, 32, 512, 128])
DEBUG 10-14 21:17:20 lpllm.py:498] reset update attn
DEBUG 10-14 21:17:20 lpllm.py:1771] update state cost 5.459785461425781e-05 s
DEBUG 10-14 21:17:20 lpllm.py:1740] restore layer func cost 0.0013408660888671875 s
DEBUG 10-14 21:17:20 lpllm.py:511] restore layer cost 0.0019881725311279297 s
DEBUG 10-14 21:17:20 lpllm.py:512] reset_next_layer_need: layer_attn_idx=3, layer_mlp_idx=2, j_loc=5
DEBUG 10-14 21:17:20 lpllm.py:1034] reset layer cost 0.0022459030151367188 s
DEBUG 10-14 21:17:20 lpllm.py:1035] have reset next layer layer_attn 3 layer_mlp 2 
DEBUG 10-14 21:17:20 lpllm.py:1041] j: 5 waiting the layer with layer_idx 3 before wait time 0.28395724296569824 s
INFO 10-14 21:17:20 client.py:117] confirm_model_loaded: Mixtral-8x7B, cc2c26d9-190e-4843-8bdb-18510684b056
DEBUG 10-14 21:17:20 lpmodule.py:433] dot attn cost 0.066587 seconds
DEBUG 10-14 21:17:20 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.024997949600219727 s
DEBUG 10-14 21:17:20 lpllm.py:2280] CPU attn cost 0.113012 seconds if batch True
DEBUG 10-14 21:17:20 lpllm.py:2289] deal attn result cost 0.000002 seconds
DEBUG 10-14 21:17:20 lpllm.py:2291] CPU compute cost 0.113733 seconds
DEBUG 10-14 21:17:20 lpllm.py:2309] free cost 0.000098 seconds
INFO 10-14 21:17:20 client.py:125] Model loaded
DEBUG 10-14 21:17:20 lpllm.py:1045] j: load cost 0.5094664096832275 s waiting cost 0.22547578811645508 s
DEBUG 10-14 21:17:20 lpllm.py:921] 
DEBUG 10-14 21:17:20 lpllm.py:921] decoder loop j: 6 cur_layer_idx: 3 layer_attn_idx: 3 layer_mlp_idx: 2
DEBUG 10-14 21:17:20 lpllm.py:930] start load next layer cur_layer_idx: 4
DEBUG 10-14 21:17:20 lpllm.py:1801] cuda memory handles: {1: <capsule object NULL at 0x7fdf3c6d9f80>} {1: b'\xe0\xaa29\x00\x00\x00\x00\xe8&\x1d\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\x86\x00\x00\x00\xff\x00\x00/\x00\x00\x00\x00\x00\x00\x00)\xd8\xd5\xc1\xa2\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-14 21:17:20 client.py:72] load_into_gpu: Mixtral-8x7B, fac5d49f-8788-4ade-9c7c-0f5b4cc7230e
INFO 10-14 21:17:20 client.py:113] Model loaded: Mixtral-8x7B, fac5d49f-8788-4ade-9c7c-0f5b4cc7230e
DEBUG 10-14 21:17:20 lpmodule.py:451] hidden_states.shape torch.Size([10, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:20 lpmodule.py:455] input_layernorm num_params: 1
DEBUG 10-14 21:17:20 lpmodule.py:457] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-14 21:17:20 lpmodule.py:498] kv_seq_len 512 past_key_value 0
DEBUG 10-14 21:17:20 lpmodule.py:504] inv_freq device cpu
DEBUG 10-14 21:17:20 lpmodule.py:505] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:20 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:20 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:20 lpmodule.py:505]         ...,
DEBUG 10-14 21:17:20 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:20 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:20 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-14 21:17:20 lpmodule.py:511] query_states device cuda:1
DEBUG 10-14 21:17:20 lpmodule.py:512] key_states device cuda:1
DEBUG 10-14 21:17:20 lpmodule.py:513] value_states device cuda:1
DEBUG 10-14 21:17:20 lpmodule.py:514] position_ids device cuda:1
DEBUG 10-14 21:17:20 lpmodule.py:515] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:20 lpmodule.py:516] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:20 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:20 lpllm.py:2262] GPU2CPU move cost 0.000693 seconds
DEBUG 10-14 21:17:20 lpmodule.py:340] layer_idx 3 attn layer_idx: 3
DEBUG 10-14 21:17:20 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-14 21:17:20 lpmodule.py:364] decoder_attn_batch update batch_dim 0-10
DEBUG 10-14 21:17:20 StaticCacheLen.py:84] static cache update layer_idx: 3, start_batch: 0, end_batch: 10
DEBUG 10-14 21:17:20 lpmodule.py:368] update for kv cache 0-10 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-14 21:17:20 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:20 lpmodule.py:374] update past key value cost 0.003731 seconds
DEBUG 10-14 21:17:20 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 512, 128]), value shape torch.Size([10, 8, 512, 128])
DEBUG 10-14 21:17:20 lpmodule.py:227] layer idx 2
DEBUG 10-14 21:17:20 lpmodule.py:399] repeat qkv cost 0.013345 seconds
DEBUG 10-14 21:17:20 lpmodule.py:400] q shape torch.Size([10, 32, 512, 128]), k shape torch.Size([10, 32, 512, 128]), v shape torch.Size([10, 32, 512, 128])
DEBUG 10-14 21:17:20 lpllm.py:503] reset update experts
DEBUG 10-14 21:17:20 lpllm.py:1771] update state cost 2.288818359375e-05 s
DEBUG 10-14 21:17:20 lpllm.py:1740] restore layer func cost 0.0012743473052978516 s
DEBUG 10-14 21:17:20 lpllm.py:511] restore layer cost 0.0016210079193115234 s
DEBUG 10-14 21:17:20 lpllm.py:512] reset_next_layer_need: layer_attn_idx=3, layer_mlp_idx=3, j_loc=6
DEBUG 10-14 21:17:20 lpllm.py:1034] reset layer cost 0.0017719268798828125 s
DEBUG 10-14 21:17:20 lpllm.py:1035] have reset next layer layer_attn 3 layer_mlp 3 
DEBUG 10-14 21:17:20 lpllm.py:921] 
DEBUG 10-14 21:17:20 lpllm.py:921] decoder loop j: 7 cur_layer_idx: 3 layer_attn_idx: 3 layer_mlp_idx: 3
DEBUG 10-14 21:17:20 lpmodule.py:451] hidden_states.shape torch.Size([10, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:20 lpmodule.py:455] input_layernorm num_params: 1
DEBUG 10-14 21:17:20 lpmodule.py:457] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-14 21:17:20 lpmodule.py:498] kv_seq_len 512 past_key_value 0
DEBUG 10-14 21:17:20 lpmodule.py:504] inv_freq device cpu
DEBUG 10-14 21:17:20 lpmodule.py:505] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:20 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:20 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:20 lpmodule.py:505]         ...,
DEBUG 10-14 21:17:20 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:20 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:20 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-14 21:17:20 lpmodule.py:511] query_states device cuda:1
DEBUG 10-14 21:17:20 lpmodule.py:512] key_states device cuda:1
DEBUG 10-14 21:17:20 lpmodule.py:513] value_states device cuda:1
DEBUG 10-14 21:17:20 lpmodule.py:514] position_ids device cuda:1
DEBUG 10-14 21:17:20 lpmodule.py:515] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:20 lpmodule.py:516] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:20 lpmodule.py:433] dot attn cost 0.064581 seconds
DEBUG 10-14 21:17:20 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-14 21:17:20 lpllm.py:2081] waiting out_queue
time cost move to cuda:1 0.025578975677490234 s
DEBUG 10-14 21:17:20 lpllm.py:2280] CPU attn cost 0.118083 seconds if batch True
DEBUG 10-14 21:17:20 lpllm.py:2289] deal attn result cost 0.000003 seconds
DEBUG 10-14 21:17:20 lpllm.py:2291] CPU compute cost 0.119272 seconds
DEBUG 10-14 21:17:20 lpllm.py:2309] free cost 0.000335 seconds
DEBUG 10-14 21:17:20 lpllm.py:2262] GPU2CPU move cost 0.000436 seconds
DEBUG 10-14 21:17:20 lpmodule.py:340] layer_idx 3 attn layer_idx: 3
DEBUG 10-14 21:17:20 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-14 21:17:20 lpmodule.py:364] decoder_attn_batch update batch_dim 10-20
DEBUG 10-14 21:17:20 StaticCacheLen.py:84] static cache update layer_idx: 3, start_batch: 10, end_batch: 20
DEBUG 10-14 21:17:20 StaticCacheLen.py:90] static cache update layer_idx: 3, update seq_length to 512
DEBUG 10-14 21:17:20 lpmodule.py:368] update for kv cache 10-20 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-14 21:17:20 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:20 lpmodule.py:374] update past key value cost 0.003916 seconds
DEBUG 10-14 21:17:20 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 512, 128]), value shape torch.Size([10, 8, 512, 128])
DEBUG 10-14 21:17:20 lpmodule.py:227] layer idx 3
DEBUG 10-14 21:17:20 lpmodule.py:399] repeat qkv cost 0.013327 seconds
DEBUG 10-14 21:17:20 lpmodule.py:400] q shape torch.Size([10, 32, 512, 128]), k shape torch.Size([10, 32, 512, 128]), v shape torch.Size([10, 32, 512, 128])
DEBUG 10-14 21:17:21 lpllm.py:498] reset update attn
DEBUG 10-14 21:17:21 lpllm.py:1771] update state cost 5.1021575927734375e-05 s
DEBUG 10-14 21:17:21 lpllm.py:1740] restore layer func cost 0.0013365745544433594 s
DEBUG 10-14 21:17:21 lpllm.py:511] restore layer cost 0.0018892288208007812 s
DEBUG 10-14 21:17:21 lpllm.py:512] reset_next_layer_need: layer_attn_idx=4, layer_mlp_idx=3, j_loc=7
DEBUG 10-14 21:17:21 lpllm.py:1034] reset layer cost 0.0020835399627685547 s
DEBUG 10-14 21:17:21 lpllm.py:1035] have reset next layer layer_attn 4 layer_mlp 3 
DEBUG 10-14 21:17:21 lpllm.py:1041] j: 7 waiting the layer with layer_idx 4 before wait time 0.22418570518493652 s
INFO 10-14 21:17:21 client.py:117] confirm_model_loaded: Mixtral-8x7B, fac5d49f-8788-4ade-9c7c-0f5b4cc7230e
DEBUG 10-14 21:17:21 lpmodule.py:433] dot attn cost 0.063218 seconds
DEBUG 10-14 21:17:21 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.024703264236450195 s
DEBUG 10-14 21:17:21 lpllm.py:2280] CPU attn cost 0.112821 seconds if batch True
DEBUG 10-14 21:17:21 lpllm.py:2289] deal attn result cost 0.000002 seconds
DEBUG 10-14 21:17:21 lpllm.py:2291] CPU compute cost 0.113555 seconds
DEBUG 10-14 21:17:21 lpllm.py:2309] free cost 0.000112 seconds
INFO 10-14 21:17:21 client.py:125] Model loaded
DEBUG 10-14 21:17:21 lpllm.py:1045] j: load cost 0.31021857261657715 s waiting cost 0.0859987735748291 s
DEBUG 10-14 21:17:21 lpllm.py:921] 
DEBUG 10-14 21:17:21 lpllm.py:921] decoder loop j: 8 cur_layer_idx: 4 layer_attn_idx: 4 layer_mlp_idx: 3
DEBUG 10-14 21:17:21 lpllm.py:930] start load next layer cur_layer_idx: 5
DEBUG 10-14 21:17:21 lpllm.py:1801] cuda memory handles: {1: <capsule object NULL at 0x7fdf3c6da010>} {1: b'\xe0\xaa29\x00\x00\x00\x00\xe8&\x1d\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\x86\x00\x00\x00\xff\x00\x000\x00\x00\x00\x00\x00\x00\x00)\xd8\xd5\xc1\xa4\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-14 21:17:21 client.py:72] load_into_gpu: Mixtral-8x7B, b7b08ce5-78a4-45de-b468-293d8783cdc8
INFO 10-14 21:17:21 client.py:113] Model loaded: Mixtral-8x7B, b7b08ce5-78a4-45de-b468-293d8783cdc8
DEBUG 10-14 21:17:21 lpmodule.py:451] hidden_states.shape torch.Size([10, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:21 lpmodule.py:455] input_layernorm num_params: 1
DEBUG 10-14 21:17:21 lpmodule.py:457] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-14 21:17:21 lpmodule.py:498] kv_seq_len 512 past_key_value 0
DEBUG 10-14 21:17:21 lpmodule.py:504] inv_freq device cpu
DEBUG 10-14 21:17:21 lpmodule.py:505] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:21 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:21 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:21 lpmodule.py:505]         ...,
DEBUG 10-14 21:17:21 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:21 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:21 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-14 21:17:21 lpmodule.py:511] query_states device cuda:1
DEBUG 10-14 21:17:21 lpmodule.py:512] key_states device cuda:1
DEBUG 10-14 21:17:21 lpmodule.py:513] value_states device cuda:1
DEBUG 10-14 21:17:21 lpmodule.py:514] position_ids device cuda:1
DEBUG 10-14 21:17:21 lpmodule.py:515] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:21 lpmodule.py:516] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:21 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:21 lpllm.py:2262] GPU2CPU move cost 0.000902 seconds
DEBUG 10-14 21:17:21 lpmodule.py:340] layer_idx 4 attn layer_idx: 4
DEBUG 10-14 21:17:21 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-14 21:17:21 lpmodule.py:364] decoder_attn_batch update batch_dim 0-10
DEBUG 10-14 21:17:21 StaticCacheLen.py:84] static cache update layer_idx: 4, start_batch: 0, end_batch: 10
DEBUG 10-14 21:17:21 lpmodule.py:368] update for kv cache 0-10 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-14 21:17:21 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:21 lpmodule.py:374] update past key value cost 0.003763 seconds
DEBUG 10-14 21:17:21 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 512, 128]), value shape torch.Size([10, 8, 512, 128])
DEBUG 10-14 21:17:21 lpmodule.py:227] layer idx 3
DEBUG 10-14 21:17:21 lpmodule.py:399] repeat qkv cost 0.013901 seconds
DEBUG 10-14 21:17:21 lpmodule.py:400] q shape torch.Size([10, 32, 512, 128]), k shape torch.Size([10, 32, 512, 128]), v shape torch.Size([10, 32, 512, 128])
DEBUG 10-14 21:17:21 lpllm.py:503] reset update experts
DEBUG 10-14 21:17:21 lpllm.py:1771] update state cost 4.601478576660156e-05 s
DEBUG 10-14 21:17:21 lpllm.py:1740] restore layer func cost 0.002517223358154297 s
DEBUG 10-14 21:17:21 lpllm.py:511] restore layer cost 0.003098726272583008 s
DEBUG 10-14 21:17:21 lpllm.py:512] reset_next_layer_need: layer_attn_idx=4, layer_mlp_idx=4, j_loc=8
DEBUG 10-14 21:17:21 lpllm.py:1034] reset layer cost 0.0032584667205810547 s
DEBUG 10-14 21:17:21 lpllm.py:1035] have reset next layer layer_attn 4 layer_mlp 4 
DEBUG 10-14 21:17:21 lpllm.py:921] 
DEBUG 10-14 21:17:21 lpllm.py:921] decoder loop j: 9 cur_layer_idx: 4 layer_attn_idx: 4 layer_mlp_idx: 4
DEBUG 10-14 21:17:21 lpmodule.py:451] hidden_states.shape torch.Size([10, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:21 lpmodule.py:455] input_layernorm num_params: 1
DEBUG 10-14 21:17:21 lpmodule.py:457] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-14 21:17:21 lpmodule.py:498] kv_seq_len 512 past_key_value 0
DEBUG 10-14 21:17:21 lpmodule.py:504] inv_freq device cpu
DEBUG 10-14 21:17:21 lpmodule.py:433] dot attn cost 0.062043 seconds
DEBUG 10-14 21:17:21 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-14 21:17:21 lpmodule.py:505] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:21 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:21 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:21 lpmodule.py:505]         ...,
DEBUG 10-14 21:17:21 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:21 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:21 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-14 21:17:21 lpmodule.py:511] query_states device cuda:1
DEBUG 10-14 21:17:21 lpmodule.py:512] key_states device cuda:1
DEBUG 10-14 21:17:21 lpmodule.py:513] value_states device cuda:1
DEBUG 10-14 21:17:21 lpmodule.py:514] position_ids device cuda:1
DEBUG 10-14 21:17:21 lpmodule.py:515] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:21 lpmodule.py:516] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:21 lpllm.py:2081] waiting out_queue
time cost move to cuda:1 0.025188684463500977 s
DEBUG 10-14 21:17:21 lpllm.py:2280] CPU attn cost 0.112088 seconds if batch True
DEBUG 10-14 21:17:21 lpllm.py:2289] deal attn result cost 0.000002 seconds
DEBUG 10-14 21:17:21 lpllm.py:2291] CPU compute cost 0.113318 seconds
DEBUG 10-14 21:17:21 lpllm.py:2309] free cost 0.000108 seconds
DEBUG 10-14 21:17:21 lpllm.py:2262] GPU2CPU move cost 0.000302 seconds
DEBUG 10-14 21:17:21 lpmodule.py:340] layer_idx 4 attn layer_idx: 4
DEBUG 10-14 21:17:21 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-14 21:17:21 lpmodule.py:364] decoder_attn_batch update batch_dim 10-20
DEBUG 10-14 21:17:21 StaticCacheLen.py:84] static cache update layer_idx: 4, start_batch: 10, end_batch: 20
DEBUG 10-14 21:17:21 StaticCacheLen.py:90] static cache update layer_idx: 4, update seq_length to 512
DEBUG 10-14 21:17:21 lpmodule.py:368] update for kv cache 10-20 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-14 21:17:21 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:21 lpmodule.py:374] update past key value cost 0.003058 seconds
DEBUG 10-14 21:17:21 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 512, 128]), value shape torch.Size([10, 8, 512, 128])
DEBUG 10-14 21:17:21 lpmodule.py:227] layer idx 4
DEBUG 10-14 21:17:21 lpmodule.py:399] repeat qkv cost 0.013811 seconds
DEBUG 10-14 21:17:21 lpmodule.py:400] q shape torch.Size([10, 32, 512, 128]), k shape torch.Size([10, 32, 512, 128]), v shape torch.Size([10, 32, 512, 128])
DEBUG 10-14 21:17:21 lpllm.py:498] reset update attn
DEBUG 10-14 21:17:21 lpllm.py:1771] update state cost 2.574920654296875e-05 s
DEBUG 10-14 21:17:21 lpllm.py:1740] restore layer func cost 0.0005707740783691406 s
DEBUG 10-14 21:17:21 lpllm.py:511] restore layer cost 0.0008792877197265625 s
DEBUG 10-14 21:17:21 lpllm.py:512] reset_next_layer_need: layer_attn_idx=5, layer_mlp_idx=4, j_loc=9
DEBUG 10-14 21:17:21 lpllm.py:1034] reset layer cost 0.0010187625885009766 s
DEBUG 10-14 21:17:21 lpllm.py:1035] have reset next layer layer_attn 5 layer_mlp 4 
DEBUG 10-14 21:17:21 lpllm.py:1041] j: 9 waiting the layer with layer_idx 5 before wait time 0.21261835098266602 s
INFO 10-14 21:17:21 client.py:117] confirm_model_loaded: Mixtral-8x7B, b7b08ce5-78a4-45de-b468-293d8783cdc8
DEBUG 10-14 21:17:21 lpmodule.py:433] dot attn cost 0.062980 seconds
DEBUG 10-14 21:17:21 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.02486872673034668 s
DEBUG 10-14 21:17:21 lpllm.py:2280] CPU attn cost 0.112066 seconds if batch True
DEBUG 10-14 21:17:21 lpllm.py:2289] deal attn result cost 0.000002 seconds
DEBUG 10-14 21:17:21 lpllm.py:2291] CPU compute cost 0.112586 seconds
DEBUG 10-14 21:17:21 lpllm.py:2309] free cost 0.000093 seconds
INFO 10-14 21:17:21 client.py:125] Model loaded
DEBUG 10-14 21:17:21 lpllm.py:1045] j: load cost 0.3116621971130371 s waiting cost 0.09902644157409668 s
DEBUG 10-14 21:17:21 lpllm.py:921] 
DEBUG 10-14 21:17:21 lpllm.py:921] decoder loop j: 10 cur_layer_idx: 5 layer_attn_idx: 5 layer_mlp_idx: 4
DEBUG 10-14 21:17:21 lpllm.py:930] start load next layer cur_layer_idx: 6
DEBUG 10-14 21:17:21 lpllm.py:1801] cuda memory handles: {1: <capsule object NULL at 0x7fdf3c6d9ec0>} {1: b'\xe0\xaa29\x00\x00\x00\x00\xe8&\x1d\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\x86\x00\x00\x00\xff\x00\x00.\x00\x00\x00\x00\x00\x00\x00)\xd8\xd5\xc1\xa0\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-14 21:17:21 client.py:72] load_into_gpu: Mixtral-8x7B, 88a7e0e1-b5fa-4b30-8853-994ff66c7057
INFO 10-14 21:17:21 client.py:113] Model loaded: Mixtral-8x7B, 88a7e0e1-b5fa-4b30-8853-994ff66c7057
DEBUG 10-14 21:17:21 lpmodule.py:451] hidden_states.shape torch.Size([10, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:21 lpmodule.py:455] input_layernorm num_params: 1
DEBUG 10-14 21:17:21 lpmodule.py:457] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-14 21:17:21 lpmodule.py:498] kv_seq_len 512 past_key_value 0
DEBUG 10-14 21:17:21 lpmodule.py:504] inv_freq device cpu
DEBUG 10-14 21:17:21 lpmodule.py:505] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:21 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:21 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:21 lpmodule.py:505]         ...,
DEBUG 10-14 21:17:21 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:21 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:21 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-14 21:17:21 lpmodule.py:511] query_states device cuda:1
DEBUG 10-14 21:17:21 lpmodule.py:512] key_states device cuda:1
DEBUG 10-14 21:17:21 lpmodule.py:513] value_states device cuda:1
DEBUG 10-14 21:17:21 lpmodule.py:514] position_ids device cuda:1
DEBUG 10-14 21:17:21 lpmodule.py:515] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:21 lpmodule.py:516] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:21 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:21 lpllm.py:2262] GPU2CPU move cost 0.000674 seconds
DEBUG 10-14 21:17:21 lpmodule.py:340] layer_idx 5 attn layer_idx: 5
DEBUG 10-14 21:17:21 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-14 21:17:21 lpmodule.py:364] decoder_attn_batch update batch_dim 0-10
DEBUG 10-14 21:17:21 StaticCacheLen.py:84] static cache update layer_idx: 5, start_batch: 0, end_batch: 10
DEBUG 10-14 21:17:21 lpmodule.py:368] update for kv cache 0-10 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-14 21:17:21 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:21 lpmodule.py:374] update past key value cost 0.003636 seconds
DEBUG 10-14 21:17:21 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 512, 128]), value shape torch.Size([10, 8, 512, 128])
DEBUG 10-14 21:17:21 lpmodule.py:227] layer idx 4
DEBUG 10-14 21:17:21 lpmodule.py:399] repeat qkv cost 0.013762 seconds
DEBUG 10-14 21:17:21 lpmodule.py:400] q shape torch.Size([10, 32, 512, 128]), k shape torch.Size([10, 32, 512, 128]), v shape torch.Size([10, 32, 512, 128])
DEBUG 10-14 21:17:21 lpllm.py:503] reset update experts
DEBUG 10-14 21:17:21 lpllm.py:1771] update state cost 4.076957702636719e-05 s
DEBUG 10-14 21:17:21 lpllm.py:1740] restore layer func cost 0.003959178924560547 s
DEBUG 10-14 21:17:21 lpllm.py:511] restore layer cost 0.004611015319824219 s
DEBUG 10-14 21:17:21 lpllm.py:512] reset_next_layer_need: layer_attn_idx=5, layer_mlp_idx=5, j_loc=10
DEBUG 10-14 21:17:21 lpllm.py:1034] reset layer cost 0.004868030548095703 s
DEBUG 10-14 21:17:21 lpllm.py:1035] have reset next layer layer_attn 5 layer_mlp 5 
DEBUG 10-14 21:17:21 lpllm.py:921] 
DEBUG 10-14 21:17:21 lpllm.py:921] decoder loop j: 11 cur_layer_idx: 5 layer_attn_idx: 5 layer_mlp_idx: 5
DEBUG 10-14 21:17:21 lpmodule.py:451] hidden_states.shape torch.Size([10, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:21 lpmodule.py:455] input_layernorm num_params: 1
DEBUG 10-14 21:17:21 lpmodule.py:457] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-14 21:17:21 lpmodule.py:433] dot attn cost 0.062451 seconds
DEBUG 10-14 21:17:21 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-14 21:17:21 lpmodule.py:498] kv_seq_len 512 past_key_value 0
DEBUG 10-14 21:17:21 lpmodule.py:504] inv_freq device cpu
DEBUG 10-14 21:17:21 lpmodule.py:505] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:21 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:21 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:21 lpmodule.py:505]         ...,
DEBUG 10-14 21:17:21 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:21 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:21 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511]])
time cost move to cuda:1 0.024580001831054688 s
DEBUG 10-14 21:17:21 lpmodule.py:511] query_states device cuda:1
DEBUG 10-14 21:17:21 lpmodule.py:512] key_states device cuda:1
DEBUG 10-14 21:17:21 lpmodule.py:513] value_states device cuda:1
DEBUG 10-14 21:17:21 lpmodule.py:514] position_ids device cuda:1
DEBUG 10-14 21:17:21 lpmodule.py:515] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:21 lpmodule.py:516] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:21 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:21 lpllm.py:2280] CPU attn cost 0.111900 seconds if batch True
DEBUG 10-14 21:17:21 lpllm.py:2289] deal attn result cost 0.000002 seconds
DEBUG 10-14 21:17:21 lpllm.py:2291] CPU compute cost 0.112918 seconds
DEBUG 10-14 21:17:21 lpllm.py:2309] free cost 0.000114 seconds
DEBUG 10-14 21:17:21 lpllm.py:2262] GPU2CPU move cost 0.000271 seconds
DEBUG 10-14 21:17:21 lpmodule.py:340] layer_idx 5 attn layer_idx: 5
DEBUG 10-14 21:17:21 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-14 21:17:21 lpmodule.py:364] decoder_attn_batch update batch_dim 10-20
DEBUG 10-14 21:17:21 StaticCacheLen.py:84] static cache update layer_idx: 5, start_batch: 10, end_batch: 20
DEBUG 10-14 21:17:21 StaticCacheLen.py:90] static cache update layer_idx: 5, update seq_length to 512
DEBUG 10-14 21:17:21 lpmodule.py:368] update for kv cache 10-20 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-14 21:17:21 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:21 lpmodule.py:374] update past key value cost 0.003356 seconds
DEBUG 10-14 21:17:21 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 512, 128]), value shape torch.Size([10, 8, 512, 128])
DEBUG 10-14 21:17:21 lpmodule.py:227] layer idx 5
DEBUG 10-14 21:17:21 lpmodule.py:399] repeat qkv cost 0.013363 seconds
DEBUG 10-14 21:17:21 lpmodule.py:400] q shape torch.Size([10, 32, 512, 128]), k shape torch.Size([10, 32, 512, 128]), v shape torch.Size([10, 32, 512, 128])
DEBUG 10-14 21:17:21 lpllm.py:498] reset update attn
DEBUG 10-14 21:17:21 lpllm.py:1771] update state cost 2.6702880859375e-05 s
DEBUG 10-14 21:17:21 lpllm.py:1740] restore layer func cost 0.0006046295166015625 s
DEBUG 10-14 21:17:21 lpllm.py:511] restore layer cost 0.0009224414825439453 s
DEBUG 10-14 21:17:21 lpllm.py:512] reset_next_layer_need: layer_attn_idx=6, layer_mlp_idx=5, j_loc=11
DEBUG 10-14 21:17:21 lpllm.py:1034] reset layer cost 0.0010416507720947266 s
DEBUG 10-14 21:17:21 lpllm.py:1035] have reset next layer layer_attn 6 layer_mlp 5 
DEBUG 10-14 21:17:21 lpllm.py:1041] j: 11 waiting the layer with layer_idx 6 before wait time 0.2179563045501709 s
INFO 10-14 21:17:21 client.py:117] confirm_model_loaded: Mixtral-8x7B, 88a7e0e1-b5fa-4b30-8853-994ff66c7057
DEBUG 10-14 21:17:21 lpmodule.py:433] dot attn cost 0.061558 seconds
DEBUG 10-14 21:17:21 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.023529529571533203 s
DEBUG 10-14 21:17:21 lpllm.py:2280] CPU attn cost 0.109398 seconds if batch True
DEBUG 10-14 21:17:21 lpllm.py:2289] deal attn result cost 0.000002 seconds
DEBUG 10-14 21:17:21 lpllm.py:2291] CPU compute cost 0.109917 seconds
DEBUG 10-14 21:17:21 lpllm.py:2309] free cost 0.000124 seconds
INFO 10-14 21:17:21 client.py:125] Model loaded
DEBUG 10-14 21:17:21 lpllm.py:1045] j: load cost 0.31699275970458984 s waiting cost 0.0990152359008789 s
DEBUG 10-14 21:17:21 lpllm.py:921] 
DEBUG 10-14 21:17:21 lpllm.py:921] decoder loop j: 12 cur_layer_idx: 6 layer_attn_idx: 6 layer_mlp_idx: 5
DEBUG 10-14 21:17:21 lpllm.py:930] start load next layer cur_layer_idx: 7
DEBUG 10-14 21:17:21 lpllm.py:1801] cuda memory handles: {1: <capsule object NULL at 0x7fdf3c6d9f80>} {1: b'\xe0\xaa29\x00\x00\x00\x00\xe8&\x1d\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\x86\x00\x00\x00\xff\x00\x00/\x00\x00\x00\x00\x00\x00\x00)\xd8\xd5\xc1\xa2\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-14 21:17:21 client.py:72] load_into_gpu: Mixtral-8x7B, 2214d462-b643-48d7-bbf2-a191ccdef29f
INFO 10-14 21:17:21 client.py:113] Model loaded: Mixtral-8x7B, 2214d462-b643-48d7-bbf2-a191ccdef29f
DEBUG 10-14 21:17:21 lpmodule.py:451] hidden_states.shape torch.Size([10, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:21 lpmodule.py:455] input_layernorm num_params: 1
DEBUG 10-14 21:17:21 lpmodule.py:457] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-14 21:17:21 lpmodule.py:498] kv_seq_len 512 past_key_value 0
DEBUG 10-14 21:17:21 lpmodule.py:504] inv_freq device cpu
DEBUG 10-14 21:17:21 lpmodule.py:505] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:21 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:21 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:21 lpmodule.py:505]         ...,
DEBUG 10-14 21:17:21 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:21 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:21 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-14 21:17:21 lpmodule.py:511] query_states device cuda:1
DEBUG 10-14 21:17:21 lpmodule.py:512] key_states device cuda:1
DEBUG 10-14 21:17:21 lpmodule.py:513] value_states device cuda:1
DEBUG 10-14 21:17:21 lpmodule.py:514] position_ids device cuda:1
DEBUG 10-14 21:17:21 lpmodule.py:515] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:21 lpmodule.py:516] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:21 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:21 lpllm.py:2262] GPU2CPU move cost 0.000857 seconds
DEBUG 10-14 21:17:21 lpmodule.py:340] layer_idx 6 attn layer_idx: 6
DEBUG 10-14 21:17:21 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-14 21:17:21 lpmodule.py:364] decoder_attn_batch update batch_dim 0-10
DEBUG 10-14 21:17:21 StaticCacheLen.py:84] static cache update layer_idx: 6, start_batch: 0, end_batch: 10
DEBUG 10-14 21:17:21 lpmodule.py:368] update for kv cache 0-10 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-14 21:17:21 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:21 lpmodule.py:374] update past key value cost 0.003910 seconds
DEBUG 10-14 21:17:21 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 512, 128]), value shape torch.Size([10, 8, 512, 128])
DEBUG 10-14 21:17:21 lpmodule.py:227] layer idx 5
DEBUG 10-14 21:17:21 lpmodule.py:399] repeat qkv cost 0.012669 seconds
DEBUG 10-14 21:17:21 lpmodule.py:400] q shape torch.Size([10, 32, 512, 128]), k shape torch.Size([10, 32, 512, 128]), v shape torch.Size([10, 32, 512, 128])
DEBUG 10-14 21:17:21 lpmodule.py:433] dot attn cost 0.065295 seconds
DEBUG 10-14 21:17:21 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-14 21:17:21 lpllm.py:503] reset update experts
DEBUG 10-14 21:17:21 lpllm.py:1771] update state cost 2.956390380859375e-05 s
DEBUG 10-14 21:17:21 lpllm.py:1740] restore layer func cost 0.002338886260986328 s
DEBUG 10-14 21:17:21 lpllm.py:511] restore layer cost 0.0027434825897216797 s
DEBUG 10-14 21:17:21 lpllm.py:512] reset_next_layer_need: layer_attn_idx=6, layer_mlp_idx=6, j_loc=12
DEBUG 10-14 21:17:21 lpllm.py:1034] reset layer cost 0.002911090850830078 s
DEBUG 10-14 21:17:21 lpllm.py:1035] have reset next layer layer_attn 6 layer_mlp 6 
DEBUG 10-14 21:17:21 lpllm.py:921] 
DEBUG 10-14 21:17:21 lpllm.py:921] decoder loop j: 13 cur_layer_idx: 6 layer_attn_idx: 6 layer_mlp_idx: 6
DEBUG 10-14 21:17:21 lpmodule.py:451] hidden_states.shape torch.Size([10, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:21 lpmodule.py:455] input_layernorm num_params: 1
DEBUG 10-14 21:17:21 lpmodule.py:457] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
time cost move to cuda:1 0.025747060775756836 s
DEBUG 10-14 21:17:21 lpmodule.py:498] kv_seq_len 512 past_key_value 0
DEBUG 10-14 21:17:21 lpmodule.py:504] inv_freq device cpu
DEBUG 10-14 21:17:21 lpllm.py:2280] CPU attn cost 0.114840 seconds if batch True
DEBUG 10-14 21:17:21 lpllm.py:2289] deal attn result cost 0.000002 seconds
DEBUG 10-14 21:17:21 lpllm.py:2291] CPU compute cost 0.116086 seconds
DEBUG 10-14 21:17:21 lpllm.py:2309] free cost 0.000090 seconds
DEBUG 10-14 21:17:21 lpmodule.py:505] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:21 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:21 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:21 lpmodule.py:505]         ...,
DEBUG 10-14 21:17:21 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:21 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:21 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-14 21:17:21 lpmodule.py:511] query_states device cuda:1
DEBUG 10-14 21:17:21 lpmodule.py:512] key_states device cuda:1
DEBUG 10-14 21:17:21 lpmodule.py:513] value_states device cuda:1
DEBUG 10-14 21:17:21 lpmodule.py:514] position_ids device cuda:1
DEBUG 10-14 21:17:21 lpmodule.py:515] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:21 lpmodule.py:516] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:21 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:21 lpllm.py:2262] GPU2CPU move cost 0.000269 seconds
DEBUG 10-14 21:17:21 lpmodule.py:340] layer_idx 6 attn layer_idx: 6
DEBUG 10-14 21:17:21 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-14 21:17:21 lpmodule.py:364] decoder_attn_batch update batch_dim 10-20
DEBUG 10-14 21:17:21 StaticCacheLen.py:84] static cache update layer_idx: 6, start_batch: 10, end_batch: 20
DEBUG 10-14 21:17:21 StaticCacheLen.py:90] static cache update layer_idx: 6, update seq_length to 512
DEBUG 10-14 21:17:21 lpmodule.py:368] update for kv cache 10-20 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-14 21:17:21 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:21 lpmodule.py:374] update past key value cost 0.002974 seconds
DEBUG 10-14 21:17:21 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 512, 128]), value shape torch.Size([10, 8, 512, 128])
DEBUG 10-14 21:17:21 lpmodule.py:227] layer idx 6
DEBUG 10-14 21:17:21 lpmodule.py:399] repeat qkv cost 0.012643 seconds
DEBUG 10-14 21:17:21 lpmodule.py:400] q shape torch.Size([10, 32, 512, 128]), k shape torch.Size([10, 32, 512, 128]), v shape torch.Size([10, 32, 512, 128])
DEBUG 10-14 21:17:21 lpllm.py:498] reset update attn
DEBUG 10-14 21:17:21 lpllm.py:1771] update state cost 3.457069396972656e-05 s
DEBUG 10-14 21:17:21 lpllm.py:1740] restore layer func cost 0.0008723735809326172 s
DEBUG 10-14 21:17:21 lpllm.py:511] restore layer cost 0.0012450218200683594 s
DEBUG 10-14 21:17:21 lpllm.py:512] reset_next_layer_need: layer_attn_idx=7, layer_mlp_idx=6, j_loc=13
DEBUG 10-14 21:17:21 lpllm.py:1034] reset layer cost 0.00139617919921875 s
DEBUG 10-14 21:17:21 lpllm.py:1035] have reset next layer layer_attn 7 layer_mlp 6 
DEBUG 10-14 21:17:21 lpllm.py:1041] j: 13 waiting the layer with layer_idx 7 before wait time 0.21932482719421387 s
INFO 10-14 21:17:21 client.py:117] confirm_model_loaded: Mixtral-8x7B, 2214d462-b643-48d7-bbf2-a191ccdef29f
DEBUG 10-14 21:17:21 lpmodule.py:433] dot attn cost 0.072560 seconds
DEBUG 10-14 21:17:21 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.02599024772644043 s
DEBUG 10-14 21:17:22 lpllm.py:2280] CPU attn cost 0.121497 seconds if batch True
DEBUG 10-14 21:17:22 lpllm.py:2289] deal attn result cost 0.000002 seconds
DEBUG 10-14 21:17:22 lpllm.py:2291] CPU compute cost 0.122004 seconds
DEBUG 10-14 21:17:22 lpllm.py:2309] free cost 0.000098 seconds
INFO 10-14 21:17:22 client.py:125] Model loaded
DEBUG 10-14 21:17:22 lpllm.py:1045] j: load cost 0.3039524555206299 s waiting cost 0.08459258079528809 s
DEBUG 10-14 21:17:22 lpllm.py:921] 
DEBUG 10-14 21:17:22 lpllm.py:921] decoder loop j: 14 cur_layer_idx: 7 layer_attn_idx: 7 layer_mlp_idx: 6
DEBUG 10-14 21:17:22 lpllm.py:930] start load next layer cur_layer_idx: 8
DEBUG 10-14 21:17:22 lpllm.py:1801] cuda memory handles: {1: <capsule object NULL at 0x7fdf3c6da010>} {1: b'\xe0\xaa29\x00\x00\x00\x00\xe8&\x1d\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\x86\x00\x00\x00\xff\x00\x000\x00\x00\x00\x00\x00\x00\x00)\xd8\xd5\xc1\xa4\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-14 21:17:22 client.py:72] load_into_gpu: Mixtral-8x7B, 7b89110f-5611-4334-b7a4-e0dd0a834efb
INFO 10-14 21:17:22 client.py:113] Model loaded: Mixtral-8x7B, 7b89110f-5611-4334-b7a4-e0dd0a834efb
DEBUG 10-14 21:17:22 lpmodule.py:451] hidden_states.shape torch.Size([10, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:22 lpmodule.py:455] input_layernorm num_params: 1
DEBUG 10-14 21:17:22 lpmodule.py:457] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-14 21:17:22 lpmodule.py:498] kv_seq_len 512 past_key_value 0
DEBUG 10-14 21:17:22 lpmodule.py:504] inv_freq device cpu
DEBUG 10-14 21:17:22 lpmodule.py:505] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:22 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:22 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:22 lpmodule.py:505]         ...,
DEBUG 10-14 21:17:22 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:22 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:22 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-14 21:17:22 lpmodule.py:511] query_states device cuda:1
DEBUG 10-14 21:17:22 lpmodule.py:512] key_states device cuda:1
DEBUG 10-14 21:17:22 lpmodule.py:513] value_states device cuda:1
DEBUG 10-14 21:17:22 lpmodule.py:514] position_ids device cuda:1
DEBUG 10-14 21:17:22 lpmodule.py:515] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:22 lpmodule.py:516] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:22 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:22 lpllm.py:2262] GPU2CPU move cost 0.000904 seconds
DEBUG 10-14 21:17:22 lpmodule.py:340] layer_idx 7 attn layer_idx: 7
DEBUG 10-14 21:17:22 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-14 21:17:22 lpmodule.py:364] decoder_attn_batch update batch_dim 0-10
DEBUG 10-14 21:17:22 StaticCacheLen.py:84] static cache update layer_idx: 7, start_batch: 0, end_batch: 10
DEBUG 10-14 21:17:22 lpmodule.py:368] update for kv cache 0-10 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-14 21:17:22 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:22 lpmodule.py:374] update past key value cost 0.003853 seconds
DEBUG 10-14 21:17:22 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 512, 128]), value shape torch.Size([10, 8, 512, 128])
DEBUG 10-14 21:17:22 lpmodule.py:227] layer idx 6
DEBUG 10-14 21:17:22 lpmodule.py:399] repeat qkv cost 0.012703 seconds
DEBUG 10-14 21:17:22 lpmodule.py:400] q shape torch.Size([10, 32, 512, 128]), k shape torch.Size([10, 32, 512, 128]), v shape torch.Size([10, 32, 512, 128])
DEBUG 10-14 21:17:22 lpmodule.py:433] dot attn cost 0.065276 seconds
DEBUG 10-14 21:17:22 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.011139154434204102 s
DEBUG 10-14 21:17:22 lpllm.py:2280] CPU attn cost 0.101592 seconds if batch True
DEBUG 10-14 21:17:22 lpllm.py:2289] deal attn result cost 0.000002 seconds
DEBUG 10-14 21:17:22 lpllm.py:2291] CPU compute cost 0.103020 seconds
DEBUG 10-14 21:17:22 lpllm.py:2309] free cost 0.000134 seconds
DEBUG 10-14 21:17:22 lpllm.py:503] reset update experts
DEBUG 10-14 21:17:22 lpllm.py:1771] update state cost 2.09808349609375e-05 s
DEBUG 10-14 21:17:22 lpllm.py:1740] restore layer func cost 0.0011301040649414062 s
DEBUG 10-14 21:17:22 lpllm.py:511] restore layer cost 0.0013935565948486328 s
DEBUG 10-14 21:17:22 lpllm.py:512] reset_next_layer_need: layer_attn_idx=7, layer_mlp_idx=7, j_loc=14
DEBUG 10-14 21:17:22 lpllm.py:1034] reset layer cost 0.0014905929565429688 s
DEBUG 10-14 21:17:22 lpllm.py:1035] have reset next layer layer_attn 7 layer_mlp 7 
DEBUG 10-14 21:17:22 lpllm.py:921] 
DEBUG 10-14 21:17:22 lpllm.py:921] decoder loop j: 15 cur_layer_idx: 7 layer_attn_idx: 7 layer_mlp_idx: 7
DEBUG 10-14 21:17:22 lpmodule.py:451] hidden_states.shape torch.Size([10, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:22 lpmodule.py:455] input_layernorm num_params: 1
DEBUG 10-14 21:17:22 lpmodule.py:457] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-14 21:17:22 lpmodule.py:498] kv_seq_len 512 past_key_value 0
DEBUG 10-14 21:17:22 lpmodule.py:504] inv_freq device cpu
DEBUG 10-14 21:17:22 lpmodule.py:505] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:22 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:22 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:22 lpmodule.py:505]         ...,
DEBUG 10-14 21:17:22 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:22 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:22 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-14 21:17:22 lpmodule.py:511] query_states device cuda:1
DEBUG 10-14 21:17:22 lpmodule.py:512] key_states device cuda:1
DEBUG 10-14 21:17:22 lpmodule.py:513] value_states device cuda:1
DEBUG 10-14 21:17:22 lpmodule.py:514] position_ids device cuda:1
DEBUG 10-14 21:17:22 lpmodule.py:515] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:22 lpmodule.py:516] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:22 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:22 lpllm.py:2262] GPU2CPU move cost 0.000667 seconds
DEBUG 10-14 21:17:22 lpmodule.py:340] layer_idx 7 attn layer_idx: 7
DEBUG 10-14 21:17:22 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-14 21:17:22 lpmodule.py:364] decoder_attn_batch update batch_dim 10-20
DEBUG 10-14 21:17:22 StaticCacheLen.py:84] static cache update layer_idx: 7, start_batch: 10, end_batch: 20
DEBUG 10-14 21:17:22 StaticCacheLen.py:90] static cache update layer_idx: 7, update seq_length to 512
DEBUG 10-14 21:17:22 lpmodule.py:368] update for kv cache 10-20 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-14 21:17:22 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:22 lpmodule.py:374] update past key value cost 0.003839 seconds
DEBUG 10-14 21:17:22 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 512, 128]), value shape torch.Size([10, 8, 512, 128])
DEBUG 10-14 21:17:22 lpmodule.py:227] layer idx 7
DEBUG 10-14 21:17:22 lpmodule.py:399] repeat qkv cost 0.012744 seconds
DEBUG 10-14 21:17:22 lpmodule.py:400] q shape torch.Size([10, 32, 512, 128]), k shape torch.Size([10, 32, 512, 128]), v shape torch.Size([10, 32, 512, 128])
DEBUG 10-14 21:17:22 lpllm.py:498] reset update attn
DEBUG 10-14 21:17:22 lpllm.py:1771] update state cost 5.245208740234375e-05 s
DEBUG 10-14 21:17:22 lpllm.py:1740] restore layer func cost 0.0013110637664794922 s
DEBUG 10-14 21:17:22 lpllm.py:511] restore layer cost 0.0019016265869140625 s
DEBUG 10-14 21:17:22 lpllm.py:512] reset_next_layer_need: layer_attn_idx=8, layer_mlp_idx=7, j_loc=15
DEBUG 10-14 21:17:22 lpllm.py:1034] reset layer cost 0.0021615028381347656 s
DEBUG 10-14 21:17:22 lpllm.py:1035] have reset next layer layer_attn 8 layer_mlp 7 
DEBUG 10-14 21:17:22 lpllm.py:1041] j: 15 waiting the layer with layer_idx 8 before wait time 0.2580690383911133 s
INFO 10-14 21:17:22 client.py:117] confirm_model_loaded: Mixtral-8x7B, 7b89110f-5611-4334-b7a4-e0dd0a834efb
DEBUG 10-14 21:17:22 lpmodule.py:433] dot attn cost 0.066068 seconds
DEBUG 10-14 21:17:22 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.02546834945678711 s
DEBUG 10-14 21:17:22 lpllm.py:2280] CPU attn cost 0.115559 seconds if batch True
DEBUG 10-14 21:17:22 lpllm.py:2289] deal attn result cost 0.000003 seconds
DEBUG 10-14 21:17:22 lpllm.py:2291] CPU compute cost 0.116653 seconds
DEBUG 10-14 21:17:22 lpllm.py:2309] free cost 0.000150 seconds
INFO 10-14 21:17:22 client.py:125] Model loaded
DEBUG 10-14 21:17:22 lpllm.py:1045] j: load cost 0.40218544006347656 s waiting cost 0.14401698112487793 s
DEBUG 10-14 21:17:22 lpllm.py:921] 
DEBUG 10-14 21:17:22 lpllm.py:921] decoder loop j: 16 cur_layer_idx: 8 layer_attn_idx: 8 layer_mlp_idx: 7
DEBUG 10-14 21:17:22 lpllm.py:930] start load next layer cur_layer_idx: 9
DEBUG 10-14 21:17:22 lpllm.py:1801] cuda memory handles: {1: <capsule object NULL at 0x7fdf3c6d9ec0>} {1: b'\xe0\xaa29\x00\x00\x00\x00\xe8&\x1d\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\x86\x00\x00\x00\xff\x00\x00.\x00\x00\x00\x00\x00\x00\x00)\xd8\xd5\xc1\xa0\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-14 21:17:22 client.py:72] load_into_gpu: Mixtral-8x7B, 8e165534-e4c4-4e25-8733-e60e3b9cbd74
INFO 10-14 21:17:22 client.py:113] Model loaded: Mixtral-8x7B, 8e165534-e4c4-4e25-8733-e60e3b9cbd74
DEBUG 10-14 21:17:22 lpmodule.py:451] hidden_states.shape torch.Size([10, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:22 lpmodule.py:455] input_layernorm num_params: 1
DEBUG 10-14 21:17:22 lpmodule.py:457] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-14 21:17:22 lpmodule.py:498] kv_seq_len 512 past_key_value 0
DEBUG 10-14 21:17:22 lpmodule.py:504] inv_freq device cpu
DEBUG 10-14 21:17:22 lpmodule.py:505] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:22 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:22 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:22 lpmodule.py:505]         ...,
DEBUG 10-14 21:17:22 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:22 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:22 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-14 21:17:22 lpmodule.py:511] query_states device cuda:1
DEBUG 10-14 21:17:22 lpmodule.py:512] key_states device cuda:1
DEBUG 10-14 21:17:22 lpmodule.py:513] value_states device cuda:1
DEBUG 10-14 21:17:22 lpmodule.py:514] position_ids device cuda:1
DEBUG 10-14 21:17:22 lpmodule.py:515] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:22 lpmodule.py:516] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:22 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:22 lpllm.py:2262] GPU2CPU move cost 0.000898 seconds
DEBUG 10-14 21:17:22 lpmodule.py:340] layer_idx 8 attn layer_idx: 8
DEBUG 10-14 21:17:22 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-14 21:17:22 lpmodule.py:364] decoder_attn_batch update batch_dim 0-10
DEBUG 10-14 21:17:22 StaticCacheLen.py:84] static cache update layer_idx: 8, start_batch: 0, end_batch: 10
DEBUG 10-14 21:17:22 lpmodule.py:368] update for kv cache 0-10 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-14 21:17:22 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:22 lpmodule.py:374] update past key value cost 0.003763 seconds
DEBUG 10-14 21:17:22 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 512, 128]), value shape torch.Size([10, 8, 512, 128])
DEBUG 10-14 21:17:22 lpmodule.py:227] layer idx 7
DEBUG 10-14 21:17:22 lpmodule.py:399] repeat qkv cost 0.012949 seconds
DEBUG 10-14 21:17:22 lpmodule.py:400] q shape torch.Size([10, 32, 512, 128]), k shape torch.Size([10, 32, 512, 128]), v shape torch.Size([10, 32, 512, 128])
DEBUG 10-14 21:17:22 lpllm.py:503] reset update experts
DEBUG 10-14 21:17:22 lpllm.py:1771] update state cost 4.00543212890625e-05 s
DEBUG 10-14 21:17:22 lpllm.py:1740] restore layer func cost 0.003116607666015625 s
DEBUG 10-14 21:17:22 lpllm.py:511] restore layer cost 0.0037369728088378906 s
DEBUG 10-14 21:17:22 lpllm.py:512] reset_next_layer_need: layer_attn_idx=8, layer_mlp_idx=8, j_loc=16
DEBUG 10-14 21:17:22 lpllm.py:1034] reset layer cost 0.003995180130004883 s
DEBUG 10-14 21:17:22 lpllm.py:1035] have reset next layer layer_attn 8 layer_mlp 8 
DEBUG 10-14 21:17:22 lpllm.py:921] 
DEBUG 10-14 21:17:22 lpllm.py:921] decoder loop j: 17 cur_layer_idx: 8 layer_attn_idx: 8 layer_mlp_idx: 8
DEBUG 10-14 21:17:22 lpmodule.py:451] hidden_states.shape torch.Size([10, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:22 lpmodule.py:455] input_layernorm num_params: 1
DEBUG 10-14 21:17:22 lpmodule.py:457] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-14 21:17:22 lpmodule.py:498] kv_seq_len 512 past_key_value 0
DEBUG 10-14 21:17:22 lpmodule.py:504] inv_freq device cpu
DEBUG 10-14 21:17:22 lpmodule.py:505] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:22 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:22 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:22 lpmodule.py:505]         ...,
DEBUG 10-14 21:17:22 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:22 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:22 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-14 21:17:22 lpmodule.py:511] query_states device cuda:1
DEBUG 10-14 21:17:22 lpmodule.py:512] key_states device cuda:1
DEBUG 10-14 21:17:22 lpmodule.py:513] value_states device cuda:1
DEBUG 10-14 21:17:22 lpmodule.py:514] position_ids device cuda:1
DEBUG 10-14 21:17:22 lpmodule.py:515] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:22 lpmodule.py:516] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:22 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:22 lpmodule.py:433] dot attn cost 0.068475 seconds
DEBUG 10-14 21:17:22 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.024998903274536133 s
DEBUG 10-14 21:17:22 lpllm.py:2280] CPU attn cost 0.117972 seconds if batch True
DEBUG 10-14 21:17:22 lpllm.py:2289] deal attn result cost 0.000002 seconds
DEBUG 10-14 21:17:22 lpllm.py:2291] CPU compute cost 0.119273 seconds
DEBUG 10-14 21:17:22 lpllm.py:2309] free cost 0.000237 seconds
DEBUG 10-14 21:17:22 lpllm.py:2262] GPU2CPU move cost 0.000267 seconds
DEBUG 10-14 21:17:22 lpmodule.py:340] layer_idx 8 attn layer_idx: 8
DEBUG 10-14 21:17:22 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-14 21:17:22 lpmodule.py:364] decoder_attn_batch update batch_dim 10-20
DEBUG 10-14 21:17:22 StaticCacheLen.py:84] static cache update layer_idx: 8, start_batch: 10, end_batch: 20
DEBUG 10-14 21:17:22 StaticCacheLen.py:90] static cache update layer_idx: 8, update seq_length to 512
DEBUG 10-14 21:17:22 lpmodule.py:368] update for kv cache 10-20 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-14 21:17:22 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:22 lpmodule.py:374] update past key value cost 0.003501 seconds
DEBUG 10-14 21:17:22 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 512, 128]), value shape torch.Size([10, 8, 512, 128])
DEBUG 10-14 21:17:22 lpmodule.py:227] layer idx 8
DEBUG 10-14 21:17:22 lpmodule.py:399] repeat qkv cost 0.012920 seconds
DEBUG 10-14 21:17:22 lpmodule.py:400] q shape torch.Size([10, 32, 512, 128]), k shape torch.Size([10, 32, 512, 128]), v shape torch.Size([10, 32, 512, 128])
DEBUG 10-14 21:17:22 lpllm.py:498] reset update attn
DEBUG 10-14 21:17:22 lpllm.py:1771] update state cost 2.5510787963867188e-05 s
DEBUG 10-14 21:17:22 lpllm.py:1740] restore layer func cost 0.0005803108215332031 s
DEBUG 10-14 21:17:22 lpllm.py:511] restore layer cost 0.0008847713470458984 s
DEBUG 10-14 21:17:22 lpllm.py:512] reset_next_layer_need: layer_attn_idx=9, layer_mlp_idx=8, j_loc=17
DEBUG 10-14 21:17:22 lpllm.py:1034] reset layer cost 0.0010061264038085938 s
DEBUG 10-14 21:17:22 lpllm.py:1035] have reset next layer layer_attn 9 layer_mlp 8 
DEBUG 10-14 21:17:22 lpllm.py:1041] j: 17 waiting the layer with layer_idx 9 before wait time 0.22413277626037598 s
INFO 10-14 21:17:22 client.py:117] confirm_model_loaded: Mixtral-8x7B, 8e165534-e4c4-4e25-8733-e60e3b9cbd74
DEBUG 10-14 21:17:22 lpmodule.py:433] dot attn cost 0.062392 seconds
DEBUG 10-14 21:17:22 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.0244901180267334 s
DEBUG 10-14 21:17:22 lpllm.py:2280] CPU attn cost 0.110711 seconds if batch True
DEBUG 10-14 21:17:22 lpllm.py:2289] deal attn result cost 0.000001 seconds
DEBUG 10-14 21:17:22 lpllm.py:2291] CPU compute cost 0.111185 seconds
DEBUG 10-14 21:17:22 lpllm.py:2309] free cost 0.000107 seconds
INFO 10-14 21:17:22 client.py:125] Model loaded
DEBUG 10-14 21:17:22 lpllm.py:1045] j: load cost 0.3072800636291504 s waiting cost 0.08312702178955078 s
DEBUG 10-14 21:17:22 lpllm.py:921] 
DEBUG 10-14 21:17:22 lpllm.py:921] decoder loop j: 18 cur_layer_idx: 9 layer_attn_idx: 9 layer_mlp_idx: 8
DEBUG 10-14 21:17:22 lpllm.py:930] start load next layer cur_layer_idx: 10
DEBUG 10-14 21:17:22 lpllm.py:1801] cuda memory handles: {1: <capsule object NULL at 0x7fdf3c6d9f80>} {1: b'\xe0\xaa29\x00\x00\x00\x00\xe8&\x1d\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\x86\x00\x00\x00\xff\x00\x00/\x00\x00\x00\x00\x00\x00\x00)\xd8\xd5\xc1\xa2\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-14 21:17:22 client.py:72] load_into_gpu: Mixtral-8x7B, 2a696d59-cc13-4d89-a366-f390589a8bc8
INFO 10-14 21:17:22 client.py:113] Model loaded: Mixtral-8x7B, 2a696d59-cc13-4d89-a366-f390589a8bc8
DEBUG 10-14 21:17:22 lpmodule.py:451] hidden_states.shape torch.Size([10, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:22 lpmodule.py:455] input_layernorm num_params: 1
DEBUG 10-14 21:17:22 lpmodule.py:457] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-14 21:17:22 lpmodule.py:498] kv_seq_len 512 past_key_value 0
DEBUG 10-14 21:17:22 lpmodule.py:504] inv_freq device cpu
DEBUG 10-14 21:17:22 lpmodule.py:505] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:22 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:22 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:22 lpmodule.py:505]         ...,
DEBUG 10-14 21:17:22 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:22 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:22 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-14 21:17:22 lpmodule.py:511] query_states device cuda:1
DEBUG 10-14 21:17:22 lpmodule.py:512] key_states device cuda:1
DEBUG 10-14 21:17:22 lpmodule.py:513] value_states device cuda:1
DEBUG 10-14 21:17:22 lpmodule.py:514] position_ids device cuda:1
DEBUG 10-14 21:17:22 lpmodule.py:515] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:22 lpmodule.py:516] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:22 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:22 lpllm.py:2262] GPU2CPU move cost 0.000796 seconds
DEBUG 10-14 21:17:22 lpmodule.py:340] layer_idx 9 attn layer_idx: 9
DEBUG 10-14 21:17:22 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-14 21:17:22 lpmodule.py:364] decoder_attn_batch update batch_dim 0-10
DEBUG 10-14 21:17:22 StaticCacheLen.py:84] static cache update layer_idx: 9, start_batch: 0, end_batch: 10
DEBUG 10-14 21:17:22 lpmodule.py:368] update for kv cache 0-10 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-14 21:17:22 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:22 lpmodule.py:374] update past key value cost 0.003972 seconds
DEBUG 10-14 21:17:22 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 512, 128]), value shape torch.Size([10, 8, 512, 128])
DEBUG 10-14 21:17:22 lpmodule.py:227] layer idx 8
DEBUG 10-14 21:17:22 lpmodule.py:399] repeat qkv cost 0.014270 seconds
DEBUG 10-14 21:17:22 lpmodule.py:400] q shape torch.Size([10, 32, 512, 128]), k shape torch.Size([10, 32, 512, 128]), v shape torch.Size([10, 32, 512, 128])
DEBUG 10-14 21:17:22 lpmodule.py:433] dot attn cost 0.063393 seconds
DEBUG 10-14 21:17:22 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.02611231803894043 s
DEBUG 10-14 21:17:22 lpllm.py:2280] CPU attn cost 0.119538 seconds if batch True
DEBUG 10-14 21:17:22 lpllm.py:2289] deal attn result cost 0.000002 seconds
DEBUG 10-14 21:17:22 lpllm.py:2291] CPU compute cost 0.120723 seconds
DEBUG 10-14 21:17:22 lpllm.py:2309] free cost 0.000108 seconds
DEBUG 10-14 21:17:22 lpllm.py:503] reset update experts
DEBUG 10-14 21:17:22 lpllm.py:1771] update state cost 2.09808349609375e-05 s
DEBUG 10-14 21:17:22 lpllm.py:1740] restore layer func cost 0.0011141300201416016 s
DEBUG 10-14 21:17:22 lpllm.py:511] restore layer cost 0.00135040283203125 s
DEBUG 10-14 21:17:22 lpllm.py:512] reset_next_layer_need: layer_attn_idx=9, layer_mlp_idx=9, j_loc=18
DEBUG 10-14 21:17:22 lpllm.py:1034] reset layer cost 0.0014464855194091797 s
DEBUG 10-14 21:17:22 lpllm.py:1035] have reset next layer layer_attn 9 layer_mlp 9 
DEBUG 10-14 21:17:22 lpllm.py:921] 
DEBUG 10-14 21:17:22 lpllm.py:921] decoder loop j: 19 cur_layer_idx: 9 layer_attn_idx: 9 layer_mlp_idx: 9
DEBUG 10-14 21:17:22 lpmodule.py:451] hidden_states.shape torch.Size([10, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:22 lpmodule.py:455] input_layernorm num_params: 1
DEBUG 10-14 21:17:22 lpmodule.py:457] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-14 21:17:22 lpmodule.py:498] kv_seq_len 512 past_key_value 0
DEBUG 10-14 21:17:22 lpmodule.py:504] inv_freq device cpu
DEBUG 10-14 21:17:22 lpmodule.py:505] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:22 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:22 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:22 lpmodule.py:505]         ...,
DEBUG 10-14 21:17:22 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:22 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:22 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-14 21:17:22 lpmodule.py:511] query_states device cuda:1
DEBUG 10-14 21:17:22 lpmodule.py:512] key_states device cuda:1
DEBUG 10-14 21:17:22 lpmodule.py:513] value_states device cuda:1
DEBUG 10-14 21:17:22 lpmodule.py:514] position_ids device cuda:1
DEBUG 10-14 21:17:22 lpmodule.py:515] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:22 lpmodule.py:516] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:22 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:22 lpllm.py:2262] GPU2CPU move cost 0.000302 seconds
DEBUG 10-14 21:17:22 lpmodule.py:340] layer_idx 9 attn layer_idx: 9
DEBUG 10-14 21:17:22 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-14 21:17:22 lpmodule.py:364] decoder_attn_batch update batch_dim 10-20
DEBUG 10-14 21:17:22 StaticCacheLen.py:84] static cache update layer_idx: 9, start_batch: 10, end_batch: 20
DEBUG 10-14 21:17:22 StaticCacheLen.py:90] static cache update layer_idx: 9, update seq_length to 512
DEBUG 10-14 21:17:22 lpmodule.py:368] update for kv cache 10-20 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-14 21:17:22 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:22 lpmodule.py:374] update past key value cost 0.003483 seconds
DEBUG 10-14 21:17:22 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 512, 128]), value shape torch.Size([10, 8, 512, 128])
DEBUG 10-14 21:17:22 lpmodule.py:227] layer idx 9
DEBUG 10-14 21:17:22 lpmodule.py:399] repeat qkv cost 0.013460 seconds
DEBUG 10-14 21:17:22 lpmodule.py:400] q shape torch.Size([10, 32, 512, 128]), k shape torch.Size([10, 32, 512, 128]), v shape torch.Size([10, 32, 512, 128])
DEBUG 10-14 21:17:23 lpmodule.py:433] dot attn cost 0.064633 seconds
DEBUG 10-14 21:17:23 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.024578094482421875 s
DEBUG 10-14 21:17:23 lpllm.py:2280] CPU attn cost 0.114148 seconds if batch True
DEBUG 10-14 21:17:23 lpllm.py:2289] deal attn result cost 0.000001 seconds
DEBUG 10-14 21:17:23 lpllm.py:2291] CPU compute cost 0.114710 seconds
DEBUG 10-14 21:17:23 lpllm.py:2309] free cost 0.000110 seconds
DEBUG 10-14 21:17:23 lpllm.py:498] reset update attn
DEBUG 10-14 21:17:23 lpllm.py:1771] update state cost 2.7894973754882812e-05 s
DEBUG 10-14 21:17:23 lpllm.py:1740] restore layer func cost 0.0006780624389648438 s
DEBUG 10-14 21:17:23 lpllm.py:511] restore layer cost 0.0009708404541015625 s
DEBUG 10-14 21:17:23 lpllm.py:512] reset_next_layer_need: layer_attn_idx=10, layer_mlp_idx=9, j_loc=19
DEBUG 10-14 21:17:23 lpllm.py:1034] reset layer cost 0.0011043548583984375 s
DEBUG 10-14 21:17:23 lpllm.py:1035] have reset next layer layer_attn 10 layer_mlp 9 
DEBUG 10-14 21:17:23 lpllm.py:1041] j: 19 waiting the layer with layer_idx 10 before wait time 0.30197834968566895 s
INFO 10-14 21:17:23 client.py:117] confirm_model_loaded: Mixtral-8x7B, 2a696d59-cc13-4d89-a366-f390589a8bc8
INFO 10-14 21:17:23 client.py:125] Model loaded
DEBUG 10-14 21:17:23 lpllm.py:1045] j: load cost 0.30350661277770996 s waiting cost 0.0014944076538085938 s
DEBUG 10-14 21:17:23 lpllm.py:921] 
DEBUG 10-14 21:17:23 lpllm.py:921] decoder loop j: 20 cur_layer_idx: 10 layer_attn_idx: 10 layer_mlp_idx: 9
DEBUG 10-14 21:17:23 lpllm.py:930] start load next layer cur_layer_idx: 11
DEBUG 10-14 21:17:23 lpllm.py:1801] cuda memory handles: {1: <capsule object NULL at 0x7fdf3c6da010>} {1: b'\xe0\xaa29\x00\x00\x00\x00\xe8&\x1d\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\x86\x00\x00\x00\xff\x00\x000\x00\x00\x00\x00\x00\x00\x00)\xd8\xd5\xc1\xa4\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-14 21:17:23 client.py:72] load_into_gpu: Mixtral-8x7B, 1044cda0-9d72-45a9-bdbf-958f48f8bde8
INFO 10-14 21:17:23 client.py:113] Model loaded: Mixtral-8x7B, 1044cda0-9d72-45a9-bdbf-958f48f8bde8
DEBUG 10-14 21:17:23 lpmodule.py:451] hidden_states.shape torch.Size([10, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:23 lpmodule.py:455] input_layernorm num_params: 1
DEBUG 10-14 21:17:23 lpmodule.py:457] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-14 21:17:23 lpmodule.py:498] kv_seq_len 512 past_key_value 0
DEBUG 10-14 21:17:23 lpmodule.py:504] inv_freq device cpu
DEBUG 10-14 21:17:23 lpmodule.py:505] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:23 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:23 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:23 lpmodule.py:505]         ...,
DEBUG 10-14 21:17:23 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:23 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:23 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-14 21:17:23 lpmodule.py:511] query_states device cuda:1
DEBUG 10-14 21:17:23 lpmodule.py:512] key_states device cuda:1
DEBUG 10-14 21:17:23 lpmodule.py:513] value_states device cuda:1
DEBUG 10-14 21:17:23 lpmodule.py:514] position_ids device cuda:1
DEBUG 10-14 21:17:23 lpmodule.py:515] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:23 lpmodule.py:516] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:23 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:23 lpllm.py:2262] GPU2CPU move cost 0.000531 seconds
DEBUG 10-14 21:17:23 lpmodule.py:340] layer_idx 10 attn layer_idx: 10
DEBUG 10-14 21:17:23 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-14 21:17:23 lpmodule.py:364] decoder_attn_batch update batch_dim 0-10
DEBUG 10-14 21:17:23 StaticCacheLen.py:84] static cache update layer_idx: 10, start_batch: 0, end_batch: 10
DEBUG 10-14 21:17:23 lpmodule.py:368] update for kv cache 0-10 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-14 21:17:23 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:23 lpmodule.py:374] update past key value cost 0.003584 seconds
DEBUG 10-14 21:17:23 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 512, 128]), value shape torch.Size([10, 8, 512, 128])
DEBUG 10-14 21:17:23 lpmodule.py:227] layer idx 9
DEBUG 10-14 21:17:23 lpmodule.py:399] repeat qkv cost 0.015421 seconds
DEBUG 10-14 21:17:23 lpmodule.py:400] q shape torch.Size([10, 32, 512, 128]), k shape torch.Size([10, 32, 512, 128]), v shape torch.Size([10, 32, 512, 128])
DEBUG 10-14 21:17:23 lpllm.py:503] reset update experts
DEBUG 10-14 21:17:23 lpllm.py:1771] update state cost 3.743171691894531e-05 s
DEBUG 10-14 21:17:23 lpllm.py:1740] restore layer func cost 0.003208160400390625 s
DEBUG 10-14 21:17:23 lpllm.py:511] restore layer cost 0.003941059112548828 s
DEBUG 10-14 21:17:23 lpllm.py:512] reset_next_layer_need: layer_attn_idx=10, layer_mlp_idx=10, j_loc=20
DEBUG 10-14 21:17:23 lpllm.py:1034] reset layer cost 0.004204273223876953 s
DEBUG 10-14 21:17:23 lpllm.py:1035] have reset next layer layer_attn 10 layer_mlp 10 
DEBUG 10-14 21:17:23 lpllm.py:921] 
DEBUG 10-14 21:17:23 lpllm.py:921] decoder loop j: 21 cur_layer_idx: 10 layer_attn_idx: 10 layer_mlp_idx: 10
DEBUG 10-14 21:17:23 lpmodule.py:451] hidden_states.shape torch.Size([10, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:23 lpmodule.py:455] input_layernorm num_params: 1
DEBUG 10-14 21:17:23 lpmodule.py:457] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-14 21:17:23 lpmodule.py:433] dot attn cost 0.062150 seconds
DEBUG 10-14 21:17:23 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-14 21:17:23 lpmodule.py:498] kv_seq_len 512 past_key_value 0
DEBUG 10-14 21:17:23 lpmodule.py:504] inv_freq device cpu
DEBUG 10-14 21:17:23 lpmodule.py:505] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:23 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:23 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:23 lpmodule.py:505]         ...,
DEBUG 10-14 21:17:23 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:23 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:23 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511]])
time cost move to cuda:1 0.024224042892456055 s
DEBUG 10-14 21:17:23 lpmodule.py:511] query_states device cuda:1
DEBUG 10-14 21:17:23 lpmodule.py:512] key_states device cuda:1
DEBUG 10-14 21:17:23 lpmodule.py:513] value_states device cuda:1
DEBUG 10-14 21:17:23 lpmodule.py:514] position_ids device cuda:1
DEBUG 10-14 21:17:23 lpmodule.py:515] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:23 lpmodule.py:516] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:23 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:23 lpllm.py:2280] CPU attn cost 0.113493 seconds if batch True
DEBUG 10-14 21:17:23 lpllm.py:2289] deal attn result cost 0.000002 seconds
DEBUG 10-14 21:17:23 lpllm.py:2291] CPU compute cost 0.114275 seconds
DEBUG 10-14 21:17:23 lpllm.py:2309] free cost 0.000086 seconds
DEBUG 10-14 21:17:23 lpllm.py:2262] GPU2CPU move cost 0.000273 seconds
DEBUG 10-14 21:17:23 lpmodule.py:340] layer_idx 10 attn layer_idx: 10
DEBUG 10-14 21:17:23 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-14 21:17:23 lpmodule.py:364] decoder_attn_batch update batch_dim 10-20
DEBUG 10-14 21:17:23 StaticCacheLen.py:84] static cache update layer_idx: 10, start_batch: 10, end_batch: 20
DEBUG 10-14 21:17:23 StaticCacheLen.py:90] static cache update layer_idx: 10, update seq_length to 512
DEBUG 10-14 21:17:23 lpmodule.py:368] update for kv cache 10-20 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-14 21:17:23 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:23 lpmodule.py:374] update past key value cost 0.002737 seconds
DEBUG 10-14 21:17:23 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 512, 128]), value shape torch.Size([10, 8, 512, 128])
DEBUG 10-14 21:17:23 lpmodule.py:227] layer idx 10
DEBUG 10-14 21:17:23 lpmodule.py:399] repeat qkv cost 0.012890 seconds
DEBUG 10-14 21:17:23 lpmodule.py:400] q shape torch.Size([10, 32, 512, 128]), k shape torch.Size([10, 32, 512, 128]), v shape torch.Size([10, 32, 512, 128])
DEBUG 10-14 21:17:23 lpllm.py:498] reset update attn
DEBUG 10-14 21:17:23 lpllm.py:1771] update state cost 2.4557113647460938e-05 s
DEBUG 10-14 21:17:23 lpllm.py:1740] restore layer func cost 0.0005085468292236328 s
DEBUG 10-14 21:17:23 lpllm.py:511] restore layer cost 0.0007982254028320312 s
DEBUG 10-14 21:17:23 lpllm.py:512] reset_next_layer_need: layer_attn_idx=11, layer_mlp_idx=10, j_loc=21
DEBUG 10-14 21:17:23 lpllm.py:1034] reset layer cost 0.0009069442749023438 s
DEBUG 10-14 21:17:23 lpllm.py:1035] have reset next layer layer_attn 11 layer_mlp 10 
DEBUG 10-14 21:17:23 lpllm.py:1041] j: 21 waiting the layer with layer_idx 11 before wait time 0.21138215065002441 s
INFO 10-14 21:17:23 client.py:117] confirm_model_loaded: Mixtral-8x7B, 1044cda0-9d72-45a9-bdbf-958f48f8bde8
DEBUG 10-14 21:17:23 lpmodule.py:433] dot attn cost 0.064834 seconds
DEBUG 10-14 21:17:23 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.024099111557006836 s
DEBUG 10-14 21:17:23 lpllm.py:2280] CPU attn cost 0.112184 seconds if batch True
DEBUG 10-14 21:17:23 lpllm.py:2289] deal attn result cost 0.000001 seconds
DEBUG 10-14 21:17:23 lpllm.py:2291] CPU compute cost 0.112661 seconds
DEBUG 10-14 21:17:23 lpllm.py:2309] free cost 0.000087 seconds
INFO 10-14 21:17:23 client.py:125] Model loaded
DEBUG 10-14 21:17:23 lpllm.py:1045] j: load cost 0.3154428005218506 s waiting cost 0.10404324531555176 s
DEBUG 10-14 21:17:23 lpllm.py:921] 
DEBUG 10-14 21:17:23 lpllm.py:921] decoder loop j: 22 cur_layer_idx: 11 layer_attn_idx: 11 layer_mlp_idx: 10
DEBUG 10-14 21:17:23 lpllm.py:930] start load next layer cur_layer_idx: 12
DEBUG 10-14 21:17:23 lpllm.py:1801] cuda memory handles: {1: <capsule object NULL at 0x7fdf3c6d9ec0>} {1: b'\xe0\xaa29\x00\x00\x00\x00\xe8&\x1d\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\x86\x00\x00\x00\xff\x00\x00.\x00\x00\x00\x00\x00\x00\x00)\xd8\xd5\xc1\xa0\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-14 21:17:23 client.py:72] load_into_gpu: Mixtral-8x7B, a01d6ccd-4117-442b-9ecb-9d237a23728a
INFO 10-14 21:17:23 client.py:113] Model loaded: Mixtral-8x7B, a01d6ccd-4117-442b-9ecb-9d237a23728a
DEBUG 10-14 21:17:23 lpmodule.py:451] hidden_states.shape torch.Size([10, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:23 lpmodule.py:455] input_layernorm num_params: 1
DEBUG 10-14 21:17:23 lpmodule.py:457] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-14 21:17:23 lpmodule.py:498] kv_seq_len 512 past_key_value 0
DEBUG 10-14 21:17:23 lpmodule.py:504] inv_freq device cpu
DEBUG 10-14 21:17:23 lpmodule.py:505] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:23 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:23 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:23 lpmodule.py:505]         ...,
DEBUG 10-14 21:17:23 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:23 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:23 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-14 21:17:23 lpmodule.py:511] query_states device cuda:1
DEBUG 10-14 21:17:23 lpmodule.py:512] key_states device cuda:1
DEBUG 10-14 21:17:23 lpmodule.py:513] value_states device cuda:1
DEBUG 10-14 21:17:23 lpmodule.py:514] position_ids device cuda:1
DEBUG 10-14 21:17:23 lpmodule.py:515] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:23 lpmodule.py:516] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:23 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:23 lpllm.py:2262] GPU2CPU move cost 0.000715 seconds
DEBUG 10-14 21:17:23 lpmodule.py:340] layer_idx 11 attn layer_idx: 11
DEBUG 10-14 21:17:23 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-14 21:17:23 lpmodule.py:364] decoder_attn_batch update batch_dim 0-10
DEBUG 10-14 21:17:23 StaticCacheLen.py:84] static cache update layer_idx: 11, start_batch: 0, end_batch: 10
DEBUG 10-14 21:17:23 lpmodule.py:368] update for kv cache 0-10 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-14 21:17:23 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:23 lpmodule.py:374] update past key value cost 0.003598 seconds
DEBUG 10-14 21:17:23 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 512, 128]), value shape torch.Size([10, 8, 512, 128])
DEBUG 10-14 21:17:23 lpmodule.py:227] layer idx 10
DEBUG 10-14 21:17:23 lpmodule.py:399] repeat qkv cost 0.013430 seconds
DEBUG 10-14 21:17:23 lpmodule.py:400] q shape torch.Size([10, 32, 512, 128]), k shape torch.Size([10, 32, 512, 128]), v shape torch.Size([10, 32, 512, 128])
DEBUG 10-14 21:17:23 lpllm.py:503] reset update experts
DEBUG 10-14 21:17:23 lpllm.py:1771] update state cost 4.00543212890625e-05 s
DEBUG 10-14 21:17:23 lpllm.py:1740] restore layer func cost 0.0031452178955078125 s
DEBUG 10-14 21:17:23 lpllm.py:511] restore layer cost 0.0037741661071777344 s
DEBUG 10-14 21:17:23 lpllm.py:512] reset_next_layer_need: layer_attn_idx=11, layer_mlp_idx=11, j_loc=22
DEBUG 10-14 21:17:23 lpllm.py:1034] reset layer cost 0.004074811935424805 s
DEBUG 10-14 21:17:23 lpllm.py:1035] have reset next layer layer_attn 11 layer_mlp 11 
DEBUG 10-14 21:17:23 lpllm.py:921] 
DEBUG 10-14 21:17:23 lpllm.py:921] decoder loop j: 23 cur_layer_idx: 11 layer_attn_idx: 11 layer_mlp_idx: 11
DEBUG 10-14 21:17:23 lpmodule.py:451] hidden_states.shape torch.Size([10, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:23 lpmodule.py:455] input_layernorm num_params: 1
DEBUG 10-14 21:17:23 lpmodule.py:457] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-14 21:17:23 lpmodule.py:498] kv_seq_len 512 past_key_value 0
DEBUG 10-14 21:17:23 lpmodule.py:504] inv_freq device cpu
DEBUG 10-14 21:17:23 lpmodule.py:433] dot attn cost 0.065536 seconds
DEBUG 10-14 21:17:23 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-14 21:17:23 lpmodule.py:505] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:23 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:23 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:23 lpmodule.py:505]         ...,
DEBUG 10-14 21:17:23 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:23 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:23 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-14 21:17:23 lpmodule.py:511] query_states device cuda:1
DEBUG 10-14 21:17:23 lpmodule.py:512] key_states device cuda:1
DEBUG 10-14 21:17:23 lpmodule.py:513] value_states device cuda:1
DEBUG 10-14 21:17:23 lpmodule.py:514] position_ids device cuda:1
DEBUG 10-14 21:17:23 lpmodule.py:515] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:23 lpmodule.py:516] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:23 lpllm.py:2081] waiting out_queue
time cost move to cuda:1 0.02485370635986328 s
DEBUG 10-14 21:17:23 lpllm.py:2280] CPU attn cost 0.114447 seconds if batch True
DEBUG 10-14 21:17:23 lpllm.py:2289] deal attn result cost 0.000002 seconds
DEBUG 10-14 21:17:23 lpllm.py:2291] CPU compute cost 0.115502 seconds
DEBUG 10-14 21:17:23 lpllm.py:2309] free cost 0.000238 seconds
DEBUG 10-14 21:17:23 lpllm.py:2262] GPU2CPU move cost 0.000299 seconds
DEBUG 10-14 21:17:23 lpmodule.py:340] layer_idx 11 attn layer_idx: 11
DEBUG 10-14 21:17:23 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-14 21:17:23 lpmodule.py:364] decoder_attn_batch update batch_dim 10-20
DEBUG 10-14 21:17:23 StaticCacheLen.py:84] static cache update layer_idx: 11, start_batch: 10, end_batch: 20
DEBUG 10-14 21:17:23 StaticCacheLen.py:90] static cache update layer_idx: 11, update seq_length to 512
DEBUG 10-14 21:17:23 lpmodule.py:368] update for kv cache 10-20 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-14 21:17:23 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:23 lpmodule.py:374] update past key value cost 0.003219 seconds
DEBUG 10-14 21:17:23 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 512, 128]), value shape torch.Size([10, 8, 512, 128])
DEBUG 10-14 21:17:23 lpmodule.py:227] layer idx 11
DEBUG 10-14 21:17:23 lpmodule.py:399] repeat qkv cost 0.012401 seconds
DEBUG 10-14 21:17:23 lpmodule.py:400] q shape torch.Size([10, 32, 512, 128]), k shape torch.Size([10, 32, 512, 128]), v shape torch.Size([10, 32, 512, 128])
DEBUG 10-14 21:17:23 lpllm.py:498] reset update attn
DEBUG 10-14 21:17:23 lpllm.py:1771] update state cost 2.47955322265625e-05 s
DEBUG 10-14 21:17:23 lpllm.py:1740] restore layer func cost 0.0005552768707275391 s
DEBUG 10-14 21:17:23 lpllm.py:511] restore layer cost 0.0008444786071777344 s
DEBUG 10-14 21:17:23 lpllm.py:512] reset_next_layer_need: layer_attn_idx=12, layer_mlp_idx=11, j_loc=23
DEBUG 10-14 21:17:23 lpllm.py:1034] reset layer cost 0.0009546279907226562 s
DEBUG 10-14 21:17:23 lpllm.py:1035] have reset next layer layer_attn 12 layer_mlp 11 
DEBUG 10-14 21:17:23 lpllm.py:1041] j: 23 waiting the layer with layer_idx 12 before wait time 0.22180962562561035 s
INFO 10-14 21:17:23 client.py:117] confirm_model_loaded: Mixtral-8x7B, a01d6ccd-4117-442b-9ecb-9d237a23728a
DEBUG 10-14 21:17:23 lpmodule.py:433] dot attn cost 0.066212 seconds
DEBUG 10-14 21:17:23 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.023850679397583008 s
DEBUG 10-14 21:17:23 lpllm.py:2280] CPU attn cost 0.112669 seconds if batch True
DEBUG 10-14 21:17:23 lpllm.py:2289] deal attn result cost 0.000003 seconds
DEBUG 10-14 21:17:23 lpllm.py:2291] CPU compute cost 0.113201 seconds
DEBUG 10-14 21:17:23 lpllm.py:2309] free cost 0.000095 seconds
INFO 10-14 21:17:23 client.py:125] Model loaded
DEBUG 10-14 21:17:23 lpllm.py:1045] j: load cost 0.36513805389404297 s waiting cost 0.1433100700378418 s
DEBUG 10-14 21:17:23 lpllm.py:921] 
DEBUG 10-14 21:17:23 lpllm.py:921] decoder loop j: 24 cur_layer_idx: 12 layer_attn_idx: 12 layer_mlp_idx: 11
DEBUG 10-14 21:17:23 lpllm.py:930] start load next layer cur_layer_idx: 13
DEBUG 10-14 21:17:23 lpllm.py:1801] cuda memory handles: {1: <capsule object NULL at 0x7fdf3c6d9f80>} {1: b'\xe0\xaa29\x00\x00\x00\x00\xe8&\x1d\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\x86\x00\x00\x00\xff\x00\x00/\x00\x00\x00\x00\x00\x00\x00)\xd8\xd5\xc1\xa2\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-14 21:17:23 client.py:72] load_into_gpu: Mixtral-8x7B, fb0632ac-5ad9-4cf2-8e15-79a6df917168
INFO 10-14 21:17:23 client.py:113] Model loaded: Mixtral-8x7B, fb0632ac-5ad9-4cf2-8e15-79a6df917168
DEBUG 10-14 21:17:23 lpmodule.py:451] hidden_states.shape torch.Size([10, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:23 lpmodule.py:455] input_layernorm num_params: 1
DEBUG 10-14 21:17:23 lpmodule.py:457] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-14 21:17:23 lpmodule.py:498] kv_seq_len 512 past_key_value 0
DEBUG 10-14 21:17:23 lpmodule.py:504] inv_freq device cpu
DEBUG 10-14 21:17:23 lpmodule.py:505] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:23 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:23 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:23 lpmodule.py:505]         ...,
DEBUG 10-14 21:17:23 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:23 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:23 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-14 21:17:23 lpmodule.py:511] query_states device cuda:1
DEBUG 10-14 21:17:23 lpmodule.py:512] key_states device cuda:1
DEBUG 10-14 21:17:23 lpmodule.py:513] value_states device cuda:1
DEBUG 10-14 21:17:23 lpmodule.py:514] position_ids device cuda:1
DEBUG 10-14 21:17:23 lpmodule.py:515] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:23 lpmodule.py:516] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:23 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:23 lpllm.py:2262] GPU2CPU move cost 0.000473 seconds
DEBUG 10-14 21:17:23 lpmodule.py:340] layer_idx 12 attn layer_idx: 12
DEBUG 10-14 21:17:23 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-14 21:17:23 lpmodule.py:364] decoder_attn_batch update batch_dim 0-10
DEBUG 10-14 21:17:23 StaticCacheLen.py:84] static cache update layer_idx: 12, start_batch: 0, end_batch: 10
DEBUG 10-14 21:17:23 lpmodule.py:368] update for kv cache 0-10 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-14 21:17:23 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:23 lpmodule.py:374] update past key value cost 0.005175 seconds
DEBUG 10-14 21:17:23 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 512, 128]), value shape torch.Size([10, 8, 512, 128])
DEBUG 10-14 21:17:23 lpmodule.py:227] layer idx 11
DEBUG 10-14 21:17:23 lpmodule.py:399] repeat qkv cost 0.012102 seconds
DEBUG 10-14 21:17:23 lpmodule.py:400] q shape torch.Size([10, 32, 512, 128]), k shape torch.Size([10, 32, 512, 128]), v shape torch.Size([10, 32, 512, 128])
DEBUG 10-14 21:17:23 lpllm.py:503] reset update experts
DEBUG 10-14 21:17:23 lpllm.py:1771] update state cost 4.0531158447265625e-05 s
DEBUG 10-14 21:17:23 lpllm.py:1740] restore layer func cost 0.0014090538024902344 s
DEBUG 10-14 21:17:23 lpllm.py:511] restore layer cost 0.0018470287322998047 s
DEBUG 10-14 21:17:23 lpllm.py:512] reset_next_layer_need: layer_attn_idx=12, layer_mlp_idx=12, j_loc=24
DEBUG 10-14 21:17:23 lpllm.py:1034] reset layer cost 0.0019812583923339844 s
DEBUG 10-14 21:17:23 lpllm.py:1035] have reset next layer layer_attn 12 layer_mlp 12 
DEBUG 10-14 21:17:23 lpllm.py:921] 
DEBUG 10-14 21:17:23 lpllm.py:921] decoder loop j: 25 cur_layer_idx: 12 layer_attn_idx: 12 layer_mlp_idx: 12
DEBUG 10-14 21:17:23 lpmodule.py:451] hidden_states.shape torch.Size([10, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:23 lpmodule.py:455] input_layernorm num_params: 1
DEBUG 10-14 21:17:23 lpmodule.py:457] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-14 21:17:23 lpmodule.py:433] dot attn cost 0.062525 seconds
DEBUG 10-14 21:17:23 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-14 21:17:23 lpmodule.py:498] kv_seq_len 512 past_key_value 0
DEBUG 10-14 21:17:23 lpmodule.py:504] inv_freq device cpu
DEBUG 10-14 21:17:23 lpmodule.py:505] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:23 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:23 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:23 lpmodule.py:505]         ...,
DEBUG 10-14 21:17:23 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:23 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:23 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-14 21:17:23 lpmodule.py:511] query_states device cuda:1
DEBUG 10-14 21:17:23 lpmodule.py:512] key_states device cuda:1
DEBUG 10-14 21:17:23 lpmodule.py:513] value_states device cuda:1
DEBUG 10-14 21:17:23 lpmodule.py:514] position_ids device cuda:1
DEBUG 10-14 21:17:23 lpmodule.py:515] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:23 lpmodule.py:516] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:23 lpllm.py:2081] waiting out_queue
time cost move to cuda:1 0.00933384895324707 s
DEBUG 10-14 21:17:23 lpllm.py:2280] CPU attn cost 0.097014 seconds if batch True
DEBUG 10-14 21:17:23 lpllm.py:2289] deal attn result cost 0.000003 seconds
DEBUG 10-14 21:17:23 lpllm.py:2291] CPU compute cost 0.097768 seconds
DEBUG 10-14 21:17:23 lpllm.py:2309] free cost 0.000110 seconds
DEBUG 10-14 21:17:23 lpllm.py:2262] GPU2CPU move cost 0.000321 seconds
DEBUG 10-14 21:17:23 lpmodule.py:340] layer_idx 12 attn layer_idx: 12
DEBUG 10-14 21:17:23 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-14 21:17:23 lpmodule.py:364] decoder_attn_batch update batch_dim 10-20
DEBUG 10-14 21:17:23 StaticCacheLen.py:84] static cache update layer_idx: 12, start_batch: 10, end_batch: 20
DEBUG 10-14 21:17:23 StaticCacheLen.py:90] static cache update layer_idx: 12, update seq_length to 512
DEBUG 10-14 21:17:23 lpmodule.py:368] update for kv cache 10-20 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-14 21:17:23 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:23 lpmodule.py:374] update past key value cost 0.003067 seconds
DEBUG 10-14 21:17:23 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 512, 128]), value shape torch.Size([10, 8, 512, 128])
DEBUG 10-14 21:17:23 lpmodule.py:227] layer idx 12
DEBUG 10-14 21:17:23 lpmodule.py:399] repeat qkv cost 0.008821 seconds
DEBUG 10-14 21:17:23 lpmodule.py:400] q shape torch.Size([10, 32, 512, 128]), k shape torch.Size([10, 32, 512, 128]), v shape torch.Size([10, 32, 512, 128])
DEBUG 10-14 21:17:23 lpmodule.py:433] dot attn cost 0.064221 seconds
DEBUG 10-14 21:17:23 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.025841236114501953 s
DEBUG 10-14 21:17:23 lpllm.py:2280] CPU attn cost 0.112399 seconds if batch True
DEBUG 10-14 21:17:23 lpllm.py:2289] deal attn result cost 0.000003 seconds
DEBUG 10-14 21:17:23 lpllm.py:2291] CPU compute cost 0.113050 seconds
DEBUG 10-14 21:17:23 lpllm.py:2309] free cost 0.000148 seconds
DEBUG 10-14 21:17:24 lpllm.py:498] reset update attn
DEBUG 10-14 21:17:24 lpllm.py:1771] update state cost 2.4557113647460938e-05 s
DEBUG 10-14 21:17:24 lpllm.py:1740] restore layer func cost 0.0005462169647216797 s
DEBUG 10-14 21:17:24 lpllm.py:511] restore layer cost 0.0008625984191894531 s
DEBUG 10-14 21:17:24 lpllm.py:512] reset_next_layer_need: layer_attn_idx=13, layer_mlp_idx=12, j_loc=25
DEBUG 10-14 21:17:24 lpllm.py:1034] reset layer cost 0.0009667873382568359 s
DEBUG 10-14 21:17:24 lpllm.py:1035] have reset next layer layer_attn 13 layer_mlp 12 
DEBUG 10-14 21:17:24 lpllm.py:1041] j: 25 waiting the layer with layer_idx 13 before wait time 0.2758142948150635 s
INFO 10-14 21:17:24 client.py:117] confirm_model_loaded: Mixtral-8x7B, fb0632ac-5ad9-4cf2-8e15-79a6df917168
INFO 10-14 21:17:24 client.py:125] Model loaded
DEBUG 10-14 21:17:24 lpllm.py:1045] j: load cost 0.4422750473022461 s waiting cost 0.1664435863494873 s
DEBUG 10-14 21:17:24 lpllm.py:921] 
DEBUG 10-14 21:17:24 lpllm.py:921] decoder loop j: 26 cur_layer_idx: 13 layer_attn_idx: 13 layer_mlp_idx: 12
DEBUG 10-14 21:17:24 lpllm.py:930] start load next layer cur_layer_idx: 14
DEBUG 10-14 21:17:24 lpllm.py:1801] cuda memory handles: {1: <capsule object NULL at 0x7fdf3c6da010>} {1: b'\xe0\xaa29\x00\x00\x00\x00\xe8&\x1d\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\x86\x00\x00\x00\xff\x00\x000\x00\x00\x00\x00\x00\x00\x00)\xd8\xd5\xc1\xa4\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-14 21:17:24 client.py:72] load_into_gpu: Mixtral-8x7B, 121add32-f795-40a3-8f88-abf17e7c0871
INFO 10-14 21:17:24 client.py:113] Model loaded: Mixtral-8x7B, 121add32-f795-40a3-8f88-abf17e7c0871
DEBUG 10-14 21:17:24 lpmodule.py:451] hidden_states.shape torch.Size([10, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:24 lpmodule.py:455] input_layernorm num_params: 1
DEBUG 10-14 21:17:24 lpmodule.py:457] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-14 21:17:24 lpmodule.py:498] kv_seq_len 512 past_key_value 0
DEBUG 10-14 21:17:24 lpmodule.py:504] inv_freq device cpu
DEBUG 10-14 21:17:24 lpmodule.py:505] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:24 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:24 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:24 lpmodule.py:505]         ...,
DEBUG 10-14 21:17:24 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:24 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:24 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-14 21:17:24 lpmodule.py:511] query_states device cuda:1
DEBUG 10-14 21:17:24 lpmodule.py:512] key_states device cuda:1
DEBUG 10-14 21:17:24 lpmodule.py:513] value_states device cuda:1
DEBUG 10-14 21:17:24 lpmodule.py:514] position_ids device cuda:1
DEBUG 10-14 21:17:24 lpmodule.py:515] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:24 lpmodule.py:516] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:24 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:24 lpllm.py:2262] GPU2CPU move cost 0.000914 seconds
DEBUG 10-14 21:17:24 lpmodule.py:340] layer_idx 13 attn layer_idx: 13
DEBUG 10-14 21:17:24 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-14 21:17:24 lpmodule.py:364] decoder_attn_batch update batch_dim 0-10
DEBUG 10-14 21:17:24 StaticCacheLen.py:84] static cache update layer_idx: 13, start_batch: 0, end_batch: 10
DEBUG 10-14 21:17:24 lpmodule.py:368] update for kv cache 0-10 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-14 21:17:24 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:24 lpmodule.py:374] update past key value cost 0.004008 seconds
DEBUG 10-14 21:17:24 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 512, 128]), value shape torch.Size([10, 8, 512, 128])
DEBUG 10-14 21:17:24 lpmodule.py:227] layer idx 12
DEBUG 10-14 21:17:24 lpmodule.py:399] repeat qkv cost 0.017628 seconds
DEBUG 10-14 21:17:24 lpmodule.py:400] q shape torch.Size([10, 32, 512, 128]), k shape torch.Size([10, 32, 512, 128]), v shape torch.Size([10, 32, 512, 128])
DEBUG 10-14 21:17:24 lpllm.py:503] reset update experts
DEBUG 10-14 21:17:24 lpllm.py:1771] update state cost 3.790855407714844e-05 s
DEBUG 10-14 21:17:24 lpllm.py:1740] restore layer func cost 0.0030059814453125 s
DEBUG 10-14 21:17:24 lpllm.py:511] restore layer cost 0.0035943984985351562 s
DEBUG 10-14 21:17:24 lpllm.py:512] reset_next_layer_need: layer_attn_idx=13, layer_mlp_idx=13, j_loc=26
DEBUG 10-14 21:17:24 lpllm.py:1034] reset layer cost 0.003773212432861328 s
DEBUG 10-14 21:17:24 lpllm.py:1035] have reset next layer layer_attn 13 layer_mlp 13 
DEBUG 10-14 21:17:24 lpllm.py:921] 
DEBUG 10-14 21:17:24 lpllm.py:921] decoder loop j: 27 cur_layer_idx: 13 layer_attn_idx: 13 layer_mlp_idx: 13
DEBUG 10-14 21:17:24 lpmodule.py:451] hidden_states.shape torch.Size([10, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:24 lpmodule.py:455] input_layernorm num_params: 1
DEBUG 10-14 21:17:24 lpmodule.py:457] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-14 21:17:24 lpmodule.py:498] kv_seq_len 512 past_key_value 0
DEBUG 10-14 21:17:24 lpmodule.py:504] inv_freq device cpu
DEBUG 10-14 21:17:24 lpmodule.py:505] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:24 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:24 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:24 lpmodule.py:505]         ...,
DEBUG 10-14 21:17:24 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:24 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:24 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-14 21:17:24 lpmodule.py:511] query_states device cuda:1
DEBUG 10-14 21:17:24 lpmodule.py:512] key_states device cuda:1
DEBUG 10-14 21:17:24 lpmodule.py:513] value_states device cuda:1
DEBUG 10-14 21:17:24 lpmodule.py:514] position_ids device cuda:1
DEBUG 10-14 21:17:24 lpmodule.py:515] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:24 lpmodule.py:516] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:24 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:24 lpmodule.py:433] dot attn cost 0.062527 seconds
DEBUG 10-14 21:17:24 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.024134397506713867 s
DEBUG 10-14 21:17:24 lpllm.py:2280] CPU attn cost 0.116436 seconds if batch True
DEBUG 10-14 21:17:24 lpllm.py:2289] deal attn result cost 0.000002 seconds
DEBUG 10-14 21:17:24 lpllm.py:2291] CPU compute cost 0.117747 seconds
DEBUG 10-14 21:17:24 lpllm.py:2309] free cost 0.000114 seconds
DEBUG 10-14 21:17:24 lpllm.py:2262] GPU2CPU move cost 0.000345 seconds
DEBUG 10-14 21:17:24 lpmodule.py:340] layer_idx 13 attn layer_idx: 13
DEBUG 10-14 21:17:24 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-14 21:17:24 lpmodule.py:364] decoder_attn_batch update batch_dim 10-20
DEBUG 10-14 21:17:24 StaticCacheLen.py:84] static cache update layer_idx: 13, start_batch: 10, end_batch: 20
DEBUG 10-14 21:17:24 StaticCacheLen.py:90] static cache update layer_idx: 13, update seq_length to 512
DEBUG 10-14 21:17:24 lpmodule.py:368] update for kv cache 10-20 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-14 21:17:24 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:24 lpmodule.py:374] update past key value cost 0.004666 seconds
DEBUG 10-14 21:17:24 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 512, 128]), value shape torch.Size([10, 8, 512, 128])
DEBUG 10-14 21:17:24 lpmodule.py:227] layer idx 13
DEBUG 10-14 21:17:24 lpmodule.py:399] repeat qkv cost 0.015082 seconds
DEBUG 10-14 21:17:24 lpmodule.py:400] q shape torch.Size([10, 32, 512, 128]), k shape torch.Size([10, 32, 512, 128]), v shape torch.Size([10, 32, 512, 128])
DEBUG 10-14 21:17:24 lpllm.py:498] reset update attn
DEBUG 10-14 21:17:24 lpllm.py:1771] update state cost 2.4557113647460938e-05 s
DEBUG 10-14 21:17:24 lpllm.py:1740] restore layer func cost 0.0004999637603759766 s
DEBUG 10-14 21:17:24 lpllm.py:511] restore layer cost 0.0007777214050292969 s
DEBUG 10-14 21:17:24 lpllm.py:512] reset_next_layer_need: layer_attn_idx=14, layer_mlp_idx=13, j_loc=27
DEBUG 10-14 21:17:24 lpllm.py:1034] reset layer cost 0.00087738037109375 s
DEBUG 10-14 21:17:24 lpllm.py:1035] have reset next layer layer_attn 14 layer_mlp 13 
DEBUG 10-14 21:17:24 lpllm.py:1041] j: 27 waiting the layer with layer_idx 14 before wait time 0.21741318702697754 s
INFO 10-14 21:17:24 client.py:117] confirm_model_loaded: Mixtral-8x7B, 121add32-f795-40a3-8f88-abf17e7c0871
DEBUG 10-14 21:17:24 lpmodule.py:433] dot attn cost 0.066567 seconds
DEBUG 10-14 21:17:24 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.02417922019958496 s
DEBUG 10-14 21:17:24 lpllm.py:2280] CPU attn cost 0.118707 seconds if batch True
DEBUG 10-14 21:17:24 lpllm.py:2289] deal attn result cost 0.000002 seconds
DEBUG 10-14 21:17:24 lpllm.py:2291] CPU compute cost 0.119268 seconds
DEBUG 10-14 21:17:24 lpllm.py:2309] free cost 0.000111 seconds
INFO 10-14 21:17:24 client.py:125] Model loaded
DEBUG 10-14 21:17:24 lpllm.py:1045] j: load cost 0.32076311111450195 s waiting cost 0.10333466529846191 s
DEBUG 10-14 21:17:24 lpllm.py:921] 
DEBUG 10-14 21:17:24 lpllm.py:921] decoder loop j: 28 cur_layer_idx: 14 layer_attn_idx: 14 layer_mlp_idx: 13
DEBUG 10-14 21:17:24 lpllm.py:930] start load next layer cur_layer_idx: 15
DEBUG 10-14 21:17:24 lpllm.py:1801] cuda memory handles: {1: <capsule object NULL at 0x7fdf3c6d9ec0>} {1: b'\xe0\xaa29\x00\x00\x00\x00\xe8&\x1d\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\x86\x00\x00\x00\xff\x00\x00.\x00\x00\x00\x00\x00\x00\x00)\xd8\xd5\xc1\xa0\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-14 21:17:24 client.py:72] load_into_gpu: Mixtral-8x7B, d77c2546-39d9-4fb0-b270-ac53c9551dba
INFO 10-14 21:17:24 client.py:113] Model loaded: Mixtral-8x7B, d77c2546-39d9-4fb0-b270-ac53c9551dba
DEBUG 10-14 21:17:24 lpmodule.py:451] hidden_states.shape torch.Size([10, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:24 lpmodule.py:455] input_layernorm num_params: 1
DEBUG 10-14 21:17:24 lpmodule.py:457] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-14 21:17:24 lpmodule.py:498] kv_seq_len 512 past_key_value 0
DEBUG 10-14 21:17:24 lpmodule.py:504] inv_freq device cpu
DEBUG 10-14 21:17:24 lpmodule.py:505] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:24 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:24 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:24 lpmodule.py:505]         ...,
DEBUG 10-14 21:17:24 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:24 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:24 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-14 21:17:24 lpmodule.py:511] query_states device cuda:1
DEBUG 10-14 21:17:24 lpmodule.py:512] key_states device cuda:1
DEBUG 10-14 21:17:24 lpmodule.py:513] value_states device cuda:1
DEBUG 10-14 21:17:24 lpmodule.py:514] position_ids device cuda:1
DEBUG 10-14 21:17:24 lpmodule.py:515] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:24 lpmodule.py:516] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:24 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:24 lpllm.py:2262] GPU2CPU move cost 0.000850 seconds
DEBUG 10-14 21:17:24 lpmodule.py:340] layer_idx 14 attn layer_idx: 14
DEBUG 10-14 21:17:24 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-14 21:17:24 lpmodule.py:364] decoder_attn_batch update batch_dim 0-10
DEBUG 10-14 21:17:24 lpmodule.py:227] layer idx 13
DEBUG 10-14 21:17:24 StaticCacheLen.py:84] static cache update layer_idx: 14, start_batch: 0, end_batch: 10
DEBUG 10-14 21:17:24 lpmodule.py:368] update for kv cache 0-10 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-14 21:17:24 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:24 lpmodule.py:374] update past key value cost 0.007257 seconds
DEBUG 10-14 21:17:24 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 512, 128]), value shape torch.Size([10, 8, 512, 128])
DEBUG 10-14 21:17:24 lpmodule.py:399] repeat qkv cost 0.013443 seconds
DEBUG 10-14 21:17:24 lpmodule.py:400] q shape torch.Size([10, 32, 512, 128]), k shape torch.Size([10, 32, 512, 128]), v shape torch.Size([10, 32, 512, 128])
DEBUG 10-14 21:17:24 lpllm.py:503] reset update experts
DEBUG 10-14 21:17:24 lpllm.py:1771] update state cost 5.936622619628906e-05 s
DEBUG 10-14 21:17:24 lpllm.py:1740] restore layer func cost 0.003113985061645508 s
DEBUG 10-14 21:17:24 lpllm.py:511] restore layer cost 0.0037746429443359375 s
DEBUG 10-14 21:17:24 lpllm.py:512] reset_next_layer_need: layer_attn_idx=14, layer_mlp_idx=14, j_loc=28
DEBUG 10-14 21:17:24 lpllm.py:1034] reset layer cost 0.004045963287353516 s
DEBUG 10-14 21:17:24 lpllm.py:1035] have reset next layer layer_attn 14 layer_mlp 14 
DEBUG 10-14 21:17:24 lpllm.py:921] 
DEBUG 10-14 21:17:24 lpllm.py:921] decoder loop j: 29 cur_layer_idx: 14 layer_attn_idx: 14 layer_mlp_idx: 14
DEBUG 10-14 21:17:24 lpmodule.py:451] hidden_states.shape torch.Size([10, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:24 lpmodule.py:455] input_layernorm num_params: 1
DEBUG 10-14 21:17:24 lpmodule.py:457] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-14 21:17:24 lpmodule.py:498] kv_seq_len 512 past_key_value 0
DEBUG 10-14 21:17:24 lpmodule.py:504] inv_freq device cpu
DEBUG 10-14 21:17:24 lpmodule.py:505] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:24 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:24 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:24 lpmodule.py:505]         ...,
DEBUG 10-14 21:17:24 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:24 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:24 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-14 21:17:24 lpmodule.py:511] query_states device cuda:1
DEBUG 10-14 21:17:24 lpmodule.py:512] key_states device cuda:1
DEBUG 10-14 21:17:24 lpmodule.py:513] value_states device cuda:1
DEBUG 10-14 21:17:24 lpmodule.py:514] position_ids device cuda:1
DEBUG 10-14 21:17:24 lpmodule.py:515] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:24 lpmodule.py:516] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:24 lpmodule.py:433] dot attn cost 0.063087 seconds
DEBUG 10-14 21:17:24 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-14 21:17:24 lpllm.py:2081] waiting out_queue
time cost move to cuda:1 0.02456498146057129 s
DEBUG 10-14 21:17:24 lpllm.py:2280] CPU attn cost 0.116316 seconds if batch True
DEBUG 10-14 21:17:24 lpllm.py:2289] deal attn result cost 0.000003 seconds
DEBUG 10-14 21:17:24 lpllm.py:2291] CPU compute cost 0.117517 seconds
DEBUG 10-14 21:17:24 lpllm.py:2309] free cost 0.000089 seconds
DEBUG 10-14 21:17:24 lpllm.py:2262] GPU2CPU move cost 0.000265 seconds
DEBUG 10-14 21:17:24 lpmodule.py:340] layer_idx 14 attn layer_idx: 14
DEBUG 10-14 21:17:24 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-14 21:17:24 lpmodule.py:364] decoder_attn_batch update batch_dim 10-20
DEBUG 10-14 21:17:24 StaticCacheLen.py:84] static cache update layer_idx: 14, start_batch: 10, end_batch: 20
DEBUG 10-14 21:17:24 StaticCacheLen.py:90] static cache update layer_idx: 14, update seq_length to 512
DEBUG 10-14 21:17:24 lpmodule.py:368] update for kv cache 10-20 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-14 21:17:24 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:24 lpmodule.py:374] update past key value cost 0.006209 seconds
DEBUG 10-14 21:17:24 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 512, 128]), value shape torch.Size([10, 8, 512, 128])
DEBUG 10-14 21:17:24 lpmodule.py:227] layer idx 14
DEBUG 10-14 21:17:24 lpmodule.py:399] repeat qkv cost 0.011942 seconds
DEBUG 10-14 21:17:24 lpmodule.py:400] q shape torch.Size([10, 32, 512, 128]), k shape torch.Size([10, 32, 512, 128]), v shape torch.Size([10, 32, 512, 128])
DEBUG 10-14 21:17:24 lpllm.py:498] reset update attn
DEBUG 10-14 21:17:24 lpllm.py:1771] update state cost 4.792213439941406e-05 s
DEBUG 10-14 21:17:24 lpllm.py:1740] restore layer func cost 0.0013036727905273438 s
DEBUG 10-14 21:17:24 lpllm.py:511] restore layer cost 0.0018918514251708984 s
DEBUG 10-14 21:17:24 lpllm.py:512] reset_next_layer_need: layer_attn_idx=15, layer_mlp_idx=14, j_loc=29
DEBUG 10-14 21:17:24 lpllm.py:1034] reset layer cost 0.0021352767944335938 s
DEBUG 10-14 21:17:24 lpllm.py:1035] have reset next layer layer_attn 15 layer_mlp 14 
DEBUG 10-14 21:17:24 lpllm.py:1041] j: 29 waiting the layer with layer_idx 15 before wait time 0.21923613548278809 s
INFO 10-14 21:17:24 client.py:117] confirm_model_loaded: Mixtral-8x7B, d77c2546-39d9-4fb0-b270-ac53c9551dba
DEBUG 10-14 21:17:24 lpmodule.py:433] dot attn cost 0.064019 seconds
DEBUG 10-14 21:17:24 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.024118661880493164 s
DEBUG 10-14 21:17:24 lpllm.py:2280] CPU attn cost 0.114453 seconds if batch True
DEBUG 10-14 21:17:24 lpllm.py:2289] deal attn result cost 0.000002 seconds
DEBUG 10-14 21:17:24 lpllm.py:2291] CPU compute cost 0.114932 seconds
DEBUG 10-14 21:17:24 lpllm.py:2309] free cost 0.000100 seconds
INFO 10-14 21:17:24 client.py:125] Model loaded
DEBUG 10-14 21:17:24 lpllm.py:1045] j: load cost 0.32031869888305664 s waiting cost 0.10102176666259766 s
DEBUG 10-14 21:17:24 lpllm.py:921] 
DEBUG 10-14 21:17:24 lpllm.py:921] decoder loop j: 30 cur_layer_idx: 15 layer_attn_idx: 15 layer_mlp_idx: 14
DEBUG 10-14 21:17:24 lpllm.py:930] start load next layer cur_layer_idx: 16
DEBUG 10-14 21:17:24 lpllm.py:1801] cuda memory handles: {1: <capsule object NULL at 0x7fdf3c6d9f80>} {1: b'\xe0\xaa29\x00\x00\x00\x00\xe8&\x1d\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\x86\x00\x00\x00\xff\x00\x00/\x00\x00\x00\x00\x00\x00\x00)\xd8\xd5\xc1\xa2\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-14 21:17:24 client.py:72] load_into_gpu: Mixtral-8x7B, 272ec71c-4025-4a35-99d3-bcd13196a999
INFO 10-14 21:17:24 client.py:113] Model loaded: Mixtral-8x7B, 272ec71c-4025-4a35-99d3-bcd13196a999
DEBUG 10-14 21:17:24 lpmodule.py:451] hidden_states.shape torch.Size([10, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:24 lpmodule.py:455] input_layernorm num_params: 1
DEBUG 10-14 21:17:24 lpmodule.py:457] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-14 21:17:24 lpmodule.py:498] kv_seq_len 512 past_key_value 0
DEBUG 10-14 21:17:24 lpmodule.py:504] inv_freq device cpu
DEBUG 10-14 21:17:24 lpmodule.py:505] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:24 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:24 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:24 lpmodule.py:505]         ...,
DEBUG 10-14 21:17:24 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:24 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:24 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-14 21:17:24 lpmodule.py:511] query_states device cuda:1
DEBUG 10-14 21:17:24 lpmodule.py:512] key_states device cuda:1
DEBUG 10-14 21:17:24 lpmodule.py:513] value_states device cuda:1
DEBUG 10-14 21:17:24 lpmodule.py:514] position_ids device cuda:1
DEBUG 10-14 21:17:24 lpmodule.py:515] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:24 lpmodule.py:516] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:24 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:24 lpllm.py:2262] GPU2CPU move cost 0.000701 seconds
DEBUG 10-14 21:17:24 lpmodule.py:340] layer_idx 15 attn layer_idx: 15
DEBUG 10-14 21:17:24 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-14 21:17:24 lpmodule.py:364] decoder_attn_batch update batch_dim 0-10
DEBUG 10-14 21:17:24 lpmodule.py:227] layer idx 14
DEBUG 10-14 21:17:24 StaticCacheLen.py:84] static cache update layer_idx: 15, start_batch: 0, end_batch: 10
DEBUG 10-14 21:17:24 lpmodule.py:368] update for kv cache 0-10 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-14 21:17:24 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:24 lpmodule.py:374] update past key value cost 0.008842 seconds
DEBUG 10-14 21:17:24 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 512, 128]), value shape torch.Size([10, 8, 512, 128])
DEBUG 10-14 21:17:24 lpmodule.py:399] repeat qkv cost 0.012851 seconds
DEBUG 10-14 21:17:24 lpmodule.py:400] q shape torch.Size([10, 32, 512, 128]), k shape torch.Size([10, 32, 512, 128]), v shape torch.Size([10, 32, 512, 128])
DEBUG 10-14 21:17:24 lpllm.py:503] reset update experts
DEBUG 10-14 21:17:24 lpllm.py:1771] update state cost 4.100799560546875e-05 s
DEBUG 10-14 21:17:24 lpllm.py:1740] restore layer func cost 0.0031280517578125 s
DEBUG 10-14 21:17:24 lpllm.py:511] restore layer cost 0.00376129150390625 s
DEBUG 10-14 21:17:24 lpllm.py:512] reset_next_layer_need: layer_attn_idx=15, layer_mlp_idx=15, j_loc=30
DEBUG 10-14 21:17:24 lpllm.py:1034] reset layer cost 0.004010677337646484 s
DEBUG 10-14 21:17:24 lpllm.py:1035] have reset next layer layer_attn 15 layer_mlp 15 
DEBUG 10-14 21:17:24 lpllm.py:921] 
DEBUG 10-14 21:17:24 lpllm.py:921] decoder loop j: 31 cur_layer_idx: 15 layer_attn_idx: 15 layer_mlp_idx: 15
DEBUG 10-14 21:17:24 lpmodule.py:451] hidden_states.shape torch.Size([10, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:24 lpmodule.py:455] input_layernorm num_params: 1
DEBUG 10-14 21:17:24 lpmodule.py:457] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-14 21:17:24 lpmodule.py:498] kv_seq_len 512 past_key_value 0
DEBUG 10-14 21:17:24 lpmodule.py:504] inv_freq device cpu
DEBUG 10-14 21:17:24 lpmodule.py:505] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:24 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:24 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:24 lpmodule.py:505]         ...,
DEBUG 10-14 21:17:24 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:24 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:24 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-14 21:17:24 lpmodule.py:433] dot attn cost 0.061629 seconds
DEBUG 10-14 21:17:24 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-14 21:17:24 lpmodule.py:511] query_states device cuda:1
DEBUG 10-14 21:17:24 lpmodule.py:512] key_states device cuda:1
DEBUG 10-14 21:17:24 lpmodule.py:513] value_states device cuda:1
DEBUG 10-14 21:17:24 lpmodule.py:514] position_ids device cuda:1
DEBUG 10-14 21:17:24 lpmodule.py:515] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:24 lpmodule.py:516] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:24 lpllm.py:2081] waiting out_queue
time cost move to cuda:1 0.02430868148803711 s
DEBUG 10-14 21:17:24 lpllm.py:2280] CPU attn cost 0.115106 seconds if batch True
DEBUG 10-14 21:17:24 lpllm.py:2289] deal attn result cost 0.000002 seconds
DEBUG 10-14 21:17:24 lpllm.py:2291] CPU compute cost 0.116170 seconds
DEBUG 10-14 21:17:24 lpllm.py:2309] free cost 0.000113 seconds
DEBUG 10-14 21:17:24 lpllm.py:2262] GPU2CPU move cost 0.000298 seconds
DEBUG 10-14 21:17:24 lpmodule.py:340] layer_idx 15 attn layer_idx: 15
DEBUG 10-14 21:17:24 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-14 21:17:24 lpmodule.py:364] decoder_attn_batch update batch_dim 10-20
DEBUG 10-14 21:17:24 StaticCacheLen.py:84] static cache update layer_idx: 15, start_batch: 10, end_batch: 20
DEBUG 10-14 21:17:24 StaticCacheLen.py:90] static cache update layer_idx: 15, update seq_length to 512
DEBUG 10-14 21:17:24 lpmodule.py:368] update for kv cache 10-20 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-14 21:17:24 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:24 lpmodule.py:374] update past key value cost 0.006144 seconds
DEBUG 10-14 21:17:24 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 512, 128]), value shape torch.Size([10, 8, 512, 128])
DEBUG 10-14 21:17:24 lpmodule.py:227] layer idx 15
DEBUG 10-14 21:17:24 lpmodule.py:399] repeat qkv cost 0.012216 seconds
DEBUG 10-14 21:17:24 lpmodule.py:400] q shape torch.Size([10, 32, 512, 128]), k shape torch.Size([10, 32, 512, 128]), v shape torch.Size([10, 32, 512, 128])
DEBUG 10-14 21:17:25 lpllm.py:498] reset update attn
DEBUG 10-14 21:17:25 lpllm.py:1771] update state cost 4.9591064453125e-05 s
DEBUG 10-14 21:17:25 lpllm.py:1740] restore layer func cost 0.0013034343719482422 s
DEBUG 10-14 21:17:25 lpllm.py:511] restore layer cost 0.0018792152404785156 s
DEBUG 10-14 21:17:25 lpllm.py:512] reset_next_layer_need: layer_attn_idx=16, layer_mlp_idx=15, j_loc=31
DEBUG 10-14 21:17:25 lpllm.py:1034] reset layer cost 0.002127408981323242 s
DEBUG 10-14 21:17:25 lpllm.py:1035] have reset next layer layer_attn 16 layer_mlp 15 
DEBUG 10-14 21:17:25 lpllm.py:1041] j: 31 waiting the layer with layer_idx 16 before wait time 0.2197732925415039 s
INFO 10-14 21:17:25 client.py:117] confirm_model_loaded: Mixtral-8x7B, 272ec71c-4025-4a35-99d3-bcd13196a999
DEBUG 10-14 21:17:25 lpmodule.py:433] dot attn cost 0.062038 seconds
DEBUG 10-14 21:17:25 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.02458047866821289 s
DEBUG 10-14 21:17:25 lpllm.py:2280] CPU attn cost 0.112355 seconds if batch True
DEBUG 10-14 21:17:25 lpllm.py:2289] deal attn result cost 0.000002 seconds
DEBUG 10-14 21:17:25 lpllm.py:2291] CPU compute cost 0.112868 seconds
DEBUG 10-14 21:17:25 lpllm.py:2309] free cost 0.000112 seconds
INFO 10-14 21:17:25 client.py:125] Model loaded
DEBUG 10-14 21:17:25 lpllm.py:1045] j: load cost 0.304368257522583 s waiting cost 0.08453226089477539 s
DEBUG 10-14 21:17:25 lpllm.py:921] 
DEBUG 10-14 21:17:25 lpllm.py:921] decoder loop j: 32 cur_layer_idx: 16 layer_attn_idx: 16 layer_mlp_idx: 15
DEBUG 10-14 21:17:25 lpllm.py:930] start load next layer cur_layer_idx: 17
DEBUG 10-14 21:17:25 lpllm.py:1801] cuda memory handles: {1: <capsule object NULL at 0x7fdf3c6da010>} {1: b'\xe0\xaa29\x00\x00\x00\x00\xe8&\x1d\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\x86\x00\x00\x00\xff\x00\x000\x00\x00\x00\x00\x00\x00\x00)\xd8\xd5\xc1\xa4\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-14 21:17:25 client.py:72] load_into_gpu: Mixtral-8x7B, f9760b5a-4cf5-421f-ac22-0df0273927b6
INFO 10-14 21:17:25 client.py:113] Model loaded: Mixtral-8x7B, f9760b5a-4cf5-421f-ac22-0df0273927b6
DEBUG 10-14 21:17:25 lpmodule.py:451] hidden_states.shape torch.Size([10, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:25 lpmodule.py:455] input_layernorm num_params: 1
DEBUG 10-14 21:17:25 lpmodule.py:457] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-14 21:17:25 lpmodule.py:498] kv_seq_len 512 past_key_value 0
DEBUG 10-14 21:17:25 lpmodule.py:504] inv_freq device cpu
DEBUG 10-14 21:17:25 lpmodule.py:505] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:25 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:25 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:25 lpmodule.py:505]         ...,
DEBUG 10-14 21:17:25 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:25 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:25 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-14 21:17:25 lpmodule.py:511] query_states device cuda:1
DEBUG 10-14 21:17:25 lpmodule.py:512] key_states device cuda:1
DEBUG 10-14 21:17:25 lpmodule.py:513] value_states device cuda:1
DEBUG 10-14 21:17:25 lpmodule.py:514] position_ids device cuda:1
DEBUG 10-14 21:17:25 lpmodule.py:515] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:25 lpmodule.py:516] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:25 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:25 lpllm.py:2262] GPU2CPU move cost 0.000743 seconds
DEBUG 10-14 21:17:25 lpmodule.py:340] layer_idx 16 attn layer_idx: 16
DEBUG 10-14 21:17:25 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-14 21:17:25 lpmodule.py:364] decoder_attn_batch update batch_dim 0-10
DEBUG 10-14 21:17:25 lpmodule.py:227] layer idx 15
DEBUG 10-14 21:17:25 StaticCacheLen.py:84] static cache update layer_idx: 16, start_batch: 0, end_batch: 10
DEBUG 10-14 21:17:25 lpmodule.py:368] update for kv cache 0-10 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-14 21:17:25 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:25 lpmodule.py:374] update past key value cost 0.008119 seconds
DEBUG 10-14 21:17:25 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 512, 128]), value shape torch.Size([10, 8, 512, 128])
DEBUG 10-14 21:17:25 lpmodule.py:399] repeat qkv cost 0.012590 seconds
DEBUG 10-14 21:17:25 lpmodule.py:400] q shape torch.Size([10, 32, 512, 128]), k shape torch.Size([10, 32, 512, 128]), v shape torch.Size([10, 32, 512, 128])
DEBUG 10-14 21:17:25 lpllm.py:503] reset update experts
DEBUG 10-14 21:17:25 lpllm.py:1771] update state cost 4.315376281738281e-05 s
DEBUG 10-14 21:17:25 lpllm.py:1740] restore layer func cost 0.0031261444091796875 s
DEBUG 10-14 21:17:25 lpllm.py:511] restore layer cost 0.0037453174591064453 s
DEBUG 10-14 21:17:25 lpllm.py:512] reset_next_layer_need: layer_attn_idx=16, layer_mlp_idx=16, j_loc=32
DEBUG 10-14 21:17:25 lpllm.py:1034] reset layer cost 0.004006862640380859 s
DEBUG 10-14 21:17:25 lpllm.py:1035] have reset next layer layer_attn 16 layer_mlp 16 
DEBUG 10-14 21:17:25 lpllm.py:921] 
DEBUG 10-14 21:17:25 lpllm.py:921] decoder loop j: 33 cur_layer_idx: 16 layer_attn_idx: 16 layer_mlp_idx: 16
DEBUG 10-14 21:17:25 lpmodule.py:451] hidden_states.shape torch.Size([10, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:25 lpmodule.py:455] input_layernorm num_params: 1
DEBUG 10-14 21:17:25 lpmodule.py:457] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-14 21:17:25 lpmodule.py:498] kv_seq_len 512 past_key_value 0
DEBUG 10-14 21:17:25 lpmodule.py:504] inv_freq device cpu
DEBUG 10-14 21:17:25 lpmodule.py:505] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:25 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:25 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:25 lpmodule.py:505]         ...,
DEBUG 10-14 21:17:25 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:25 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:25 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-14 21:17:25 lpmodule.py:511] query_states device cuda:1
DEBUG 10-14 21:17:25 lpmodule.py:512] key_states device cuda:1
DEBUG 10-14 21:17:25 lpmodule.py:513] value_states device cuda:1
DEBUG 10-14 21:17:25 lpmodule.py:514] position_ids device cuda:1
DEBUG 10-14 21:17:25 lpmodule.py:515] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:25 lpmodule.py:516] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:25 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:25 lpmodule.py:433] dot attn cost 0.064984 seconds
DEBUG 10-14 21:17:25 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.015772581100463867 s
DEBUG 10-14 21:17:25 lpllm.py:2280] CPU attn cost 0.111666 seconds if batch True
DEBUG 10-14 21:17:25 lpllm.py:2289] deal attn result cost 0.000002 seconds
DEBUG 10-14 21:17:25 lpllm.py:2291] CPU compute cost 0.112879 seconds
DEBUG 10-14 21:17:25 lpllm.py:2309] free cost 0.000132 seconds
DEBUG 10-14 21:17:25 lpllm.py:2262] GPU2CPU move cost 0.000386 seconds
DEBUG 10-14 21:17:25 lpmodule.py:340] layer_idx 16 attn layer_idx: 16
DEBUG 10-14 21:17:25 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-14 21:17:25 lpmodule.py:364] decoder_attn_batch update batch_dim 10-20
DEBUG 10-14 21:17:25 StaticCacheLen.py:84] static cache update layer_idx: 16, start_batch: 10, end_batch: 20
DEBUG 10-14 21:17:25 StaticCacheLen.py:90] static cache update layer_idx: 16, update seq_length to 512
DEBUG 10-14 21:17:25 lpmodule.py:368] update for kv cache 10-20 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-14 21:17:25 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:25 lpmodule.py:374] update past key value cost 0.006483 seconds
DEBUG 10-14 21:17:25 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 512, 128]), value shape torch.Size([10, 8, 512, 128])
DEBUG 10-14 21:17:25 lpmodule.py:227] layer idx 16
DEBUG 10-14 21:17:25 lpmodule.py:399] repeat qkv cost 0.012571 seconds
DEBUG 10-14 21:17:25 lpmodule.py:400] q shape torch.Size([10, 32, 512, 128]), k shape torch.Size([10, 32, 512, 128]), v shape torch.Size([10, 32, 512, 128])
DEBUG 10-14 21:17:25 lpllm.py:498] reset update attn
DEBUG 10-14 21:17:25 lpllm.py:1771] update state cost 5.0067901611328125e-05 s
DEBUG 10-14 21:17:25 lpllm.py:1740] restore layer func cost 0.0013203620910644531 s
DEBUG 10-14 21:17:25 lpllm.py:511] restore layer cost 0.0018854141235351562 s
DEBUG 10-14 21:17:25 lpllm.py:512] reset_next_layer_need: layer_attn_idx=17, layer_mlp_idx=16, j_loc=33
DEBUG 10-14 21:17:25 lpllm.py:1034] reset layer cost 0.0021376609802246094 s
DEBUG 10-14 21:17:25 lpllm.py:1035] have reset next layer layer_attn 17 layer_mlp 16 
DEBUG 10-14 21:17:25 lpllm.py:1041] j: 33 waiting the layer with layer_idx 17 before wait time 0.2459118366241455 s
INFO 10-14 21:17:25 client.py:117] confirm_model_loaded: Mixtral-8x7B, f9760b5a-4cf5-421f-ac22-0df0273927b6
DEBUG 10-14 21:17:25 lpmodule.py:433] dot attn cost 0.066990 seconds
DEBUG 10-14 21:17:25 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.025205612182617188 s
DEBUG 10-14 21:17:25 lpllm.py:2280] CPU attn cost 0.118073 seconds if batch True
DEBUG 10-14 21:17:25 lpllm.py:2289] deal attn result cost 0.000002 seconds
DEBUG 10-14 21:17:25 lpllm.py:2291] CPU compute cost 0.118680 seconds
DEBUG 10-14 21:17:25 lpllm.py:2309] free cost 0.000121 seconds
INFO 10-14 21:17:25 client.py:125] Model loaded
DEBUG 10-14 21:17:25 lpllm.py:1045] j: load cost 0.42911744117736816 s waiting cost 0.18314194679260254 s
DEBUG 10-14 21:17:25 lpllm.py:921] 
DEBUG 10-14 21:17:25 lpllm.py:921] decoder loop j: 34 cur_layer_idx: 17 layer_attn_idx: 17 layer_mlp_idx: 16
DEBUG 10-14 21:17:25 lpllm.py:930] start load next layer cur_layer_idx: 18
DEBUG 10-14 21:17:25 lpllm.py:1801] cuda memory handles: {1: <capsule object NULL at 0x7fdf3c6d9ec0>} {1: b'\xe0\xaa29\x00\x00\x00\x00\xe8&\x1d\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\x86\x00\x00\x00\xff\x00\x00.\x00\x00\x00\x00\x00\x00\x00)\xd8\xd5\xc1\xa0\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-14 21:17:25 client.py:72] load_into_gpu: Mixtral-8x7B, 6bcf2a0a-33c6-48e7-9c93-fb17ec34e9da
INFO 10-14 21:17:25 client.py:113] Model loaded: Mixtral-8x7B, 6bcf2a0a-33c6-48e7-9c93-fb17ec34e9da
DEBUG 10-14 21:17:25 lpmodule.py:451] hidden_states.shape torch.Size([10, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:25 lpmodule.py:455] input_layernorm num_params: 1
DEBUG 10-14 21:17:25 lpmodule.py:457] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-14 21:17:25 lpmodule.py:498] kv_seq_len 512 past_key_value 0
DEBUG 10-14 21:17:25 lpmodule.py:504] inv_freq device cpu
DEBUG 10-14 21:17:25 lpmodule.py:505] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:25 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:25 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:25 lpmodule.py:505]         ...,
DEBUG 10-14 21:17:25 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:25 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:25 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-14 21:17:25 lpmodule.py:511] query_states device cuda:1
DEBUG 10-14 21:17:25 lpmodule.py:512] key_states device cuda:1
DEBUG 10-14 21:17:25 lpmodule.py:513] value_states device cuda:1
DEBUG 10-14 21:17:25 lpmodule.py:514] position_ids device cuda:1
DEBUG 10-14 21:17:25 lpmodule.py:515] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:25 lpmodule.py:516] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:25 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:25 lpllm.py:2262] GPU2CPU move cost 0.000509 seconds
DEBUG 10-14 21:17:25 lpmodule.py:340] layer_idx 17 attn layer_idx: 17
DEBUG 10-14 21:17:25 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-14 21:17:25 lpmodule.py:364] decoder_attn_batch update batch_dim 0-10
DEBUG 10-14 21:17:25 lpmodule.py:227] layer idx 16
DEBUG 10-14 21:17:25 StaticCacheLen.py:84] static cache update layer_idx: 17, start_batch: 0, end_batch: 10
DEBUG 10-14 21:17:25 lpmodule.py:368] update for kv cache 0-10 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-14 21:17:25 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:25 lpmodule.py:374] update past key value cost 0.007183 seconds
DEBUG 10-14 21:17:25 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 512, 128]), value shape torch.Size([10, 8, 512, 128])
DEBUG 10-14 21:17:25 lpmodule.py:399] repeat qkv cost 0.012127 seconds
DEBUG 10-14 21:17:25 lpmodule.py:400] q shape torch.Size([10, 32, 512, 128]), k shape torch.Size([10, 32, 512, 128]), v shape torch.Size([10, 32, 512, 128])
DEBUG 10-14 21:17:25 lpllm.py:503] reset update experts
DEBUG 10-14 21:17:25 lpllm.py:1771] update state cost 3.9577484130859375e-05 s
DEBUG 10-14 21:17:25 lpllm.py:1740] restore layer func cost 0.0031304359436035156 s
DEBUG 10-14 21:17:25 lpllm.py:511] restore layer cost 0.0037734508514404297 s
DEBUG 10-14 21:17:25 lpllm.py:512] reset_next_layer_need: layer_attn_idx=17, layer_mlp_idx=17, j_loc=34
DEBUG 10-14 21:17:25 lpllm.py:1034] reset layer cost 0.004028797149658203 s
DEBUG 10-14 21:17:25 lpllm.py:1035] have reset next layer layer_attn 17 layer_mlp 17 
DEBUG 10-14 21:17:25 lpllm.py:921] 
DEBUG 10-14 21:17:25 lpllm.py:921] decoder loop j: 35 cur_layer_idx: 17 layer_attn_idx: 17 layer_mlp_idx: 17
DEBUG 10-14 21:17:25 lpmodule.py:451] hidden_states.shape torch.Size([10, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:25 lpmodule.py:455] input_layernorm num_params: 1
DEBUG 10-14 21:17:25 lpmodule.py:457] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-14 21:17:25 lpmodule.py:498] kv_seq_len 512 past_key_value 0
DEBUG 10-14 21:17:25 lpmodule.py:504] inv_freq device cpu
DEBUG 10-14 21:17:25 lpmodule.py:505] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:25 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:25 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:25 lpmodule.py:505]         ...,
DEBUG 10-14 21:17:25 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:25 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:25 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-14 21:17:25 lpmodule.py:433] dot attn cost 0.064410 seconds
DEBUG 10-14 21:17:25 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-14 21:17:25 lpmodule.py:511] query_states device cuda:1
DEBUG 10-14 21:17:25 lpmodule.py:512] key_states device cuda:1
DEBUG 10-14 21:17:25 lpmodule.py:513] value_states device cuda:1
DEBUG 10-14 21:17:25 lpmodule.py:514] position_ids device cuda:1
DEBUG 10-14 21:17:25 lpmodule.py:515] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:25 lpmodule.py:516] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:25 lpllm.py:2081] waiting out_queue
time cost move to cuda:1 0.024861574172973633 s
DEBUG 10-14 21:17:25 lpllm.py:2280] CPU attn cost 0.115948 seconds if batch True
DEBUG 10-14 21:17:25 lpllm.py:2289] deal attn result cost 0.000002 seconds
DEBUG 10-14 21:17:25 lpllm.py:2291] CPU compute cost 0.116696 seconds
DEBUG 10-14 21:17:25 lpllm.py:2309] free cost 0.000104 seconds
DEBUG 10-14 21:17:25 lpllm.py:2262] GPU2CPU move cost 0.000277 seconds
DEBUG 10-14 21:17:25 lpmodule.py:340] layer_idx 17 attn layer_idx: 17
DEBUG 10-14 21:17:25 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-14 21:17:25 lpmodule.py:364] decoder_attn_batch update batch_dim 10-20
DEBUG 10-14 21:17:25 StaticCacheLen.py:84] static cache update layer_idx: 17, start_batch: 10, end_batch: 20
DEBUG 10-14 21:17:25 StaticCacheLen.py:90] static cache update layer_idx: 17, update seq_length to 512
DEBUG 10-14 21:17:25 lpmodule.py:368] update for kv cache 10-20 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-14 21:17:25 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:25 lpmodule.py:374] update past key value cost 0.006063 seconds
DEBUG 10-14 21:17:25 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 512, 128]), value shape torch.Size([10, 8, 512, 128])
DEBUG 10-14 21:17:25 lpmodule.py:227] layer idx 17
DEBUG 10-14 21:17:25 lpmodule.py:399] repeat qkv cost 0.012876 seconds
DEBUG 10-14 21:17:25 lpmodule.py:400] q shape torch.Size([10, 32, 512, 128]), k shape torch.Size([10, 32, 512, 128]), v shape torch.Size([10, 32, 512, 128])
DEBUG 10-14 21:17:25 lpllm.py:498] reset update attn
DEBUG 10-14 21:17:25 lpllm.py:1771] update state cost 4.8160552978515625e-05 s
DEBUG 10-14 21:17:25 lpllm.py:1740] restore layer func cost 0.0012726783752441406 s
DEBUG 10-14 21:17:25 lpllm.py:511] restore layer cost 0.0018510818481445312 s
DEBUG 10-14 21:17:25 lpllm.py:512] reset_next_layer_need: layer_attn_idx=18, layer_mlp_idx=17, j_loc=35
DEBUG 10-14 21:17:25 lpllm.py:1034] reset layer cost 0.0020914077758789062 s
DEBUG 10-14 21:17:25 lpllm.py:1035] have reset next layer layer_attn 18 layer_mlp 17 
DEBUG 10-14 21:17:25 lpllm.py:1041] j: 35 waiting the layer with layer_idx 18 before wait time 0.21779322624206543 s
INFO 10-14 21:17:25 client.py:117] confirm_model_loaded: Mixtral-8x7B, 6bcf2a0a-33c6-48e7-9c93-fb17ec34e9da
DEBUG 10-14 21:17:25 lpmodule.py:433] dot attn cost 0.062710 seconds
DEBUG 10-14 21:17:25 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.024873018264770508 s
DEBUG 10-14 21:17:25 lpllm.py:2280] CPU attn cost 0.114459 seconds if batch True
DEBUG 10-14 21:17:25 lpllm.py:2289] deal attn result cost 0.000002 seconds
DEBUG 10-14 21:17:25 lpllm.py:2291] CPU compute cost 0.114944 seconds
DEBUG 10-14 21:17:25 lpllm.py:2309] free cost 0.000133 seconds
INFO 10-14 21:17:25 client.py:125] Model loaded
DEBUG 10-14 21:17:25 lpllm.py:1045] j: load cost 0.3074493408203125 s waiting cost 0.08960294723510742 s
DEBUG 10-14 21:17:25 lpllm.py:921] 
DEBUG 10-14 21:17:25 lpllm.py:921] decoder loop j: 36 cur_layer_idx: 18 layer_attn_idx: 18 layer_mlp_idx: 17
DEBUG 10-14 21:17:25 lpllm.py:930] start load next layer cur_layer_idx: 19
DEBUG 10-14 21:17:25 lpllm.py:1801] cuda memory handles: {1: <capsule object NULL at 0x7fdf3c6d9f80>} {1: b'\xe0\xaa29\x00\x00\x00\x00\xe8&\x1d\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\x86\x00\x00\x00\xff\x00\x00/\x00\x00\x00\x00\x00\x00\x00)\xd8\xd5\xc1\xa2\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-14 21:17:25 client.py:72] load_into_gpu: Mixtral-8x7B, 1f936a7c-d5be-434e-adfe-3e6414936d23
INFO 10-14 21:17:25 client.py:113] Model loaded: Mixtral-8x7B, 1f936a7c-d5be-434e-adfe-3e6414936d23
DEBUG 10-14 21:17:25 lpmodule.py:451] hidden_states.shape torch.Size([10, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:25 lpmodule.py:455] input_layernorm num_params: 1
DEBUG 10-14 21:17:25 lpmodule.py:457] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-14 21:17:25 lpmodule.py:498] kv_seq_len 512 past_key_value 0
DEBUG 10-14 21:17:25 lpmodule.py:504] inv_freq device cpu
DEBUG 10-14 21:17:25 lpmodule.py:505] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:25 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:25 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:25 lpmodule.py:505]         ...,
DEBUG 10-14 21:17:25 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:25 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:25 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-14 21:17:25 lpmodule.py:511] query_states device cuda:1
DEBUG 10-14 21:17:25 lpmodule.py:512] key_states device cuda:1
DEBUG 10-14 21:17:25 lpmodule.py:513] value_states device cuda:1
DEBUG 10-14 21:17:25 lpmodule.py:514] position_ids device cuda:1
DEBUG 10-14 21:17:25 lpmodule.py:515] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:25 lpmodule.py:516] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:25 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:25 lpllm.py:2262] GPU2CPU move cost 0.000897 seconds
DEBUG 10-14 21:17:25 lpmodule.py:340] layer_idx 18 attn layer_idx: 18
DEBUG 10-14 21:17:25 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-14 21:17:25 lpmodule.py:364] decoder_attn_batch update batch_dim 0-10
DEBUG 10-14 21:17:25 lpmodule.py:227] layer idx 17
DEBUG 10-14 21:17:25 StaticCacheLen.py:84] static cache update layer_idx: 18, start_batch: 0, end_batch: 10
DEBUG 10-14 21:17:25 lpmodule.py:368] update for kv cache 0-10 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-14 21:17:25 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:25 lpmodule.py:374] update past key value cost 0.006720 seconds
DEBUG 10-14 21:17:25 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 512, 128]), value shape torch.Size([10, 8, 512, 128])
DEBUG 10-14 21:17:25 lpmodule.py:399] repeat qkv cost 0.012430 seconds
DEBUG 10-14 21:17:25 lpmodule.py:400] q shape torch.Size([10, 32, 512, 128]), k shape torch.Size([10, 32, 512, 128]), v shape torch.Size([10, 32, 512, 128])
DEBUG 10-14 21:17:25 lpmodule.py:433] dot attn cost 0.064717 seconds
DEBUG 10-14 21:17:25 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-14 21:17:26 lpllm.py:503] reset update experts
DEBUG 10-14 21:17:26 lpllm.py:1771] update state cost 2.5510787963867188e-05 s
DEBUG 10-14 21:17:26 lpllm.py:1740] restore layer func cost 0.0022308826446533203 s
DEBUG 10-14 21:17:26 lpllm.py:511] restore layer cost 0.0025587081909179688 s
DEBUG 10-14 21:17:26 lpllm.py:512] reset_next_layer_need: layer_attn_idx=18, layer_mlp_idx=18, j_loc=36
DEBUG 10-14 21:17:26 lpllm.py:1034] reset layer cost 0.0027208328247070312 s
DEBUG 10-14 21:17:26 lpllm.py:1035] have reset next layer layer_attn 18 layer_mlp 18 
DEBUG 10-14 21:17:26 lpllm.py:921] 
DEBUG 10-14 21:17:26 lpllm.py:921] decoder loop j: 37 cur_layer_idx: 18 layer_attn_idx: 18 layer_mlp_idx: 18
DEBUG 10-14 21:17:26 lpmodule.py:451] hidden_states.shape torch.Size([10, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:26 lpmodule.py:455] input_layernorm num_params: 1
DEBUG 10-14 21:17:26 lpmodule.py:457] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
time cost move to cuda:1 0.026914358139038086 s
DEBUG 10-14 21:17:26 lpmodule.py:498] kv_seq_len 512 past_key_value 0
DEBUG 10-14 21:17:26 lpmodule.py:504] inv_freq device cpu
DEBUG 10-14 21:17:26 lpmodule.py:505] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:26 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:26 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:26 lpmodule.py:505]         ...,
DEBUG 10-14 21:17:26 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:26 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:26 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-14 21:17:26 lpllm.py:2280] CPU attn cost 0.117941 seconds if batch True
DEBUG 10-14 21:17:26 lpllm.py:2289] deal attn result cost 0.000002 seconds
DEBUG 10-14 21:17:26 lpllm.py:2291] CPU compute cost 0.119176 seconds
DEBUG 10-14 21:17:26 lpllm.py:2309] free cost 0.000077 seconds
DEBUG 10-14 21:17:26 lpmodule.py:511] query_states device cuda:1
DEBUG 10-14 21:17:26 lpmodule.py:512] key_states device cuda:1
DEBUG 10-14 21:17:26 lpmodule.py:513] value_states device cuda:1
DEBUG 10-14 21:17:26 lpmodule.py:514] position_ids device cuda:1
DEBUG 10-14 21:17:26 lpmodule.py:515] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:26 lpmodule.py:516] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:26 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:26 lpllm.py:2262] GPU2CPU move cost 0.000250 seconds
DEBUG 10-14 21:17:26 lpmodule.py:340] layer_idx 18 attn layer_idx: 18
DEBUG 10-14 21:17:26 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-14 21:17:26 lpmodule.py:364] decoder_attn_batch update batch_dim 10-20
DEBUG 10-14 21:17:26 lpmodule.py:227] layer idx 18
DEBUG 10-14 21:17:26 StaticCacheLen.py:84] static cache update layer_idx: 18, start_batch: 10, end_batch: 20
DEBUG 10-14 21:17:26 StaticCacheLen.py:90] static cache update layer_idx: 18, update seq_length to 512
DEBUG 10-14 21:17:26 lpmodule.py:368] update for kv cache 10-20 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-14 21:17:26 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:26 lpmodule.py:374] update past key value cost 0.010808 seconds
DEBUG 10-14 21:17:26 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 512, 128]), value shape torch.Size([10, 8, 512, 128])
DEBUG 10-14 21:17:26 lpmodule.py:399] repeat qkv cost 0.012455 seconds
DEBUG 10-14 21:17:26 lpmodule.py:400] q shape torch.Size([10, 32, 512, 128]), k shape torch.Size([10, 32, 512, 128]), v shape torch.Size([10, 32, 512, 128])
DEBUG 10-14 21:17:26 lpllm.py:498] reset update attn
DEBUG 10-14 21:17:26 lpllm.py:1771] update state cost 5.3882598876953125e-05 s
DEBUG 10-14 21:17:26 lpllm.py:1740] restore layer func cost 0.0013370513916015625 s
DEBUG 10-14 21:17:26 lpllm.py:511] restore layer cost 0.0019736289978027344 s
DEBUG 10-14 21:17:26 lpllm.py:512] reset_next_layer_need: layer_attn_idx=19, layer_mlp_idx=18, j_loc=37
DEBUG 10-14 21:17:26 lpllm.py:1034] reset layer cost 0.0021941661834716797 s
DEBUG 10-14 21:17:26 lpllm.py:1035] have reset next layer layer_attn 19 layer_mlp 18 
DEBUG 10-14 21:17:26 lpllm.py:1041] j: 37 waiting the layer with layer_idx 19 before wait time 0.21874094009399414 s
INFO 10-14 21:17:26 client.py:117] confirm_model_loaded: Mixtral-8x7B, 1f936a7c-d5be-434e-adfe-3e6414936d23
DEBUG 10-14 21:17:26 lpmodule.py:433] dot attn cost 0.062912 seconds
DEBUG 10-14 21:17:26 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.024245023727416992 s
DEBUG 10-14 21:17:26 lpllm.py:2280] CPU attn cost 0.119554 seconds if batch True
DEBUG 10-14 21:17:26 lpllm.py:2289] deal attn result cost 0.000002 seconds
DEBUG 10-14 21:17:26 lpllm.py:2291] CPU compute cost 0.120031 seconds
DEBUG 10-14 21:17:26 lpllm.py:2309] free cost 0.000097 seconds
INFO 10-14 21:17:26 client.py:125] Model loaded
DEBUG 10-14 21:17:26 lpllm.py:1045] j: load cost 0.3240354061126709 s waiting cost 0.10522270202636719 s
DEBUG 10-14 21:17:26 lpllm.py:921] 
DEBUG 10-14 21:17:26 lpllm.py:921] decoder loop j: 38 cur_layer_idx: 19 layer_attn_idx: 19 layer_mlp_idx: 18
DEBUG 10-14 21:17:26 lpllm.py:930] start load next layer cur_layer_idx: 20
DEBUG 10-14 21:17:26 lpllm.py:1801] cuda memory handles: {1: <capsule object NULL at 0x7fdf3c6da010>} {1: b'\xe0\xaa29\x00\x00\x00\x00\xe8&\x1d\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\x86\x00\x00\x00\xff\x00\x000\x00\x00\x00\x00\x00\x00\x00)\xd8\xd5\xc1\xa4\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-14 21:17:26 client.py:72] load_into_gpu: Mixtral-8x7B, 53b0453e-a471-467b-9940-a3f128bd154a
INFO 10-14 21:17:26 client.py:113] Model loaded: Mixtral-8x7B, 53b0453e-a471-467b-9940-a3f128bd154a
DEBUG 10-14 21:17:26 lpmodule.py:451] hidden_states.shape torch.Size([10, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:26 lpmodule.py:455] input_layernorm num_params: 1
DEBUG 10-14 21:17:26 lpmodule.py:457] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-14 21:17:26 lpmodule.py:498] kv_seq_len 512 past_key_value 0
DEBUG 10-14 21:17:26 lpmodule.py:504] inv_freq device cpu
DEBUG 10-14 21:17:26 lpmodule.py:505] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:26 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:26 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:26 lpmodule.py:505]         ...,
DEBUG 10-14 21:17:26 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:26 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:26 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-14 21:17:26 lpmodule.py:511] query_states device cuda:1
DEBUG 10-14 21:17:26 lpmodule.py:512] key_states device cuda:1
DEBUG 10-14 21:17:26 lpmodule.py:513] value_states device cuda:1
DEBUG 10-14 21:17:26 lpmodule.py:514] position_ids device cuda:1
DEBUG 10-14 21:17:26 lpmodule.py:515] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:26 lpmodule.py:516] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:26 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:26 lpllm.py:2262] GPU2CPU move cost 0.000604 seconds
DEBUG 10-14 21:17:26 lpmodule.py:340] layer_idx 19 attn layer_idx: 19
DEBUG 10-14 21:17:26 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-14 21:17:26 lpmodule.py:364] decoder_attn_batch update batch_dim 0-10
DEBUG 10-14 21:17:26 lpmodule.py:227] layer idx 18
DEBUG 10-14 21:17:26 StaticCacheLen.py:84] static cache update layer_idx: 19, start_batch: 0, end_batch: 10
DEBUG 10-14 21:17:26 lpmodule.py:368] update for kv cache 0-10 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-14 21:17:26 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:26 lpmodule.py:374] update past key value cost 0.007372 seconds
DEBUG 10-14 21:17:26 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 512, 128]), value shape torch.Size([10, 8, 512, 128])
DEBUG 10-14 21:17:26 lpmodule.py:399] repeat qkv cost 0.016153 seconds
DEBUG 10-14 21:17:26 lpmodule.py:400] q shape torch.Size([10, 32, 512, 128]), k shape torch.Size([10, 32, 512, 128]), v shape torch.Size([10, 32, 512, 128])
DEBUG 10-14 21:17:26 lpllm.py:503] reset update experts
DEBUG 10-14 21:17:26 lpllm.py:1771] update state cost 5.221366882324219e-05 s
DEBUG 10-14 21:17:26 lpllm.py:1740] restore layer func cost 0.003256559371948242 s
DEBUG 10-14 21:17:26 lpllm.py:511] restore layer cost 0.004178524017333984 s
DEBUG 10-14 21:17:26 lpllm.py:512] reset_next_layer_need: layer_attn_idx=19, layer_mlp_idx=19, j_loc=38
DEBUG 10-14 21:17:26 lpllm.py:1034] reset layer cost 0.00446009635925293 s
DEBUG 10-14 21:17:26 lpllm.py:1035] have reset next layer layer_attn 19 layer_mlp 19 
DEBUG 10-14 21:17:26 lpllm.py:921] 
DEBUG 10-14 21:17:26 lpllm.py:921] decoder loop j: 39 cur_layer_idx: 19 layer_attn_idx: 19 layer_mlp_idx: 19
DEBUG 10-14 21:17:26 lpmodule.py:451] hidden_states.shape torch.Size([10, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:26 lpmodule.py:455] input_layernorm num_params: 1
DEBUG 10-14 21:17:26 lpmodule.py:457] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-14 21:17:26 lpmodule.py:498] kv_seq_len 512 past_key_value 0
DEBUG 10-14 21:17:26 lpmodule.py:504] inv_freq device cpu
DEBUG 10-14 21:17:26 lpmodule.py:505] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:26 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:26 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:26 lpmodule.py:505]         ...,
DEBUG 10-14 21:17:26 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:26 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:26 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-14 21:17:26 lpmodule.py:511] query_states device cuda:1
DEBUG 10-14 21:17:26 lpmodule.py:512] key_states device cuda:1
DEBUG 10-14 21:17:26 lpmodule.py:513] value_states device cuda:1
DEBUG 10-14 21:17:26 lpmodule.py:514] position_ids device cuda:1
DEBUG 10-14 21:17:26 lpmodule.py:515] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:26 lpmodule.py:516] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:26 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:26 lpmodule.py:433] dot attn cost 0.065189 seconds
DEBUG 10-14 21:17:26 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.024271488189697266 s
DEBUG 10-14 21:17:26 lpllm.py:2280] CPU attn cost 0.120600 seconds if batch True
DEBUG 10-14 21:17:26 lpllm.py:2289] deal attn result cost 0.000002 seconds
DEBUG 10-14 21:17:26 lpllm.py:2291] CPU compute cost 0.121550 seconds
DEBUG 10-14 21:17:26 lpllm.py:2309] free cost 0.000095 seconds
DEBUG 10-14 21:17:26 lpllm.py:2262] GPU2CPU move cost 0.000286 seconds
DEBUG 10-14 21:17:26 lpmodule.py:340] layer_idx 19 attn layer_idx: 19
DEBUG 10-14 21:17:26 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-14 21:17:26 lpmodule.py:364] decoder_attn_batch update batch_dim 10-20
DEBUG 10-14 21:17:26 StaticCacheLen.py:84] static cache update layer_idx: 19, start_batch: 10, end_batch: 20
DEBUG 10-14 21:17:26 StaticCacheLen.py:90] static cache update layer_idx: 19, update seq_length to 512
DEBUG 10-14 21:17:26 lpmodule.py:368] update for kv cache 10-20 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-14 21:17:26 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:26 lpmodule.py:374] update past key value cost 0.006677 seconds
DEBUG 10-14 21:17:26 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 512, 128]), value shape torch.Size([10, 8, 512, 128])
DEBUG 10-14 21:17:26 lpmodule.py:227] layer idx 19
DEBUG 10-14 21:17:26 lpmodule.py:399] repeat qkv cost 0.013483 seconds
DEBUG 10-14 21:17:26 lpmodule.py:400] q shape torch.Size([10, 32, 512, 128]), k shape torch.Size([10, 32, 512, 128]), v shape torch.Size([10, 32, 512, 128])
DEBUG 10-14 21:17:26 lpllm.py:498] reset update attn
DEBUG 10-14 21:17:26 lpllm.py:1771] update state cost 7.343292236328125e-05 s
DEBUG 10-14 21:17:26 lpllm.py:1740] restore layer func cost 0.0015857219696044922 s
DEBUG 10-14 21:17:26 lpllm.py:511] restore layer cost 0.002276897430419922 s
DEBUG 10-14 21:17:26 lpllm.py:512] reset_next_layer_need: layer_attn_idx=20, layer_mlp_idx=19, j_loc=39
DEBUG 10-14 21:17:26 lpllm.py:1034] reset layer cost 0.0026161670684814453 s
DEBUG 10-14 21:17:26 lpllm.py:1035] have reset next layer layer_attn 20 layer_mlp 19 
DEBUG 10-14 21:17:26 lpllm.py:1041] j: 39 waiting the layer with layer_idx 20 before wait time 0.222243070602417 s
INFO 10-14 21:17:26 client.py:117] confirm_model_loaded: Mixtral-8x7B, 53b0453e-a471-467b-9940-a3f128bd154a
DEBUG 10-14 21:17:26 lpmodule.py:433] dot attn cost 0.065272 seconds
DEBUG 10-14 21:17:26 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.02479720115661621 s
DEBUG 10-14 21:17:26 lpllm.py:2280] CPU attn cost 0.118181 seconds if batch True
DEBUG 10-14 21:17:26 lpllm.py:2289] deal attn result cost 0.000003 seconds
DEBUG 10-14 21:17:26 lpllm.py:2291] CPU compute cost 0.118836 seconds
DEBUG 10-14 21:17:26 lpllm.py:2309] free cost 0.000160 seconds
INFO 10-14 21:17:26 client.py:125] Model loaded
DEBUG 10-14 21:17:26 lpllm.py:1045] j: load cost 0.29065465927124023 s waiting cost 0.06838202476501465 s
DEBUG 10-14 21:17:26 lpllm.py:921] 
DEBUG 10-14 21:17:26 lpllm.py:921] decoder loop j: 40 cur_layer_idx: 20 layer_attn_idx: 20 layer_mlp_idx: 19
DEBUG 10-14 21:17:26 lpllm.py:930] start load next layer cur_layer_idx: 21
DEBUG 10-14 21:17:26 lpllm.py:1801] cuda memory handles: {1: <capsule object NULL at 0x7fdf3c6d9ec0>} {1: b'\xe0\xaa29\x00\x00\x00\x00\xe8&\x1d\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\x86\x00\x00\x00\xff\x00\x00.\x00\x00\x00\x00\x00\x00\x00)\xd8\xd5\xc1\xa0\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-14 21:17:26 client.py:72] load_into_gpu: Mixtral-8x7B, 7797e0cc-7150-4c46-9cde-8fa265bd8c97
INFO 10-14 21:17:26 client.py:113] Model loaded: Mixtral-8x7B, 7797e0cc-7150-4c46-9cde-8fa265bd8c97
DEBUG 10-14 21:17:26 lpmodule.py:451] hidden_states.shape torch.Size([10, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:26 lpmodule.py:455] input_layernorm num_params: 1
DEBUG 10-14 21:17:26 lpmodule.py:457] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-14 21:17:26 lpmodule.py:498] kv_seq_len 512 past_key_value 0
DEBUG 10-14 21:17:26 lpmodule.py:504] inv_freq device cpu
DEBUG 10-14 21:17:26 lpmodule.py:505] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:26 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:26 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:26 lpmodule.py:505]         ...,
DEBUG 10-14 21:17:26 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:26 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:26 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-14 21:17:26 lpmodule.py:511] query_states device cuda:1
DEBUG 10-14 21:17:26 lpmodule.py:512] key_states device cuda:1
DEBUG 10-14 21:17:26 lpmodule.py:513] value_states device cuda:1
DEBUG 10-14 21:17:26 lpmodule.py:514] position_ids device cuda:1
DEBUG 10-14 21:17:26 lpmodule.py:515] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:26 lpmodule.py:516] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:26 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:26 lpllm.py:2262] GPU2CPU move cost 0.000740 seconds
DEBUG 10-14 21:17:26 lpmodule.py:340] layer_idx 20 attn layer_idx: 20
DEBUG 10-14 21:17:26 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-14 21:17:26 lpmodule.py:364] decoder_attn_batch update batch_dim 0-10
DEBUG 10-14 21:17:26 lpmodule.py:227] layer idx 19
DEBUG 10-14 21:17:26 StaticCacheLen.py:84] static cache update layer_idx: 20, start_batch: 0, end_batch: 10
DEBUG 10-14 21:17:26 lpmodule.py:368] update for kv cache 0-10 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-14 21:17:26 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:26 lpmodule.py:374] update past key value cost 0.006715 seconds
DEBUG 10-14 21:17:26 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 512, 128]), value shape torch.Size([10, 8, 512, 128])
DEBUG 10-14 21:17:26 lpmodule.py:399] repeat qkv cost 0.012747 seconds
DEBUG 10-14 21:17:26 lpmodule.py:400] q shape torch.Size([10, 32, 512, 128]), k shape torch.Size([10, 32, 512, 128]), v shape torch.Size([10, 32, 512, 128])
DEBUG 10-14 21:17:26 lpllm.py:503] reset update experts
DEBUG 10-14 21:17:26 lpllm.py:1771] update state cost 4.291534423828125e-05 s
DEBUG 10-14 21:17:26 lpllm.py:1740] restore layer func cost 0.002877950668334961 s
DEBUG 10-14 21:17:26 lpllm.py:511] restore layer cost 0.0033991336822509766 s
DEBUG 10-14 21:17:26 lpllm.py:512] reset_next_layer_need: layer_attn_idx=20, layer_mlp_idx=20, j_loc=40
DEBUG 10-14 21:17:26 lpllm.py:1034] reset layer cost 0.003605365753173828 s
DEBUG 10-14 21:17:26 lpllm.py:1035] have reset next layer layer_attn 20 layer_mlp 20 
DEBUG 10-14 21:17:26 lpllm.py:921] 
DEBUG 10-14 21:17:26 lpllm.py:921] decoder loop j: 41 cur_layer_idx: 20 layer_attn_idx: 20 layer_mlp_idx: 20
DEBUG 10-14 21:17:26 lpmodule.py:451] hidden_states.shape torch.Size([10, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:26 lpmodule.py:455] input_layernorm num_params: 1
DEBUG 10-14 21:17:26 lpmodule.py:457] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-14 21:17:26 lpmodule.py:433] dot attn cost 0.066299 seconds
DEBUG 10-14 21:17:26 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-14 21:17:26 lpmodule.py:498] kv_seq_len 512 past_key_value 0
DEBUG 10-14 21:17:26 lpmodule.py:504] inv_freq device cpu
time cost move to cuda:1 0.024138450622558594 s
DEBUG 10-14 21:17:26 lpmodule.py:505] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:26 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:26 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:26 lpmodule.py:505]         ...,
DEBUG 10-14 21:17:26 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:26 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:26 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-14 21:17:26 lpmodule.py:511] query_states device cuda:1
DEBUG 10-14 21:17:26 lpmodule.py:512] key_states device cuda:1
DEBUG 10-14 21:17:26 lpmodule.py:513] value_states device cuda:1
DEBUG 10-14 21:17:26 lpmodule.py:514] position_ids device cuda:1
DEBUG 10-14 21:17:26 lpmodule.py:515] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:26 lpmodule.py:516] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:26 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:26 lpllm.py:2280] CPU attn cost 0.117950 seconds if batch True
DEBUG 10-14 21:17:26 lpllm.py:2289] deal attn result cost 0.000002 seconds
DEBUG 10-14 21:17:26 lpllm.py:2291] CPU compute cost 0.119034 seconds
DEBUG 10-14 21:17:26 lpllm.py:2309] free cost 0.000111 seconds
DEBUG 10-14 21:17:26 lpllm.py:2262] GPU2CPU move cost 0.000335 seconds
DEBUG 10-14 21:17:26 lpmodule.py:340] layer_idx 20 attn layer_idx: 20
DEBUG 10-14 21:17:26 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-14 21:17:26 lpmodule.py:364] decoder_attn_batch update batch_dim 10-20
DEBUG 10-14 21:17:26 StaticCacheLen.py:84] static cache update layer_idx: 20, start_batch: 10, end_batch: 20
DEBUG 10-14 21:17:26 StaticCacheLen.py:90] static cache update layer_idx: 20, update seq_length to 512
DEBUG 10-14 21:17:26 lpmodule.py:368] update for kv cache 10-20 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-14 21:17:26 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:26 lpmodule.py:374] update past key value cost 0.005555 seconds
DEBUG 10-14 21:17:26 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 512, 128]), value shape torch.Size([10, 8, 512, 128])
DEBUG 10-14 21:17:26 lpmodule.py:227] layer idx 20
DEBUG 10-14 21:17:26 lpmodule.py:399] repeat qkv cost 0.011702 seconds
DEBUG 10-14 21:17:26 lpmodule.py:400] q shape torch.Size([10, 32, 512, 128]), k shape torch.Size([10, 32, 512, 128]), v shape torch.Size([10, 32, 512, 128])
DEBUG 10-14 21:17:26 lpllm.py:498] reset update attn
DEBUG 10-14 21:17:26 lpllm.py:1771] update state cost 4.76837158203125e-05 s
DEBUG 10-14 21:17:26 lpllm.py:1740] restore layer func cost 0.0013802051544189453 s
DEBUG 10-14 21:17:26 lpllm.py:511] restore layer cost 0.0019931793212890625 s
DEBUG 10-14 21:17:26 lpllm.py:512] reset_next_layer_need: layer_attn_idx=21, layer_mlp_idx=20, j_loc=41
DEBUG 10-14 21:17:26 lpllm.py:1034] reset layer cost 0.002240896224975586 s
DEBUG 10-14 21:17:26 lpllm.py:1035] have reset next layer layer_attn 21 layer_mlp 20 
DEBUG 10-14 21:17:26 lpllm.py:1041] j: 41 waiting the layer with layer_idx 21 before wait time 0.22071123123168945 s
INFO 10-14 21:17:26 client.py:117] confirm_model_loaded: Mixtral-8x7B, 7797e0cc-7150-4c46-9cde-8fa265bd8c97
DEBUG 10-14 21:17:26 lpmodule.py:433] dot attn cost 0.062364 seconds
DEBUG 10-14 21:17:26 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.024500131607055664 s
DEBUG 10-14 21:17:26 lpllm.py:2280] CPU attn cost 0.110993 seconds if batch True
DEBUG 10-14 21:17:26 lpllm.py:2289] deal attn result cost 0.000002 seconds
DEBUG 10-14 21:17:26 lpllm.py:2291] CPU compute cost 0.111537 seconds
DEBUG 10-14 21:17:26 lpllm.py:2309] free cost 0.000101 seconds
INFO 10-14 21:17:26 client.py:125] Model loaded
DEBUG 10-14 21:17:26 lpllm.py:1045] j: load cost 0.32216811180114746 s waiting cost 0.10139966011047363 s
DEBUG 10-14 21:17:26 lpllm.py:921] 
DEBUG 10-14 21:17:26 lpllm.py:921] decoder loop j: 42 cur_layer_idx: 21 layer_attn_idx: 21 layer_mlp_idx: 20
DEBUG 10-14 21:17:26 lpllm.py:930] start load next layer cur_layer_idx: 22
DEBUG 10-14 21:17:26 lpllm.py:1801] cuda memory handles: {1: <capsule object NULL at 0x7fdf3c6d9f80>} {1: b'\xe0\xaa29\x00\x00\x00\x00\xe8&\x1d\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\x86\x00\x00\x00\xff\x00\x00/\x00\x00\x00\x00\x00\x00\x00)\xd8\xd5\xc1\xa2\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-14 21:17:26 client.py:72] load_into_gpu: Mixtral-8x7B, bc59c792-2360-4855-8cdb-2fb37ab89b9d
INFO 10-14 21:17:26 client.py:113] Model loaded: Mixtral-8x7B, bc59c792-2360-4855-8cdb-2fb37ab89b9d
DEBUG 10-14 21:17:26 lpmodule.py:451] hidden_states.shape torch.Size([10, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:26 lpmodule.py:455] input_layernorm num_params: 1
DEBUG 10-14 21:17:26 lpmodule.py:457] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-14 21:17:26 lpmodule.py:498] kv_seq_len 512 past_key_value 0
DEBUG 10-14 21:17:26 lpmodule.py:504] inv_freq device cpu
DEBUG 10-14 21:17:26 lpmodule.py:505] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:26 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:26 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:26 lpmodule.py:505]         ...,
DEBUG 10-14 21:17:26 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:26 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:26 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-14 21:17:26 lpmodule.py:511] query_states device cuda:1
DEBUG 10-14 21:17:26 lpmodule.py:512] key_states device cuda:1
DEBUG 10-14 21:17:26 lpmodule.py:513] value_states device cuda:1
DEBUG 10-14 21:17:26 lpmodule.py:514] position_ids device cuda:1
DEBUG 10-14 21:17:26 lpmodule.py:515] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:26 lpmodule.py:516] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:26 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:26 lpllm.py:2262] GPU2CPU move cost 0.000755 seconds
DEBUG 10-14 21:17:26 lpmodule.py:340] layer_idx 21 attn layer_idx: 21
DEBUG 10-14 21:17:26 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-14 21:17:26 lpmodule.py:364] decoder_attn_batch update batch_dim 0-10
DEBUG 10-14 21:17:26 lpmodule.py:227] layer idx 20
DEBUG 10-14 21:17:26 StaticCacheLen.py:84] static cache update layer_idx: 21, start_batch: 0, end_batch: 10
DEBUG 10-14 21:17:26 lpmodule.py:368] update for kv cache 0-10 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-14 21:17:26 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:26 lpmodule.py:374] update past key value cost 0.006315 seconds
DEBUG 10-14 21:17:26 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 512, 128]), value shape torch.Size([10, 8, 512, 128])
DEBUG 10-14 21:17:26 lpmodule.py:399] repeat qkv cost 0.012140 seconds
DEBUG 10-14 21:17:26 lpmodule.py:400] q shape torch.Size([10, 32, 512, 128]), k shape torch.Size([10, 32, 512, 128]), v shape torch.Size([10, 32, 512, 128])
DEBUG 10-14 21:17:26 lpllm.py:503] reset update experts
DEBUG 10-14 21:17:26 lpllm.py:1771] update state cost 4.696846008300781e-05 s
DEBUG 10-14 21:17:26 lpllm.py:1740] restore layer func cost 0.0031735897064208984 s
DEBUG 10-14 21:17:26 lpllm.py:511] restore layer cost 0.003764629364013672 s
DEBUG 10-14 21:17:26 lpllm.py:512] reset_next_layer_need: layer_attn_idx=21, layer_mlp_idx=21, j_loc=42
DEBUG 10-14 21:17:26 lpllm.py:1034] reset layer cost 0.004040241241455078 s
DEBUG 10-14 21:17:26 lpllm.py:1035] have reset next layer layer_attn 21 layer_mlp 21 
DEBUG 10-14 21:17:26 lpllm.py:921] 
DEBUG 10-14 21:17:26 lpllm.py:921] decoder loop j: 43 cur_layer_idx: 21 layer_attn_idx: 21 layer_mlp_idx: 21
DEBUG 10-14 21:17:26 lpmodule.py:451] hidden_states.shape torch.Size([10, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:26 lpmodule.py:455] input_layernorm num_params: 1
DEBUG 10-14 21:17:26 lpmodule.py:457] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-14 21:17:26 lpmodule.py:433] dot attn cost 0.065187 seconds
DEBUG 10-14 21:17:26 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-14 21:17:26 lpmodule.py:498] kv_seq_len 512 past_key_value 0
DEBUG 10-14 21:17:26 lpmodule.py:504] inv_freq device cpu
DEBUG 10-14 21:17:26 lpmodule.py:505] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:26 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:26 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:26 lpmodule.py:505]         ...,
DEBUG 10-14 21:17:26 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:26 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:26 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-14 21:17:26 lpmodule.py:511] query_states device cuda:1
DEBUG 10-14 21:17:26 lpmodule.py:512] key_states device cuda:1
time cost move to cuda:1 0.01634812355041504 s
DEBUG 10-14 21:17:26 lpmodule.py:513] value_states device cuda:1
DEBUG 10-14 21:17:26 lpmodule.py:514] position_ids device cuda:1
DEBUG 10-14 21:17:26 lpmodule.py:515] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:26 lpmodule.py:516] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:26 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:26 lpllm.py:2280] CPU attn cost 0.107338 seconds if batch True
DEBUG 10-14 21:17:26 lpllm.py:2289] deal attn result cost 0.000002 seconds
DEBUG 10-14 21:17:26 lpllm.py:2291] CPU compute cost 0.108471 seconds
DEBUG 10-14 21:17:26 lpllm.py:2309] free cost 0.000093 seconds
DEBUG 10-14 21:17:26 lpllm.py:2262] GPU2CPU move cost 0.000265 seconds
DEBUG 10-14 21:17:26 lpmodule.py:340] layer_idx 21 attn layer_idx: 21
DEBUG 10-14 21:17:26 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-14 21:17:26 lpmodule.py:364] decoder_attn_batch update batch_dim 10-20
DEBUG 10-14 21:17:26 StaticCacheLen.py:84] static cache update layer_idx: 21, start_batch: 10, end_batch: 20
DEBUG 10-14 21:17:26 StaticCacheLen.py:90] static cache update layer_idx: 21, update seq_length to 512
DEBUG 10-14 21:17:26 lpmodule.py:368] update for kv cache 10-20 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-14 21:17:26 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:26 lpmodule.py:374] update past key value cost 0.005427 seconds
DEBUG 10-14 21:17:26 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 512, 128]), value shape torch.Size([10, 8, 512, 128])
DEBUG 10-14 21:17:26 lpmodule.py:227] layer idx 21
DEBUG 10-14 21:17:27 lpmodule.py:399] repeat qkv cost 0.011749 seconds
DEBUG 10-14 21:17:27 lpmodule.py:400] q shape torch.Size([10, 32, 512, 128]), k shape torch.Size([10, 32, 512, 128]), v shape torch.Size([10, 32, 512, 128])
DEBUG 10-14 21:17:27 lpllm.py:498] reset update attn
DEBUG 10-14 21:17:27 lpllm.py:1771] update state cost 4.9114227294921875e-05 s
DEBUG 10-14 21:17:27 lpllm.py:1740] restore layer func cost 0.0013630390167236328 s
DEBUG 10-14 21:17:27 lpllm.py:511] restore layer cost 0.0019314289093017578 s
DEBUG 10-14 21:17:27 lpllm.py:512] reset_next_layer_need: layer_attn_idx=22, layer_mlp_idx=21, j_loc=43
DEBUG 10-14 21:17:27 lpllm.py:1034] reset layer cost 0.002123117446899414 s
DEBUG 10-14 21:17:27 lpllm.py:1035] have reset next layer layer_attn 22 layer_mlp 21 
DEBUG 10-14 21:17:27 lpllm.py:1041] j: 43 waiting the layer with layer_idx 22 before wait time 0.2520895004272461 s
INFO 10-14 21:17:27 client.py:117] confirm_model_loaded: Mixtral-8x7B, bc59c792-2360-4855-8cdb-2fb37ab89b9d
DEBUG 10-14 21:17:27 lpmodule.py:433] dot attn cost 0.064372 seconds
DEBUG 10-14 21:17:27 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.024123430252075195 s
DEBUG 10-14 21:17:27 lpllm.py:2280] CPU attn cost 0.112452 seconds if batch True
DEBUG 10-14 21:17:27 lpllm.py:2289] deal attn result cost 0.000002 seconds
DEBUG 10-14 21:17:27 lpllm.py:2291] CPU compute cost 0.112916 seconds
DEBUG 10-14 21:17:27 lpllm.py:2309] free cost 0.000092 seconds
INFO 10-14 21:17:27 client.py:125] Model loaded
DEBUG 10-14 21:17:27 lpllm.py:1045] j: load cost 0.40726780891418457 s waiting cost 0.15514874458312988 s
DEBUG 10-14 21:17:27 lpllm.py:921] 
DEBUG 10-14 21:17:27 lpllm.py:921] decoder loop j: 44 cur_layer_idx: 22 layer_attn_idx: 22 layer_mlp_idx: 21
DEBUG 10-14 21:17:27 lpllm.py:930] start load next layer cur_layer_idx: 23
DEBUG 10-14 21:17:27 lpllm.py:1801] cuda memory handles: {1: <capsule object NULL at 0x7fdf3c6da010>} {1: b'\xe0\xaa29\x00\x00\x00\x00\xe8&\x1d\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\x86\x00\x00\x00\xff\x00\x000\x00\x00\x00\x00\x00\x00\x00)\xd8\xd5\xc1\xa4\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-14 21:17:27 client.py:72] load_into_gpu: Mixtral-8x7B, f7410f63-2bbe-4ac0-969a-751295cfb66c
INFO 10-14 21:17:27 client.py:113] Model loaded: Mixtral-8x7B, f7410f63-2bbe-4ac0-969a-751295cfb66c
DEBUG 10-14 21:17:27 lpmodule.py:451] hidden_states.shape torch.Size([10, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:27 lpmodule.py:455] input_layernorm num_params: 1
DEBUG 10-14 21:17:27 lpmodule.py:457] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-14 21:17:27 lpmodule.py:498] kv_seq_len 512 past_key_value 0
DEBUG 10-14 21:17:27 lpmodule.py:504] inv_freq device cpu
DEBUG 10-14 21:17:27 lpmodule.py:505] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:27 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:27 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:27 lpmodule.py:505]         ...,
DEBUG 10-14 21:17:27 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:27 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:27 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-14 21:17:27 lpmodule.py:511] query_states device cuda:1
DEBUG 10-14 21:17:27 lpmodule.py:512] key_states device cuda:1
DEBUG 10-14 21:17:27 lpmodule.py:513] value_states device cuda:1
DEBUG 10-14 21:17:27 lpmodule.py:514] position_ids device cuda:1
DEBUG 10-14 21:17:27 lpmodule.py:515] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:27 lpmodule.py:516] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:27 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:27 lpllm.py:2262] GPU2CPU move cost 0.000828 seconds
DEBUG 10-14 21:17:27 lpmodule.py:340] layer_idx 22 attn layer_idx: 22
DEBUG 10-14 21:17:27 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-14 21:17:27 lpmodule.py:364] decoder_attn_batch update batch_dim 0-10
DEBUG 10-14 21:17:27 lpmodule.py:227] layer idx 21
DEBUG 10-14 21:17:27 StaticCacheLen.py:84] static cache update layer_idx: 22, start_batch: 0, end_batch: 10
DEBUG 10-14 21:17:27 lpmodule.py:368] update for kv cache 0-10 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-14 21:17:27 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:27 lpmodule.py:374] update past key value cost 0.006860 seconds
DEBUG 10-14 21:17:27 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 512, 128]), value shape torch.Size([10, 8, 512, 128])
DEBUG 10-14 21:17:27 lpmodule.py:399] repeat qkv cost 0.013159 seconds
DEBUG 10-14 21:17:27 lpmodule.py:400] q shape torch.Size([10, 32, 512, 128]), k shape torch.Size([10, 32, 512, 128]), v shape torch.Size([10, 32, 512, 128])
DEBUG 10-14 21:17:27 lpllm.py:503] reset update experts
DEBUG 10-14 21:17:27 lpllm.py:1771] update state cost 2.288818359375e-05 s
DEBUG 10-14 21:17:27 lpllm.py:1740] restore layer func cost 0.0012698173522949219 s
DEBUG 10-14 21:17:27 lpllm.py:511] restore layer cost 0.001581430435180664 s
DEBUG 10-14 21:17:27 lpllm.py:512] reset_next_layer_need: layer_attn_idx=22, layer_mlp_idx=22, j_loc=44
DEBUG 10-14 21:17:27 lpllm.py:1034] reset layer cost 0.0016989707946777344 s
DEBUG 10-14 21:17:27 lpllm.py:1035] have reset next layer layer_attn 22 layer_mlp 22 
DEBUG 10-14 21:17:27 lpllm.py:921] 
DEBUG 10-14 21:17:27 lpllm.py:921] decoder loop j: 45 cur_layer_idx: 22 layer_attn_idx: 22 layer_mlp_idx: 22
DEBUG 10-14 21:17:27 lpmodule.py:451] hidden_states.shape torch.Size([10, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:27 lpmodule.py:455] input_layernorm num_params: 1
DEBUG 10-14 21:17:27 lpmodule.py:457] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-14 21:17:27 lpmodule.py:433] dot attn cost 0.061600 seconds
DEBUG 10-14 21:17:27 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-14 21:17:27 lpmodule.py:498] kv_seq_len 512 past_key_value 0
DEBUG 10-14 21:17:27 lpmodule.py:504] inv_freq device cpu
DEBUG 10-14 21:17:27 lpmodule.py:505] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:27 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:27 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:27 lpmodule.py:505]         ...,
DEBUG 10-14 21:17:27 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:27 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:27 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511]])
time cost move to cuda:1 0.024768590927124023 s
DEBUG 10-14 21:17:27 lpmodule.py:511] query_states device cuda:1
DEBUG 10-14 21:17:27 lpmodule.py:512] key_states device cuda:1
DEBUG 10-14 21:17:27 lpmodule.py:513] value_states device cuda:1
DEBUG 10-14 21:17:27 lpmodule.py:514] position_ids device cuda:1
DEBUG 10-14 21:17:27 lpmodule.py:515] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:27 lpmodule.py:516] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:27 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:27 lpllm.py:2280] CPU attn cost 0.113837 seconds if batch True
DEBUG 10-14 21:17:27 lpllm.py:2289] deal attn result cost 0.000002 seconds
DEBUG 10-14 21:17:27 lpllm.py:2291] CPU compute cost 0.115018 seconds
DEBUG 10-14 21:17:27 lpllm.py:2309] free cost 0.000085 seconds
DEBUG 10-14 21:17:27 lpllm.py:2262] GPU2CPU move cost 0.000269 seconds
DEBUG 10-14 21:17:27 lpmodule.py:340] layer_idx 22 attn layer_idx: 22
DEBUG 10-14 21:17:27 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-14 21:17:27 lpmodule.py:364] decoder_attn_batch update batch_dim 10-20
DEBUG 10-14 21:17:27 StaticCacheLen.py:84] static cache update layer_idx: 22, start_batch: 10, end_batch: 20
DEBUG 10-14 21:17:27 StaticCacheLen.py:90] static cache update layer_idx: 22, update seq_length to 512
DEBUG 10-14 21:17:27 lpmodule.py:368] update for kv cache 10-20 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-14 21:17:27 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:27 lpmodule.py:374] update past key value cost 0.006594 seconds
DEBUG 10-14 21:17:27 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 512, 128]), value shape torch.Size([10, 8, 512, 128])
DEBUG 10-14 21:17:27 lpmodule.py:227] layer idx 22
DEBUG 10-14 21:17:27 lpmodule.py:399] repeat qkv cost 0.012045 seconds
DEBUG 10-14 21:17:27 lpmodule.py:400] q shape torch.Size([10, 32, 512, 128]), k shape torch.Size([10, 32, 512, 128]), v shape torch.Size([10, 32, 512, 128])
DEBUG 10-14 21:17:27 lpllm.py:498] reset update attn
DEBUG 10-14 21:17:27 lpllm.py:1771] update state cost 5.602836608886719e-05 s
DEBUG 10-14 21:17:27 lpllm.py:1740] restore layer func cost 0.0013473033905029297 s
DEBUG 10-14 21:17:27 lpllm.py:511] restore layer cost 0.0019588470458984375 s
DEBUG 10-14 21:17:27 lpllm.py:512] reset_next_layer_need: layer_attn_idx=23, layer_mlp_idx=22, j_loc=45
DEBUG 10-14 21:17:27 lpllm.py:1034] reset layer cost 0.002203702926635742 s
DEBUG 10-14 21:17:27 lpllm.py:1035] have reset next layer layer_attn 23 layer_mlp 22 
DEBUG 10-14 21:17:27 lpllm.py:1041] j: 45 waiting the layer with layer_idx 23 before wait time 0.2169349193572998 s
INFO 10-14 21:17:27 client.py:117] confirm_model_loaded: Mixtral-8x7B, f7410f63-2bbe-4ac0-969a-751295cfb66c
DEBUG 10-14 21:17:27 lpmodule.py:433] dot attn cost 0.063614 seconds
DEBUG 10-14 21:17:27 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.02480626106262207 s
DEBUG 10-14 21:17:27 lpllm.py:2280] CPU attn cost 0.114174 seconds if batch True
DEBUG 10-14 21:17:27 lpllm.py:2289] deal attn result cost 0.000002 seconds
DEBUG 10-14 21:17:27 lpllm.py:2291] CPU compute cost 0.114634 seconds
DEBUG 10-14 21:17:27 lpllm.py:2309] free cost 0.000094 seconds
INFO 10-14 21:17:27 client.py:125] Model loaded
DEBUG 10-14 21:17:27 lpllm.py:1045] j: load cost 0.3352017402648926 s waiting cost 0.1182091236114502 s
DEBUG 10-14 21:17:27 lpllm.py:921] 
DEBUG 10-14 21:17:27 lpllm.py:921] decoder loop j: 46 cur_layer_idx: 23 layer_attn_idx: 23 layer_mlp_idx: 22
DEBUG 10-14 21:17:27 lpllm.py:930] start load next layer cur_layer_idx: 24
DEBUG 10-14 21:17:27 lpllm.py:1801] cuda memory handles: {1: <capsule object NULL at 0x7fdf3c6d9ec0>} {1: b'\xe0\xaa29\x00\x00\x00\x00\xe8&\x1d\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\x86\x00\x00\x00\xff\x00\x00.\x00\x00\x00\x00\x00\x00\x00)\xd8\xd5\xc1\xa0\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-14 21:17:27 client.py:72] load_into_gpu: Mixtral-8x7B, b6d480bf-0823-470f-a0b7-6c62d0504376
INFO 10-14 21:17:27 client.py:113] Model loaded: Mixtral-8x7B, b6d480bf-0823-470f-a0b7-6c62d0504376
DEBUG 10-14 21:17:27 lpmodule.py:451] hidden_states.shape torch.Size([10, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:27 lpmodule.py:455] input_layernorm num_params: 1
DEBUG 10-14 21:17:27 lpmodule.py:457] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-14 21:17:27 lpmodule.py:498] kv_seq_len 512 past_key_value 0
DEBUG 10-14 21:17:27 lpmodule.py:504] inv_freq device cpu
DEBUG 10-14 21:17:27 lpmodule.py:505] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:27 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:27 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:27 lpmodule.py:505]         ...,
DEBUG 10-14 21:17:27 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:27 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:27 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-14 21:17:27 lpmodule.py:511] query_states device cuda:1
DEBUG 10-14 21:17:27 lpmodule.py:512] key_states device cuda:1
DEBUG 10-14 21:17:27 lpmodule.py:513] value_states device cuda:1
DEBUG 10-14 21:17:27 lpmodule.py:514] position_ids device cuda:1
DEBUG 10-14 21:17:27 lpmodule.py:515] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:27 lpmodule.py:516] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:27 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:27 lpllm.py:2262] GPU2CPU move cost 0.000661 seconds
DEBUG 10-14 21:17:27 lpmodule.py:340] layer_idx 23 attn layer_idx: 23
DEBUG 10-14 21:17:27 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-14 21:17:27 lpmodule.py:364] decoder_attn_batch update batch_dim 0-10
DEBUG 10-14 21:17:27 lpmodule.py:227] layer idx 22
DEBUG 10-14 21:17:27 StaticCacheLen.py:84] static cache update layer_idx: 23, start_batch: 0, end_batch: 10
DEBUG 10-14 21:17:27 lpmodule.py:368] update for kv cache 0-10 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-14 21:17:27 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:27 lpmodule.py:374] update past key value cost 0.007614 seconds
DEBUG 10-14 21:17:27 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 512, 128]), value shape torch.Size([10, 8, 512, 128])
DEBUG 10-14 21:17:27 lpmodule.py:399] repeat qkv cost 0.013252 seconds
DEBUG 10-14 21:17:27 lpmodule.py:400] q shape torch.Size([10, 32, 512, 128]), k shape torch.Size([10, 32, 512, 128]), v shape torch.Size([10, 32, 512, 128])
DEBUG 10-14 21:17:27 lpllm.py:503] reset update experts
DEBUG 10-14 21:17:27 lpllm.py:1771] update state cost 4.3392181396484375e-05 s
DEBUG 10-14 21:17:27 lpllm.py:1740] restore layer func cost 0.003179311752319336 s
DEBUG 10-14 21:17:27 lpllm.py:511] restore layer cost 0.0037860870361328125 s
DEBUG 10-14 21:17:27 lpllm.py:512] reset_next_layer_need: layer_attn_idx=23, layer_mlp_idx=23, j_loc=46
DEBUG 10-14 21:17:27 lpllm.py:1034] reset layer cost 0.0039844512939453125 s
DEBUG 10-14 21:17:27 lpllm.py:1035] have reset next layer layer_attn 23 layer_mlp 23 
DEBUG 10-14 21:17:27 lpllm.py:921] 
DEBUG 10-14 21:17:27 lpllm.py:921] decoder loop j: 47 cur_layer_idx: 23 layer_attn_idx: 23 layer_mlp_idx: 23
DEBUG 10-14 21:17:27 lpmodule.py:451] hidden_states.shape torch.Size([10, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:27 lpmodule.py:455] input_layernorm num_params: 1
DEBUG 10-14 21:17:27 lpmodule.py:457] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-14 21:17:27 lpmodule.py:498] kv_seq_len 512 past_key_value 0
DEBUG 10-14 21:17:27 lpmodule.py:504] inv_freq device cpu
DEBUG 10-14 21:17:27 lpmodule.py:505] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:27 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:27 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:27 lpmodule.py:505]         ...,
DEBUG 10-14 21:17:27 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:27 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:27 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-14 21:17:27 lpmodule.py:433] dot attn cost 0.061356 seconds
DEBUG 10-14 21:17:27 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-14 21:17:27 lpmodule.py:511] query_states device cuda:1
DEBUG 10-14 21:17:27 lpmodule.py:512] key_states device cuda:1
DEBUG 10-14 21:17:27 lpmodule.py:513] value_states device cuda:1
DEBUG 10-14 21:17:27 lpmodule.py:514] position_ids device cuda:1
DEBUG 10-14 21:17:27 lpmodule.py:515] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:27 lpmodule.py:516] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:27 lpllm.py:2081] waiting out_queue
time cost move to cuda:1 0.02450728416442871 s
DEBUG 10-14 21:17:27 lpllm.py:2280] CPU attn cost 0.114514 seconds if batch True
DEBUG 10-14 21:17:27 lpllm.py:2289] deal attn result cost 0.000002 seconds
DEBUG 10-14 21:17:27 lpllm.py:2291] CPU compute cost 0.115510 seconds
DEBUG 10-14 21:17:27 lpllm.py:2309] free cost 0.000225 seconds
DEBUG 10-14 21:17:27 lpllm.py:2262] GPU2CPU move cost 0.000347 seconds
DEBUG 10-14 21:17:27 lpmodule.py:340] layer_idx 23 attn layer_idx: 23
DEBUG 10-14 21:17:27 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-14 21:17:27 lpmodule.py:364] decoder_attn_batch update batch_dim 10-20
DEBUG 10-14 21:17:27 StaticCacheLen.py:84] static cache update layer_idx: 23, start_batch: 10, end_batch: 20
DEBUG 10-14 21:17:27 StaticCacheLen.py:90] static cache update layer_idx: 23, update seq_length to 512
DEBUG 10-14 21:17:27 lpmodule.py:368] update for kv cache 10-20 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-14 21:17:27 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:27 lpmodule.py:374] update past key value cost 0.006650 seconds
DEBUG 10-14 21:17:27 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 512, 128]), value shape torch.Size([10, 8, 512, 128])
DEBUG 10-14 21:17:27 lpmodule.py:227] layer idx 23
DEBUG 10-14 21:17:27 lpmodule.py:399] repeat qkv cost 0.012739 seconds
DEBUG 10-14 21:17:27 lpmodule.py:400] q shape torch.Size([10, 32, 512, 128]), k shape torch.Size([10, 32, 512, 128]), v shape torch.Size([10, 32, 512, 128])
DEBUG 10-14 21:17:27 lpllm.py:498] reset update attn
DEBUG 10-14 21:17:27 lpllm.py:1771] update state cost 4.9114227294921875e-05 s
DEBUG 10-14 21:17:27 lpllm.py:1740] restore layer func cost 0.0008397102355957031 s
DEBUG 10-14 21:17:27 lpllm.py:511] restore layer cost 0.0013277530670166016 s
DEBUG 10-14 21:17:27 lpllm.py:512] reset_next_layer_need: layer_attn_idx=24, layer_mlp_idx=23, j_loc=47
DEBUG 10-14 21:17:27 lpllm.py:1034] reset layer cost 0.0014789104461669922 s
DEBUG 10-14 21:17:27 lpllm.py:1035] have reset next layer layer_attn 24 layer_mlp 23 
DEBUG 10-14 21:17:27 lpllm.py:1041] j: 47 waiting the layer with layer_idx 24 before wait time 0.2176203727722168 s
INFO 10-14 21:17:27 client.py:117] confirm_model_loaded: Mixtral-8x7B, b6d480bf-0823-470f-a0b7-6c62d0504376
DEBUG 10-14 21:17:27 lpmodule.py:433] dot attn cost 0.061541 seconds
DEBUG 10-14 21:17:27 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.024973392486572266 s
DEBUG 10-14 21:17:27 lpllm.py:2280] CPU attn cost 0.113045 seconds if batch True
DEBUG 10-14 21:17:27 lpllm.py:2289] deal attn result cost 0.000001 seconds
DEBUG 10-14 21:17:27 lpllm.py:2291] CPU compute cost 0.113590 seconds
DEBUG 10-14 21:17:27 lpllm.py:2309] free cost 0.000093 seconds
INFO 10-14 21:17:27 client.py:125] Model loaded
DEBUG 10-14 21:17:27 lpllm.py:1045] j: load cost 0.3445398807525635 s waiting cost 0.12690210342407227 s
DEBUG 10-14 21:17:27 lpllm.py:921] 
DEBUG 10-14 21:17:27 lpllm.py:921] decoder loop j: 48 cur_layer_idx: 24 layer_attn_idx: 24 layer_mlp_idx: 23
DEBUG 10-14 21:17:27 lpllm.py:930] start load next layer cur_layer_idx: 25
DEBUG 10-14 21:17:27 lpllm.py:1801] cuda memory handles: {1: <capsule object NULL at 0x7fdf3c6d9f80>} {1: b'\xe0\xaa29\x00\x00\x00\x00\xe8&\x1d\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\x86\x00\x00\x00\xff\x00\x00/\x00\x00\x00\x00\x00\x00\x00)\xd8\xd5\xc1\xa2\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-14 21:17:27 client.py:72] load_into_gpu: Mixtral-8x7B, 6b742d0c-9b08-479e-9a6e-379962fada91
INFO 10-14 21:17:27 client.py:113] Model loaded: Mixtral-8x7B, 6b742d0c-9b08-479e-9a6e-379962fada91
DEBUG 10-14 21:17:27 lpmodule.py:451] hidden_states.shape torch.Size([10, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:27 lpmodule.py:455] input_layernorm num_params: 1
DEBUG 10-14 21:17:27 lpmodule.py:457] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-14 21:17:27 lpmodule.py:498] kv_seq_len 512 past_key_value 0
DEBUG 10-14 21:17:27 lpmodule.py:504] inv_freq device cpu
DEBUG 10-14 21:17:27 lpmodule.py:505] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:27 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:27 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:27 lpmodule.py:505]         ...,
DEBUG 10-14 21:17:27 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:27 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:27 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-14 21:17:27 lpmodule.py:511] query_states device cuda:1
DEBUG 10-14 21:17:27 lpmodule.py:512] key_states device cuda:1
DEBUG 10-14 21:17:27 lpmodule.py:513] value_states device cuda:1
DEBUG 10-14 21:17:27 lpmodule.py:514] position_ids device cuda:1
DEBUG 10-14 21:17:27 lpmodule.py:515] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:27 lpmodule.py:516] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:27 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:27 lpllm.py:2262] GPU2CPU move cost 0.000708 seconds
DEBUG 10-14 21:17:27 lpmodule.py:340] layer_idx 24 attn layer_idx: 24
DEBUG 10-14 21:17:27 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-14 21:17:27 lpmodule.py:364] decoder_attn_batch update batch_dim 0-10
DEBUG 10-14 21:17:27 lpmodule.py:227] layer idx 23
DEBUG 10-14 21:17:27 StaticCacheLen.py:84] static cache update layer_idx: 24, start_batch: 0, end_batch: 10
DEBUG 10-14 21:17:27 lpmodule.py:368] update for kv cache 0-10 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-14 21:17:27 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:27 lpmodule.py:374] update past key value cost 0.008045 seconds
DEBUG 10-14 21:17:27 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 512, 128]), value shape torch.Size([10, 8, 512, 128])
DEBUG 10-14 21:17:27 lpmodule.py:399] repeat qkv cost 0.012261 seconds
DEBUG 10-14 21:17:27 lpmodule.py:400] q shape torch.Size([10, 32, 512, 128]), k shape torch.Size([10, 32, 512, 128]), v shape torch.Size([10, 32, 512, 128])
DEBUG 10-14 21:17:27 lpllm.py:503] reset update experts
DEBUG 10-14 21:17:27 lpllm.py:1771] update state cost 2.9802322387695312e-05 s
DEBUG 10-14 21:17:27 lpllm.py:1740] restore layer func cost 0.0018470287322998047 s
DEBUG 10-14 21:17:27 lpllm.py:511] restore layer cost 0.0022666454315185547 s
DEBUG 10-14 21:17:27 lpllm.py:512] reset_next_layer_need: layer_attn_idx=24, layer_mlp_idx=24, j_loc=48
DEBUG 10-14 21:17:27 lpllm.py:1034] reset layer cost 0.0024247169494628906 s
DEBUG 10-14 21:17:27 lpllm.py:1035] have reset next layer layer_attn 24 layer_mlp 24 
DEBUG 10-14 21:17:27 lpllm.py:921] 
DEBUG 10-14 21:17:27 lpllm.py:921] decoder loop j: 49 cur_layer_idx: 24 layer_attn_idx: 24 layer_mlp_idx: 24
DEBUG 10-14 21:17:27 lpmodule.py:451] hidden_states.shape torch.Size([10, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:27 lpmodule.py:455] input_layernorm num_params: 1
DEBUG 10-14 21:17:27 lpmodule.py:457] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-14 21:17:28 lpmodule.py:498] kv_seq_len 512 past_key_value 0
DEBUG 10-14 21:17:28 lpmodule.py:504] inv_freq device cpu
DEBUG 10-14 21:17:28 lpmodule.py:505] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:28 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:28 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:28 lpmodule.py:505]         ...,
DEBUG 10-14 21:17:28 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:28 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:28 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-14 21:17:28 lpmodule.py:511] query_states device cuda:1
DEBUG 10-14 21:17:28 lpmodule.py:512] key_states device cuda:1
DEBUG 10-14 21:17:28 lpmodule.py:513] value_states device cuda:1
DEBUG 10-14 21:17:28 lpmodule.py:514] position_ids device cuda:1
DEBUG 10-14 21:17:28 lpmodule.py:515] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:28 lpmodule.py:516] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:28 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:28 lpmodule.py:433] dot attn cost 0.065170 seconds
DEBUG 10-14 21:17:28 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.024486064910888672 s
DEBUG 10-14 21:17:28 lpllm.py:2280] CPU attn cost 0.117144 seconds if batch True
DEBUG 10-14 21:17:28 lpllm.py:2289] deal attn result cost 0.000002 seconds
DEBUG 10-14 21:17:28 lpllm.py:2291] CPU compute cost 0.118254 seconds
DEBUG 10-14 21:17:28 lpllm.py:2309] free cost 0.000130 seconds
DEBUG 10-14 21:17:28 lpllm.py:2262] GPU2CPU move cost 0.000405 seconds
DEBUG 10-14 21:17:28 lpmodule.py:340] layer_idx 24 attn layer_idx: 24
DEBUG 10-14 21:17:28 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-14 21:17:28 lpmodule.py:364] decoder_attn_batch update batch_dim 10-20
DEBUG 10-14 21:17:28 StaticCacheLen.py:84] static cache update layer_idx: 24, start_batch: 10, end_batch: 20
DEBUG 10-14 21:17:28 StaticCacheLen.py:90] static cache update layer_idx: 24, update seq_length to 512
DEBUG 10-14 21:17:28 lpmodule.py:368] update for kv cache 10-20 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-14 21:17:28 lpmodule.py:227] layer idx 24
DEBUG 10-14 21:17:28 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:28 lpmodule.py:374] update past key value cost 0.007079 seconds
DEBUG 10-14 21:17:28 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 512, 128]), value shape torch.Size([10, 8, 512, 128])
DEBUG 10-14 21:17:28 lpmodule.py:399] repeat qkv cost 0.012228 seconds
DEBUG 10-14 21:17:28 lpmodule.py:400] q shape torch.Size([10, 32, 512, 128]), k shape torch.Size([10, 32, 512, 128]), v shape torch.Size([10, 32, 512, 128])
DEBUG 10-14 21:17:28 lpllm.py:498] reset update attn
DEBUG 10-14 21:17:28 lpllm.py:1771] update state cost 4.863739013671875e-05 s
DEBUG 10-14 21:17:28 lpllm.py:1740] restore layer func cost 0.0013966560363769531 s
DEBUG 10-14 21:17:28 lpllm.py:511] restore layer cost 0.0020074844360351562 s
DEBUG 10-14 21:17:28 lpllm.py:512] reset_next_layer_need: layer_attn_idx=25, layer_mlp_idx=24, j_loc=49
DEBUG 10-14 21:17:28 lpllm.py:1034] reset layer cost 0.0022449493408203125 s
DEBUG 10-14 21:17:28 lpllm.py:1035] have reset next layer layer_attn 25 layer_mlp 24 
DEBUG 10-14 21:17:28 lpllm.py:1041] j: 49 waiting the layer with layer_idx 25 before wait time 0.2172834873199463 s
INFO 10-14 21:17:28 client.py:117] confirm_model_loaded: Mixtral-8x7B, 6b742d0c-9b08-479e-9a6e-379962fada91
DEBUG 10-14 21:17:28 lpmodule.py:433] dot attn cost 0.064632 seconds
DEBUG 10-14 21:17:28 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.024610280990600586 s
DEBUG 10-14 21:17:28 lpllm.py:2280] CPU attn cost 0.116002 seconds if batch True
DEBUG 10-14 21:17:28 lpllm.py:2289] deal attn result cost 0.000002 seconds
DEBUG 10-14 21:17:28 lpllm.py:2291] CPU compute cost 0.116671 seconds
DEBUG 10-14 21:17:28 lpllm.py:2309] free cost 0.000097 seconds
INFO 10-14 21:17:28 client.py:125] Model loaded
DEBUG 10-14 21:17:28 lpllm.py:1045] j: load cost 0.33876848220825195 s waiting cost 0.12142801284790039 s
DEBUG 10-14 21:17:28 lpllm.py:921] 
DEBUG 10-14 21:17:28 lpllm.py:921] decoder loop j: 50 cur_layer_idx: 25 layer_attn_idx: 25 layer_mlp_idx: 24
DEBUG 10-14 21:17:28 lpllm.py:930] start load next layer cur_layer_idx: 26
DEBUG 10-14 21:17:28 lpllm.py:1801] cuda memory handles: {1: <capsule object NULL at 0x7fdf3c6da010>} {1: b'\xe0\xaa29\x00\x00\x00\x00\xe8&\x1d\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\x86\x00\x00\x00\xff\x00\x000\x00\x00\x00\x00\x00\x00\x00)\xd8\xd5\xc1\xa4\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-14 21:17:28 client.py:72] load_into_gpu: Mixtral-8x7B, e5582ef5-f46e-4e93-af25-9b5f3fe58393
INFO 10-14 21:17:28 client.py:113] Model loaded: Mixtral-8x7B, e5582ef5-f46e-4e93-af25-9b5f3fe58393
DEBUG 10-14 21:17:28 lpmodule.py:451] hidden_states.shape torch.Size([10, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:28 lpmodule.py:455] input_layernorm num_params: 1
DEBUG 10-14 21:17:28 lpmodule.py:457] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-14 21:17:28 lpmodule.py:498] kv_seq_len 512 past_key_value 0
DEBUG 10-14 21:17:28 lpmodule.py:504] inv_freq device cpu
DEBUG 10-14 21:17:28 lpmodule.py:505] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:28 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:28 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:28 lpmodule.py:505]         ...,
DEBUG 10-14 21:17:28 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:28 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:28 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-14 21:17:28 lpmodule.py:511] query_states device cuda:1
DEBUG 10-14 21:17:28 lpmodule.py:512] key_states device cuda:1
DEBUG 10-14 21:17:28 lpmodule.py:513] value_states device cuda:1
DEBUG 10-14 21:17:28 lpmodule.py:514] position_ids device cuda:1
DEBUG 10-14 21:17:28 lpmodule.py:515] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:28 lpmodule.py:516] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:28 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:28 lpllm.py:2262] GPU2CPU move cost 0.000713 seconds
DEBUG 10-14 21:17:28 lpmodule.py:340] layer_idx 25 attn layer_idx: 25
DEBUG 10-14 21:17:28 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-14 21:17:28 lpmodule.py:364] decoder_attn_batch update batch_dim 0-10
DEBUG 10-14 21:17:28 lpmodule.py:227] layer idx 24
DEBUG 10-14 21:17:28 StaticCacheLen.py:84] static cache update layer_idx: 25, start_batch: 0, end_batch: 10
DEBUG 10-14 21:17:28 lpmodule.py:368] update for kv cache 0-10 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-14 21:17:28 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:28 lpmodule.py:374] update past key value cost 0.007497 seconds
DEBUG 10-14 21:17:28 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 512, 128]), value shape torch.Size([10, 8, 512, 128])
DEBUG 10-14 21:17:28 lpmodule.py:399] repeat qkv cost 0.011809 seconds
DEBUG 10-14 21:17:28 lpmodule.py:400] q shape torch.Size([10, 32, 512, 128]), k shape torch.Size([10, 32, 512, 128]), v shape torch.Size([10, 32, 512, 128])
DEBUG 10-14 21:17:28 lpllm.py:503] reset update experts
DEBUG 10-14 21:17:28 lpllm.py:1771] update state cost 4.076957702636719e-05 s
DEBUG 10-14 21:17:28 lpllm.py:1740] restore layer func cost 0.0021631717681884766 s
DEBUG 10-14 21:17:28 lpllm.py:511] restore layer cost 0.0028159618377685547 s
DEBUG 10-14 21:17:28 lpllm.py:512] reset_next_layer_need: layer_attn_idx=25, layer_mlp_idx=25, j_loc=50
DEBUG 10-14 21:17:28 lpllm.py:1034] reset layer cost 0.002972841262817383 s
DEBUG 10-14 21:17:28 lpllm.py:1035] have reset next layer layer_attn 25 layer_mlp 25 
DEBUG 10-14 21:17:28 lpllm.py:921] 
DEBUG 10-14 21:17:28 lpllm.py:921] decoder loop j: 51 cur_layer_idx: 25 layer_attn_idx: 25 layer_mlp_idx: 25
DEBUG 10-14 21:17:28 lpmodule.py:451] hidden_states.shape torch.Size([10, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:28 lpmodule.py:455] input_layernorm num_params: 1
DEBUG 10-14 21:17:28 lpmodule.py:457] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-14 21:17:28 lpmodule.py:498] kv_seq_len 512 past_key_value 0
DEBUG 10-14 21:17:28 lpmodule.py:504] inv_freq device cpu
DEBUG 10-14 21:17:28 lpmodule.py:505] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:28 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:28 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:28 lpmodule.py:505]         ...,
DEBUG 10-14 21:17:28 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:28 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:28 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-14 21:17:28 lpmodule.py:511] query_states device cuda:1
DEBUG 10-14 21:17:28 lpmodule.py:512] key_states device cuda:1
DEBUG 10-14 21:17:28 lpmodule.py:513] value_states device cuda:1
DEBUG 10-14 21:17:28 lpmodule.py:514] position_ids device cuda:1
DEBUG 10-14 21:17:28 lpmodule.py:515] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:28 lpmodule.py:516] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:28 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:28 lpmodule.py:433] dot attn cost 0.066871 seconds
DEBUG 10-14 21:17:28 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.01372528076171875 s
DEBUG 10-14 21:17:28 lpllm.py:2280] CPU attn cost 0.107267 seconds if batch True
DEBUG 10-14 21:17:28 lpllm.py:2289] deal attn result cost 0.000002 seconds
DEBUG 10-14 21:17:28 lpllm.py:2291] CPU compute cost 0.108387 seconds
DEBUG 10-14 21:17:28 lpllm.py:2309] free cost 0.000113 seconds
DEBUG 10-14 21:17:28 lpllm.py:2262] GPU2CPU move cost 0.000305 seconds
DEBUG 10-14 21:17:28 lpmodule.py:340] layer_idx 25 attn layer_idx: 25
DEBUG 10-14 21:17:28 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-14 21:17:28 lpmodule.py:364] decoder_attn_batch update batch_dim 10-20
DEBUG 10-14 21:17:28 lpmodule.py:227] layer idx 25
DEBUG 10-14 21:17:28 StaticCacheLen.py:84] static cache update layer_idx: 25, start_batch: 10, end_batch: 20
DEBUG 10-14 21:17:28 StaticCacheLen.py:90] static cache update layer_idx: 25, update seq_length to 512
DEBUG 10-14 21:17:28 lpmodule.py:368] update for kv cache 10-20 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-14 21:17:28 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:28 lpmodule.py:374] update past key value cost 0.007338 seconds
DEBUG 10-14 21:17:28 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 512, 128]), value shape torch.Size([10, 8, 512, 128])
DEBUG 10-14 21:17:28 lpmodule.py:399] repeat qkv cost 0.011286 seconds
DEBUG 10-14 21:17:28 lpmodule.py:400] q shape torch.Size([10, 32, 512, 128]), k shape torch.Size([10, 32, 512, 128]), v shape torch.Size([10, 32, 512, 128])
DEBUG 10-14 21:17:28 lpllm.py:498] reset update attn
DEBUG 10-14 21:17:28 lpllm.py:1771] update state cost 2.6941299438476562e-05 s
DEBUG 10-14 21:17:28 lpllm.py:1740] restore layer func cost 0.000637054443359375 s
DEBUG 10-14 21:17:28 lpllm.py:511] restore layer cost 0.0009891986846923828 s
DEBUG 10-14 21:17:28 lpllm.py:512] reset_next_layer_need: layer_attn_idx=26, layer_mlp_idx=25, j_loc=51
DEBUG 10-14 21:17:28 lpllm.py:1034] reset layer cost 0.0011518001556396484 s
DEBUG 10-14 21:17:28 lpllm.py:1035] have reset next layer layer_attn 26 layer_mlp 25 
DEBUG 10-14 21:17:28 lpllm.py:1041] j: 51 waiting the layer with layer_idx 26 before wait time 0.23133325576782227 s
INFO 10-14 21:17:28 client.py:117] confirm_model_loaded: Mixtral-8x7B, e5582ef5-f46e-4e93-af25-9b5f3fe58393
DEBUG 10-14 21:17:28 lpmodule.py:433] dot attn cost 0.066588 seconds
DEBUG 10-14 21:17:28 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.0256807804107666 s
DEBUG 10-14 21:17:28 lpllm.py:2280] CPU attn cost 0.117472 seconds if batch True
DEBUG 10-14 21:17:28 lpllm.py:2289] deal attn result cost 0.000002 seconds
DEBUG 10-14 21:17:28 lpllm.py:2291] CPU compute cost 0.118012 seconds
DEBUG 10-14 21:17:28 lpllm.py:2309] free cost 0.000100 seconds
INFO 10-14 21:17:28 client.py:125] Model loaded
DEBUG 10-14 21:17:28 lpllm.py:1045] j: load cost 0.43001389503479004 s waiting cost 0.19865846633911133 s
DEBUG 10-14 21:17:28 lpllm.py:921] 
DEBUG 10-14 21:17:28 lpllm.py:921] decoder loop j: 52 cur_layer_idx: 26 layer_attn_idx: 26 layer_mlp_idx: 25
DEBUG 10-14 21:17:28 lpllm.py:930] start load next layer cur_layer_idx: 27
DEBUG 10-14 21:17:28 lpllm.py:1801] cuda memory handles: {1: <capsule object NULL at 0x7fdf3c6d9ec0>} {1: b'\xe0\xaa29\x00\x00\x00\x00\xe8&\x1d\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\x86\x00\x00\x00\xff\x00\x00.\x00\x00\x00\x00\x00\x00\x00)\xd8\xd5\xc1\xa0\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-14 21:17:28 client.py:72] load_into_gpu: Mixtral-8x7B, 5a442991-86b7-4124-ad51-d37dbac4a388
INFO 10-14 21:17:28 client.py:113] Model loaded: Mixtral-8x7B, 5a442991-86b7-4124-ad51-d37dbac4a388
DEBUG 10-14 21:17:28 lpmodule.py:451] hidden_states.shape torch.Size([10, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:28 lpmodule.py:455] input_layernorm num_params: 1
DEBUG 10-14 21:17:28 lpmodule.py:457] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-14 21:17:28 lpmodule.py:498] kv_seq_len 512 past_key_value 0
DEBUG 10-14 21:17:28 lpmodule.py:504] inv_freq device cpu
DEBUG 10-14 21:17:28 lpmodule.py:505] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:28 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:28 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:28 lpmodule.py:505]         ...,
DEBUG 10-14 21:17:28 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:28 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:28 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-14 21:17:28 lpmodule.py:511] query_states device cuda:1
DEBUG 10-14 21:17:28 lpmodule.py:512] key_states device cuda:1
DEBUG 10-14 21:17:28 lpmodule.py:513] value_states device cuda:1
DEBUG 10-14 21:17:28 lpmodule.py:514] position_ids device cuda:1
DEBUG 10-14 21:17:28 lpmodule.py:515] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:28 lpmodule.py:516] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:28 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:28 lpllm.py:2262] GPU2CPU move cost 0.000395 seconds
DEBUG 10-14 21:17:28 lpmodule.py:340] layer_idx 26 attn layer_idx: 26
DEBUG 10-14 21:17:28 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-14 21:17:28 lpmodule.py:364] decoder_attn_batch update batch_dim 0-10
DEBUG 10-14 21:17:28 lpmodule.py:227] layer idx 25
DEBUG 10-14 21:17:28 StaticCacheLen.py:84] static cache update layer_idx: 26, start_batch: 0, end_batch: 10
DEBUG 10-14 21:17:28 lpmodule.py:368] update for kv cache 0-10 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-14 21:17:28 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:28 lpmodule.py:374] update past key value cost 0.007329 seconds
DEBUG 10-14 21:17:28 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 512, 128]), value shape torch.Size([10, 8, 512, 128])
DEBUG 10-14 21:17:28 lpmodule.py:399] repeat qkv cost 0.012027 seconds
DEBUG 10-14 21:17:28 lpmodule.py:400] q shape torch.Size([10, 32, 512, 128]), k shape torch.Size([10, 32, 512, 128]), v shape torch.Size([10, 32, 512, 128])
DEBUG 10-14 21:17:28 lpllm.py:503] reset update experts
DEBUG 10-14 21:17:28 lpllm.py:1771] update state cost 3.0517578125e-05 s
DEBUG 10-14 21:17:28 lpllm.py:1740] restore layer func cost 0.0019004344940185547 s
DEBUG 10-14 21:17:28 lpllm.py:511] restore layer cost 0.002364635467529297 s
DEBUG 10-14 21:17:28 lpllm.py:512] reset_next_layer_need: layer_attn_idx=26, layer_mlp_idx=26, j_loc=52
DEBUG 10-14 21:17:28 lpllm.py:1034] reset layer cost 0.0025281906127929688 s
DEBUG 10-14 21:17:28 lpllm.py:1035] have reset next layer layer_attn 26 layer_mlp 26 
DEBUG 10-14 21:17:28 lpllm.py:921] 
DEBUG 10-14 21:17:28 lpllm.py:921] decoder loop j: 53 cur_layer_idx: 26 layer_attn_idx: 26 layer_mlp_idx: 26
DEBUG 10-14 21:17:28 lpmodule.py:451] hidden_states.shape torch.Size([10, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:28 lpmodule.py:455] input_layernorm num_params: 1
DEBUG 10-14 21:17:28 lpmodule.py:457] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-14 21:17:28 lpmodule.py:498] kv_seq_len 512 past_key_value 0
DEBUG 10-14 21:17:28 lpmodule.py:504] inv_freq device cpu
DEBUG 10-14 21:17:28 lpmodule.py:505] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:28 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:28 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:28 lpmodule.py:505]         ...,
DEBUG 10-14 21:17:28 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:28 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:28 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-14 21:17:28 lpmodule.py:433] dot attn cost 0.063933 seconds
DEBUG 10-14 21:17:28 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-14 21:17:28 lpmodule.py:511] query_states device cuda:1
DEBUG 10-14 21:17:28 lpmodule.py:512] key_states device cuda:1
DEBUG 10-14 21:17:28 lpmodule.py:513] value_states device cuda:1
DEBUG 10-14 21:17:28 lpmodule.py:514] position_ids device cuda:1
DEBUG 10-14 21:17:28 lpmodule.py:515] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:28 lpmodule.py:516] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:28 lpllm.py:2081] waiting out_queue
time cost move to cuda:1 0.024941444396972656 s
DEBUG 10-14 21:17:28 lpllm.py:2280] CPU attn cost 0.115301 seconds if batch True
DEBUG 10-14 21:17:28 lpllm.py:2289] deal attn result cost 0.000001 seconds
DEBUG 10-14 21:17:28 lpllm.py:2291] CPU compute cost 0.115952 seconds
DEBUG 10-14 21:17:28 lpllm.py:2309] free cost 0.000084 seconds
DEBUG 10-14 21:17:28 lpllm.py:2262] GPU2CPU move cost 0.000263 seconds
DEBUG 10-14 21:17:28 lpmodule.py:340] layer_idx 26 attn layer_idx: 26
DEBUG 10-14 21:17:28 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-14 21:17:28 lpmodule.py:364] decoder_attn_batch update batch_dim 10-20
DEBUG 10-14 21:17:28 StaticCacheLen.py:84] static cache update layer_idx: 26, start_batch: 10, end_batch: 20
DEBUG 10-14 21:17:28 StaticCacheLen.py:90] static cache update layer_idx: 26, update seq_length to 512
DEBUG 10-14 21:17:28 lpmodule.py:368] update for kv cache 10-20 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-14 21:17:28 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:28 lpmodule.py:374] update past key value cost 0.006242 seconds
DEBUG 10-14 21:17:28 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 512, 128]), value shape torch.Size([10, 8, 512, 128])
DEBUG 10-14 21:17:28 lpmodule.py:227] layer idx 26
DEBUG 10-14 21:17:28 lpmodule.py:399] repeat qkv cost 0.011959 seconds
DEBUG 10-14 21:17:28 lpmodule.py:400] q shape torch.Size([10, 32, 512, 128]), k shape torch.Size([10, 32, 512, 128]), v shape torch.Size([10, 32, 512, 128])
DEBUG 10-14 21:17:28 lpllm.py:498] reset update attn
DEBUG 10-14 21:17:28 lpllm.py:1771] update state cost 3.838539123535156e-05 s
DEBUG 10-14 21:17:28 lpllm.py:1740] restore layer func cost 0.0009160041809082031 s
DEBUG 10-14 21:17:28 lpllm.py:511] restore layer cost 0.0013375282287597656 s
DEBUG 10-14 21:17:28 lpllm.py:512] reset_next_layer_need: layer_attn_idx=27, layer_mlp_idx=26, j_loc=53
DEBUG 10-14 21:17:28 lpllm.py:1034] reset layer cost 0.0014982223510742188 s
DEBUG 10-14 21:17:28 lpllm.py:1035] have reset next layer layer_attn 27 layer_mlp 26 
DEBUG 10-14 21:17:28 lpllm.py:1041] j: 53 waiting the layer with layer_idx 27 before wait time 0.21735668182373047 s
INFO 10-14 21:17:28 client.py:117] confirm_model_loaded: Mixtral-8x7B, 5a442991-86b7-4124-ad51-d37dbac4a388
DEBUG 10-14 21:17:28 lpmodule.py:433] dot attn cost 0.067984 seconds
DEBUG 10-14 21:17:28 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.024690628051757812 s
DEBUG 10-14 21:17:28 lpllm.py:2280] CPU attn cost 0.117703 seconds if batch True
DEBUG 10-14 21:17:28 lpllm.py:2289] deal attn result cost 0.000002 seconds
DEBUG 10-14 21:17:28 lpllm.py:2291] CPU compute cost 0.118184 seconds
DEBUG 10-14 21:17:28 lpllm.py:2309] free cost 0.000096 seconds
INFO 10-14 21:17:29 client.py:125] Model loaded
DEBUG 10-14 21:17:29 lpllm.py:1045] j: load cost 0.34143948554992676 s waiting cost 0.1240544319152832 s
DEBUG 10-14 21:17:29 lpllm.py:921] 
DEBUG 10-14 21:17:29 lpllm.py:921] decoder loop j: 54 cur_layer_idx: 27 layer_attn_idx: 27 layer_mlp_idx: 26
DEBUG 10-14 21:17:29 lpllm.py:930] start load next layer cur_layer_idx: 28
DEBUG 10-14 21:17:29 lpllm.py:1801] cuda memory handles: {1: <capsule object NULL at 0x7fdf3c6d9f80>} {1: b'\xe0\xaa29\x00\x00\x00\x00\xe8&\x1d\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\x86\x00\x00\x00\xff\x00\x00/\x00\x00\x00\x00\x00\x00\x00)\xd8\xd5\xc1\xa2\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-14 21:17:29 client.py:72] load_into_gpu: Mixtral-8x7B, ae4a1821-7cbc-4256-9620-16b9a3d34aa6
INFO 10-14 21:17:29 client.py:113] Model loaded: Mixtral-8x7B, ae4a1821-7cbc-4256-9620-16b9a3d34aa6
DEBUG 10-14 21:17:29 lpmodule.py:451] hidden_states.shape torch.Size([10, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:29 lpmodule.py:455] input_layernorm num_params: 1
DEBUG 10-14 21:17:29 lpmodule.py:457] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-14 21:17:29 lpmodule.py:498] kv_seq_len 512 past_key_value 0
DEBUG 10-14 21:17:29 lpmodule.py:504] inv_freq device cpu
DEBUG 10-14 21:17:29 lpmodule.py:505] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:29 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:29 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:29 lpmodule.py:505]         ...,
DEBUG 10-14 21:17:29 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:29 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:29 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-14 21:17:29 lpmodule.py:511] query_states device cuda:1
DEBUG 10-14 21:17:29 lpmodule.py:512] key_states device cuda:1
DEBUG 10-14 21:17:29 lpmodule.py:513] value_states device cuda:1
DEBUG 10-14 21:17:29 lpmodule.py:514] position_ids device cuda:1
DEBUG 10-14 21:17:29 lpmodule.py:515] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:29 lpmodule.py:516] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:29 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:29 lpllm.py:2262] GPU2CPU move cost 0.000704 seconds
DEBUG 10-14 21:17:29 lpmodule.py:340] layer_idx 27 attn layer_idx: 27
DEBUG 10-14 21:17:29 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-14 21:17:29 lpmodule.py:364] decoder_attn_batch update batch_dim 0-10
DEBUG 10-14 21:17:29 lpmodule.py:227] layer idx 26
DEBUG 10-14 21:17:29 StaticCacheLen.py:84] static cache update layer_idx: 27, start_batch: 0, end_batch: 10
DEBUG 10-14 21:17:29 lpmodule.py:368] update for kv cache 0-10 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-14 21:17:29 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:29 lpmodule.py:374] update past key value cost 0.006713 seconds
DEBUG 10-14 21:17:29 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 512, 128]), value shape torch.Size([10, 8, 512, 128])
DEBUG 10-14 21:17:29 lpmodule.py:399] repeat qkv cost 0.013332 seconds
DEBUG 10-14 21:17:29 lpmodule.py:400] q shape torch.Size([10, 32, 512, 128]), k shape torch.Size([10, 32, 512, 128]), v shape torch.Size([10, 32, 512, 128])
DEBUG 10-14 21:17:29 lpllm.py:503] reset update experts
DEBUG 10-14 21:17:29 lpllm.py:1771] update state cost 4.029273986816406e-05 s
DEBUG 10-14 21:17:29 lpllm.py:1740] restore layer func cost 0.002697467803955078 s
DEBUG 10-14 21:17:29 lpllm.py:511] restore layer cost 0.0032417774200439453 s
DEBUG 10-14 21:17:29 lpllm.py:512] reset_next_layer_need: layer_attn_idx=27, layer_mlp_idx=27, j_loc=54
DEBUG 10-14 21:17:29 lpllm.py:1034] reset layer cost 0.003405332565307617 s
DEBUG 10-14 21:17:29 lpllm.py:1035] have reset next layer layer_attn 27 layer_mlp 27 
DEBUG 10-14 21:17:29 lpllm.py:921] 
DEBUG 10-14 21:17:29 lpllm.py:921] decoder loop j: 55 cur_layer_idx: 27 layer_attn_idx: 27 layer_mlp_idx: 27
DEBUG 10-14 21:17:29 lpmodule.py:451] hidden_states.shape torch.Size([10, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:29 lpmodule.py:455] input_layernorm num_params: 1
DEBUG 10-14 21:17:29 lpmodule.py:457] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-14 21:17:29 lpmodule.py:498] kv_seq_len 512 past_key_value 0
DEBUG 10-14 21:17:29 lpmodule.py:504] inv_freq device cpu
DEBUG 10-14 21:17:29 lpmodule.py:433] dot attn cost 0.060673 seconds
DEBUG 10-14 21:17:29 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-14 21:17:29 lpmodule.py:505] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:29 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:29 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:29 lpmodule.py:505]         ...,
DEBUG 10-14 21:17:29 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:29 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:29 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-14 21:17:29 lpmodule.py:511] query_states device cuda:1
DEBUG 10-14 21:17:29 lpmodule.py:512] key_states device cuda:1
DEBUG 10-14 21:17:29 lpmodule.py:513] value_states device cuda:1
DEBUG 10-14 21:17:29 lpmodule.py:514] position_ids device cuda:1
DEBUG 10-14 21:17:29 lpmodule.py:515] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:29 lpmodule.py:516] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:29 lpllm.py:2081] waiting out_queue
time cost move to cuda:1 0.02451634407043457 s
DEBUG 10-14 21:17:29 lpllm.py:2280] CPU attn cost 0.113132 seconds if batch True
DEBUG 10-14 21:17:29 lpllm.py:2289] deal attn result cost 0.000002 seconds
DEBUG 10-14 21:17:29 lpllm.py:2291] CPU compute cost 0.114192 seconds
DEBUG 10-14 21:17:29 lpllm.py:2309] free cost 0.000242 seconds
DEBUG 10-14 21:17:29 lpllm.py:2262] GPU2CPU move cost 0.000273 seconds
DEBUG 10-14 21:17:29 lpmodule.py:340] layer_idx 27 attn layer_idx: 27
DEBUG 10-14 21:17:29 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-14 21:17:29 lpmodule.py:364] decoder_attn_batch update batch_dim 10-20
DEBUG 10-14 21:17:29 StaticCacheLen.py:84] static cache update layer_idx: 27, start_batch: 10, end_batch: 20
DEBUG 10-14 21:17:29 StaticCacheLen.py:90] static cache update layer_idx: 27, update seq_length to 512
DEBUG 10-14 21:17:29 lpmodule.py:368] update for kv cache 10-20 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-14 21:17:29 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:29 lpmodule.py:374] update past key value cost 0.006068 seconds
DEBUG 10-14 21:17:29 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 512, 128]), value shape torch.Size([10, 8, 512, 128])
DEBUG 10-14 21:17:29 lpmodule.py:227] layer idx 27
DEBUG 10-14 21:17:29 lpmodule.py:399] repeat qkv cost 0.012501 seconds
DEBUG 10-14 21:17:29 lpmodule.py:400] q shape torch.Size([10, 32, 512, 128]), k shape torch.Size([10, 32, 512, 128]), v shape torch.Size([10, 32, 512, 128])
DEBUG 10-14 21:17:29 lpllm.py:498] reset update attn
DEBUG 10-14 21:17:29 lpllm.py:1771] update state cost 2.6702880859375e-05 s
DEBUG 10-14 21:17:29 lpllm.py:1740] restore layer func cost 0.000591278076171875 s
DEBUG 10-14 21:17:29 lpllm.py:511] restore layer cost 0.0009088516235351562 s
DEBUG 10-14 21:17:29 lpllm.py:512] reset_next_layer_need: layer_attn_idx=28, layer_mlp_idx=27, j_loc=55
DEBUG 10-14 21:17:29 lpllm.py:1034] reset layer cost 0.0010287761688232422 s
DEBUG 10-14 21:17:29 lpllm.py:1035] have reset next layer layer_attn 28 layer_mlp 27 
DEBUG 10-14 21:17:29 lpllm.py:1041] j: 55 waiting the layer with layer_idx 28 before wait time 0.2162158489227295 s
INFO 10-14 21:17:29 client.py:117] confirm_model_loaded: Mixtral-8x7B, ae4a1821-7cbc-4256-9620-16b9a3d34aa6
DEBUG 10-14 21:17:29 lpmodule.py:433] dot attn cost 0.060344 seconds
DEBUG 10-14 21:17:29 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.02414536476135254 s
DEBUG 10-14 21:17:29 lpllm.py:2280] CPU attn cost 0.110191 seconds if batch True
DEBUG 10-14 21:17:29 lpllm.py:2289] deal attn result cost 0.000002 seconds
DEBUG 10-14 21:17:29 lpllm.py:2291] CPU compute cost 0.110652 seconds
DEBUG 10-14 21:17:29 lpllm.py:2309] free cost 0.000089 seconds
INFO 10-14 21:17:29 client.py:125] Model loaded
DEBUG 10-14 21:17:29 lpllm.py:1045] j: load cost 0.3488137722015381 s waiting cost 0.13258051872253418 s
DEBUG 10-14 21:17:29 lpllm.py:921] 
DEBUG 10-14 21:17:29 lpllm.py:921] decoder loop j: 56 cur_layer_idx: 28 layer_attn_idx: 28 layer_mlp_idx: 27
DEBUG 10-14 21:17:29 lpllm.py:930] start load next layer cur_layer_idx: 29
DEBUG 10-14 21:17:29 lpllm.py:1801] cuda memory handles: {1: <capsule object NULL at 0x7fdf3c6da010>} {1: b'\xe0\xaa29\x00\x00\x00\x00\xe8&\x1d\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\x86\x00\x00\x00\xff\x00\x000\x00\x00\x00\x00\x00\x00\x00)\xd8\xd5\xc1\xa4\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-14 21:17:29 client.py:72] load_into_gpu: Mixtral-8x7B, 44e9f9ef-4deb-4a7e-a7b5-d9f06d809e30
INFO 10-14 21:17:29 client.py:113] Model loaded: Mixtral-8x7B, 44e9f9ef-4deb-4a7e-a7b5-d9f06d809e30
DEBUG 10-14 21:17:29 lpmodule.py:451] hidden_states.shape torch.Size([10, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:29 lpmodule.py:455] input_layernorm num_params: 1
DEBUG 10-14 21:17:29 lpmodule.py:457] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-14 21:17:29 lpmodule.py:498] kv_seq_len 512 past_key_value 0
DEBUG 10-14 21:17:29 lpmodule.py:504] inv_freq device cpu
DEBUG 10-14 21:17:29 lpmodule.py:505] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:29 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:29 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:29 lpmodule.py:505]         ...,
DEBUG 10-14 21:17:29 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:29 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:29 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-14 21:17:29 lpmodule.py:511] query_states device cuda:1
DEBUG 10-14 21:17:29 lpmodule.py:512] key_states device cuda:1
DEBUG 10-14 21:17:29 lpmodule.py:513] value_states device cuda:1
DEBUG 10-14 21:17:29 lpmodule.py:514] position_ids device cuda:1
DEBUG 10-14 21:17:29 lpmodule.py:515] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:29 lpmodule.py:516] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:29 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:29 lpllm.py:2262] GPU2CPU move cost 0.000633 seconds
DEBUG 10-14 21:17:29 lpmodule.py:340] layer_idx 28 attn layer_idx: 28
DEBUG 10-14 21:17:29 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-14 21:17:29 lpmodule.py:364] decoder_attn_batch update batch_dim 0-10
DEBUG 10-14 21:17:29 lpmodule.py:227] layer idx 27
DEBUG 10-14 21:17:29 StaticCacheLen.py:84] static cache update layer_idx: 28, start_batch: 0, end_batch: 10
DEBUG 10-14 21:17:29 lpmodule.py:368] update for kv cache 0-10 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-14 21:17:29 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:29 lpmodule.py:374] update past key value cost 0.006918 seconds
DEBUG 10-14 21:17:29 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 512, 128]), value shape torch.Size([10, 8, 512, 128])
DEBUG 10-14 21:17:29 lpmodule.py:399] repeat qkv cost 0.013358 seconds
DEBUG 10-14 21:17:29 lpmodule.py:400] q shape torch.Size([10, 32, 512, 128]), k shape torch.Size([10, 32, 512, 128]), v shape torch.Size([10, 32, 512, 128])
DEBUG 10-14 21:17:29 lpllm.py:503] reset update experts
DEBUG 10-14 21:17:29 lpllm.py:1771] update state cost 3.123283386230469e-05 s
DEBUG 10-14 21:17:29 lpllm.py:1740] restore layer func cost 0.002031564712524414 s
DEBUG 10-14 21:17:29 lpllm.py:511] restore layer cost 0.002424478530883789 s
DEBUG 10-14 21:17:29 lpllm.py:512] reset_next_layer_need: layer_attn_idx=28, layer_mlp_idx=28, j_loc=56
DEBUG 10-14 21:17:29 lpllm.py:1034] reset layer cost 0.0025615692138671875 s
DEBUG 10-14 21:17:29 lpllm.py:1035] have reset next layer layer_attn 28 layer_mlp 28 
DEBUG 10-14 21:17:29 lpllm.py:921] 
DEBUG 10-14 21:17:29 lpllm.py:921] decoder loop j: 57 cur_layer_idx: 28 layer_attn_idx: 28 layer_mlp_idx: 28
DEBUG 10-14 21:17:29 lpmodule.py:451] hidden_states.shape torch.Size([10, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:29 lpmodule.py:455] input_layernorm num_params: 1
DEBUG 10-14 21:17:29 lpmodule.py:457] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-14 21:17:29 lpmodule.py:498] kv_seq_len 512 past_key_value 0
DEBUG 10-14 21:17:29 lpmodule.py:504] inv_freq device cpu
DEBUG 10-14 21:17:29 lpmodule.py:505] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:29 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:29 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:29 lpmodule.py:505]         ...,
DEBUG 10-14 21:17:29 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:29 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:29 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-14 21:17:29 lpmodule.py:433] dot attn cost 0.065798 seconds
DEBUG 10-14 21:17:29 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-14 21:17:29 lpmodule.py:511] query_states device cuda:1
DEBUG 10-14 21:17:29 lpmodule.py:512] key_states device cuda:1
DEBUG 10-14 21:17:29 lpmodule.py:513] value_states device cuda:1
DEBUG 10-14 21:17:29 lpmodule.py:514] position_ids device cuda:1
DEBUG 10-14 21:17:29 lpmodule.py:515] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:29 lpmodule.py:516] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:29 lpllm.py:2081] waiting out_queue
time cost move to cuda:1 0.023962974548339844 s
DEBUG 10-14 21:17:29 lpllm.py:2280] CPU attn cost 0.121353 seconds if batch True
DEBUG 10-14 21:17:29 lpllm.py:2289] deal attn result cost 0.000001 seconds
DEBUG 10-14 21:17:29 lpllm.py:2291] CPU compute cost 0.122346 seconds
DEBUG 10-14 21:17:29 lpllm.py:2309] free cost 0.000219 seconds
DEBUG 10-14 21:17:29 lpllm.py:2262] GPU2CPU move cost 0.000262 seconds
DEBUG 10-14 21:17:29 lpmodule.py:340] layer_idx 28 attn layer_idx: 28
DEBUG 10-14 21:17:29 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-14 21:17:29 lpmodule.py:364] decoder_attn_batch update batch_dim 10-20
DEBUG 10-14 21:17:29 StaticCacheLen.py:84] static cache update layer_idx: 28, start_batch: 10, end_batch: 20
DEBUG 10-14 21:17:29 StaticCacheLen.py:90] static cache update layer_idx: 28, update seq_length to 512
DEBUG 10-14 21:17:29 lpmodule.py:368] update for kv cache 10-20 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-14 21:17:29 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:29 lpmodule.py:374] update past key value cost 0.006828 seconds
DEBUG 10-14 21:17:29 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 512, 128]), value shape torch.Size([10, 8, 512, 128])
DEBUG 10-14 21:17:29 lpmodule.py:227] layer idx 28
DEBUG 10-14 21:17:29 lpmodule.py:399] repeat qkv cost 0.012732 seconds
DEBUG 10-14 21:17:29 lpmodule.py:400] q shape torch.Size([10, 32, 512, 128]), k shape torch.Size([10, 32, 512, 128]), v shape torch.Size([10, 32, 512, 128])
DEBUG 10-14 21:17:29 lpllm.py:498] reset update attn
DEBUG 10-14 21:17:29 lpllm.py:1771] update state cost 4.982948303222656e-05 s
DEBUG 10-14 21:17:29 lpllm.py:1740] restore layer func cost 0.001341104507446289 s
DEBUG 10-14 21:17:29 lpllm.py:511] restore layer cost 0.001918792724609375 s
DEBUG 10-14 21:17:29 lpllm.py:512] reset_next_layer_need: layer_attn_idx=29, layer_mlp_idx=28, j_loc=57
DEBUG 10-14 21:17:29 lpllm.py:1034] reset layer cost 0.002157449722290039 s
DEBUG 10-14 21:17:29 lpllm.py:1035] have reset next layer layer_attn 29 layer_mlp 28 
DEBUG 10-14 21:17:29 lpllm.py:1041] j: 57 waiting the layer with layer_idx 29 before wait time 0.22838258743286133 s
INFO 10-14 21:17:29 client.py:117] confirm_model_loaded: Mixtral-8x7B, 44e9f9ef-4deb-4a7e-a7b5-d9f06d809e30
DEBUG 10-14 21:17:29 lpmodule.py:433] dot attn cost 0.062271 seconds
DEBUG 10-14 21:17:29 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.024363994598388672 s
DEBUG 10-14 21:17:29 lpllm.py:2280] CPU attn cost 0.113280 seconds if batch True
DEBUG 10-14 21:17:29 lpllm.py:2289] deal attn result cost 0.000002 seconds
DEBUG 10-14 21:17:29 lpllm.py:2291] CPU compute cost 0.113809 seconds
DEBUG 10-14 21:17:29 lpllm.py:2309] free cost 0.000106 seconds
INFO 10-14 21:17:29 client.py:125] Model loaded
DEBUG 10-14 21:17:29 lpllm.py:1045] j: load cost 0.3585078716278076 s waiting cost 0.1300649642944336 s
DEBUG 10-14 21:17:29 lpllm.py:921] 
DEBUG 10-14 21:17:29 lpllm.py:921] decoder loop j: 58 cur_layer_idx: 29 layer_attn_idx: 29 layer_mlp_idx: 28
DEBUG 10-14 21:17:29 lpllm.py:930] start load next layer cur_layer_idx: 30
DEBUG 10-14 21:17:29 lpllm.py:1801] cuda memory handles: {1: <capsule object NULL at 0x7fdf3c6d9ec0>} {1: b'\xe0\xaa29\x00\x00\x00\x00\xe8&\x1d\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\x86\x00\x00\x00\xff\x00\x00.\x00\x00\x00\x00\x00\x00\x00)\xd8\xd5\xc1\xa0\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-14 21:17:29 client.py:72] load_into_gpu: Mixtral-8x7B, 8a0f4663-07d4-4340-9cf2-1bc1765f43fb
INFO 10-14 21:17:29 client.py:113] Model loaded: Mixtral-8x7B, 8a0f4663-07d4-4340-9cf2-1bc1765f43fb
DEBUG 10-14 21:17:29 lpmodule.py:451] hidden_states.shape torch.Size([10, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:29 lpmodule.py:455] input_layernorm num_params: 1
DEBUG 10-14 21:17:29 lpmodule.py:457] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-14 21:17:29 lpmodule.py:498] kv_seq_len 512 past_key_value 0
DEBUG 10-14 21:17:29 lpmodule.py:504] inv_freq device cpu
DEBUG 10-14 21:17:29 lpmodule.py:505] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:29 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:29 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:29 lpmodule.py:505]         ...,
DEBUG 10-14 21:17:29 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:29 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:29 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-14 21:17:29 lpmodule.py:511] query_states device cuda:1
DEBUG 10-14 21:17:29 lpmodule.py:512] key_states device cuda:1
DEBUG 10-14 21:17:29 lpmodule.py:513] value_states device cuda:1
DEBUG 10-14 21:17:29 lpmodule.py:514] position_ids device cuda:1
DEBUG 10-14 21:17:29 lpmodule.py:515] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:29 lpmodule.py:516] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:29 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:29 lpllm.py:2262] GPU2CPU move cost 0.000660 seconds
DEBUG 10-14 21:17:29 lpmodule.py:340] layer_idx 29 attn layer_idx: 29
DEBUG 10-14 21:17:29 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-14 21:17:29 lpmodule.py:364] decoder_attn_batch update batch_dim 0-10
DEBUG 10-14 21:17:29 lpmodule.py:227] layer idx 28
DEBUG 10-14 21:17:29 StaticCacheLen.py:84] static cache update layer_idx: 29, start_batch: 0, end_batch: 10
DEBUG 10-14 21:17:29 lpmodule.py:368] update for kv cache 0-10 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-14 21:17:29 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:29 lpmodule.py:374] update past key value cost 0.011486 seconds
DEBUG 10-14 21:17:29 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 512, 128]), value shape torch.Size([10, 8, 512, 128])
DEBUG 10-14 21:17:29 lpmodule.py:399] repeat qkv cost 0.011877 seconds
DEBUG 10-14 21:17:29 lpmodule.py:400] q shape torch.Size([10, 32, 512, 128]), k shape torch.Size([10, 32, 512, 128]), v shape torch.Size([10, 32, 512, 128])
DEBUG 10-14 21:17:29 lpllm.py:503] reset update experts
DEBUG 10-14 21:17:29 lpllm.py:1771] update state cost 4.6253204345703125e-05 s
DEBUG 10-14 21:17:29 lpllm.py:1740] restore layer func cost 0.00281524658203125 s
DEBUG 10-14 21:17:29 lpllm.py:511] restore layer cost 0.0033919811248779297 s
DEBUG 10-14 21:17:29 lpllm.py:512] reset_next_layer_need: layer_attn_idx=29, layer_mlp_idx=29, j_loc=58
DEBUG 10-14 21:17:29 lpllm.py:1034] reset layer cost 0.003559589385986328 s
DEBUG 10-14 21:17:29 lpllm.py:1035] have reset next layer layer_attn 29 layer_mlp 29 
DEBUG 10-14 21:17:29 lpllm.py:921] 
DEBUG 10-14 21:17:29 lpllm.py:921] decoder loop j: 59 cur_layer_idx: 29 layer_attn_idx: 29 layer_mlp_idx: 29
DEBUG 10-14 21:17:29 lpmodule.py:451] hidden_states.shape torch.Size([10, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:29 lpmodule.py:455] input_layernorm num_params: 1
DEBUG 10-14 21:17:29 lpmodule.py:457] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-14 21:17:29 lpmodule.py:498] kv_seq_len 512 past_key_value 0
DEBUG 10-14 21:17:29 lpmodule.py:504] inv_freq device cpu
DEBUG 10-14 21:17:29 lpmodule.py:505] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:29 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:29 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:29 lpmodule.py:505]         ...,
DEBUG 10-14 21:17:29 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:29 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:29 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-14 21:17:29 lpmodule.py:511] query_states device cuda:1
DEBUG 10-14 21:17:29 lpmodule.py:512] key_states device cuda:1
DEBUG 10-14 21:17:29 lpmodule.py:513] value_states device cuda:1
DEBUG 10-14 21:17:29 lpmodule.py:514] position_ids device cuda:1
DEBUG 10-14 21:17:29 lpmodule.py:515] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:29 lpmodule.py:516] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:29 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:29 lpmodule.py:433] dot attn cost 0.062791 seconds
DEBUG 10-14 21:17:29 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.02196502685546875 s
DEBUG 10-14 21:17:29 lpllm.py:2280] CPU attn cost 0.115153 seconds if batch True
DEBUG 10-14 21:17:29 lpllm.py:2289] deal attn result cost 0.000002 seconds
DEBUG 10-14 21:17:29 lpllm.py:2291] CPU compute cost 0.116213 seconds
DEBUG 10-14 21:17:29 lpllm.py:2309] free cost 0.000103 seconds
DEBUG 10-14 21:17:29 lpllm.py:2262] GPU2CPU move cost 0.000297 seconds
DEBUG 10-14 21:17:29 lpmodule.py:340] layer_idx 29 attn layer_idx: 29
DEBUG 10-14 21:17:29 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-14 21:17:29 lpmodule.py:364] decoder_attn_batch update batch_dim 10-20
DEBUG 10-14 21:17:29 lpmodule.py:227] layer idx 29
DEBUG 10-14 21:17:29 StaticCacheLen.py:84] static cache update layer_idx: 29, start_batch: 10, end_batch: 20
DEBUG 10-14 21:17:29 StaticCacheLen.py:90] static cache update layer_idx: 29, update seq_length to 512
DEBUG 10-14 21:17:29 lpmodule.py:368] update for kv cache 10-20 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-14 21:17:29 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:29 lpmodule.py:374] update past key value cost 0.008828 seconds
DEBUG 10-14 21:17:29 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 512, 128]), value shape torch.Size([10, 8, 512, 128])
DEBUG 10-14 21:17:29 lpmodule.py:399] repeat qkv cost 0.012109 seconds
DEBUG 10-14 21:17:29 lpmodule.py:400] q shape torch.Size([10, 32, 512, 128]), k shape torch.Size([10, 32, 512, 128]), v shape torch.Size([10, 32, 512, 128])
DEBUG 10-14 21:17:29 lpllm.py:498] reset update attn
DEBUG 10-14 21:17:29 lpllm.py:1771] update state cost 2.765655517578125e-05 s
DEBUG 10-14 21:17:29 lpllm.py:1740] restore layer func cost 0.0013391971588134766 s
DEBUG 10-14 21:17:29 lpllm.py:511] restore layer cost 0.0018358230590820312 s
DEBUG 10-14 21:17:29 lpllm.py:512] reset_next_layer_need: layer_attn_idx=30, layer_mlp_idx=29, j_loc=59
DEBUG 10-14 21:17:29 lpllm.py:1034] reset layer cost 0.002048015594482422 s
DEBUG 10-14 21:17:29 lpllm.py:1035] have reset next layer layer_attn 30 layer_mlp 29 
DEBUG 10-14 21:17:29 lpllm.py:1041] j: 59 waiting the layer with layer_idx 30 before wait time 0.21677827835083008 s
INFO 10-14 21:17:29 client.py:117] confirm_model_loaded: Mixtral-8x7B, 8a0f4663-07d4-4340-9cf2-1bc1765f43fb
DEBUG 10-14 21:17:29 lpmodule.py:433] dot attn cost 0.067300 seconds
DEBUG 10-14 21:17:29 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.024786710739135742 s
DEBUG 10-14 21:17:29 lpllm.py:2280] CPU attn cost 0.120020 seconds if batch True
DEBUG 10-14 21:17:29 lpllm.py:2289] deal attn result cost 0.000002 seconds
DEBUG 10-14 21:17:29 lpllm.py:2291] CPU compute cost 0.120533 seconds
DEBUG 10-14 21:17:29 lpllm.py:2309] free cost 0.000094 seconds
INFO 10-14 21:17:30 client.py:125] Model loaded
DEBUG 10-14 21:17:30 lpllm.py:1045] j: load cost 0.3702521324157715 s waiting cost 0.15341496467590332 s
DEBUG 10-14 21:17:30 lpllm.py:921] 
DEBUG 10-14 21:17:30 lpllm.py:921] decoder loop j: 60 cur_layer_idx: 30 layer_attn_idx: 30 layer_mlp_idx: 29
DEBUG 10-14 21:17:30 lpllm.py:930] start load next layer cur_layer_idx: 31
DEBUG 10-14 21:17:30 lpllm.py:1801] cuda memory handles: {1: <capsule object NULL at 0x7fdf3c6d9f80>} {1: b'\xe0\xaa29\x00\x00\x00\x00\xe8&\x1d\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\x86\x00\x00\x00\xff\x00\x00/\x00\x00\x00\x00\x00\x00\x00)\xd8\xd5\xc1\xa2\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-14 21:17:30 client.py:72] load_into_gpu: Mixtral-8x7B, 2f00bc28-3a3b-46f6-8c08-b1f9c7847065
INFO 10-14 21:17:30 client.py:113] Model loaded: Mixtral-8x7B, 2f00bc28-3a3b-46f6-8c08-b1f9c7847065
DEBUG 10-14 21:17:30 lpmodule.py:451] hidden_states.shape torch.Size([10, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:30 lpmodule.py:455] input_layernorm num_params: 1
DEBUG 10-14 21:17:30 lpmodule.py:457] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-14 21:17:30 lpmodule.py:498] kv_seq_len 512 past_key_value 0
DEBUG 10-14 21:17:30 lpmodule.py:504] inv_freq device cpu
DEBUG 10-14 21:17:30 lpmodule.py:505] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:30 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:30 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:30 lpmodule.py:505]         ...,
DEBUG 10-14 21:17:30 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:30 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:30 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-14 21:17:30 lpmodule.py:511] query_states device cuda:1
DEBUG 10-14 21:17:30 lpmodule.py:512] key_states device cuda:1
DEBUG 10-14 21:17:30 lpmodule.py:513] value_states device cuda:1
DEBUG 10-14 21:17:30 lpmodule.py:514] position_ids device cuda:1
DEBUG 10-14 21:17:30 lpmodule.py:515] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:30 lpmodule.py:516] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:30 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:30 lpllm.py:2262] GPU2CPU move cost 0.000495 seconds
DEBUG 10-14 21:17:30 lpmodule.py:340] layer_idx 30 attn layer_idx: 30
DEBUG 10-14 21:17:30 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-14 21:17:30 lpmodule.py:364] decoder_attn_batch update batch_dim 0-10
DEBUG 10-14 21:17:30 lpmodule.py:227] layer idx 29
DEBUG 10-14 21:17:30 StaticCacheLen.py:84] static cache update layer_idx: 30, start_batch: 0, end_batch: 10
DEBUG 10-14 21:17:30 lpmodule.py:368] update for kv cache 0-10 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-14 21:17:30 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:30 lpmodule.py:374] update past key value cost 0.007476 seconds
DEBUG 10-14 21:17:30 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 512, 128]), value shape torch.Size([10, 8, 512, 128])
DEBUG 10-14 21:17:30 lpmodule.py:399] repeat qkv cost 0.018088 seconds
DEBUG 10-14 21:17:30 lpmodule.py:400] q shape torch.Size([10, 32, 512, 128]), k shape torch.Size([10, 32, 512, 128]), v shape torch.Size([10, 32, 512, 128])
DEBUG 10-14 21:17:30 lpllm.py:503] reset update experts
DEBUG 10-14 21:17:30 lpllm.py:1771] update state cost 2.2172927856445312e-05 s
DEBUG 10-14 21:17:30 lpllm.py:1740] restore layer func cost 0.0011327266693115234 s
DEBUG 10-14 21:17:30 lpllm.py:511] restore layer cost 0.001402139663696289 s
DEBUG 10-14 21:17:30 lpllm.py:512] reset_next_layer_need: layer_attn_idx=30, layer_mlp_idx=30, j_loc=60
DEBUG 10-14 21:17:30 lpllm.py:1034] reset layer cost 0.0015141963958740234 s
DEBUG 10-14 21:17:30 lpllm.py:1035] have reset next layer layer_attn 30 layer_mlp 30 
DEBUG 10-14 21:17:30 lpllm.py:921] 
DEBUG 10-14 21:17:30 lpllm.py:921] decoder loop j: 61 cur_layer_idx: 30 layer_attn_idx: 30 layer_mlp_idx: 30
DEBUG 10-14 21:17:30 lpmodule.py:451] hidden_states.shape torch.Size([10, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:30 lpmodule.py:455] input_layernorm num_params: 1
DEBUG 10-14 21:17:30 lpmodule.py:457] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-14 21:17:30 lpmodule.py:433] dot attn cost 0.067547 seconds
DEBUG 10-14 21:17:30 lpmodule.py:437] layer self attn q_proj device cuda:1
DEBUG 10-14 21:17:30 lpmodule.py:498] kv_seq_len 512 past_key_value 0
DEBUG 10-14 21:17:30 lpmodule.py:504] inv_freq device cpu
DEBUG 10-14 21:17:30 lpmodule.py:505] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:30 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:30 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:30 lpmodule.py:505]         ...,
DEBUG 10-14 21:17:30 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:30 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:30 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-14 21:17:30 lpmodule.py:511] query_states device cuda:1
DEBUG 10-14 21:17:30 lpmodule.py:512] key_states device cuda:1
DEBUG 10-14 21:17:30 lpmodule.py:513] value_states device cuda:1
DEBUG 10-14 21:17:30 lpmodule.py:514] position_ids device cuda:1
DEBUG 10-14 21:17:30 lpmodule.py:515] cos device cuda:1 shape torch.Size([512, 128])
time cost move to cuda:1 0.02567911148071289 s
DEBUG 10-14 21:17:30 lpmodule.py:516] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:30 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:30 lpllm.py:2280] CPU attn cost 0.126010 seconds if batch True
DEBUG 10-14 21:17:30 lpllm.py:2289] deal attn result cost 0.000002 seconds
DEBUG 10-14 21:17:30 lpllm.py:2291] CPU compute cost 0.126809 seconds
DEBUG 10-14 21:17:30 lpllm.py:2309] free cost 0.000228 seconds
DEBUG 10-14 21:17:30 lpllm.py:2262] GPU2CPU move cost 0.000366 seconds
DEBUG 10-14 21:17:30 lpmodule.py:340] layer_idx 30 attn layer_idx: 30
DEBUG 10-14 21:17:30 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-14 21:17:30 lpmodule.py:364] decoder_attn_batch update batch_dim 10-20
DEBUG 10-14 21:17:30 StaticCacheLen.py:84] static cache update layer_idx: 30, start_batch: 10, end_batch: 20
DEBUG 10-14 21:17:30 StaticCacheLen.py:90] static cache update layer_idx: 30, update seq_length to 512
DEBUG 10-14 21:17:30 lpmodule.py:368] update for kv cache 10-20 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-14 21:17:30 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:30 lpmodule.py:374] update past key value cost 0.006476 seconds
DEBUG 10-14 21:17:30 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 512, 128]), value shape torch.Size([10, 8, 512, 128])
DEBUG 10-14 21:17:30 lpmodule.py:227] layer idx 30
DEBUG 10-14 21:17:30 lpmodule.py:399] repeat qkv cost 0.012952 seconds
DEBUG 10-14 21:17:30 lpmodule.py:400] q shape torch.Size([10, 32, 512, 128]), k shape torch.Size([10, 32, 512, 128]), v shape torch.Size([10, 32, 512, 128])
DEBUG 10-14 21:17:30 lpllm.py:498] reset update attn
DEBUG 10-14 21:17:30 lpllm.py:1771] update state cost 7.939338684082031e-05 s
DEBUG 10-14 21:17:30 lpllm.py:1740] restore layer func cost 0.0015676021575927734 s
DEBUG 10-14 21:17:30 lpllm.py:511] restore layer cost 0.0022423267364501953 s
DEBUG 10-14 21:17:30 lpllm.py:512] reset_next_layer_need: layer_attn_idx=31, layer_mlp_idx=30, j_loc=61
DEBUG 10-14 21:17:30 lpllm.py:1034] reset layer cost 0.0025827884674072266 s
DEBUG 10-14 21:17:30 lpllm.py:1035] have reset next layer layer_attn 31 layer_mlp 30 
DEBUG 10-14 21:17:30 lpllm.py:1041] j: 61 waiting the layer with layer_idx 31 before wait time 0.26020097732543945 s
INFO 10-14 21:17:30 client.py:117] confirm_model_loaded: Mixtral-8x7B, 2f00bc28-3a3b-46f6-8c08-b1f9c7847065
DEBUG 10-14 21:17:30 lpmodule.py:433] dot attn cost 0.064752 seconds
DEBUG 10-14 21:17:30 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.025628089904785156 s
DEBUG 10-14 21:17:30 lpllm.py:2280] CPU attn cost 0.116832 seconds if batch True
DEBUG 10-14 21:17:30 lpllm.py:2289] deal attn result cost 0.000002 seconds
DEBUG 10-14 21:17:30 lpllm.py:2291] CPU compute cost 0.117453 seconds
DEBUG 10-14 21:17:30 lpllm.py:2309] free cost 0.000139 seconds
INFO 10-14 21:17:30 client.py:125] Model loaded
DEBUG 10-14 21:17:30 lpllm.py:1045] j: load cost 0.44228315353393555 s waiting cost 0.18202543258666992 s
DEBUG 10-14 21:17:30 lpllm.py:921] 
DEBUG 10-14 21:17:30 lpllm.py:921] decoder loop j: 62 cur_layer_idx: 31 layer_attn_idx: 31 layer_mlp_idx: 30
DEBUG 10-14 21:17:30 lpmodule.py:451] hidden_states.shape torch.Size([10, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:30 lpmodule.py:455] input_layernorm num_params: 1
DEBUG 10-14 21:17:30 lpmodule.py:457] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-14 21:17:30 lpmodule.py:498] kv_seq_len 512 past_key_value 0
DEBUG 10-14 21:17:30 lpmodule.py:504] inv_freq device cpu
DEBUG 10-14 21:17:30 lpmodule.py:505] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:30 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:30 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:30 lpmodule.py:505]         ...,
DEBUG 10-14 21:17:30 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:30 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:30 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-14 21:17:30 lpmodule.py:511] query_states device cuda:1
DEBUG 10-14 21:17:30 lpmodule.py:512] key_states device cuda:1
DEBUG 10-14 21:17:30 lpmodule.py:513] value_states device cuda:1
DEBUG 10-14 21:17:30 lpmodule.py:514] position_ids device cuda:1
DEBUG 10-14 21:17:30 lpmodule.py:515] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:30 lpmodule.py:516] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:30 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:30 lpllm.py:2262] GPU2CPU move cost 0.001064 seconds
DEBUG 10-14 21:17:30 lpmodule.py:340] layer_idx 31 attn layer_idx: 31
DEBUG 10-14 21:17:30 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-14 21:17:30 lpmodule.py:364] decoder_attn_batch update batch_dim 0-10
DEBUG 10-14 21:17:30 lpmodule.py:227] layer idx 30
DEBUG 10-14 21:17:30 StaticCacheLen.py:84] static cache update layer_idx: 31, start_batch: 0, end_batch: 10
DEBUG 10-14 21:17:30 lpmodule.py:368] update for kv cache 0-10 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-14 21:17:30 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:30 lpmodule.py:374] update past key value cost 0.007117 seconds
DEBUG 10-14 21:17:30 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 512, 128]), value shape torch.Size([10, 8, 512, 128])
DEBUG 10-14 21:17:30 lpmodule.py:399] repeat qkv cost 0.013425 seconds
DEBUG 10-14 21:17:30 lpmodule.py:400] q shape torch.Size([10, 32, 512, 128]), k shape torch.Size([10, 32, 512, 128]), v shape torch.Size([10, 32, 512, 128])
DEBUG 10-14 21:17:30 lpllm.py:503] reset update experts
DEBUG 10-14 21:17:30 lpllm.py:1771] update state cost 2.8371810913085938e-05 s
DEBUG 10-14 21:17:30 lpllm.py:1740] restore layer func cost 0.0019137859344482422 s
DEBUG 10-14 21:17:30 lpllm.py:511] restore layer cost 0.0023040771484375 s
DEBUG 10-14 21:17:30 lpllm.py:512] reset_next_layer_need: layer_attn_idx=31, layer_mlp_idx=31, j_loc=62
DEBUG 10-14 21:17:30 lpllm.py:1034] reset layer cost 0.002461671829223633 s
DEBUG 10-14 21:17:30 lpllm.py:1035] have reset next layer layer_attn 31 layer_mlp 31 
DEBUG 10-14 21:17:30 lpllm.py:921] 
DEBUG 10-14 21:17:30 lpllm.py:921] decoder loop j: 63 cur_layer_idx: 31 layer_attn_idx: 31 layer_mlp_idx: 31
DEBUG 10-14 21:17:30 lpmodule.py:451] hidden_states.shape torch.Size([10, 512, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:30 lpmodule.py:455] input_layernorm num_params: 1
DEBUG 10-14 21:17:30 lpmodule.py:457] input_layernorm param 0: shape=torch.Size([4096]), dtype=torch.bfloat16, device=cuda:1
DEBUG 10-14 21:17:30 lpmodule.py:498] kv_seq_len 512 past_key_value 0
DEBUG 10-14 21:17:30 lpmodule.py:504] inv_freq device cpu
DEBUG 10-14 21:17:30 lpmodule.py:505] kv_seq_len 512 position_ids tensor([[  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:30 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:30 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:30 lpmodule.py:505]         ...,
DEBUG 10-14 21:17:30 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:30 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511],
DEBUG 10-14 21:17:30 lpmodule.py:505]         [  0,   1,   2,  ..., 509, 510, 511]])
DEBUG 10-14 21:17:30 lpmodule.py:511] query_states device cuda:1
DEBUG 10-14 21:17:30 lpmodule.py:512] key_states device cuda:1
DEBUG 10-14 21:17:30 lpmodule.py:513] value_states device cuda:1
DEBUG 10-14 21:17:30 lpmodule.py:514] position_ids device cuda:1
DEBUG 10-14 21:17:30 lpmodule.py:515] cos device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:30 lpmodule.py:516] sin device cuda:1 shape torch.Size([512, 128])
DEBUG 10-14 21:17:30 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:30 lpmodule.py:433] dot attn cost 0.063254 seconds
DEBUG 10-14 21:17:30 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.012484312057495117 s
DEBUG 10-14 21:17:30 lpllm.py:2280] CPU attn cost 0.103713 seconds if batch True
DEBUG 10-14 21:17:30 lpllm.py:2289] deal attn result cost 0.000002 seconds
DEBUG 10-14 21:17:30 lpllm.py:2291] CPU compute cost 0.105128 seconds
DEBUG 10-14 21:17:30 lpllm.py:2309] free cost 0.000112 seconds
DEBUG 10-14 21:17:30 lpllm.py:2262] GPU2CPU move cost 0.000285 seconds
DEBUG 10-14 21:17:30 lpmodule.py:340] layer_idx 31 attn layer_idx: 31
DEBUG 10-14 21:17:30 lpmodule.py:352] get seq_length 0 in decoder_attn_batch
DEBUG 10-14 21:17:30 lpmodule.py:364] decoder_attn_batch update batch_dim 10-20
DEBUG 10-14 21:17:30 lpmodule.py:227] layer idx 31
DEBUG 10-14 21:17:30 StaticCacheLen.py:84] static cache update layer_idx: 31, start_batch: 10, end_batch: 20
DEBUG 10-14 21:17:30 StaticCacheLen.py:90] static cache update layer_idx: 31, update seq_length to 512
DEBUG 10-14 21:17:30 lpmodule.py:368] update for kv cache 10-20 kv_seq_len 512 in decoder_attn_batch
DEBUG 10-14 21:17:30 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:30 lpmodule.py:374] update past key value cost 0.006456 seconds
DEBUG 10-14 21:17:30 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 512, 128]), value shape torch.Size([10, 8, 512, 128])
DEBUG 10-14 21:17:30 lpmodule.py:399] repeat qkv cost 0.012930 seconds
DEBUG 10-14 21:17:30 lpmodule.py:400] q shape torch.Size([10, 32, 512, 128]), k shape torch.Size([10, 32, 512, 128]), v shape torch.Size([10, 32, 512, 128])
DEBUG 10-14 21:17:30 lpllm.py:921] 
DEBUG 10-14 21:17:30 lpllm.py:921] decoder loop j: 64 cur_layer_idx: 32 layer_attn_idx: 32 layer_mlp_idx: 31
DEBUG 10-14 21:17:30 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:30 lpmodule.py:433] dot attn cost 0.063234 seconds
DEBUG 10-14 21:17:30 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.011214971542358398 s
DEBUG 10-14 21:17:30 lpllm.py:2280] CPU attn cost 0.102051 seconds if batch True
DEBUG 10-14 21:17:30 lpllm.py:2289] deal attn result cost 0.000006 seconds
DEBUG 10-14 21:17:30 lpllm.py:2291] CPU compute cost 0.102914 seconds
DEBUG 10-14 21:17:30 lpllm.py:2309] free cost 0.000316 seconds
DEBUG 10-14 21:17:30 lpmodule.py:227] layer idx 31
DEBUG 10-14 21:17:30 lpllm.py:1082] last_mlp_output_chunk shape: torch.Size([10, 512, 4096]), mlp_output_chunk shape: torch.Size([10, 512, 4096])
DEBUG 10-14 21:17:30 lpllm.py:1083] last_mlp_output_chunk len 1, mlp_output_chunk len 1
DEBUG 10-14 21:17:30 lpllm.py:618] decoders batch for 0 cost 11.860949993133545 s
DEBUG 10-14 21:17:30 lpllm.py:670] last_hidden_states shape torch.Size([10, 1, 4096])
DEBUG 10-14 21:17:30 lpllm.py:689] next_token_logits shape torch.Size([10, 32000])
DEBUG 10-14 21:17:30 lpllm.py:670] last_hidden_states shape torch.Size([10, 1, 4096])
DEBUG 10-14 21:17:30 lpllm.py:689] next_token_logits shape torch.Size([10, 32000])
DEBUG 10-14 21:17:30 lpllm.py:708] next_token_ids shape torch.Size([10, 1])
DEBUG 10-14 21:17:30 lpllm.py:724] 
DEBUG 10-14 21:17:30 lpllm.py:724] 
DEBUG 10-14 21:17:30 lpllm.py:724] step 1 decoders
DEBUG 10-14 21:17:30 lpmodule.py:77] before sdpa attention attention_mask tensor([[1, 1, 1,  ..., 1, 1, 1],
DEBUG 10-14 21:17:30 lpmodule.py:77]         [1, 1, 1,  ..., 1, 1, 1],
DEBUG 10-14 21:17:30 lpmodule.py:77]         [1, 1, 1,  ..., 1, 1, 1],
DEBUG 10-14 21:17:30 lpmodule.py:77]         ...,
DEBUG 10-14 21:17:30 lpmodule.py:77]         [1, 1, 1,  ..., 1, 1, 1],
DEBUG 10-14 21:17:30 lpmodule.py:77]         [1, 1, 1,  ..., 1, 1, 1],
DEBUG 10-14 21:17:30 lpmodule.py:77]         [1, 1, 1,  ..., 1, 1, 1]])
DEBUG 10-14 21:17:30 lpmodule.py:85] after sdpa attention attention_mask None
DEBUG 10-14 21:17:30 lpmodule.py:77] before sdpa attention attention_mask tensor([[1, 1, 1,  ..., 1, 1, 1],
DEBUG 10-14 21:17:30 lpmodule.py:77]         [1, 1, 1,  ..., 1, 1, 1],
DEBUG 10-14 21:17:30 lpmodule.py:77]         [1, 1, 1,  ..., 1, 1, 1],
DEBUG 10-14 21:17:30 lpmodule.py:77]         ...,
DEBUG 10-14 21:17:30 lpmodule.py:77]         [1, 1, 1,  ..., 1, 1, 1],
DEBUG 10-14 21:17:30 lpmodule.py:77]         [1, 1, 1,  ..., 1, 1, 1],
DEBUG 10-14 21:17:30 lpmodule.py:77]         [1, 1, 1,  ..., 1, 1, 1]])
DEBUG 10-14 21:17:30 lpmodule.py:85] after sdpa attention attention_mask None
DEBUG 10-14 21:17:30 lpllm.py:774] decoders non batch pre cost 0.00421452522277832 s
DEBUG 10-14 21:17:30 lpllm.py:1801] cuda memory handles: {1: <capsule object NULL at 0x7fdf3c6da010>} {1: b'\xe0\xaa29\x00\x00\x00\x00\xe8&\x1d\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\x86\x00\x00\x00\xff\x00\x000\x00\x00\x00\x00\x00\x00\x00)\xd8\xd5\xc1\xa4\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-14 21:17:30 client.py:72] load_into_gpu: Mixtral-8x7B, e3f04571-b714-4007-b71b-5a3f46a06f97
INFO 10-14 21:17:30 client.py:113] Model loaded: Mixtral-8x7B, e3f04571-b714-4007-b71b-5a3f46a06f97
DEBUG 10-14 21:17:30 lpllm.py:1740] restore layer func cost 0.0013647079467773438 s
INFO 10-14 21:17:30 client.py:117] confirm_model_loaded: Mixtral-8x7B, e3f04571-b714-4007-b71b-5a3f46a06f97
INFO 10-14 21:17:31 client.py:125] Model loaded
DEBUG 10-14 21:17:31 lpllm.py:422] prepare layer cost 0.28867006301879883 s
DEBUG 10-14 21:17:31 lpllm.py:1801] cuda memory handles: {1: <capsule object NULL at 0x7fdf3c6d9ec0>} {1: b'\xe0\xaa29\x00\x00\x00\x00\xe8&\x1d\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\x86\x00\x00\x00\xff\x00\x00.\x00\x00\x00\x00\x00\x00\x00)\xd8\xd5\xc1\xa0\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-14 21:17:31 client.py:72] load_into_gpu: Mixtral-8x7B, 0d2b07f7-0fe2-455e-9381-3e8bd6ec1561
INFO 10-14 21:17:31 client.py:113] Model loaded: Mixtral-8x7B, 0d2b07f7-0fe2-455e-9381-3e8bd6ec1561
DEBUG 10-14 21:17:31 lpllm.py:1130] start first layer
DEBUG 10-14 21:17:31 lpmodule.py:526] hidden_states.shape torch.Size([10, 1, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:31 lpmodule.py:541] kv_seq_len 1 past_key_value 512
DEBUG 10-14 21:17:31 lpmodule.py:545] inv_freq device cpu
DEBUG 10-14 21:17:31 lpmodule.py:546] kv_seq_len 513 position_ids torch.Size([10, 1]) value tensor([512])
DEBUG 10-14 21:17:31 lpmodule.py:551] query_states device cuda:1
DEBUG 10-14 21:17:31 lpmodule.py:552] key_states device cuda:1
DEBUG 10-14 21:17:31 lpmodule.py:553] value_states device cuda:1
DEBUG 10-14 21:17:31 lpmodule.py:554] position_ids device cpu
DEBUG 10-14 21:17:31 lpmodule.py:555] cos device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:31 lpmodule.py:556] sin device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:31 lpllm.py:1200] 
DEBUG 10-14 21:17:31 lpllm.py:1200] one decoder loop j: 1 cur_layer_idx: 0
DEBUG 10-14 21:17:31 lpllm.py:1207] start decoder qkv layer_attn 0 layer_mlp 0
DEBUG 10-14 21:17:31 lpmodule.py:526] hidden_states.shape torch.Size([10, 1, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:31 lpllm.py:2262] GPU2CPU move cost 0.000572 seconds
DEBUG 10-14 21:17:31 lpmodule.py:340] layer_idx 0 attn layer_idx: 0
DEBUG 10-14 21:17:31 lpmodule.py:352] get seq_length 512 in decoder_attn_batch
WARNING 10-14 21:17:31 lpmodule.py:355] generally decoder_attn_batch called with q_len == 1
DEBUG 10-14 21:17:31 lpmodule.py:364] decoder_attn_batch update batch_dim 0-10
DEBUG 10-14 21:17:31 lpmodule.py:541] kv_seq_len 1 past_key_value 512
DEBUG 10-14 21:17:31 lpmodule.py:545] inv_freq device cpu
DEBUG 10-14 21:17:31 lpmodule.py:546] kv_seq_len 513 position_ids torch.Size([10, 1]) value tensor([512])
DEBUG 10-14 21:17:31 lpmodule.py:551] query_states device cuda:1
DEBUG 10-14 21:17:31 lpmodule.py:552] key_states device cuda:1
DEBUG 10-14 21:17:31 lpmodule.py:553] value_states device cuda:1
DEBUG 10-14 21:17:31 lpmodule.py:554] position_ids device cpu
DEBUG 10-14 21:17:31 lpmodule.py:555] cos device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:31 lpmodule.py:556] sin device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:31 StaticCacheLen.py:84] static cache update layer_idx: 0, start_batch: 0, end_batch: 10
DEBUG 10-14 21:17:31 lpmodule.py:368] update for kv cache 0-10 kv_seq_len 513 in decoder_attn_batch
DEBUG 10-14 21:17:31 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:31 lpmodule.py:374] update past key value cost 0.003098 seconds
DEBUG 10-14 21:17:31 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 513, 128]), value shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:31 lpmodule.py:399] repeat qkv cost 0.000001 seconds
DEBUG 10-14 21:17:31 lpmodule.py:400] q shape torch.Size([10, 32, 1, 128]), k shape torch.Size([10, 8, 513, 128]), v shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:31 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:31 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:31 tutils.py:81] single group 0 real attn out cost 0.008239507675170898 s
DEBUG 10-14 21:17:31 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:31 tutils.py:81] single group 1 real attn out cost 0.004338502883911133 s
DEBUG 10-14 21:17:31 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:31 tutils.py:81] single group 2 real attn out cost 0.0029358863830566406 s
DEBUG 10-14 21:17:31 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:31 tutils.py:81] single group 3 real attn out cost 0.0033042430877685547 s
DEBUG 10-14 21:17:31 tutils.py:88] dot attn help cost 0.025805 seconds
DEBUG 10-14 21:17:31 lpmodule.py:433] dot attn cost 0.025958 seconds
DEBUG 10-14 21:17:31 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.002138376235961914 s
DEBUG 10-14 21:17:31 lpllm.py:2280] CPU attn cost 0.032067 seconds if batch True
DEBUG 10-14 21:17:31 lpllm.py:2289] deal attn result cost 0.000004 seconds
DEBUG 10-14 21:17:31 lpllm.py:2291] CPU compute cost 0.033091 seconds
DEBUG 10-14 21:17:31 lpllm.py:2309] free cost 0.000192 seconds
DEBUG 10-14 21:17:31 lpllm.py:2262] GPU2CPU move cost 0.000385 seconds
DEBUG 10-14 21:17:31 lpmodule.py:340] layer_idx 0 attn layer_idx: 0
DEBUG 10-14 21:17:31 lpmodule.py:352] get seq_length 512 in decoder_attn_batch
WARNING 10-14 21:17:31 lpmodule.py:355] generally decoder_attn_batch called with q_len == 1
DEBUG 10-14 21:17:31 lpmodule.py:227] layer idx 0
DEBUG 10-14 21:17:31 lpmodule.py:364] decoder_attn_batch update batch_dim 10-20
DEBUG 10-14 21:17:31 StaticCacheLen.py:84] static cache update layer_idx: 0, start_batch: 10, end_batch: 20
DEBUG 10-14 21:17:31 StaticCacheLen.py:90] static cache update layer_idx: 0, update seq_length to 513
DEBUG 10-14 21:17:31 lpmodule.py:368] update for kv cache 10-20 kv_seq_len 513 in decoder_attn_batch
DEBUG 10-14 21:17:31 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:31 lpmodule.py:374] update past key value cost 0.002476 seconds
DEBUG 10-14 21:17:31 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 513, 128]), value shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:31 lpmodule.py:399] repeat qkv cost 0.000001 seconds
DEBUG 10-14 21:17:31 lpmodule.py:400] q shape torch.Size([10, 32, 1, 128]), k shape torch.Size([10, 8, 513, 128]), v shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:31 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:31 tutils.py:81] single group 0 real attn out cost 0.0069234371185302734 s
DEBUG 10-14 21:17:31 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:31 tutils.py:81] single group 1 real attn out cost 0.0033445358276367188 s
DEBUG 10-14 21:17:31 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:31 lpllm.py:498] reset update attn
DEBUG 10-14 21:17:31 lpllm.py:1771] update state cost 3.552436828613281e-05 s
DEBUG 10-14 21:17:31 lpllm.py:1740] restore layer func cost 0.0008985996246337891 s
DEBUG 10-14 21:17:31 lpllm.py:511] restore layer cost 0.0012536048889160156 s
DEBUG 10-14 21:17:31 lpllm.py:512] reset_next_layer_need: layer_attn_idx=1, layer_mlp_idx=0, j_loc=1
DEBUG 10-14 21:17:31 lpllm.py:1280] reset layer cost 0.0013947486877441406 s
DEBUG 10-14 21:17:31 lpllm.py:1281] have reset next layer layer_attn 0 layer_mlp 0 
DEBUG 10-14 21:17:31 lpllm.py:1287] j: 1 waiting the layer with layer_idx 1 before wait time 0.07428550720214844 s
INFO 10-14 21:17:31 client.py:117] confirm_model_loaded: Mixtral-8x7B, 0d2b07f7-0fe2-455e-9381-3e8bd6ec1561
DEBUG 10-14 21:17:31 tutils.py:81] single group 2 real attn out cost 0.0031769275665283203 s
DEBUG 10-14 21:17:31 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:31 tutils.py:81] single group 3 real attn out cost 0.0031905174255371094 s
DEBUG 10-14 21:17:31 tutils.py:88] dot attn help cost 0.022862 seconds
DEBUG 10-14 21:17:31 lpmodule.py:433] dot attn cost 0.023020 seconds
DEBUG 10-14 21:17:31 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.00044345855712890625 s
DEBUG 10-14 21:17:31 lpllm.py:2280] CPU attn cost 0.026540 seconds if batch True
DEBUG 10-14 21:17:31 lpllm.py:2289] deal attn result cost 0.000003 seconds
DEBUG 10-14 21:17:31 lpllm.py:2291] CPU compute cost 0.027239 seconds
DEBUG 10-14 21:17:31 lpllm.py:2309] free cost 0.000171 seconds
INFO 10-14 21:17:31 client.py:125] Model loaded
DEBUG 10-14 21:17:31 lpllm.py:1291] j: load cost 0.3030252456665039 s waiting cost 0.22871661186218262 s
DEBUG 10-14 21:17:31 lpllm.py:1200] 
DEBUG 10-14 21:17:31 lpllm.py:1200] one decoder loop j: 2 cur_layer_idx: 1
DEBUG 10-14 21:17:31 lpllm.py:1205] start load next layer cur_layer_idx: 2
DEBUG 10-14 21:17:31 lpllm.py:1801] cuda memory handles: {1: <capsule object NULL at 0x7fdf3c6d9f80>} {1: b'\xe0\xaa29\x00\x00\x00\x00\xe8&\x1d\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\x86\x00\x00\x00\xff\x00\x00/\x00\x00\x00\x00\x00\x00\x00)\xd8\xd5\xc1\xa2\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-14 21:17:31 client.py:72] load_into_gpu: Mixtral-8x7B, 86658ff2-e2a7-43b1-b8f4-30756fd279a7
INFO 10-14 21:17:31 client.py:113] Model loaded: Mixtral-8x7B, 86658ff2-e2a7-43b1-b8f4-30756fd279a7
DEBUG 10-14 21:17:31 lpllm.py:1207] start decoder qkv layer_attn 1 layer_mlp 0
DEBUG 10-14 21:17:31 lpmodule.py:526] hidden_states.shape torch.Size([10, 1, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:31 lpmodule.py:541] kv_seq_len 1 past_key_value 512
DEBUG 10-14 21:17:31 lpmodule.py:545] inv_freq device cpu
DEBUG 10-14 21:17:31 lpmodule.py:546] kv_seq_len 513 position_ids torch.Size([10, 1]) value tensor([512])
DEBUG 10-14 21:17:31 lpmodule.py:551] query_states device cuda:1
DEBUG 10-14 21:17:31 lpmodule.py:552] key_states device cuda:1
DEBUG 10-14 21:17:31 lpmodule.py:553] value_states device cuda:1
DEBUG 10-14 21:17:31 lpmodule.py:554] position_ids device cpu
DEBUG 10-14 21:17:31 lpmodule.py:555] cos device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:31 lpmodule.py:556] sin device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:31 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:31 lpllm.py:2262] GPU2CPU move cost 0.000361 seconds
DEBUG 10-14 21:17:31 lpmodule.py:340] layer_idx 1 attn layer_idx: 1
DEBUG 10-14 21:17:31 lpmodule.py:352] get seq_length 512 in decoder_attn_batch
DEBUG 10-14 21:17:31 lpmodule.py:227] layer idx 0
WARNING 10-14 21:17:31 lpmodule.py:355] generally decoder_attn_batch called with q_len == 1
DEBUG 10-14 21:17:31 lpmodule.py:364] decoder_attn_batch update batch_dim 0-10
DEBUG 10-14 21:17:31 StaticCacheLen.py:84] static cache update layer_idx: 1, start_batch: 0, end_batch: 10
DEBUG 10-14 21:17:31 lpmodule.py:368] update for kv cache 0-10 kv_seq_len 513 in decoder_attn_batch
DEBUG 10-14 21:17:31 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:31 lpmodule.py:374] update past key value cost 0.003086 seconds
DEBUG 10-14 21:17:31 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 513, 128]), value shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:31 lpmodule.py:399] repeat qkv cost 0.000001 seconds
DEBUG 10-14 21:17:31 lpmodule.py:400] q shape torch.Size([10, 32, 1, 128]), k shape torch.Size([10, 8, 513, 128]), v shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:31 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:31 lpllm.py:503] reset update experts
DEBUG 10-14 21:17:31 lpllm.py:1771] update state cost 3.218650817871094e-05 s
DEBUG 10-14 21:17:31 tutils.py:81] single group 0 real attn out cost 0.007971763610839844 s
DEBUG 10-14 21:17:31 lpllm.py:1740] restore layer func cost 0.002542257308959961 s
DEBUG 10-14 21:17:31 lpllm.py:511] restore layer cost 0.002919435501098633 s
DEBUG 10-14 21:17:31 lpllm.py:512] reset_next_layer_need: layer_attn_idx=1, layer_mlp_idx=1, j_loc=2
DEBUG 10-14 21:17:31 lpllm.py:1280] reset layer cost 0.0030570030212402344 s
DEBUG 10-14 21:17:31 lpllm.py:1281] have reset next layer layer_attn 1 layer_mlp 0 
DEBUG 10-14 21:17:31 lpllm.py:1200] 
DEBUG 10-14 21:17:31 lpllm.py:1200] one decoder loop j: 3 cur_layer_idx: 1
DEBUG 10-14 21:17:31 lpllm.py:1207] start decoder qkv layer_attn 1 layer_mlp 1
DEBUG 10-14 21:17:31 lpmodule.py:526] hidden_states.shape torch.Size([10, 1, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:31 lpmodule.py:541] kv_seq_len 1 past_key_value 512
DEBUG 10-14 21:17:31 lpmodule.py:545] inv_freq device cpu
DEBUG 10-14 21:17:31 lpmodule.py:546] kv_seq_len 513 position_ids torch.Size([10, 1]) value tensor([512])
DEBUG 10-14 21:17:31 lpmodule.py:551] query_states device cuda:1
DEBUG 10-14 21:17:31 lpmodule.py:552] key_states device cuda:1
DEBUG 10-14 21:17:31 lpmodule.py:553] value_states device cuda:1
DEBUG 10-14 21:17:31 lpmodule.py:554] position_ids device cpu
DEBUG 10-14 21:17:31 lpmodule.py:555] cos device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:31 lpmodule.py:556] sin device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:31 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:31 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:31 tutils.py:81] single group 1 real attn out cost 0.005064964294433594 s
DEBUG 10-14 21:17:31 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:31 tutils.py:81] single group 2 real attn out cost 0.0026993751525878906 s
DEBUG 10-14 21:17:31 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:31 tutils.py:81] single group 3 real attn out cost 0.003531932830810547 s
DEBUG 10-14 21:17:31 tutils.py:88] dot attn help cost 0.028539 seconds
DEBUG 10-14 21:17:31 lpmodule.py:433] dot attn cost 0.028599 seconds
DEBUG 10-14 21:17:31 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.0006060600280761719 s
DEBUG 10-14 21:17:31 lpllm.py:2280] CPU attn cost 0.032866 seconds if batch True
DEBUG 10-14 21:17:31 lpllm.py:2289] deal attn result cost 0.000002 seconds
DEBUG 10-14 21:17:31 lpllm.py:2291] CPU compute cost 0.033510 seconds
DEBUG 10-14 21:17:31 lpllm.py:2309] free cost 0.000138 seconds
DEBUG 10-14 21:17:31 lpmodule.py:162] call mlp prepare
DEBUG 10-14 21:17:31 lpmodule.py:163] layer idx 1
DEBUG 10-14 21:17:31 lpmodule.py:123] Tokens per expert: [0, 0, 8, 0, 2, 0, 3, 7]
DEBUG 10-14 21:17:31 lpmodule.py:124] Number of active experts: 4
DEBUG 10-14 21:17:31 lpmodule.py:195] call mlp post
DEBUG 10-14 21:17:31 lpllm.py:2262] GPU2CPU move cost 0.000245 seconds
DEBUG 10-14 21:17:31 lpmodule.py:340] layer_idx 1 attn layer_idx: 1
DEBUG 10-14 21:17:31 lpmodule.py:352] get seq_length 512 in decoder_attn_batch
WARNING 10-14 21:17:31 lpmodule.py:355] generally decoder_attn_batch called with q_len == 1
DEBUG 10-14 21:17:31 lpmodule.py:364] decoder_attn_batch update batch_dim 10-20
DEBUG 10-14 21:17:31 StaticCacheLen.py:84] static cache update layer_idx: 1, start_batch: 10, end_batch: 20
DEBUG 10-14 21:17:31 StaticCacheLen.py:90] static cache update layer_idx: 1, update seq_length to 513
DEBUG 10-14 21:17:31 lpmodule.py:368] update for kv cache 10-20 kv_seq_len 513 in decoder_attn_batch
DEBUG 10-14 21:17:31 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:31 lpmodule.py:374] update past key value cost 0.001973 seconds
DEBUG 10-14 21:17:31 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 513, 128]), value shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:31 lpmodule.py:399] repeat qkv cost 0.000001 seconds
DEBUG 10-14 21:17:31 lpmodule.py:400] q shape torch.Size([10, 32, 1, 128]), k shape torch.Size([10, 8, 513, 128]), v shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:31 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:31 lpllm.py:498] reset update attn
DEBUG 10-14 21:17:31 lpllm.py:1771] update state cost 3.6716461181640625e-05 s
DEBUG 10-14 21:17:31 lpllm.py:1740] restore layer func cost 0.0005323886871337891 s
DEBUG 10-14 21:17:31 lpllm.py:511] restore layer cost 0.0008215904235839844 s
DEBUG 10-14 21:17:31 lpllm.py:512] reset_next_layer_need: layer_attn_idx=2, layer_mlp_idx=1, j_loc=3
DEBUG 10-14 21:17:31 lpllm.py:1280] reset layer cost 0.0009243488311767578 s
DEBUG 10-14 21:17:31 lpllm.py:1281] have reset next layer layer_attn 1 layer_mlp 1 
DEBUG 10-14 21:17:31 lpllm.py:1287] j: 3 waiting the layer with layer_idx 2 before wait time 0.12659001350402832 s
INFO 10-14 21:17:31 client.py:117] confirm_model_loaded: Mixtral-8x7B, 86658ff2-e2a7-43b1-b8f4-30756fd279a7
DEBUG 10-14 21:17:31 tutils.py:81] single group 0 real attn out cost 0.005585908889770508 s
DEBUG 10-14 21:17:31 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:31 tutils.py:81] single group 1 real attn out cost 0.0034737586975097656 s
DEBUG 10-14 21:17:31 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:31 tutils.py:81] single group 2 real attn out cost 0.0027234554290771484 s
DEBUG 10-14 21:17:31 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:31 tutils.py:81] single group 3 real attn out cost 0.003470182418823242 s
DEBUG 10-14 21:17:31 tutils.py:88] dot attn help cost 0.020810 seconds
DEBUG 10-14 21:17:31 lpmodule.py:433] dot attn cost 0.020882 seconds
DEBUG 10-14 21:17:31 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.0005743503570556641 s
DEBUG 10-14 21:17:31 lpllm.py:2280] CPU attn cost 0.023791 seconds if batch True
DEBUG 10-14 21:17:31 lpllm.py:2289] deal attn result cost 0.000002 seconds
DEBUG 10-14 21:17:31 lpllm.py:2291] CPU compute cost 0.024251 seconds
DEBUG 10-14 21:17:31 lpllm.py:2309] free cost 0.000097 seconds
INFO 10-14 21:17:31 client.py:125] Model loaded
DEBUG 10-14 21:17:31 lpllm.py:1291] j: load cost 0.3450784683227539 s waiting cost 0.2184748649597168 s
DEBUG 10-14 21:17:31 lpllm.py:1200] 
DEBUG 10-14 21:17:31 lpllm.py:1200] one decoder loop j: 4 cur_layer_idx: 2
DEBUG 10-14 21:17:31 lpllm.py:1205] start load next layer cur_layer_idx: 3
DEBUG 10-14 21:17:31 lpllm.py:1801] cuda memory handles: {1: <capsule object NULL at 0x7fdf3c6da010>} {1: b'\xe0\xaa29\x00\x00\x00\x00\xe8&\x1d\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\x86\x00\x00\x00\xff\x00\x000\x00\x00\x00\x00\x00\x00\x00)\xd8\xd5\xc1\xa4\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-14 21:17:31 client.py:72] load_into_gpu: Mixtral-8x7B, 8018eb63-71cc-495e-83b1-67d391e25a86
INFO 10-14 21:17:31 client.py:113] Model loaded: Mixtral-8x7B, 8018eb63-71cc-495e-83b1-67d391e25a86
DEBUG 10-14 21:17:31 lpllm.py:1207] start decoder qkv layer_attn 2 layer_mlp 1
DEBUG 10-14 21:17:31 lpmodule.py:526] hidden_states.shape torch.Size([10, 1, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:31 lpmodule.py:541] kv_seq_len 1 past_key_value 512
DEBUG 10-14 21:17:31 lpmodule.py:545] inv_freq device cpu
DEBUG 10-14 21:17:31 lpmodule.py:546] kv_seq_len 513 position_ids torch.Size([10, 1]) value tensor([512])
DEBUG 10-14 21:17:31 lpmodule.py:551] query_states device cuda:1
DEBUG 10-14 21:17:31 lpmodule.py:552] key_states device cuda:1
DEBUG 10-14 21:17:31 lpmodule.py:553] value_states device cuda:1
DEBUG 10-14 21:17:31 lpmodule.py:554] position_ids device cpu
DEBUG 10-14 21:17:31 lpmodule.py:555] cos device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:31 lpmodule.py:556] sin device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:31 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:31 lpmodule.py:162] call mlp prepare
DEBUG 10-14 21:17:31 lpmodule.py:163] layer idx 1
DEBUG 10-14 21:17:31 lpmodule.py:123] Tokens per expert: [0, 0, 9, 1, 9, 0, 1, 0]
DEBUG 10-14 21:17:31 lpmodule.py:124] Number of active experts: 4
DEBUG 10-14 21:17:31 lpmodule.py:195] call mlp post
DEBUG 10-14 21:17:31 lpllm.py:2262] GPU2CPU move cost 0.000560 seconds
DEBUG 10-14 21:17:31 lpmodule.py:340] layer_idx 2 attn layer_idx: 2
DEBUG 10-14 21:17:31 lpmodule.py:352] get seq_length 512 in decoder_attn_batch
WARNING 10-14 21:17:31 lpmodule.py:355] generally decoder_attn_batch called with q_len == 1
DEBUG 10-14 21:17:31 lpmodule.py:364] decoder_attn_batch update batch_dim 0-10
DEBUG 10-14 21:17:31 StaticCacheLen.py:84] static cache update layer_idx: 2, start_batch: 0, end_batch: 10
DEBUG 10-14 21:17:31 lpmodule.py:368] update for kv cache 0-10 kv_seq_len 513 in decoder_attn_batch
DEBUG 10-14 21:17:31 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:31 lpmodule.py:374] update past key value cost 0.002540 seconds
DEBUG 10-14 21:17:31 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 513, 128]), value shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:31 lpmodule.py:399] repeat qkv cost 0.000001 seconds
DEBUG 10-14 21:17:31 lpmodule.py:400] q shape torch.Size([10, 32, 1, 128]), k shape torch.Size([10, 8, 513, 128]), v shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:31 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:31 tutils.py:81] single group 0 real attn out cost 0.005524635314941406 s
DEBUG 10-14 21:17:31 lpllm.py:503] reset update experts
DEBUG 10-14 21:17:31 lpllm.py:1771] update state cost 3.7670135498046875e-05 s
DEBUG 10-14 21:17:31 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:31 lpllm.py:1740] restore layer func cost 0.0032112598419189453 s
DEBUG 10-14 21:17:31 lpllm.py:511] restore layer cost 0.0036573410034179688 s
DEBUG 10-14 21:17:31 lpllm.py:512] reset_next_layer_need: layer_attn_idx=2, layer_mlp_idx=2, j_loc=4
DEBUG 10-14 21:17:31 lpllm.py:1280] reset layer cost 0.0038809776306152344 s
DEBUG 10-14 21:17:31 lpllm.py:1281] have reset next layer layer_attn 2 layer_mlp 1 
DEBUG 10-14 21:17:31 lpllm.py:1200] 
DEBUG 10-14 21:17:31 lpllm.py:1200] one decoder loop j: 5 cur_layer_idx: 2
DEBUG 10-14 21:17:31 lpllm.py:1207] start decoder qkv layer_attn 2 layer_mlp 2
DEBUG 10-14 21:17:31 lpmodule.py:526] hidden_states.shape torch.Size([10, 1, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:31 tutils.py:81] single group 1 real attn out cost 0.0032181739807128906 s
DEBUG 10-14 21:17:31 lpmodule.py:541] kv_seq_len 1 past_key_value 512
DEBUG 10-14 21:17:31 lpmodule.py:545] inv_freq device cpu
DEBUG 10-14 21:17:31 lpmodule.py:546] kv_seq_len 513 position_ids torch.Size([10, 1]) value tensor([512])
DEBUG 10-14 21:17:31 lpmodule.py:551] query_states device cuda:1
DEBUG 10-14 21:17:31 lpmodule.py:552] key_states device cuda:1
DEBUG 10-14 21:17:31 lpmodule.py:553] value_states device cuda:1
DEBUG 10-14 21:17:31 lpmodule.py:554] position_ids device cpu
DEBUG 10-14 21:17:31 lpmodule.py:555] cos device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:31 lpmodule.py:556] sin device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:31 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:31 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:31 tutils.py:81] single group 2 real attn out cost 0.003583192825317383 s
DEBUG 10-14 21:17:31 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:31 tutils.py:81] single group 3 real attn out cost 0.0034592151641845703 s
DEBUG 10-14 21:17:31 tutils.py:88] dot attn help cost 0.023086 seconds
DEBUG 10-14 21:17:31 lpmodule.py:433] dot attn cost 0.023171 seconds
DEBUG 10-14 21:17:31 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.00024580955505371094 s
DEBUG 10-14 21:17:31 lpllm.py:2280] CPU attn cost 0.026735 seconds if batch True
DEBUG 10-14 21:17:31 lpllm.py:2289] deal attn result cost 0.000002 seconds
DEBUG 10-14 21:17:31 lpllm.py:2291] CPU compute cost 0.027580 seconds
DEBUG 10-14 21:17:31 lpllm.py:2309] free cost 0.000139 seconds
DEBUG 10-14 21:17:31 lpmodule.py:162] call mlp prepare
DEBUG 10-14 21:17:31 lpmodule.py:163] layer idx 2
DEBUG 10-14 21:17:31 lpmodule.py:123] Tokens per expert: [8, 8, 2, 0, 2, 0, 0, 0]
DEBUG 10-14 21:17:31 lpmodule.py:124] Number of active experts: 4
DEBUG 10-14 21:17:31 lpmodule.py:195] call mlp post
DEBUG 10-14 21:17:31 lpllm.py:2262] GPU2CPU move cost 0.000305 seconds
DEBUG 10-14 21:17:31 lpmodule.py:340] layer_idx 2 attn layer_idx: 2
DEBUG 10-14 21:17:31 lpmodule.py:352] get seq_length 512 in decoder_attn_batch
WARNING 10-14 21:17:31 lpmodule.py:355] generally decoder_attn_batch called with q_len == 1
DEBUG 10-14 21:17:31 lpmodule.py:364] decoder_attn_batch update batch_dim 10-20
DEBUG 10-14 21:17:31 StaticCacheLen.py:84] static cache update layer_idx: 2, start_batch: 10, end_batch: 20
DEBUG 10-14 21:17:31 StaticCacheLen.py:90] static cache update layer_idx: 2, update seq_length to 513
DEBUG 10-14 21:17:31 lpmodule.py:368] update for kv cache 10-20 kv_seq_len 513 in decoder_attn_batch
DEBUG 10-14 21:17:31 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:31 lpmodule.py:374] update past key value cost 0.001804 seconds
DEBUG 10-14 21:17:31 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 513, 128]), value shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:31 lpmodule.py:399] repeat qkv cost 0.000000 seconds
DEBUG 10-14 21:17:31 lpmodule.py:400] q shape torch.Size([10, 32, 1, 128]), k shape torch.Size([10, 8, 513, 128]), v shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:31 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:31 lpllm.py:498] reset update attn
DEBUG 10-14 21:17:31 lpllm.py:1771] update state cost 3.0040740966796875e-05 s
DEBUG 10-14 21:17:31 lpllm.py:1740] restore layer func cost 0.0007169246673583984 s
DEBUG 10-14 21:17:31 lpllm.py:511] restore layer cost 0.0009877681732177734 s
DEBUG 10-14 21:17:31 lpllm.py:512] reset_next_layer_need: layer_attn_idx=3, layer_mlp_idx=2, j_loc=5
DEBUG 10-14 21:17:31 lpllm.py:1280] reset layer cost 0.0011296272277832031 s
DEBUG 10-14 21:17:31 lpllm.py:1281] have reset next layer layer_attn 2 layer_mlp 2 
DEBUG 10-14 21:17:31 lpllm.py:1287] j: 5 waiting the layer with layer_idx 3 before wait time 0.10344886779785156 s
INFO 10-14 21:17:31 client.py:117] confirm_model_loaded: Mixtral-8x7B, 8018eb63-71cc-495e-83b1-67d391e25a86
DEBUG 10-14 21:17:31 tutils.py:81] single group 0 real attn out cost 0.006387948989868164 s
DEBUG 10-14 21:17:31 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:31 tutils.py:81] single group 1 real attn out cost 0.0030603408813476562 s
DEBUG 10-14 21:17:31 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:31 tutils.py:81] single group 2 real attn out cost 0.0029535293579101562 s
DEBUG 10-14 21:17:31 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:31 tutils.py:81] single group 3 real attn out cost 0.002810239791870117 s
DEBUG 10-14 21:17:31 tutils.py:88] dot attn help cost 0.020610 seconds
DEBUG 10-14 21:17:31 lpmodule.py:433] dot attn cost 0.020740 seconds
DEBUG 10-14 21:17:31 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.00030922889709472656 s
DEBUG 10-14 21:17:31 lpllm.py:2280] CPU attn cost 0.023167 seconds if batch True
DEBUG 10-14 21:17:31 lpllm.py:2289] deal attn result cost 0.000003 seconds
DEBUG 10-14 21:17:31 lpllm.py:2291] CPU compute cost 0.023660 seconds
DEBUG 10-14 21:17:31 lpllm.py:2309] free cost 0.000113 seconds
INFO 10-14 21:17:32 client.py:125] Model loaded
DEBUG 10-14 21:17:32 lpllm.py:1291] j: load cost 0.3365025520324707 s waiting cost 0.23302459716796875 s
DEBUG 10-14 21:17:32 lpllm.py:1200] 
DEBUG 10-14 21:17:32 lpllm.py:1200] one decoder loop j: 6 cur_layer_idx: 3
DEBUG 10-14 21:17:32 lpllm.py:1205] start load next layer cur_layer_idx: 4
DEBUG 10-14 21:17:32 lpllm.py:1801] cuda memory handles: {1: <capsule object NULL at 0x7fdf3c6d9ec0>} {1: b'\xe0\xaa29\x00\x00\x00\x00\xe8&\x1d\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\x86\x00\x00\x00\xff\x00\x00.\x00\x00\x00\x00\x00\x00\x00)\xd8\xd5\xc1\xa0\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-14 21:17:32 client.py:72] load_into_gpu: Mixtral-8x7B, a995f49c-6e68-4eee-88e6-f52657d18cba
INFO 10-14 21:17:32 client.py:113] Model loaded: Mixtral-8x7B, a995f49c-6e68-4eee-88e6-f52657d18cba
DEBUG 10-14 21:17:32 lpllm.py:1207] start decoder qkv layer_attn 3 layer_mlp 2
DEBUG 10-14 21:17:32 lpmodule.py:526] hidden_states.shape torch.Size([10, 1, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:32 lpmodule.py:541] kv_seq_len 1 past_key_value 512
DEBUG 10-14 21:17:32 lpmodule.py:545] inv_freq device cpu
DEBUG 10-14 21:17:32 lpmodule.py:546] kv_seq_len 513 position_ids torch.Size([10, 1]) value tensor([512])
DEBUG 10-14 21:17:32 lpmodule.py:551] query_states device cuda:1
DEBUG 10-14 21:17:32 lpmodule.py:552] key_states device cuda:1
DEBUG 10-14 21:17:32 lpmodule.py:553] value_states device cuda:1
DEBUG 10-14 21:17:32 lpmodule.py:554] position_ids device cpu
DEBUG 10-14 21:17:32 lpmodule.py:555] cos device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:32 lpmodule.py:556] sin device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:32 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:32 lpmodule.py:162] call mlp prepare
DEBUG 10-14 21:17:32 lpmodule.py:163] layer idx 2
DEBUG 10-14 21:17:32 lpmodule.py:123] Tokens per expert: [1, 0, 0, 0, 9, 0, 10, 0]
DEBUG 10-14 21:17:32 lpmodule.py:124] Number of active experts: 3
DEBUG 10-14 21:17:32 lpmodule.py:195] call mlp post
DEBUG 10-14 21:17:32 lpllm.py:2262] GPU2CPU move cost 0.000334 seconds
DEBUG 10-14 21:17:32 lpmodule.py:340] layer_idx 3 attn layer_idx: 3
DEBUG 10-14 21:17:32 lpmodule.py:352] get seq_length 512 in decoder_attn_batch
WARNING 10-14 21:17:32 lpmodule.py:355] generally decoder_attn_batch called with q_len == 1
DEBUG 10-14 21:17:32 lpmodule.py:364] decoder_attn_batch update batch_dim 0-10
DEBUG 10-14 21:17:32 StaticCacheLen.py:84] static cache update layer_idx: 3, start_batch: 0, end_batch: 10
DEBUG 10-14 21:17:32 lpmodule.py:368] update for kv cache 0-10 kv_seq_len 513 in decoder_attn_batch
DEBUG 10-14 21:17:32 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:32 lpmodule.py:374] update past key value cost 0.002540 seconds
DEBUG 10-14 21:17:32 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 513, 128]), value shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:32 lpmodule.py:399] repeat qkv cost 0.000001 seconds
DEBUG 10-14 21:17:32 lpmodule.py:400] q shape torch.Size([10, 32, 1, 128]), k shape torch.Size([10, 8, 513, 128]), v shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:32 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:32 lpllm.py:503] reset update experts
DEBUG 10-14 21:17:32 lpllm.py:1771] update state cost 2.574920654296875e-05 s
DEBUG 10-14 21:17:32 lpllm.py:1740] restore layer func cost 0.0018548965454101562 s
DEBUG 10-14 21:17:32 lpllm.py:511] restore layer cost 0.002165555953979492 s
DEBUG 10-14 21:17:32 lpllm.py:512] reset_next_layer_need: layer_attn_idx=3, layer_mlp_idx=3, j_loc=6
DEBUG 10-14 21:17:32 lpllm.py:1280] reset layer cost 0.0022945404052734375 s
DEBUG 10-14 21:17:32 lpllm.py:1281] have reset next layer layer_attn 3 layer_mlp 2 
DEBUG 10-14 21:17:32 lpllm.py:1200] 
DEBUG 10-14 21:17:32 lpllm.py:1200] one decoder loop j: 7 cur_layer_idx: 3
DEBUG 10-14 21:17:32 lpllm.py:1207] start decoder qkv layer_attn 3 layer_mlp 3
DEBUG 10-14 21:17:32 lpmodule.py:526] hidden_states.shape torch.Size([10, 1, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:32 lpmodule.py:541] kv_seq_len 1 past_key_value 512
DEBUG 10-14 21:17:32 lpmodule.py:545] inv_freq device cpu
DEBUG 10-14 21:17:32 lpmodule.py:546] kv_seq_len 513 position_ids torch.Size([10, 1]) value tensor([512])
DEBUG 10-14 21:17:32 lpmodule.py:551] query_states device cuda:1
DEBUG 10-14 21:17:32 tutils.py:81] single group 0 real attn out cost 0.006070137023925781 s
DEBUG 10-14 21:17:32 lpmodule.py:552] key_states device cuda:1
DEBUG 10-14 21:17:32 lpmodule.py:553] value_states device cuda:1
DEBUG 10-14 21:17:32 lpmodule.py:554] position_ids device cpu
DEBUG 10-14 21:17:32 lpmodule.py:555] cos device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:32 lpmodule.py:556] sin device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:32 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:32 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:32 tutils.py:81] single group 1 real attn out cost 0.0029435157775878906 s
DEBUG 10-14 21:17:32 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:32 tutils.py:81] single group 2 real attn out cost 0.0032444000244140625 s
DEBUG 10-14 21:17:32 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:32 tutils.py:81] single group 3 real attn out cost 0.003558635711669922 s
DEBUG 10-14 21:17:32 tutils.py:88] dot attn help cost 0.023727 seconds
DEBUG 10-14 21:17:32 lpmodule.py:433] dot attn cost 0.023857 seconds
DEBUG 10-14 21:17:32 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.00038623809814453125 s
DEBUG 10-14 21:17:32 lpllm.py:2280] CPU attn cost 0.027556 seconds if batch True
DEBUG 10-14 21:17:32 lpllm.py:2289] deal attn result cost 0.000004 seconds
DEBUG 10-14 21:17:32 lpllm.py:2291] CPU compute cost 0.028238 seconds
DEBUG 10-14 21:17:32 lpllm.py:2309] free cost 0.000218 seconds
DEBUG 10-14 21:17:32 lpmodule.py:162] call mlp prepare
DEBUG 10-14 21:17:32 lpmodule.py:163] layer idx 3
DEBUG 10-14 21:17:32 lpmodule.py:123] Tokens per expert: [3, 0, 5, 0, 2, 5, 0, 5]
DEBUG 10-14 21:17:32 lpmodule.py:124] Number of active experts: 5
DEBUG 10-14 21:17:32 lpmodule.py:195] call mlp post
DEBUG 10-14 21:17:32 lpllm.py:2262] GPU2CPU move cost 0.000425 seconds
DEBUG 10-14 21:17:32 lpmodule.py:340] layer_idx 3 attn layer_idx: 3
DEBUG 10-14 21:17:32 lpmodule.py:352] get seq_length 512 in decoder_attn_batch
WARNING 10-14 21:17:32 lpmodule.py:355] generally decoder_attn_batch called with q_len == 1
DEBUG 10-14 21:17:32 lpmodule.py:364] decoder_attn_batch update batch_dim 10-20
DEBUG 10-14 21:17:32 StaticCacheLen.py:84] static cache update layer_idx: 3, start_batch: 10, end_batch: 20
DEBUG 10-14 21:17:32 StaticCacheLen.py:90] static cache update layer_idx: 3, update seq_length to 513
DEBUG 10-14 21:17:32 lpmodule.py:368] update for kv cache 10-20 kv_seq_len 513 in decoder_attn_batch
DEBUG 10-14 21:17:32 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:32 lpmodule.py:374] update past key value cost 0.002583 seconds
DEBUG 10-14 21:17:32 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 513, 128]), value shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:32 lpmodule.py:399] repeat qkv cost 0.000001 seconds
DEBUG 10-14 21:17:32 lpmodule.py:400] q shape torch.Size([10, 32, 1, 128]), k shape torch.Size([10, 8, 513, 128]), v shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:32 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:32 lpllm.py:498] reset update attn
DEBUG 10-14 21:17:32 lpllm.py:1771] update state cost 3.4809112548828125e-05 s
DEBUG 10-14 21:17:32 lpllm.py:1740] restore layer func cost 0.0008466243743896484 s
DEBUG 10-14 21:17:32 lpllm.py:511] restore layer cost 0.0011641979217529297 s
DEBUG 10-14 21:17:32 lpllm.py:512] reset_next_layer_need: layer_attn_idx=4, layer_mlp_idx=3, j_loc=7
DEBUG 10-14 21:17:32 lpllm.py:1280] reset layer cost 0.0013206005096435547 s
DEBUG 10-14 21:17:32 lpllm.py:1281] have reset next layer layer_attn 3 layer_mlp 3 
DEBUG 10-14 21:17:32 lpllm.py:1287] j: 7 waiting the layer with layer_idx 4 before wait time 0.06440949440002441 s
INFO 10-14 21:17:32 client.py:117] confirm_model_loaded: Mixtral-8x7B, a995f49c-6e68-4eee-88e6-f52657d18cba
DEBUG 10-14 21:17:32 tutils.py:81] single group 0 real attn out cost 0.005903005599975586 s
DEBUG 10-14 21:17:32 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:32 tutils.py:81] single group 1 real attn out cost 0.003532886505126953 s
DEBUG 10-14 21:17:32 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:32 tutils.py:81] single group 2 real attn out cost 0.003142118453979492 s
DEBUG 10-14 21:17:32 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:32 tutils.py:81] single group 3 real attn out cost 0.003138303756713867 s
DEBUG 10-14 21:17:32 tutils.py:88] dot attn help cost 0.022495 seconds
DEBUG 10-14 21:17:32 lpmodule.py:433] dot attn cost 0.022625 seconds
DEBUG 10-14 21:17:32 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.001313924789428711 s
DEBUG 10-14 21:17:32 lpllm.py:2280] CPU attn cost 0.027253 seconds if batch True
DEBUG 10-14 21:17:32 lpllm.py:2289] deal attn result cost 0.000003 seconds
DEBUG 10-14 21:17:32 lpllm.py:2291] CPU compute cost 0.028020 seconds
DEBUG 10-14 21:17:32 lpllm.py:2309] free cost 0.000168 seconds
INFO 10-14 21:17:32 client.py:125] Model loaded
DEBUG 10-14 21:17:32 lpllm.py:1291] j: load cost 0.2937440872192383 s waiting cost 0.22929739952087402 s
DEBUG 10-14 21:17:32 lpllm.py:1200] 
DEBUG 10-14 21:17:32 lpllm.py:1200] one decoder loop j: 8 cur_layer_idx: 4
DEBUG 10-14 21:17:32 lpllm.py:1205] start load next layer cur_layer_idx: 5
DEBUG 10-14 21:17:32 lpllm.py:1801] cuda memory handles: {1: <capsule object NULL at 0x7fdf3c6d9f80>} {1: b'\xe0\xaa29\x00\x00\x00\x00\xe8&\x1d\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\x86\x00\x00\x00\xff\x00\x00/\x00\x00\x00\x00\x00\x00\x00)\xd8\xd5\xc1\xa2\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-14 21:17:32 client.py:72] load_into_gpu: Mixtral-8x7B, d0c9ba74-685a-4bd3-af3a-c3de7dc56f9d
INFO 10-14 21:17:32 client.py:113] Model loaded: Mixtral-8x7B, d0c9ba74-685a-4bd3-af3a-c3de7dc56f9d
DEBUG 10-14 21:17:32 lpllm.py:1207] start decoder qkv layer_attn 4 layer_mlp 3
DEBUG 10-14 21:17:32 lpmodule.py:526] hidden_states.shape torch.Size([10, 1, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:32 lpmodule.py:541] kv_seq_len 1 past_key_value 512
DEBUG 10-14 21:17:32 lpmodule.py:545] inv_freq device cpu
DEBUG 10-14 21:17:32 lpmodule.py:546] kv_seq_len 513 position_ids torch.Size([10, 1]) value tensor([512])
DEBUG 10-14 21:17:32 lpmodule.py:551] query_states device cuda:1
DEBUG 10-14 21:17:32 lpmodule.py:552] key_states device cuda:1
DEBUG 10-14 21:17:32 lpmodule.py:553] value_states device cuda:1
DEBUG 10-14 21:17:32 lpmodule.py:554] position_ids device cpu
DEBUG 10-14 21:17:32 lpmodule.py:555] cos device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:32 lpmodule.py:556] sin device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:32 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:32 lpmodule.py:162] call mlp prepare
DEBUG 10-14 21:17:32 lpmodule.py:163] layer idx 3
DEBUG 10-14 21:17:32 lpmodule.py:123] Tokens per expert: [0, 0, 1, 0, 9, 0, 10, 0]
DEBUG 10-14 21:17:32 lpmodule.py:124] Number of active experts: 3
DEBUG 10-14 21:17:32 lpmodule.py:195] call mlp post
DEBUG 10-14 21:17:32 lpllm.py:2262] GPU2CPU move cost 0.000348 seconds
DEBUG 10-14 21:17:32 lpmodule.py:340] layer_idx 4 attn layer_idx: 4
DEBUG 10-14 21:17:32 lpmodule.py:352] get seq_length 512 in decoder_attn_batch
WARNING 10-14 21:17:32 lpmodule.py:355] generally decoder_attn_batch called with q_len == 1
DEBUG 10-14 21:17:32 lpmodule.py:364] decoder_attn_batch update batch_dim 0-10
DEBUG 10-14 21:17:32 StaticCacheLen.py:84] static cache update layer_idx: 4, start_batch: 0, end_batch: 10
DEBUG 10-14 21:17:32 lpmodule.py:368] update for kv cache 0-10 kv_seq_len 513 in decoder_attn_batch
DEBUG 10-14 21:17:32 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:32 lpmodule.py:374] update past key value cost 0.002702 seconds
DEBUG 10-14 21:17:32 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 513, 128]), value shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:32 lpmodule.py:399] repeat qkv cost 0.000001 seconds
DEBUG 10-14 21:17:32 lpmodule.py:400] q shape torch.Size([10, 32, 1, 128]), k shape torch.Size([10, 8, 513, 128]), v shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:32 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:32 lpllm.py:503] reset update experts
DEBUG 10-14 21:17:32 lpllm.py:1771] update state cost 3.0994415283203125e-05 s
DEBUG 10-14 21:17:32 lpllm.py:1740] restore layer func cost 0.0021903514862060547 s
DEBUG 10-14 21:17:32 lpllm.py:511] restore layer cost 0.0025103092193603516 s
DEBUG 10-14 21:17:32 lpllm.py:512] reset_next_layer_need: layer_attn_idx=4, layer_mlp_idx=4, j_loc=8
DEBUG 10-14 21:17:32 lpllm.py:1280] reset layer cost 0.0026378631591796875 s
DEBUG 10-14 21:17:32 lpllm.py:1281] have reset next layer layer_attn 4 layer_mlp 3 
DEBUG 10-14 21:17:32 lpllm.py:1200] 
DEBUG 10-14 21:17:32 lpllm.py:1200] one decoder loop j: 9 cur_layer_idx: 4
DEBUG 10-14 21:17:32 lpllm.py:1207] start decoder qkv layer_attn 4 layer_mlp 4
DEBUG 10-14 21:17:32 lpmodule.py:526] hidden_states.shape torch.Size([10, 1, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:32 lpmodule.py:541] kv_seq_len 1 past_key_value 512
DEBUG 10-14 21:17:32 lpmodule.py:545] inv_freq device cpu
DEBUG 10-14 21:17:32 lpmodule.py:546] kv_seq_len 513 position_ids torch.Size([10, 1]) value tensor([512])
DEBUG 10-14 21:17:32 lpmodule.py:551] query_states device cuda:1
DEBUG 10-14 21:17:32 tutils.py:81] single group 0 real attn out cost 0.006596565246582031 s
DEBUG 10-14 21:17:32 lpmodule.py:552] key_states device cuda:1
DEBUG 10-14 21:17:32 lpmodule.py:553] value_states device cuda:1
DEBUG 10-14 21:17:32 lpmodule.py:554] position_ids device cpu
DEBUG 10-14 21:17:32 lpmodule.py:555] cos device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:32 lpmodule.py:556] sin device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:32 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:32 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:32 tutils.py:81] single group 1 real attn out cost 0.0032491683959960938 s
DEBUG 10-14 21:17:32 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:32 tutils.py:81] single group 2 real attn out cost 0.0031211376190185547 s
DEBUG 10-14 21:17:32 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:32 tutils.py:81] single group 3 real attn out cost 0.0030984878540039062 s
DEBUG 10-14 21:17:32 tutils.py:88] dot attn help cost 0.022442 seconds
DEBUG 10-14 21:17:32 lpmodule.py:433] dot attn cost 0.022566 seconds
DEBUG 10-14 21:17:32 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.0003044605255126953 s
DEBUG 10-14 21:17:32 lpllm.py:2280] CPU attn cost 0.026435 seconds if batch True
DEBUG 10-14 21:17:32 lpllm.py:2289] deal attn result cost 0.000003 seconds
DEBUG 10-14 21:17:32 lpllm.py:2291] CPU compute cost 0.027169 seconds
DEBUG 10-14 21:17:32 lpllm.py:2309] free cost 0.000165 seconds
DEBUG 10-14 21:17:32 lpmodule.py:162] call mlp prepare
DEBUG 10-14 21:17:32 lpmodule.py:163] layer idx 4
DEBUG 10-14 21:17:32 lpmodule.py:123] Tokens per expert: [5, 5, 5, 0, 0, 0, 0, 5]
DEBUG 10-14 21:17:32 lpmodule.py:124] Number of active experts: 4
DEBUG 10-14 21:17:32 lpmodule.py:195] call mlp post
DEBUG 10-14 21:17:32 lpllm.py:2262] GPU2CPU move cost 0.000272 seconds
DEBUG 10-14 21:17:32 lpmodule.py:340] layer_idx 4 attn layer_idx: 4
DEBUG 10-14 21:17:32 lpmodule.py:352] get seq_length 512 in decoder_attn_batch
WARNING 10-14 21:17:32 lpmodule.py:355] generally decoder_attn_batch called with q_len == 1
DEBUG 10-14 21:17:32 lpmodule.py:364] decoder_attn_batch update batch_dim 10-20
DEBUG 10-14 21:17:32 StaticCacheLen.py:84] static cache update layer_idx: 4, start_batch: 10, end_batch: 20
DEBUG 10-14 21:17:32 StaticCacheLen.py:90] static cache update layer_idx: 4, update seq_length to 513
DEBUG 10-14 21:17:32 lpmodule.py:368] update for kv cache 10-20 kv_seq_len 513 in decoder_attn_batch
DEBUG 10-14 21:17:32 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:32 lpmodule.py:374] update past key value cost 0.002402 seconds
DEBUG 10-14 21:17:32 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 513, 128]), value shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:32 lpmodule.py:399] repeat qkv cost 0.000001 seconds
DEBUG 10-14 21:17:32 lpmodule.py:400] q shape torch.Size([10, 32, 1, 128]), k shape torch.Size([10, 8, 513, 128]), v shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:32 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:32 lpllm.py:498] reset update attn
DEBUG 10-14 21:17:32 lpllm.py:1771] update state cost 3.24249267578125e-05 s
DEBUG 10-14 21:17:32 lpllm.py:1740] restore layer func cost 0.0007669925689697266 s
DEBUG 10-14 21:17:32 lpllm.py:511] restore layer cost 0.001055002212524414 s
DEBUG 10-14 21:17:32 lpllm.py:512] reset_next_layer_need: layer_attn_idx=5, layer_mlp_idx=4, j_loc=9
DEBUG 10-14 21:17:32 lpllm.py:1280] reset layer cost 0.001191854476928711 s
DEBUG 10-14 21:17:32 lpllm.py:1281] have reset next layer layer_attn 4 layer_mlp 4 
DEBUG 10-14 21:17:32 lpllm.py:1287] j: 9 waiting the layer with layer_idx 5 before wait time 0.05969643592834473 s
INFO 10-14 21:17:32 client.py:117] confirm_model_loaded: Mixtral-8x7B, d0c9ba74-685a-4bd3-af3a-c3de7dc56f9d
DEBUG 10-14 21:17:32 tutils.py:81] single group 0 real attn out cost 0.006072998046875 s
DEBUG 10-14 21:17:32 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:32 tutils.py:81] single group 1 real attn out cost 0.004336833953857422 s
DEBUG 10-14 21:17:32 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:32 tutils.py:81] single group 2 real attn out cost 0.003612518310546875 s
DEBUG 10-14 21:17:32 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:32 tutils.py:81] single group 3 real attn out cost 0.0030558109283447266 s
DEBUG 10-14 21:17:32 tutils.py:88] dot attn help cost 0.023519 seconds
DEBUG 10-14 21:17:32 lpmodule.py:433] dot attn cost 0.023650 seconds
DEBUG 10-14 21:17:32 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.0010037422180175781 s
DEBUG 10-14 21:17:32 lpllm.py:2280] CPU attn cost 0.027680 seconds if batch True
DEBUG 10-14 21:17:32 lpllm.py:2289] deal attn result cost 0.000004 seconds
DEBUG 10-14 21:17:32 lpllm.py:2291] CPU compute cost 0.028282 seconds
DEBUG 10-14 21:17:32 lpllm.py:2309] free cost 0.000177 seconds
INFO 10-14 21:17:32 client.py:125] Model loaded
DEBUG 10-14 21:17:32 lpllm.py:1291] j: load cost 0.30315065383911133 s waiting cost 0.2434241771697998 s
DEBUG 10-14 21:17:32 lpllm.py:1200] 
DEBUG 10-14 21:17:32 lpllm.py:1200] one decoder loop j: 10 cur_layer_idx: 5
DEBUG 10-14 21:17:32 lpllm.py:1205] start load next layer cur_layer_idx: 6
DEBUG 10-14 21:17:32 lpllm.py:1801] cuda memory handles: {1: <capsule object NULL at 0x7fdf3c6da010>} {1: b'\xe0\xaa29\x00\x00\x00\x00\xe8&\x1d\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\x86\x00\x00\x00\xff\x00\x000\x00\x00\x00\x00\x00\x00\x00)\xd8\xd5\xc1\xa4\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-14 21:17:32 client.py:72] load_into_gpu: Mixtral-8x7B, 28c9b6e8-88af-419c-bf10-a1e7f805d3ac
INFO 10-14 21:17:32 client.py:113] Model loaded: Mixtral-8x7B, 28c9b6e8-88af-419c-bf10-a1e7f805d3ac
DEBUG 10-14 21:17:32 lpllm.py:1207] start decoder qkv layer_attn 5 layer_mlp 4
DEBUG 10-14 21:17:32 lpmodule.py:526] hidden_states.shape torch.Size([10, 1, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:32 lpmodule.py:541] kv_seq_len 1 past_key_value 512
DEBUG 10-14 21:17:32 lpmodule.py:545] inv_freq device cpu
DEBUG 10-14 21:17:32 lpmodule.py:546] kv_seq_len 513 position_ids torch.Size([10, 1]) value tensor([512])
DEBUG 10-14 21:17:32 lpmodule.py:551] query_states device cuda:1
DEBUG 10-14 21:17:32 lpmodule.py:552] key_states device cuda:1
DEBUG 10-14 21:17:32 lpmodule.py:553] value_states device cuda:1
DEBUG 10-14 21:17:32 lpmodule.py:554] position_ids device cpu
DEBUG 10-14 21:17:32 lpmodule.py:555] cos device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:32 lpmodule.py:556] sin device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:32 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:32 lpmodule.py:162] call mlp prepare
DEBUG 10-14 21:17:32 lpmodule.py:163] layer idx 4
DEBUG 10-14 21:17:32 lpmodule.py:123] Tokens per expert: [0, 1, 0, 0, 9, 1, 9, 0]
DEBUG 10-14 21:17:32 lpmodule.py:124] Number of active experts: 4
DEBUG 10-14 21:17:32 lpmodule.py:195] call mlp post
DEBUG 10-14 21:17:32 lpllm.py:2262] GPU2CPU move cost 0.000471 seconds
DEBUG 10-14 21:17:32 lpmodule.py:340] layer_idx 5 attn layer_idx: 5
DEBUG 10-14 21:17:32 lpmodule.py:352] get seq_length 512 in decoder_attn_batch
WARNING 10-14 21:17:32 lpmodule.py:355] generally decoder_attn_batch called with q_len == 1
DEBUG 10-14 21:17:32 lpmodule.py:364] decoder_attn_batch update batch_dim 0-10
DEBUG 10-14 21:17:32 StaticCacheLen.py:84] static cache update layer_idx: 5, start_batch: 0, end_batch: 10
DEBUG 10-14 21:17:32 lpmodule.py:368] update for kv cache 0-10 kv_seq_len 513 in decoder_attn_batch
DEBUG 10-14 21:17:32 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:32 lpmodule.py:374] update past key value cost 0.002859 seconds
DEBUG 10-14 21:17:32 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 513, 128]), value shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:32 lpmodule.py:399] repeat qkv cost 0.000001 seconds
DEBUG 10-14 21:17:32 lpmodule.py:400] q shape torch.Size([10, 32, 1, 128]), k shape torch.Size([10, 8, 513, 128]), v shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:32 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:32 lpllm.py:503] reset update experts
DEBUG 10-14 21:17:32 lpllm.py:1771] update state cost 2.4080276489257812e-05 s
DEBUG 10-14 21:17:32 lpllm.py:1740] restore layer func cost 0.0016574859619140625 s
DEBUG 10-14 21:17:32 lpllm.py:511] restore layer cost 0.0019376277923583984 s
DEBUG 10-14 21:17:32 lpllm.py:512] reset_next_layer_need: layer_attn_idx=5, layer_mlp_idx=5, j_loc=10
DEBUG 10-14 21:17:32 lpllm.py:1280] reset layer cost 0.0020508766174316406 s
DEBUG 10-14 21:17:32 lpllm.py:1281] have reset next layer layer_attn 5 layer_mlp 4 
DEBUG 10-14 21:17:32 lpllm.py:1200] 
DEBUG 10-14 21:17:32 lpllm.py:1200] one decoder loop j: 11 cur_layer_idx: 5
DEBUG 10-14 21:17:32 lpllm.py:1207] start decoder qkv layer_attn 5 layer_mlp 5
DEBUG 10-14 21:17:32 lpmodule.py:526] hidden_states.shape torch.Size([10, 1, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:32 lpmodule.py:541] kv_seq_len 1 past_key_value 512
DEBUG 10-14 21:17:32 lpmodule.py:545] inv_freq device cpu
DEBUG 10-14 21:17:32 lpmodule.py:546] kv_seq_len 513 position_ids torch.Size([10, 1]) value tensor([512])
DEBUG 10-14 21:17:32 tutils.py:81] single group 0 real attn out cost 0.006124019622802734 s
DEBUG 10-14 21:17:32 lpmodule.py:551] query_states device cuda:1
DEBUG 10-14 21:17:32 lpmodule.py:552] key_states device cuda:1
DEBUG 10-14 21:17:32 lpmodule.py:553] value_states device cuda:1
DEBUG 10-14 21:17:32 lpmodule.py:554] position_ids device cpu
DEBUG 10-14 21:17:32 lpmodule.py:555] cos device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:32 lpmodule.py:556] sin device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:32 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:32 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:32 tutils.py:81] single group 1 real attn out cost 0.0040738582611083984 s
DEBUG 10-14 21:17:32 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:32 tutils.py:81] single group 2 real attn out cost 0.0027909278869628906 s
DEBUG 10-14 21:17:32 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:32 tutils.py:81] single group 3 real attn out cost 0.0027282238006591797 s
DEBUG 10-14 21:17:32 tutils.py:88] dot attn help cost 0.021482 seconds
DEBUG 10-14 21:17:32 lpmodule.py:433] dot attn cost 0.021622 seconds
DEBUG 10-14 21:17:32 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.00032448768615722656 s
DEBUG 10-14 21:17:32 lpllm.py:2280] CPU attn cost 0.025785 seconds if batch True
DEBUG 10-14 21:17:32 lpllm.py:2289] deal attn result cost 0.000004 seconds
DEBUG 10-14 21:17:32 lpllm.py:2291] CPU compute cost 0.026628 seconds
DEBUG 10-14 21:17:32 lpllm.py:2309] free cost 0.000191 seconds
DEBUG 10-14 21:17:32 lpmodule.py:162] call mlp prepare
DEBUG 10-14 21:17:32 lpmodule.py:163] layer idx 5
DEBUG 10-14 21:17:32 lpmodule.py:123] Tokens per expert: [5, 0, 0, 7, 5, 0, 3, 0]
DEBUG 10-14 21:17:32 lpmodule.py:124] Number of active experts: 4
DEBUG 10-14 21:17:32 lpmodule.py:195] call mlp post
DEBUG 10-14 21:17:32 lpllm.py:2262] GPU2CPU move cost 0.000357 seconds
DEBUG 10-14 21:17:32 lpmodule.py:340] layer_idx 5 attn layer_idx: 5
DEBUG 10-14 21:17:32 lpmodule.py:352] get seq_length 512 in decoder_attn_batch
WARNING 10-14 21:17:32 lpmodule.py:355] generally decoder_attn_batch called with q_len == 1
DEBUG 10-14 21:17:32 lpmodule.py:364] decoder_attn_batch update batch_dim 10-20
DEBUG 10-14 21:17:32 StaticCacheLen.py:84] static cache update layer_idx: 5, start_batch: 10, end_batch: 20
DEBUG 10-14 21:17:32 StaticCacheLen.py:90] static cache update layer_idx: 5, update seq_length to 513
DEBUG 10-14 21:17:32 lpmodule.py:368] update for kv cache 10-20 kv_seq_len 513 in decoder_attn_batch
DEBUG 10-14 21:17:32 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:32 lpmodule.py:374] update past key value cost 0.002475 seconds
DEBUG 10-14 21:17:32 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 513, 128]), value shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:32 lpmodule.py:399] repeat qkv cost 0.000001 seconds
DEBUG 10-14 21:17:32 lpmodule.py:400] q shape torch.Size([10, 32, 1, 128]), k shape torch.Size([10, 8, 513, 128]), v shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:32 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:32 lpllm.py:498] reset update attn
DEBUG 10-14 21:17:32 lpllm.py:1771] update state cost 3.600120544433594e-05 s
DEBUG 10-14 21:17:32 lpllm.py:1740] restore layer func cost 0.0008878707885742188 s
DEBUG 10-14 21:17:32 lpllm.py:511] restore layer cost 0.0012164115905761719 s
DEBUG 10-14 21:17:32 lpllm.py:512] reset_next_layer_need: layer_attn_idx=6, layer_mlp_idx=5, j_loc=11
DEBUG 10-14 21:17:32 lpllm.py:1280] reset layer cost 0.0013785362243652344 s
DEBUG 10-14 21:17:32 lpllm.py:1281] have reset next layer layer_attn 5 layer_mlp 5 
DEBUG 10-14 21:17:32 lpllm.py:1287] j: 11 waiting the layer with layer_idx 6 before wait time 0.060845375061035156 s
INFO 10-14 21:17:32 client.py:117] confirm_model_loaded: Mixtral-8x7B, 28c9b6e8-88af-419c-bf10-a1e7f805d3ac
DEBUG 10-14 21:17:32 tutils.py:81] single group 0 real attn out cost 0.006377458572387695 s
DEBUG 10-14 21:17:32 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:32 tutils.py:81] single group 1 real attn out cost 0.0031211376190185547 s
DEBUG 10-14 21:17:32 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:32 tutils.py:81] single group 2 real attn out cost 0.00365447998046875 s
DEBUG 10-14 21:17:32 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:32 tutils.py:81] single group 3 real attn out cost 0.003165006637573242 s
DEBUG 10-14 21:17:32 tutils.py:88] dot attn help cost 0.022345 seconds
DEBUG 10-14 21:17:32 lpmodule.py:433] dot attn cost 0.022461 seconds
DEBUG 10-14 21:17:32 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.0008497238159179688 s
DEBUG 10-14 21:17:32 lpllm.py:2280] CPU attn cost 0.026460 seconds if batch True
DEBUG 10-14 21:17:32 lpllm.py:2289] deal attn result cost 0.000003 seconds
DEBUG 10-14 21:17:32 lpllm.py:2291] CPU compute cost 0.027177 seconds
DEBUG 10-14 21:17:32 lpllm.py:2309] free cost 0.000142 seconds
INFO 10-14 21:17:33 client.py:125] Model loaded
DEBUG 10-14 21:17:33 lpllm.py:1291] j: load cost 0.30094408988952637 s waiting cost 0.24005794525146484 s
DEBUG 10-14 21:17:33 lpllm.py:1200] 
DEBUG 10-14 21:17:33 lpllm.py:1200] one decoder loop j: 12 cur_layer_idx: 6
DEBUG 10-14 21:17:33 lpllm.py:1205] start load next layer cur_layer_idx: 7
DEBUG 10-14 21:17:33 lpllm.py:1801] cuda memory handles: {1: <capsule object NULL at 0x7fdf3c6d9ec0>} {1: b'\xe0\xaa29\x00\x00\x00\x00\xe8&\x1d\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\x86\x00\x00\x00\xff\x00\x00.\x00\x00\x00\x00\x00\x00\x00)\xd8\xd5\xc1\xa0\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-14 21:17:33 client.py:72] load_into_gpu: Mixtral-8x7B, 5a2694cc-710e-4dae-a5b1-01641b674ae6
INFO 10-14 21:17:33 client.py:113] Model loaded: Mixtral-8x7B, 5a2694cc-710e-4dae-a5b1-01641b674ae6
DEBUG 10-14 21:17:33 lpllm.py:1207] start decoder qkv layer_attn 6 layer_mlp 5
DEBUG 10-14 21:17:33 lpmodule.py:526] hidden_states.shape torch.Size([10, 1, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:33 lpmodule.py:541] kv_seq_len 1 past_key_value 512
DEBUG 10-14 21:17:33 lpmodule.py:545] inv_freq device cpu
DEBUG 10-14 21:17:33 lpmodule.py:546] kv_seq_len 513 position_ids torch.Size([10, 1]) value tensor([512])
DEBUG 10-14 21:17:33 lpmodule.py:551] query_states device cuda:1
DEBUG 10-14 21:17:33 lpmodule.py:552] key_states device cuda:1
DEBUG 10-14 21:17:33 lpmodule.py:553] value_states device cuda:1
DEBUG 10-14 21:17:33 lpmodule.py:554] position_ids device cpu
DEBUG 10-14 21:17:33 lpmodule.py:555] cos device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:33 lpmodule.py:556] sin device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:33 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:33 lpmodule.py:162] call mlp prepare
DEBUG 10-14 21:17:33 lpmodule.py:163] layer idx 5
DEBUG 10-14 21:17:33 lpmodule.py:123] Tokens per expert: [0, 1, 0, 0, 9, 0, 0, 10]
DEBUG 10-14 21:17:33 lpmodule.py:124] Number of active experts: 3
DEBUG 10-14 21:17:33 lpmodule.py:195] call mlp post
DEBUG 10-14 21:17:33 lpllm.py:2262] GPU2CPU move cost 0.000373 seconds
DEBUG 10-14 21:17:33 lpmodule.py:340] layer_idx 6 attn layer_idx: 6
DEBUG 10-14 21:17:33 lpmodule.py:352] get seq_length 512 in decoder_attn_batch
WARNING 10-14 21:17:33 lpmodule.py:355] generally decoder_attn_batch called with q_len == 1
DEBUG 10-14 21:17:33 lpmodule.py:364] decoder_attn_batch update batch_dim 0-10
DEBUG 10-14 21:17:33 StaticCacheLen.py:84] static cache update layer_idx: 6, start_batch: 0, end_batch: 10
DEBUG 10-14 21:17:33 lpmodule.py:368] update for kv cache 0-10 kv_seq_len 513 in decoder_attn_batch
DEBUG 10-14 21:17:33 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:33 lpmodule.py:374] update past key value cost 0.002583 seconds
DEBUG 10-14 21:17:33 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 513, 128]), value shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:33 lpmodule.py:399] repeat qkv cost 0.000001 seconds
DEBUG 10-14 21:17:33 lpmodule.py:400] q shape torch.Size([10, 32, 1, 128]), k shape torch.Size([10, 8, 513, 128]), v shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:33 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:33 lpllm.py:503] reset update experts
DEBUG 10-14 21:17:33 lpllm.py:1771] update state cost 2.6226043701171875e-05 s
DEBUG 10-14 21:17:33 lpllm.py:1740] restore layer func cost 0.0017023086547851562 s
DEBUG 10-14 21:17:33 lpllm.py:511] restore layer cost 0.0020503997802734375 s
DEBUG 10-14 21:17:33 lpllm.py:512] reset_next_layer_need: layer_attn_idx=6, layer_mlp_idx=6, j_loc=12
DEBUG 10-14 21:17:33 lpllm.py:1280] reset layer cost 0.0021886825561523438 s
DEBUG 10-14 21:17:33 lpllm.py:1281] have reset next layer layer_attn 6 layer_mlp 5 
DEBUG 10-14 21:17:33 lpllm.py:1200] 
DEBUG 10-14 21:17:33 lpllm.py:1200] one decoder loop j: 13 cur_layer_idx: 6
DEBUG 10-14 21:17:33 lpllm.py:1207] start decoder qkv layer_attn 6 layer_mlp 6
DEBUG 10-14 21:17:33 lpmodule.py:526] hidden_states.shape torch.Size([10, 1, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:33 lpmodule.py:541] kv_seq_len 1 past_key_value 512
DEBUG 10-14 21:17:33 tutils.py:81] single group 0 real attn out cost 0.005023002624511719 s
DEBUG 10-14 21:17:33 lpmodule.py:545] inv_freq device cpu
DEBUG 10-14 21:17:33 lpmodule.py:546] kv_seq_len 513 position_ids torch.Size([10, 1]) value tensor([512])
DEBUG 10-14 21:17:33 lpmodule.py:551] query_states device cuda:1
DEBUG 10-14 21:17:33 lpmodule.py:552] key_states device cuda:1
DEBUG 10-14 21:17:33 lpmodule.py:553] value_states device cuda:1
DEBUG 10-14 21:17:33 lpmodule.py:554] position_ids device cpu
DEBUG 10-14 21:17:33 lpmodule.py:555] cos device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:33 lpmodule.py:556] sin device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:33 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:33 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:33 tutils.py:81] single group 1 real attn out cost 0.0032510757446289062 s
DEBUG 10-14 21:17:33 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:33 tutils.py:81] single group 2 real attn out cost 0.003221273422241211 s
DEBUG 10-14 21:17:33 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:33 tutils.py:81] single group 3 real attn out cost 0.0032634735107421875 s
DEBUG 10-14 21:17:33 tutils.py:88] dot attn help cost 0.022612 seconds
DEBUG 10-14 21:17:33 lpmodule.py:433] dot attn cost 0.022856 seconds
DEBUG 10-14 21:17:33 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.0003743171691894531 s
DEBUG 10-14 21:17:33 lpllm.py:2280] CPU attn cost 0.026700 seconds if batch True
DEBUG 10-14 21:17:33 lpllm.py:2289] deal attn result cost 0.000004 seconds
DEBUG 10-14 21:17:33 lpllm.py:2291] CPU compute cost 0.027467 seconds
DEBUG 10-14 21:17:33 lpllm.py:2309] free cost 0.000238 seconds
DEBUG 10-14 21:17:33 lpmodule.py:162] call mlp prepare
DEBUG 10-14 21:17:33 lpmodule.py:163] layer idx 6
DEBUG 10-14 21:17:33 lpmodule.py:123] Tokens per expert: [3, 0, 2, 2, 5, 8, 0, 0]
DEBUG 10-14 21:17:33 lpmodule.py:124] Number of active experts: 5
DEBUG 10-14 21:17:33 lpmodule.py:195] call mlp post
DEBUG 10-14 21:17:33 lpllm.py:2262] GPU2CPU move cost 0.000455 seconds
DEBUG 10-14 21:17:33 lpmodule.py:340] layer_idx 6 attn layer_idx: 6
DEBUG 10-14 21:17:33 lpmodule.py:352] get seq_length 512 in decoder_attn_batch
WARNING 10-14 21:17:33 lpmodule.py:355] generally decoder_attn_batch called with q_len == 1
DEBUG 10-14 21:17:33 lpmodule.py:364] decoder_attn_batch update batch_dim 10-20
DEBUG 10-14 21:17:33 StaticCacheLen.py:84] static cache update layer_idx: 6, start_batch: 10, end_batch: 20
DEBUG 10-14 21:17:33 StaticCacheLen.py:90] static cache update layer_idx: 6, update seq_length to 513
DEBUG 10-14 21:17:33 lpmodule.py:368] update for kv cache 10-20 kv_seq_len 513 in decoder_attn_batch
DEBUG 10-14 21:17:33 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:33 lpmodule.py:374] update past key value cost 0.002790 seconds
DEBUG 10-14 21:17:33 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 513, 128]), value shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:33 lpmodule.py:399] repeat qkv cost 0.000001 seconds
DEBUG 10-14 21:17:33 lpmodule.py:400] q shape torch.Size([10, 32, 1, 128]), k shape torch.Size([10, 8, 513, 128]), v shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:33 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:33 lpllm.py:498] reset update attn
DEBUG 10-14 21:17:33 lpllm.py:1771] update state cost 3.457069396972656e-05 s
DEBUG 10-14 21:17:33 lpllm.py:1740] restore layer func cost 0.0008742809295654297 s
DEBUG 10-14 21:17:33 lpllm.py:511] restore layer cost 0.0012192726135253906 s
DEBUG 10-14 21:17:33 lpllm.py:512] reset_next_layer_need: layer_attn_idx=7, layer_mlp_idx=6, j_loc=13
DEBUG 10-14 21:17:33 lpllm.py:1280] reset layer cost 0.0013854503631591797 s
DEBUG 10-14 21:17:33 lpllm.py:1281] have reset next layer layer_attn 6 layer_mlp 6 
DEBUG 10-14 21:17:33 lpllm.py:1287] j: 13 waiting the layer with layer_idx 7 before wait time 0.06086087226867676 s
INFO 10-14 21:17:33 client.py:117] confirm_model_loaded: Mixtral-8x7B, 5a2694cc-710e-4dae-a5b1-01641b674ae6
DEBUG 10-14 21:17:33 tutils.py:81] single group 0 real attn out cost 0.005763530731201172 s
DEBUG 10-14 21:17:33 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:33 tutils.py:81] single group 1 real attn out cost 0.0031473636627197266 s
DEBUG 10-14 21:17:33 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:33 tutils.py:81] single group 2 real attn out cost 0.002736806869506836 s
DEBUG 10-14 21:17:33 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:33 tutils.py:81] single group 3 real attn out cost 0.002719402313232422 s
DEBUG 10-14 21:17:33 tutils.py:88] dot attn help cost 0.020516 seconds
DEBUG 10-14 21:17:33 lpmodule.py:433] dot attn cost 0.020646 seconds
DEBUG 10-14 21:17:33 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.0007271766662597656 s
DEBUG 10-14 21:17:33 lpllm.py:2280] CPU attn cost 0.024957 seconds if batch True
DEBUG 10-14 21:17:33 lpllm.py:2289] deal attn result cost 0.000003 seconds
DEBUG 10-14 21:17:33 lpllm.py:2291] CPU compute cost 0.025731 seconds
DEBUG 10-14 21:17:33 lpllm.py:2309] free cost 0.000157 seconds
INFO 10-14 21:17:33 client.py:125] Model loaded
DEBUG 10-14 21:17:33 lpllm.py:1291] j: load cost 0.2907874584197998 s waiting cost 0.22988367080688477 s
DEBUG 10-14 21:17:33 lpllm.py:1200] 
DEBUG 10-14 21:17:33 lpllm.py:1200] one decoder loop j: 14 cur_layer_idx: 7
DEBUG 10-14 21:17:33 lpllm.py:1205] start load next layer cur_layer_idx: 8
DEBUG 10-14 21:17:33 lpllm.py:1801] cuda memory handles: {1: <capsule object NULL at 0x7fdf3c6d9f80>} {1: b'\xe0\xaa29\x00\x00\x00\x00\xe8&\x1d\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\x86\x00\x00\x00\xff\x00\x00/\x00\x00\x00\x00\x00\x00\x00)\xd8\xd5\xc1\xa2\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-14 21:17:33 client.py:72] load_into_gpu: Mixtral-8x7B, 552fb461-d53e-4d52-9d0f-181d5fd1aafc
INFO 10-14 21:17:33 client.py:113] Model loaded: Mixtral-8x7B, 552fb461-d53e-4d52-9d0f-181d5fd1aafc
DEBUG 10-14 21:17:33 lpllm.py:1207] start decoder qkv layer_attn 7 layer_mlp 6
DEBUG 10-14 21:17:33 lpmodule.py:526] hidden_states.shape torch.Size([10, 1, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:33 lpmodule.py:541] kv_seq_len 1 past_key_value 512
DEBUG 10-14 21:17:33 lpmodule.py:545] inv_freq device cpu
DEBUG 10-14 21:17:33 lpmodule.py:546] kv_seq_len 513 position_ids torch.Size([10, 1]) value tensor([512])
DEBUG 10-14 21:17:33 lpmodule.py:551] query_states device cuda:1
DEBUG 10-14 21:17:33 lpmodule.py:552] key_states device cuda:1
DEBUG 10-14 21:17:33 lpmodule.py:553] value_states device cuda:1
DEBUG 10-14 21:17:33 lpmodule.py:554] position_ids device cpu
DEBUG 10-14 21:17:33 lpmodule.py:555] cos device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:33 lpmodule.py:556] sin device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:33 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:33 lpmodule.py:162] call mlp prepare
DEBUG 10-14 21:17:33 lpmodule.py:163] layer idx 6
DEBUG 10-14 21:17:33 lpmodule.py:123] Tokens per expert: [9, 1, 0, 9, 0, 1, 0, 0]
DEBUG 10-14 21:17:33 lpmodule.py:124] Number of active experts: 4
DEBUG 10-14 21:17:33 lpmodule.py:195] call mlp post
DEBUG 10-14 21:17:33 lpllm.py:2262] GPU2CPU move cost 0.000535 seconds
DEBUG 10-14 21:17:33 lpmodule.py:340] layer_idx 7 attn layer_idx: 7
DEBUG 10-14 21:17:33 lpmodule.py:352] get seq_length 512 in decoder_attn_batch
WARNING 10-14 21:17:33 lpmodule.py:355] generally decoder_attn_batch called with q_len == 1
DEBUG 10-14 21:17:33 lpmodule.py:364] decoder_attn_batch update batch_dim 0-10
DEBUG 10-14 21:17:33 StaticCacheLen.py:84] static cache update layer_idx: 7, start_batch: 0, end_batch: 10
DEBUG 10-14 21:17:33 lpmodule.py:368] update for kv cache 0-10 kv_seq_len 513 in decoder_attn_batch
DEBUG 10-14 21:17:33 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:33 lpmodule.py:374] update past key value cost 0.002626 seconds
DEBUG 10-14 21:17:33 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 513, 128]), value shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:33 lpmodule.py:399] repeat qkv cost 0.000001 seconds
DEBUG 10-14 21:17:33 lpmodule.py:400] q shape torch.Size([10, 32, 1, 128]), k shape torch.Size([10, 8, 513, 128]), v shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:33 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:33 lpllm.py:503] reset update experts
DEBUG 10-14 21:17:33 lpllm.py:1771] update state cost 2.574920654296875e-05 s
DEBUG 10-14 21:17:33 lpllm.py:1740] restore layer func cost 0.001993417739868164 s
DEBUG 10-14 21:17:33 lpllm.py:511] restore layer cost 0.0022585391998291016 s
DEBUG 10-14 21:17:33 lpllm.py:512] reset_next_layer_need: layer_attn_idx=7, layer_mlp_idx=7, j_loc=14
DEBUG 10-14 21:17:33 lpllm.py:1280] reset layer cost 0.0023899078369140625 s
DEBUG 10-14 21:17:33 lpllm.py:1281] have reset next layer layer_attn 7 layer_mlp 6 
DEBUG 10-14 21:17:33 lpllm.py:1200] 
DEBUG 10-14 21:17:33 lpllm.py:1200] one decoder loop j: 15 cur_layer_idx: 7
DEBUG 10-14 21:17:33 lpllm.py:1207] start decoder qkv layer_attn 7 layer_mlp 7
DEBUG 10-14 21:17:33 lpmodule.py:526] hidden_states.shape torch.Size([10, 1, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:33 lpmodule.py:541] kv_seq_len 1 past_key_value 512
DEBUG 10-14 21:17:33 lpmodule.py:545] inv_freq device cpu
DEBUG 10-14 21:17:33 lpmodule.py:546] kv_seq_len 513 position_ids torch.Size([10, 1]) value tensor([512])
DEBUG 10-14 21:17:33 lpmodule.py:551] query_states device cuda:1
DEBUG 10-14 21:17:33 lpmodule.py:552] key_states device cuda:1
DEBUG 10-14 21:17:33 lpmodule.py:553] value_states device cuda:1
DEBUG 10-14 21:17:33 lpmodule.py:554] position_ids device cpu
DEBUG 10-14 21:17:33 lpmodule.py:555] cos device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:33 lpmodule.py:556] sin device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:33 tutils.py:81] single group 0 real attn out cost 0.0059642791748046875 s
DEBUG 10-14 21:17:33 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:33 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:33 tutils.py:81] single group 1 real attn out cost 0.003238201141357422 s
DEBUG 10-14 21:17:33 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:33 tutils.py:81] single group 2 real attn out cost 0.0031023025512695312 s
DEBUG 10-14 21:17:33 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:33 tutils.py:81] single group 3 real attn out cost 0.0030574798583984375 s
DEBUG 10-14 21:17:33 tutils.py:88] dot attn help cost 0.021788 seconds
DEBUG 10-14 21:17:33 lpmodule.py:433] dot attn cost 0.021911 seconds
DEBUG 10-14 21:17:33 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.00030350685119628906 s
DEBUG 10-14 21:17:33 lpllm.py:2280] CPU attn cost 0.025756 seconds if batch True
DEBUG 10-14 21:17:33 lpllm.py:2289] deal attn result cost 0.000003 seconds
DEBUG 10-14 21:17:33 lpllm.py:2291] CPU compute cost 0.026680 seconds
DEBUG 10-14 21:17:33 lpllm.py:2309] free cost 0.000168 seconds
DEBUG 10-14 21:17:33 lpmodule.py:162] call mlp prepare
DEBUG 10-14 21:17:33 lpmodule.py:163] layer idx 7
DEBUG 10-14 21:17:33 lpmodule.py:123] Tokens per expert: [0, 0, 8, 0, 0, 7, 0, 5]
DEBUG 10-14 21:17:33 lpmodule.py:124] Number of active experts: 3
DEBUG 10-14 21:17:33 lpmodule.py:195] call mlp post
DEBUG 10-14 21:17:33 lpllm.py:2262] GPU2CPU move cost 0.000323 seconds
DEBUG 10-14 21:17:33 lpmodule.py:340] layer_idx 7 attn layer_idx: 7
DEBUG 10-14 21:17:33 lpmodule.py:352] get seq_length 512 in decoder_attn_batch
WARNING 10-14 21:17:33 lpmodule.py:355] generally decoder_attn_batch called with q_len == 1
DEBUG 10-14 21:17:33 lpmodule.py:364] decoder_attn_batch update batch_dim 10-20
DEBUG 10-14 21:17:33 StaticCacheLen.py:84] static cache update layer_idx: 7, start_batch: 10, end_batch: 20
DEBUG 10-14 21:17:33 StaticCacheLen.py:90] static cache update layer_idx: 7, update seq_length to 513
DEBUG 10-14 21:17:33 lpmodule.py:368] update for kv cache 10-20 kv_seq_len 513 in decoder_attn_batch
DEBUG 10-14 21:17:33 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:33 lpmodule.py:374] update past key value cost 0.002506 seconds
DEBUG 10-14 21:17:33 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 513, 128]), value shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:33 lpmodule.py:399] repeat qkv cost 0.000001 seconds
DEBUG 10-14 21:17:33 lpmodule.py:400] q shape torch.Size([10, 32, 1, 128]), k shape torch.Size([10, 8, 513, 128]), v shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:33 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:33 lpllm.py:498] reset update attn
DEBUG 10-14 21:17:33 lpllm.py:1771] update state cost 3.361701965332031e-05 s
DEBUG 10-14 21:17:33 lpllm.py:1740] restore layer func cost 0.0007579326629638672 s
DEBUG 10-14 21:17:33 lpllm.py:511] restore layer cost 0.0010275840759277344 s
DEBUG 10-14 21:17:33 lpllm.py:512] reset_next_layer_need: layer_attn_idx=8, layer_mlp_idx=7, j_loc=15
DEBUG 10-14 21:17:33 lpllm.py:1280] reset layer cost 0.00115966796875 s
DEBUG 10-14 21:17:33 lpllm.py:1281] have reset next layer layer_attn 7 layer_mlp 7 
DEBUG 10-14 21:17:33 lpllm.py:1287] j: 15 waiting the layer with layer_idx 8 before wait time 0.05517578125 s
INFO 10-14 21:17:33 client.py:117] confirm_model_loaded: Mixtral-8x7B, 552fb461-d53e-4d52-9d0f-181d5fd1aafc
DEBUG 10-14 21:17:33 tutils.py:81] single group 0 real attn out cost 0.00516510009765625 s
DEBUG 10-14 21:17:33 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:33 tutils.py:81] single group 1 real attn out cost 0.0030922889709472656 s
DEBUG 10-14 21:17:33 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:33 tutils.py:81] single group 2 real attn out cost 0.0032205581665039062 s
DEBUG 10-14 21:17:33 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:33 tutils.py:81] single group 3 real attn out cost 0.004019498825073242 s
DEBUG 10-14 21:17:33 tutils.py:88] dot attn help cost 0.022761 seconds
DEBUG 10-14 21:17:33 lpmodule.py:433] dot attn cost 0.022876 seconds
DEBUG 10-14 21:17:33 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.0007586479187011719 s
DEBUG 10-14 21:17:33 lpllm.py:2280] CPU attn cost 0.026752 seconds if batch True
DEBUG 10-14 21:17:33 lpllm.py:2289] deal attn result cost 0.000003 seconds
DEBUG 10-14 21:17:33 lpllm.py:2291] CPU compute cost 0.027377 seconds
DEBUG 10-14 21:17:33 lpllm.py:2309] free cost 0.000147 seconds
INFO 10-14 21:17:33 client.py:125] Model loaded
DEBUG 10-14 21:17:33 lpllm.py:1291] j: load cost 0.29825878143310547 s waiting cost 0.24305391311645508 s
DEBUG 10-14 21:17:33 lpllm.py:1200] 
DEBUG 10-14 21:17:33 lpllm.py:1200] one decoder loop j: 16 cur_layer_idx: 8
DEBUG 10-14 21:17:33 lpllm.py:1205] start load next layer cur_layer_idx: 9
DEBUG 10-14 21:17:33 lpllm.py:1801] cuda memory handles: {1: <capsule object NULL at 0x7fdf3c6da010>} {1: b'\xe0\xaa29\x00\x00\x00\x00\xe8&\x1d\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\x86\x00\x00\x00\xff\x00\x000\x00\x00\x00\x00\x00\x00\x00)\xd8\xd5\xc1\xa4\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-14 21:17:33 client.py:72] load_into_gpu: Mixtral-8x7B, 1c17df3e-49e1-42f0-8b47-d4b500ecd201
INFO 10-14 21:17:33 client.py:113] Model loaded: Mixtral-8x7B, 1c17df3e-49e1-42f0-8b47-d4b500ecd201
DEBUG 10-14 21:17:33 lpllm.py:1207] start decoder qkv layer_attn 8 layer_mlp 7
DEBUG 10-14 21:17:33 lpmodule.py:526] hidden_states.shape torch.Size([10, 1, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:33 lpmodule.py:541] kv_seq_len 1 past_key_value 512
DEBUG 10-14 21:17:33 lpmodule.py:545] inv_freq device cpu
DEBUG 10-14 21:17:33 lpmodule.py:546] kv_seq_len 513 position_ids torch.Size([10, 1]) value tensor([512])
DEBUG 10-14 21:17:33 lpmodule.py:551] query_states device cuda:1
DEBUG 10-14 21:17:33 lpmodule.py:552] key_states device cuda:1
DEBUG 10-14 21:17:33 lpmodule.py:553] value_states device cuda:1
DEBUG 10-14 21:17:33 lpmodule.py:554] position_ids device cpu
DEBUG 10-14 21:17:33 lpmodule.py:555] cos device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:33 lpmodule.py:556] sin device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:33 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:33 lpmodule.py:162] call mlp prepare
DEBUG 10-14 21:17:33 lpmodule.py:163] layer idx 7
DEBUG 10-14 21:17:33 lpmodule.py:123] Tokens per expert: [0, 9, 1, 0, 9, 1, 0, 0]
DEBUG 10-14 21:17:33 lpmodule.py:124] Number of active experts: 4
DEBUG 10-14 21:17:33 lpmodule.py:195] call mlp post
DEBUG 10-14 21:17:33 lpllm.py:2262] GPU2CPU move cost 0.000365 seconds
DEBUG 10-14 21:17:33 lpmodule.py:340] layer_idx 8 attn layer_idx: 8
DEBUG 10-14 21:17:33 lpmodule.py:352] get seq_length 512 in decoder_attn_batch
WARNING 10-14 21:17:33 lpmodule.py:355] generally decoder_attn_batch called with q_len == 1
DEBUG 10-14 21:17:33 lpmodule.py:364] decoder_attn_batch update batch_dim 0-10
DEBUG 10-14 21:17:33 StaticCacheLen.py:84] static cache update layer_idx: 8, start_batch: 0, end_batch: 10
DEBUG 10-14 21:17:33 lpmodule.py:368] update for kv cache 0-10 kv_seq_len 513 in decoder_attn_batch
DEBUG 10-14 21:17:33 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:33 lpmodule.py:374] update past key value cost 0.002488 seconds
DEBUG 10-14 21:17:33 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 513, 128]), value shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:33 lpmodule.py:399] repeat qkv cost 0.000001 seconds
DEBUG 10-14 21:17:33 lpmodule.py:400] q shape torch.Size([10, 32, 1, 128]), k shape torch.Size([10, 8, 513, 128]), v shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:33 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:33 lpllm.py:503] reset update experts
DEBUG 10-14 21:17:33 lpllm.py:1771] update state cost 4.696846008300781e-05 s
DEBUG 10-14 21:17:33 tutils.py:81] single group 0 real attn out cost 0.00592041015625 s
DEBUG 10-14 21:17:33 lpllm.py:1740] restore layer func cost 0.002720355987548828 s
DEBUG 10-14 21:17:33 lpllm.py:511] restore layer cost 0.0032224655151367188 s
DEBUG 10-14 21:17:33 lpllm.py:512] reset_next_layer_need: layer_attn_idx=8, layer_mlp_idx=8, j_loc=16
DEBUG 10-14 21:17:33 lpllm.py:1280] reset layer cost 0.0034911632537841797 s
DEBUG 10-14 21:17:33 lpllm.py:1281] have reset next layer layer_attn 8 layer_mlp 7 
DEBUG 10-14 21:17:33 lpllm.py:1200] 
DEBUG 10-14 21:17:33 lpllm.py:1200] one decoder loop j: 17 cur_layer_idx: 8
DEBUG 10-14 21:17:33 lpllm.py:1207] start decoder qkv layer_attn 8 layer_mlp 8
DEBUG 10-14 21:17:33 lpmodule.py:526] hidden_states.shape torch.Size([10, 1, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:33 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:33 lpmodule.py:541] kv_seq_len 1 past_key_value 512
DEBUG 10-14 21:17:33 lpmodule.py:545] inv_freq device cpu
DEBUG 10-14 21:17:33 lpmodule.py:546] kv_seq_len 513 position_ids torch.Size([10, 1]) value tensor([512])
DEBUG 10-14 21:17:33 lpmodule.py:551] query_states device cuda:1
DEBUG 10-14 21:17:33 lpmodule.py:552] key_states device cuda:1
DEBUG 10-14 21:17:33 lpmodule.py:553] value_states device cuda:1
DEBUG 10-14 21:17:33 lpmodule.py:554] position_ids device cpu
DEBUG 10-14 21:17:33 lpmodule.py:555] cos device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:33 lpmodule.py:556] sin device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:33 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:33 tutils.py:81] single group 1 real attn out cost 0.003610372543334961 s
DEBUG 10-14 21:17:33 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:33 tutils.py:81] single group 2 real attn out cost 0.0031173229217529297 s
DEBUG 10-14 21:17:33 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:33 tutils.py:81] single group 3 real attn out cost 0.003286600112915039 s
DEBUG 10-14 21:17:33 tutils.py:88] dot attn help cost 0.023416 seconds
DEBUG 10-14 21:17:33 lpmodule.py:433] dot attn cost 0.023534 seconds
DEBUG 10-14 21:17:33 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.0005702972412109375 s
DEBUG 10-14 21:17:33 lpllm.py:2280] CPU attn cost 0.027393 seconds if batch True
DEBUG 10-14 21:17:33 lpllm.py:2289] deal attn result cost 0.000003 seconds
DEBUG 10-14 21:17:33 lpllm.py:2291] CPU compute cost 0.028218 seconds
DEBUG 10-14 21:17:33 lpllm.py:2309] free cost 0.000169 seconds
DEBUG 10-14 21:17:33 lpmodule.py:162] call mlp prepare
DEBUG 10-14 21:17:33 lpmodule.py:163] layer idx 8
DEBUG 10-14 21:17:33 lpmodule.py:123] Tokens per expert: [0, 2, 8, 0, 0, 5, 0, 5]
DEBUG 10-14 21:17:33 lpmodule.py:124] Number of active experts: 4
DEBUG 10-14 21:17:33 lpmodule.py:195] call mlp post
DEBUG 10-14 21:17:33 lpllm.py:2262] GPU2CPU move cost 0.000353 seconds
DEBUG 10-14 21:17:33 lpmodule.py:340] layer_idx 8 attn layer_idx: 8
DEBUG 10-14 21:17:33 lpmodule.py:352] get seq_length 512 in decoder_attn_batch
WARNING 10-14 21:17:33 lpmodule.py:355] generally decoder_attn_batch called with q_len == 1
DEBUG 10-14 21:17:33 lpmodule.py:364] decoder_attn_batch update batch_dim 10-20
DEBUG 10-14 21:17:33 StaticCacheLen.py:84] static cache update layer_idx: 8, start_batch: 10, end_batch: 20
DEBUG 10-14 21:17:33 StaticCacheLen.py:90] static cache update layer_idx: 8, update seq_length to 513
DEBUG 10-14 21:17:33 lpmodule.py:368] update for kv cache 10-20 kv_seq_len 513 in decoder_attn_batch
DEBUG 10-14 21:17:33 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:33 lpmodule.py:374] update past key value cost 0.002620 seconds
DEBUG 10-14 21:17:33 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 513, 128]), value shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:33 lpmodule.py:399] repeat qkv cost 0.000001 seconds
DEBUG 10-14 21:17:33 lpmodule.py:400] q shape torch.Size([10, 32, 1, 128]), k shape torch.Size([10, 8, 513, 128]), v shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:33 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:33 lpllm.py:498] reset update attn
DEBUG 10-14 21:17:33 lpllm.py:1771] update state cost 5.650520324707031e-05 s
DEBUG 10-14 21:17:33 lpllm.py:1740] restore layer func cost 0.0010211467742919922 s
DEBUG 10-14 21:17:33 lpllm.py:511] restore layer cost 0.00141143798828125 s
DEBUG 10-14 21:17:33 lpllm.py:512] reset_next_layer_need: layer_attn_idx=9, layer_mlp_idx=8, j_loc=17
DEBUG 10-14 21:17:33 lpllm.py:1280] reset layer cost 0.0015904903411865234 s
DEBUG 10-14 21:17:33 lpllm.py:1281] have reset next layer layer_attn 8 layer_mlp 8 
DEBUG 10-14 21:17:33 lpllm.py:1287] j: 17 waiting the layer with layer_idx 9 before wait time 0.06209373474121094 s
INFO 10-14 21:17:33 client.py:117] confirm_model_loaded: Mixtral-8x7B, 1c17df3e-49e1-42f0-8b47-d4b500ecd201
DEBUG 10-14 21:17:33 tutils.py:81] single group 0 real attn out cost 0.00575566291809082 s
DEBUG 10-14 21:17:33 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:33 tutils.py:81] single group 1 real attn out cost 0.0030410289764404297 s
DEBUG 10-14 21:17:33 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:33 tutils.py:81] single group 2 real attn out cost 0.0031201839447021484 s
DEBUG 10-14 21:17:33 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:33 tutils.py:81] single group 3 real attn out cost 0.0036139488220214844 s
DEBUG 10-14 21:17:33 tutils.py:88] dot attn help cost 0.021130 seconds
DEBUG 10-14 21:17:33 lpmodule.py:433] dot attn cost 0.021255 seconds
DEBUG 10-14 21:17:33 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.0012049674987792969 s
DEBUG 10-14 21:17:33 lpllm.py:2280] CPU attn cost 0.025820 seconds if batch True
DEBUG 10-14 21:17:33 lpllm.py:2289] deal attn result cost 0.000003 seconds
DEBUG 10-14 21:17:33 lpllm.py:2291] CPU compute cost 0.026505 seconds
DEBUG 10-14 21:17:33 lpllm.py:2309] free cost 0.000154 seconds
INFO 10-14 21:17:33 client.py:125] Model loaded
DEBUG 10-14 21:17:33 lpllm.py:1291] j: load cost 0.29045748710632324 s waiting cost 0.2283344268798828 s
DEBUG 10-14 21:17:33 lpllm.py:1200] 
DEBUG 10-14 21:17:33 lpllm.py:1200] one decoder loop j: 18 cur_layer_idx: 9
DEBUG 10-14 21:17:33 lpllm.py:1205] start load next layer cur_layer_idx: 10
DEBUG 10-14 21:17:33 lpllm.py:1801] cuda memory handles: {1: <capsule object NULL at 0x7fdf3c6d9ec0>} {1: b'\xe0\xaa29\x00\x00\x00\x00\xe8&\x1d\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\x86\x00\x00\x00\xff\x00\x00.\x00\x00\x00\x00\x00\x00\x00)\xd8\xd5\xc1\xa0\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-14 21:17:33 client.py:72] load_into_gpu: Mixtral-8x7B, 8db865e6-5831-489a-834c-afef468a9486
INFO 10-14 21:17:33 client.py:113] Model loaded: Mixtral-8x7B, 8db865e6-5831-489a-834c-afef468a9486
DEBUG 10-14 21:17:33 lpllm.py:1207] start decoder qkv layer_attn 9 layer_mlp 8
DEBUG 10-14 21:17:33 lpmodule.py:526] hidden_states.shape torch.Size([10, 1, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:33 lpmodule.py:541] kv_seq_len 1 past_key_value 512
DEBUG 10-14 21:17:33 lpmodule.py:545] inv_freq device cpu
DEBUG 10-14 21:17:33 lpmodule.py:546] kv_seq_len 513 position_ids torch.Size([10, 1]) value tensor([512])
DEBUG 10-14 21:17:33 lpmodule.py:551] query_states device cuda:1
DEBUG 10-14 21:17:33 lpmodule.py:552] key_states device cuda:1
DEBUG 10-14 21:17:33 lpmodule.py:553] value_states device cuda:1
DEBUG 10-14 21:17:33 lpmodule.py:554] position_ids device cpu
DEBUG 10-14 21:17:33 lpmodule.py:555] cos device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:33 lpmodule.py:556] sin device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:33 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:33 lpmodule.py:162] call mlp prepare
DEBUG 10-14 21:17:33 lpmodule.py:163] layer idx 8
DEBUG 10-14 21:17:33 lpmodule.py:123] Tokens per expert: [0, 10, 0, 9, 0, 1, 0, 0]
DEBUG 10-14 21:17:33 lpmodule.py:124] Number of active experts: 3
DEBUG 10-14 21:17:33 lpmodule.py:195] call mlp post
DEBUG 10-14 21:17:33 lpllm.py:2262] GPU2CPU move cost 0.000355 seconds
DEBUG 10-14 21:17:33 lpmodule.py:340] layer_idx 9 attn layer_idx: 9
DEBUG 10-14 21:17:33 lpmodule.py:352] get seq_length 512 in decoder_attn_batch
WARNING 10-14 21:17:33 lpmodule.py:355] generally decoder_attn_batch called with q_len == 1
DEBUG 10-14 21:17:33 lpmodule.py:364] decoder_attn_batch update batch_dim 0-10
DEBUG 10-14 21:17:33 StaticCacheLen.py:84] static cache update layer_idx: 9, start_batch: 0, end_batch: 10
DEBUG 10-14 21:17:33 lpmodule.py:368] update for kv cache 0-10 kv_seq_len 513 in decoder_attn_batch
DEBUG 10-14 21:17:33 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:33 lpmodule.py:374] update past key value cost 0.002654 seconds
DEBUG 10-14 21:17:33 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 513, 128]), value shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:33 lpmodule.py:399] repeat qkv cost 0.000002 seconds
DEBUG 10-14 21:17:33 lpmodule.py:400] q shape torch.Size([10, 32, 1, 128]), k shape torch.Size([10, 8, 513, 128]), v shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:33 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:33 lpllm.py:503] reset update experts
DEBUG 10-14 21:17:33 lpllm.py:1771] update state cost 2.7179718017578125e-05 s
DEBUG 10-14 21:17:33 lpllm.py:1740] restore layer func cost 0.0013685226440429688 s
DEBUG 10-14 21:17:33 lpllm.py:511] restore layer cost 0.0016503334045410156 s
DEBUG 10-14 21:17:33 lpllm.py:512] reset_next_layer_need: layer_attn_idx=9, layer_mlp_idx=9, j_loc=18
DEBUG 10-14 21:17:33 lpllm.py:1280] reset layer cost 0.0017685890197753906 s
DEBUG 10-14 21:17:33 lpllm.py:1281] have reset next layer layer_attn 9 layer_mlp 8 
DEBUG 10-14 21:17:33 lpllm.py:1200] 
DEBUG 10-14 21:17:33 lpllm.py:1200] one decoder loop j: 19 cur_layer_idx: 9
DEBUG 10-14 21:17:33 lpllm.py:1207] start decoder qkv layer_attn 9 layer_mlp 9
DEBUG 10-14 21:17:33 lpmodule.py:526] hidden_states.shape torch.Size([10, 1, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:33 lpmodule.py:541] kv_seq_len 1 past_key_value 512
DEBUG 10-14 21:17:33 lpmodule.py:545] inv_freq device cpu
DEBUG 10-14 21:17:33 lpmodule.py:546] kv_seq_len 513 position_ids torch.Size([10, 1]) value tensor([512])
DEBUG 10-14 21:17:33 tutils.py:81] single group 0 real attn out cost 0.005885124206542969 s
DEBUG 10-14 21:17:33 lpmodule.py:551] query_states device cuda:1
DEBUG 10-14 21:17:33 lpmodule.py:552] key_states device cuda:1
DEBUG 10-14 21:17:33 lpmodule.py:553] value_states device cuda:1
DEBUG 10-14 21:17:33 lpmodule.py:554] position_ids device cpu
DEBUG 10-14 21:17:33 lpmodule.py:555] cos device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:33 lpmodule.py:556] sin device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:33 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:33 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:33 tutils.py:81] single group 1 real attn out cost 0.0031049251556396484 s
DEBUG 10-14 21:17:33 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:33 tutils.py:81] single group 2 real attn out cost 0.003504514694213867 s
DEBUG 10-14 21:17:33 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:33 tutils.py:81] single group 3 real attn out cost 0.0033447742462158203 s
DEBUG 10-14 21:17:33 tutils.py:88] dot attn help cost 0.022587 seconds
DEBUG 10-14 21:17:33 lpmodule.py:433] dot attn cost 0.022794 seconds
DEBUG 10-14 21:17:33 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.0004124641418457031 s
DEBUG 10-14 21:17:33 lpllm.py:2280] CPU attn cost 0.026766 seconds if batch True
DEBUG 10-14 21:17:33 lpllm.py:2289] deal attn result cost 0.000004 seconds
DEBUG 10-14 21:17:33 lpllm.py:2291] CPU compute cost 0.027541 seconds
DEBUG 10-14 21:17:33 lpllm.py:2309] free cost 0.000242 seconds
DEBUG 10-14 21:17:33 lpmodule.py:162] call mlp prepare
DEBUG 10-14 21:17:33 lpmodule.py:163] layer idx 9
DEBUG 10-14 21:17:33 lpmodule.py:123] Tokens per expert: [3, 3, 7, 0, 0, 5, 0, 2]
DEBUG 10-14 21:17:33 lpmodule.py:124] Number of active experts: 5
DEBUG 10-14 21:17:33 lpmodule.py:195] call mlp post
DEBUG 10-14 21:17:33 lpllm.py:2262] GPU2CPU move cost 0.000578 seconds
DEBUG 10-14 21:17:33 lpmodule.py:340] layer_idx 9 attn layer_idx: 9
DEBUG 10-14 21:17:33 lpmodule.py:352] get seq_length 512 in decoder_attn_batch
WARNING 10-14 21:17:33 lpmodule.py:355] generally decoder_attn_batch called with q_len == 1
DEBUG 10-14 21:17:33 lpmodule.py:364] decoder_attn_batch update batch_dim 10-20
DEBUG 10-14 21:17:33 StaticCacheLen.py:84] static cache update layer_idx: 9, start_batch: 10, end_batch: 20
DEBUG 10-14 21:17:33 StaticCacheLen.py:90] static cache update layer_idx: 9, update seq_length to 513
DEBUG 10-14 21:17:33 lpmodule.py:368] update for kv cache 10-20 kv_seq_len 513 in decoder_attn_batch
DEBUG 10-14 21:17:33 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:33 lpmodule.py:374] update past key value cost 0.002636 seconds
DEBUG 10-14 21:17:33 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 513, 128]), value shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:33 lpmodule.py:399] repeat qkv cost 0.000001 seconds
DEBUG 10-14 21:17:33 lpmodule.py:400] q shape torch.Size([10, 32, 1, 128]), k shape torch.Size([10, 8, 513, 128]), v shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:33 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:33 lpllm.py:498] reset update attn
DEBUG 10-14 21:17:33 lpllm.py:1771] update state cost 3.266334533691406e-05 s
DEBUG 10-14 21:17:33 lpllm.py:1740] restore layer func cost 0.0008928775787353516 s
DEBUG 10-14 21:17:33 lpllm.py:511] restore layer cost 0.0011928081512451172 s
DEBUG 10-14 21:17:33 lpllm.py:512] reset_next_layer_need: layer_attn_idx=10, layer_mlp_idx=9, j_loc=19
DEBUG 10-14 21:17:33 lpllm.py:1280] reset layer cost 0.001316070556640625 s
DEBUG 10-14 21:17:33 lpllm.py:1281] have reset next layer layer_attn 9 layer_mlp 9 
DEBUG 10-14 21:17:33 lpllm.py:1287] j: 19 waiting the layer with layer_idx 10 before wait time 0.06138944625854492 s
INFO 10-14 21:17:33 client.py:117] confirm_model_loaded: Mixtral-8x7B, 8db865e6-5831-489a-834c-afef468a9486
DEBUG 10-14 21:17:33 tutils.py:81] single group 0 real attn out cost 0.0070192813873291016 s
DEBUG 10-14 21:17:33 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:33 tutils.py:81] single group 1 real attn out cost 0.003073453903198242 s
DEBUG 10-14 21:17:33 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:33 tutils.py:81] single group 2 real attn out cost 0.002653360366821289 s
DEBUG 10-14 21:17:33 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:33 tutils.py:81] single group 3 real attn out cost 0.002696990966796875 s
DEBUG 10-14 21:17:33 tutils.py:88] dot attn help cost 0.021489 seconds
DEBUG 10-14 21:17:33 lpmodule.py:433] dot attn cost 0.021619 seconds
DEBUG 10-14 21:17:33 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.00042748451232910156 s
DEBUG 10-14 21:17:33 lpllm.py:2280] CPU attn cost 0.025571 seconds if batch True
DEBUG 10-14 21:17:33 lpllm.py:2289] deal attn result cost 0.000003 seconds
DEBUG 10-14 21:17:33 lpllm.py:2291] CPU compute cost 0.026523 seconds
DEBUG 10-14 21:17:33 lpllm.py:2309] free cost 0.000194 seconds
INFO 10-14 21:17:34 client.py:125] Model loaded
DEBUG 10-14 21:17:34 lpllm.py:1291] j: load cost 0.29369473457336426 s waiting cost 0.23227930068969727 s
DEBUG 10-14 21:17:34 lpllm.py:1200] 
DEBUG 10-14 21:17:34 lpllm.py:1200] one decoder loop j: 20 cur_layer_idx: 10
DEBUG 10-14 21:17:34 lpllm.py:1205] start load next layer cur_layer_idx: 11
DEBUG 10-14 21:17:34 lpllm.py:1801] cuda memory handles: {1: <capsule object NULL at 0x7fdf3c6d9f80>} {1: b'\xe0\xaa29\x00\x00\x00\x00\xe8&\x1d\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\x86\x00\x00\x00\xff\x00\x00/\x00\x00\x00\x00\x00\x00\x00)\xd8\xd5\xc1\xa2\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-14 21:17:34 client.py:72] load_into_gpu: Mixtral-8x7B, 1d5fc42c-9461-4bfa-8b21-586691de643e
INFO 10-14 21:17:34 client.py:113] Model loaded: Mixtral-8x7B, 1d5fc42c-9461-4bfa-8b21-586691de643e
DEBUG 10-14 21:17:34 lpllm.py:1207] start decoder qkv layer_attn 10 layer_mlp 9
DEBUG 10-14 21:17:34 lpmodule.py:526] hidden_states.shape torch.Size([10, 1, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:34 lpmodule.py:541] kv_seq_len 1 past_key_value 512
DEBUG 10-14 21:17:34 lpmodule.py:545] inv_freq device cpu
DEBUG 10-14 21:17:34 lpmodule.py:546] kv_seq_len 513 position_ids torch.Size([10, 1]) value tensor([512])
DEBUG 10-14 21:17:34 lpmodule.py:551] query_states device cuda:1
DEBUG 10-14 21:17:34 lpmodule.py:552] key_states device cuda:1
DEBUG 10-14 21:17:34 lpmodule.py:553] value_states device cuda:1
DEBUG 10-14 21:17:34 lpmodule.py:554] position_ids device cpu
DEBUG 10-14 21:17:34 lpmodule.py:555] cos device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:34 lpmodule.py:556] sin device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:34 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:34 lpmodule.py:162] call mlp prepare
DEBUG 10-14 21:17:34 lpmodule.py:163] layer idx 9
DEBUG 10-14 21:17:34 lpmodule.py:123] Tokens per expert: [10, 0, 0, 0, 0, 10, 0, 0]
DEBUG 10-14 21:17:34 lpmodule.py:124] Number of active experts: 2
DEBUG 10-14 21:17:34 lpmodule.py:195] call mlp post
DEBUG 10-14 21:17:34 lpllm.py:2262] GPU2CPU move cost 0.000497 seconds
DEBUG 10-14 21:17:34 lpmodule.py:340] layer_idx 10 attn layer_idx: 10
DEBUG 10-14 21:17:34 lpmodule.py:352] get seq_length 512 in decoder_attn_batch
WARNING 10-14 21:17:34 lpmodule.py:355] generally decoder_attn_batch called with q_len == 1
DEBUG 10-14 21:17:34 lpmodule.py:364] decoder_attn_batch update batch_dim 0-10
DEBUG 10-14 21:17:34 StaticCacheLen.py:84] static cache update layer_idx: 10, start_batch: 0, end_batch: 10
DEBUG 10-14 21:17:34 lpmodule.py:368] update for kv cache 0-10 kv_seq_len 513 in decoder_attn_batch
DEBUG 10-14 21:17:34 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:34 lpmodule.py:374] update past key value cost 0.002497 seconds
DEBUG 10-14 21:17:34 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 513, 128]), value shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:34 lpmodule.py:399] repeat qkv cost 0.000001 seconds
DEBUG 10-14 21:17:34 lpmodule.py:400] q shape torch.Size([10, 32, 1, 128]), k shape torch.Size([10, 8, 513, 128]), v shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:34 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:34 lpllm.py:503] reset update experts
DEBUG 10-14 21:17:34 lpllm.py:1771] update state cost 3.24249267578125e-05 s
DEBUG 10-14 21:17:34 lpllm.py:1740] restore layer func cost 0.0022399425506591797 s
DEBUG 10-14 21:17:34 lpllm.py:511] restore layer cost 0.002613544464111328 s
DEBUG 10-14 21:17:34 lpllm.py:512] reset_next_layer_need: layer_attn_idx=10, layer_mlp_idx=10, j_loc=20
DEBUG 10-14 21:17:34 lpllm.py:1280] reset layer cost 0.0027527809143066406 s
DEBUG 10-14 21:17:34 lpllm.py:1281] have reset next layer layer_attn 10 layer_mlp 9 
DEBUG 10-14 21:17:34 lpllm.py:1200] 
DEBUG 10-14 21:17:34 lpllm.py:1200] one decoder loop j: 21 cur_layer_idx: 10
DEBUG 10-14 21:17:34 lpllm.py:1207] start decoder qkv layer_attn 10 layer_mlp 10
DEBUG 10-14 21:17:34 lpmodule.py:526] hidden_states.shape torch.Size([10, 1, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:34 tutils.py:81] single group 0 real attn out cost 0.006405830383300781 s
DEBUG 10-14 21:17:34 lpmodule.py:541] kv_seq_len 1 past_key_value 512
DEBUG 10-14 21:17:34 lpmodule.py:545] inv_freq device cpu
DEBUG 10-14 21:17:34 lpmodule.py:546] kv_seq_len 513 position_ids torch.Size([10, 1]) value tensor([512])
DEBUG 10-14 21:17:34 lpmodule.py:551] query_states device cuda:1
DEBUG 10-14 21:17:34 lpmodule.py:552] key_states device cuda:1
DEBUG 10-14 21:17:34 lpmodule.py:553] value_states device cuda:1
DEBUG 10-14 21:17:34 lpmodule.py:554] position_ids device cpu
DEBUG 10-14 21:17:34 lpmodule.py:555] cos device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:34 lpmodule.py:556] sin device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:34 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:34 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:34 tutils.py:81] single group 1 real attn out cost 0.003519773483276367 s
DEBUG 10-14 21:17:34 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:34 tutils.py:81] single group 2 real attn out cost 0.003505229949951172 s
DEBUG 10-14 21:17:34 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:34 tutils.py:81] single group 3 real attn out cost 0.003142118453979492 s
DEBUG 10-14 21:17:34 tutils.py:88] dot attn help cost 0.023506 seconds
DEBUG 10-14 21:17:34 lpmodule.py:433] dot attn cost 0.023626 seconds
DEBUG 10-14 21:17:34 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.0004417896270751953 s
DEBUG 10-14 21:17:34 lpllm.py:2280] CPU attn cost 0.027380 seconds if batch True
DEBUG 10-14 21:17:34 lpllm.py:2289] deal attn result cost 0.000003 seconds
DEBUG 10-14 21:17:34 lpllm.py:2291] CPU compute cost 0.028251 seconds
DEBUG 10-14 21:17:34 lpllm.py:2309] free cost 0.000169 seconds
DEBUG 10-14 21:17:34 lpmodule.py:162] call mlp prepare
DEBUG 10-14 21:17:34 lpmodule.py:163] layer idx 10
DEBUG 10-14 21:17:34 lpmodule.py:123] Tokens per expert: [3, 0, 3, 0, 2, 7, 5, 0]
DEBUG 10-14 21:17:34 lpmodule.py:124] Number of active experts: 5
DEBUG 10-14 21:17:34 lpmodule.py:195] call mlp post
DEBUG 10-14 21:17:34 lpllm.py:2262] GPU2CPU move cost 0.000275 seconds
DEBUG 10-14 21:17:34 lpmodule.py:340] layer_idx 10 attn layer_idx: 10
DEBUG 10-14 21:17:34 lpmodule.py:352] get seq_length 512 in decoder_attn_batch
WARNING 10-14 21:17:34 lpmodule.py:355] generally decoder_attn_batch called with q_len == 1
DEBUG 10-14 21:17:34 lpmodule.py:364] decoder_attn_batch update batch_dim 10-20
DEBUG 10-14 21:17:34 StaticCacheLen.py:84] static cache update layer_idx: 10, start_batch: 10, end_batch: 20
DEBUG 10-14 21:17:34 StaticCacheLen.py:90] static cache update layer_idx: 10, update seq_length to 513
DEBUG 10-14 21:17:34 lpmodule.py:368] update for kv cache 10-20 kv_seq_len 513 in decoder_attn_batch
DEBUG 10-14 21:17:34 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:34 lpmodule.py:374] update past key value cost 0.002795 seconds
DEBUG 10-14 21:17:34 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 513, 128]), value shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:34 lpmodule.py:399] repeat qkv cost 0.000001 seconds
DEBUG 10-14 21:17:34 lpmodule.py:400] q shape torch.Size([10, 32, 1, 128]), k shape torch.Size([10, 8, 513, 128]), v shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:34 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:34 lpllm.py:498] reset update attn
DEBUG 10-14 21:17:34 lpllm.py:1771] update state cost 3.409385681152344e-05 s
DEBUG 10-14 21:17:34 lpllm.py:1740] restore layer func cost 0.0008111000061035156 s
DEBUG 10-14 21:17:34 lpllm.py:511] restore layer cost 0.0011262893676757812 s
DEBUG 10-14 21:17:34 lpllm.py:512] reset_next_layer_need: layer_attn_idx=11, layer_mlp_idx=10, j_loc=21
DEBUG 10-14 21:17:34 lpllm.py:1280] reset layer cost 0.0012736320495605469 s
DEBUG 10-14 21:17:34 lpllm.py:1281] have reset next layer layer_attn 10 layer_mlp 10 
DEBUG 10-14 21:17:34 lpllm.py:1287] j: 21 waiting the layer with layer_idx 11 before wait time 0.06492161750793457 s
INFO 10-14 21:17:34 client.py:117] confirm_model_loaded: Mixtral-8x7B, 1d5fc42c-9461-4bfa-8b21-586691de643e
DEBUG 10-14 21:17:34 tutils.py:81] single group 0 real attn out cost 0.005896806716918945 s
DEBUG 10-14 21:17:34 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:34 tutils.py:81] single group 1 real attn out cost 0.003063201904296875 s
DEBUG 10-14 21:17:34 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:34 tutils.py:81] single group 2 real attn out cost 0.0030028820037841797 s
DEBUG 10-14 21:17:34 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:34 tutils.py:81] single group 3 real attn out cost 0.0034914016723632812 s
DEBUG 10-14 21:17:34 tutils.py:88] dot attn help cost 0.021417 seconds
DEBUG 10-14 21:17:34 lpmodule.py:433] dot attn cost 0.021576 seconds
DEBUG 10-14 21:17:34 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.0012619495391845703 s
DEBUG 10-14 21:17:34 lpllm.py:2280] CPU attn cost 0.026435 seconds if batch True
DEBUG 10-14 21:17:34 lpllm.py:2289] deal attn result cost 0.000004 seconds
DEBUG 10-14 21:17:34 lpllm.py:2291] CPU compute cost 0.027039 seconds
DEBUG 10-14 21:17:34 lpllm.py:2309] free cost 0.000214 seconds
INFO 10-14 21:17:34 client.py:125] Model loaded
DEBUG 10-14 21:17:34 lpllm.py:1291] j: load cost 0.299422025680542 s waiting cost 0.2344663143157959 s
DEBUG 10-14 21:17:34 lpllm.py:1200] 
DEBUG 10-14 21:17:34 lpllm.py:1200] one decoder loop j: 22 cur_layer_idx: 11
DEBUG 10-14 21:17:34 lpllm.py:1205] start load next layer cur_layer_idx: 12
DEBUG 10-14 21:17:34 lpllm.py:1801] cuda memory handles: {1: <capsule object NULL at 0x7fdf3c6da010>} {1: b'\xe0\xaa29\x00\x00\x00\x00\xe8&\x1d\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\x86\x00\x00\x00\xff\x00\x000\x00\x00\x00\x00\x00\x00\x00)\xd8\xd5\xc1\xa4\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-14 21:17:34 client.py:72] load_into_gpu: Mixtral-8x7B, b9d8ac7c-48a1-4a82-a37a-f4b418f7b4f8
INFO 10-14 21:17:34 client.py:113] Model loaded: Mixtral-8x7B, b9d8ac7c-48a1-4a82-a37a-f4b418f7b4f8
DEBUG 10-14 21:17:34 lpllm.py:1207] start decoder qkv layer_attn 11 layer_mlp 10
DEBUG 10-14 21:17:34 lpmodule.py:526] hidden_states.shape torch.Size([10, 1, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:34 lpmodule.py:541] kv_seq_len 1 past_key_value 512
DEBUG 10-14 21:17:34 lpmodule.py:545] inv_freq device cpu
DEBUG 10-14 21:17:34 lpmodule.py:546] kv_seq_len 513 position_ids torch.Size([10, 1]) value tensor([512])
DEBUG 10-14 21:17:34 lpmodule.py:551] query_states device cuda:1
DEBUG 10-14 21:17:34 lpmodule.py:552] key_states device cuda:1
DEBUG 10-14 21:17:34 lpmodule.py:553] value_states device cuda:1
DEBUG 10-14 21:17:34 lpmodule.py:554] position_ids device cpu
DEBUG 10-14 21:17:34 lpmodule.py:555] cos device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:34 lpmodule.py:556] sin device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:34 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:34 lpmodule.py:162] call mlp prepare
DEBUG 10-14 21:17:34 lpmodule.py:163] layer idx 10
DEBUG 10-14 21:17:34 lpmodule.py:123] Tokens per expert: [1, 0, 1, 0, 9, 0, 0, 9]
DEBUG 10-14 21:17:34 lpmodule.py:124] Number of active experts: 4
DEBUG 10-14 21:17:34 lpmodule.py:195] call mlp post
DEBUG 10-14 21:17:34 lpllm.py:2262] GPU2CPU move cost 0.000648 seconds
DEBUG 10-14 21:17:34 lpmodule.py:340] layer_idx 11 attn layer_idx: 11
DEBUG 10-14 21:17:34 lpmodule.py:352] get seq_length 512 in decoder_attn_batch
WARNING 10-14 21:17:34 lpmodule.py:355] generally decoder_attn_batch called with q_len == 1
DEBUG 10-14 21:17:34 lpmodule.py:364] decoder_attn_batch update batch_dim 0-10
DEBUG 10-14 21:17:34 StaticCacheLen.py:84] static cache update layer_idx: 11, start_batch: 0, end_batch: 10
DEBUG 10-14 21:17:34 lpmodule.py:368] update for kv cache 0-10 kv_seq_len 513 in decoder_attn_batch
DEBUG 10-14 21:17:34 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:34 lpmodule.py:374] update past key value cost 0.003358 seconds
DEBUG 10-14 21:17:34 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 513, 128]), value shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:34 lpmodule.py:399] repeat qkv cost 0.000002 seconds
DEBUG 10-14 21:17:34 lpmodule.py:400] q shape torch.Size([10, 32, 1, 128]), k shape torch.Size([10, 8, 513, 128]), v shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:34 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:34 lpllm.py:503] reset update experts
DEBUG 10-14 21:17:34 lpllm.py:1771] update state cost 2.4318695068359375e-05 s
DEBUG 10-14 21:17:34 lpllm.py:1740] restore layer func cost 0.0014307498931884766 s
DEBUG 10-14 21:17:34 lpllm.py:511] restore layer cost 0.001687765121459961 s
DEBUG 10-14 21:17:34 lpllm.py:512] reset_next_layer_need: layer_attn_idx=11, layer_mlp_idx=11, j_loc=22
DEBUG 10-14 21:17:34 lpllm.py:1280] reset layer cost 0.0018055438995361328 s
DEBUG 10-14 21:17:34 lpllm.py:1281] have reset next layer layer_attn 11 layer_mlp 10 
DEBUG 10-14 21:17:34 lpllm.py:1200] 
DEBUG 10-14 21:17:34 lpllm.py:1200] one decoder loop j: 23 cur_layer_idx: 11
DEBUG 10-14 21:17:34 lpllm.py:1207] start decoder qkv layer_attn 11 layer_mlp 11
DEBUG 10-14 21:17:34 lpmodule.py:526] hidden_states.shape torch.Size([10, 1, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:34 lpmodule.py:541] kv_seq_len 1 past_key_value 512
DEBUG 10-14 21:17:34 lpmodule.py:545] inv_freq device cpu
DEBUG 10-14 21:17:34 lpmodule.py:546] kv_seq_len 513 position_ids torch.Size([10, 1]) value tensor([512])
DEBUG 10-14 21:17:34 lpmodule.py:551] query_states device cuda:1
DEBUG 10-14 21:17:34 lpmodule.py:552] key_states device cuda:1
DEBUG 10-14 21:17:34 lpmodule.py:553] value_states device cuda:1
DEBUG 10-14 21:17:34 lpmodule.py:554] position_ids device cpu
DEBUG 10-14 21:17:34 lpmodule.py:555] cos device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:34 lpmodule.py:556] sin device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:34 tutils.py:81] single group 0 real attn out cost 0.0061075687408447266 s
DEBUG 10-14 21:17:34 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:34 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:34 tutils.py:81] single group 1 real attn out cost 0.003058195114135742 s
DEBUG 10-14 21:17:34 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:34 tutils.py:81] single group 2 real attn out cost 0.0030014514923095703 s
DEBUG 10-14 21:17:34 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:34 tutils.py:81] single group 3 real attn out cost 0.003142833709716797 s
DEBUG 10-14 21:17:34 tutils.py:88] dot attn help cost 0.021179 seconds
DEBUG 10-14 21:17:34 lpmodule.py:433] dot attn cost 0.021255 seconds
DEBUG 10-14 21:17:34 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.0002193450927734375 s
DEBUG 10-14 21:17:34 lpllm.py:2280] CPU attn cost 0.025862 seconds if batch True
DEBUG 10-14 21:17:34 lpllm.py:2289] deal attn result cost 0.000002 seconds
DEBUG 10-14 21:17:34 lpllm.py:2291] CPU compute cost 0.026901 seconds
DEBUG 10-14 21:17:34 lpllm.py:2309] free cost 0.000110 seconds
DEBUG 10-14 21:17:34 lpmodule.py:162] call mlp prepare
DEBUG 10-14 21:17:34 lpmodule.py:163] layer idx 11
DEBUG 10-14 21:17:34 lpmodule.py:123] Tokens per expert: [3, 5, 5, 7, 0, 0, 0, 0]
DEBUG 10-14 21:17:34 lpmodule.py:124] Number of active experts: 4
DEBUG 10-14 21:17:34 lpmodule.py:195] call mlp post
DEBUG 10-14 21:17:34 lpllm.py:2262] GPU2CPU move cost 0.000245 seconds
DEBUG 10-14 21:17:34 lpmodule.py:340] layer_idx 11 attn layer_idx: 11
DEBUG 10-14 21:17:34 lpmodule.py:352] get seq_length 512 in decoder_attn_batch
WARNING 10-14 21:17:34 lpmodule.py:355] generally decoder_attn_batch called with q_len == 1
DEBUG 10-14 21:17:34 lpmodule.py:364] decoder_attn_batch update batch_dim 10-20
DEBUG 10-14 21:17:34 StaticCacheLen.py:84] static cache update layer_idx: 11, start_batch: 10, end_batch: 20
DEBUG 10-14 21:17:34 StaticCacheLen.py:90] static cache update layer_idx: 11, update seq_length to 513
DEBUG 10-14 21:17:34 lpmodule.py:368] update for kv cache 10-20 kv_seq_len 513 in decoder_attn_batch
DEBUG 10-14 21:17:34 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:34 lpmodule.py:374] update past key value cost 0.001597 seconds
DEBUG 10-14 21:17:34 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 513, 128]), value shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:34 lpmodule.py:399] repeat qkv cost 0.000000 seconds
DEBUG 10-14 21:17:34 lpmodule.py:400] q shape torch.Size([10, 32, 1, 128]), k shape torch.Size([10, 8, 513, 128]), v shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:34 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:34 lpllm.py:498] reset update attn
DEBUG 10-14 21:17:34 lpllm.py:1771] update state cost 2.7179718017578125e-05 s
DEBUG 10-14 21:17:34 lpllm.py:1740] restore layer func cost 0.0006382465362548828 s
DEBUG 10-14 21:17:34 lpllm.py:511] restore layer cost 0.0008764266967773438 s
DEBUG 10-14 21:17:34 lpllm.py:512] reset_next_layer_need: layer_attn_idx=12, layer_mlp_idx=11, j_loc=23
DEBUG 10-14 21:17:34 lpllm.py:1280] reset layer cost 0.0010116100311279297 s
DEBUG 10-14 21:17:34 lpllm.py:1281] have reset next layer layer_attn 11 layer_mlp 11 
DEBUG 10-14 21:17:34 lpllm.py:1287] j: 23 waiting the layer with layer_idx 12 before wait time 0.05873227119445801 s
INFO 10-14 21:17:34 client.py:117] confirm_model_loaded: Mixtral-8x7B, b9d8ac7c-48a1-4a82-a37a-f4b418f7b4f8
DEBUG 10-14 21:17:34 tutils.py:81] single group 0 real attn out cost 0.006581544876098633 s
DEBUG 10-14 21:17:34 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:34 tutils.py:81] single group 1 real attn out cost 0.0030269622802734375 s
DEBUG 10-14 21:17:34 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:34 tutils.py:81] single group 2 real attn out cost 0.003140687942504883 s
DEBUG 10-14 21:17:34 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:34 tutils.py:81] single group 3 real attn out cost 0.0030736923217773438 s
DEBUG 10-14 21:17:34 tutils.py:88] dot attn help cost 0.022436 seconds
DEBUG 10-14 21:17:34 lpmodule.py:433] dot attn cost 0.022565 seconds
DEBUG 10-14 21:17:34 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.0004680156707763672 s
DEBUG 10-14 21:17:34 lpllm.py:2280] CPU attn cost 0.024981 seconds if batch True
DEBUG 10-14 21:17:34 lpllm.py:2289] deal attn result cost 0.000003 seconds
DEBUG 10-14 21:17:34 lpllm.py:2291] CPU compute cost 0.025482 seconds
DEBUG 10-14 21:17:34 lpllm.py:2309] free cost 0.000168 seconds
INFO 10-14 21:17:34 client.py:125] Model loaded
DEBUG 10-14 21:17:34 lpllm.py:1291] j: load cost 0.29894590377807617 s waiting cost 0.240189790725708 s
DEBUG 10-14 21:17:34 lpllm.py:1200] 
DEBUG 10-14 21:17:34 lpllm.py:1200] one decoder loop j: 24 cur_layer_idx: 12
DEBUG 10-14 21:17:34 lpllm.py:1205] start load next layer cur_layer_idx: 13
DEBUG 10-14 21:17:34 lpllm.py:1801] cuda memory handles: {1: <capsule object NULL at 0x7fdf3c6d9ec0>} {1: b'\xe0\xaa29\x00\x00\x00\x00\xe8&\x1d\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\x86\x00\x00\x00\xff\x00\x00.\x00\x00\x00\x00\x00\x00\x00)\xd8\xd5\xc1\xa0\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-14 21:17:34 client.py:72] load_into_gpu: Mixtral-8x7B, 931f4123-8e24-454d-8b26-39c325d49118
INFO 10-14 21:17:34 client.py:113] Model loaded: Mixtral-8x7B, 931f4123-8e24-454d-8b26-39c325d49118
DEBUG 10-14 21:17:34 lpllm.py:1207] start decoder qkv layer_attn 12 layer_mlp 11
DEBUG 10-14 21:17:34 lpmodule.py:526] hidden_states.shape torch.Size([10, 1, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:34 lpmodule.py:541] kv_seq_len 1 past_key_value 512
DEBUG 10-14 21:17:34 lpmodule.py:545] inv_freq device cpu
DEBUG 10-14 21:17:34 lpmodule.py:546] kv_seq_len 513 position_ids torch.Size([10, 1]) value tensor([512])
DEBUG 10-14 21:17:34 lpmodule.py:551] query_states device cuda:1
DEBUG 10-14 21:17:34 lpmodule.py:552] key_states device cuda:1
DEBUG 10-14 21:17:34 lpmodule.py:553] value_states device cuda:1
DEBUG 10-14 21:17:34 lpmodule.py:554] position_ids device cpu
DEBUG 10-14 21:17:34 lpmodule.py:555] cos device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:34 lpmodule.py:556] sin device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:34 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:34 lpmodule.py:162] call mlp prepare
DEBUG 10-14 21:17:34 lpmodule.py:163] layer idx 11
DEBUG 10-14 21:17:34 lpmodule.py:123] Tokens per expert: [10, 0, 0, 9, 1, 0, 0, 0]
DEBUG 10-14 21:17:34 lpmodule.py:124] Number of active experts: 3
DEBUG 10-14 21:17:34 lpmodule.py:195] call mlp post
DEBUG 10-14 21:17:34 lpllm.py:2262] GPU2CPU move cost 0.000530 seconds
DEBUG 10-14 21:17:34 lpmodule.py:340] layer_idx 12 attn layer_idx: 12
DEBUG 10-14 21:17:34 lpmodule.py:352] get seq_length 512 in decoder_attn_batch
WARNING 10-14 21:17:34 lpmodule.py:355] generally decoder_attn_batch called with q_len == 1
DEBUG 10-14 21:17:34 lpmodule.py:364] decoder_attn_batch update batch_dim 0-10
DEBUG 10-14 21:17:34 StaticCacheLen.py:84] static cache update layer_idx: 12, start_batch: 0, end_batch: 10
DEBUG 10-14 21:17:34 lpmodule.py:368] update for kv cache 0-10 kv_seq_len 513 in decoder_attn_batch
DEBUG 10-14 21:17:34 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:34 lpmodule.py:374] update past key value cost 0.003295 seconds
DEBUG 10-14 21:17:34 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 513, 128]), value shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:34 lpmodule.py:399] repeat qkv cost 0.000002 seconds
DEBUG 10-14 21:17:34 lpmodule.py:400] q shape torch.Size([10, 32, 1, 128]), k shape torch.Size([10, 8, 513, 128]), v shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:34 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:34 lpllm.py:503] reset update experts
DEBUG 10-14 21:17:34 lpllm.py:1771] update state cost 2.4557113647460938e-05 s
DEBUG 10-14 21:17:34 lpllm.py:1740] restore layer func cost 0.0027618408203125 s
DEBUG 10-14 21:17:34 lpllm.py:511] restore layer cost 0.003069639205932617 s
DEBUG 10-14 21:17:34 lpllm.py:512] reset_next_layer_need: layer_attn_idx=12, layer_mlp_idx=12, j_loc=24
DEBUG 10-14 21:17:34 lpllm.py:1280] reset layer cost 0.003243684768676758 s
DEBUG 10-14 21:17:34 lpllm.py:1281] have reset next layer layer_attn 12 layer_mlp 11 
DEBUG 10-14 21:17:34 lpllm.py:1200] 
DEBUG 10-14 21:17:34 lpllm.py:1200] one decoder loop j: 25 cur_layer_idx: 12
DEBUG 10-14 21:17:34 lpllm.py:1207] start decoder qkv layer_attn 12 layer_mlp 12
DEBUG 10-14 21:17:34 lpmodule.py:526] hidden_states.shape torch.Size([10, 1, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:34 lpmodule.py:541] kv_seq_len 1 past_key_value 512
DEBUG 10-14 21:17:34 lpmodule.py:545] inv_freq device cpu
DEBUG 10-14 21:17:34 lpmodule.py:546] kv_seq_len 513 position_ids torch.Size([10, 1]) value tensor([512])
DEBUG 10-14 21:17:34 lpmodule.py:551] query_states device cuda:1
DEBUG 10-14 21:17:34 lpmodule.py:552] key_states device cuda:1
DEBUG 10-14 21:17:34 lpmodule.py:553] value_states device cuda:1
DEBUG 10-14 21:17:34 lpmodule.py:554] position_ids device cpu
DEBUG 10-14 21:17:34 lpmodule.py:555] cos device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:34 lpmodule.py:556] sin device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:34 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:34 tutils.py:81] single group 0 real attn out cost 0.009752035140991211 s
DEBUG 10-14 21:17:34 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:34 tutils.py:81] single group 1 real attn out cost 0.0029282569885253906 s
DEBUG 10-14 21:17:34 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:34 tutils.py:81] single group 2 real attn out cost 0.0030646324157714844 s
DEBUG 10-14 21:17:34 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:34 tutils.py:81] single group 3 real attn out cost 0.003147125244140625 s
DEBUG 10-14 21:17:34 tutils.py:88] dot attn help cost 0.024875 seconds
DEBUG 10-14 21:17:34 lpmodule.py:433] dot attn cost 0.024956 seconds
DEBUG 10-14 21:17:34 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.00024819374084472656 s
DEBUG 10-14 21:17:34 lpllm.py:2280] CPU attn cost 0.029531 seconds if batch True
DEBUG 10-14 21:17:34 lpllm.py:2289] deal attn result cost 0.000003 seconds
DEBUG 10-14 21:17:34 lpllm.py:2291] CPU compute cost 0.030413 seconds
DEBUG 10-14 21:17:34 lpllm.py:2309] free cost 0.000130 seconds
DEBUG 10-14 21:17:34 lpmodule.py:162] call mlp prepare
DEBUG 10-14 21:17:34 lpmodule.py:163] layer idx 12
DEBUG 10-14 21:17:34 lpmodule.py:123] Tokens per expert: [3, 9, 0, 0, 1, 0, 7, 0]
DEBUG 10-14 21:17:34 lpmodule.py:124] Number of active experts: 4
DEBUG 10-14 21:17:34 lpmodule.py:195] call mlp post
DEBUG 10-14 21:17:34 lpllm.py:2262] GPU2CPU move cost 0.000283 seconds
DEBUG 10-14 21:17:34 lpmodule.py:340] layer_idx 12 attn layer_idx: 12
DEBUG 10-14 21:17:34 lpmodule.py:352] get seq_length 512 in decoder_attn_batch
WARNING 10-14 21:17:34 lpmodule.py:355] generally decoder_attn_batch called with q_len == 1
DEBUG 10-14 21:17:34 lpmodule.py:364] decoder_attn_batch update batch_dim 10-20
DEBUG 10-14 21:17:34 StaticCacheLen.py:84] static cache update layer_idx: 12, start_batch: 10, end_batch: 20
DEBUG 10-14 21:17:34 StaticCacheLen.py:90] static cache update layer_idx: 12, update seq_length to 513
DEBUG 10-14 21:17:34 lpmodule.py:368] update for kv cache 10-20 kv_seq_len 513 in decoder_attn_batch
DEBUG 10-14 21:17:34 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:34 lpmodule.py:374] update past key value cost 0.001775 seconds
DEBUG 10-14 21:17:34 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 513, 128]), value shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:34 lpmodule.py:399] repeat qkv cost 0.000000 seconds
DEBUG 10-14 21:17:34 lpmodule.py:400] q shape torch.Size([10, 32, 1, 128]), k shape torch.Size([10, 8, 513, 128]), v shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:34 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:34 lpllm.py:498] reset update attn
DEBUG 10-14 21:17:34 lpllm.py:1771] update state cost 3.0517578125e-05 s
DEBUG 10-14 21:17:34 lpllm.py:1740] restore layer func cost 0.0006885528564453125 s
DEBUG 10-14 21:17:34 lpllm.py:511] restore layer cost 0.0009510517120361328 s
DEBUG 10-14 21:17:34 lpllm.py:512] reset_next_layer_need: layer_attn_idx=13, layer_mlp_idx=12, j_loc=25
DEBUG 10-14 21:17:34 lpllm.py:1280] reset layer cost 0.0010776519775390625 s
DEBUG 10-14 21:17:34 lpllm.py:1281] have reset next layer layer_attn 12 layer_mlp 12 
DEBUG 10-14 21:17:34 lpllm.py:1287] j: 25 waiting the layer with layer_idx 13 before wait time 0.09543991088867188 s
INFO 10-14 21:17:34 client.py:117] confirm_model_loaded: Mixtral-8x7B, 931f4123-8e24-454d-8b26-39c325d49118
DEBUG 10-14 21:17:34 tutils.py:81] single group 0 real attn out cost 0.006168365478515625 s
DEBUG 10-14 21:17:34 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:34 tutils.py:81] single group 1 real attn out cost 0.003065347671508789 s
DEBUG 10-14 21:17:34 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:34 tutils.py:81] single group 2 real attn out cost 0.003098726272583008 s
DEBUG 10-14 21:17:34 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:34 tutils.py:81] single group 3 real attn out cost 0.0030012130737304688 s
DEBUG 10-14 21:17:34 tutils.py:88] dot attn help cost 0.020477 seconds
DEBUG 10-14 21:17:34 lpmodule.py:433] dot attn cost 0.020546 seconds
DEBUG 10-14 21:17:34 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.001035928726196289 s
DEBUG 10-14 21:17:34 lpllm.py:2280] CPU attn cost 0.023666 seconds if batch True
DEBUG 10-14 21:17:34 lpllm.py:2289] deal attn result cost 0.000002 seconds
DEBUG 10-14 21:17:34 lpllm.py:2291] CPU compute cost 0.024131 seconds
DEBUG 10-14 21:17:34 lpllm.py:2309] free cost 0.000086 seconds
INFO 10-14 21:17:35 client.py:125] Model loaded
DEBUG 10-14 21:17:35 lpllm.py:1291] j: load cost 0.3278782367706299 s waiting cost 0.23241162300109863 s
DEBUG 10-14 21:17:35 lpllm.py:1200] 
DEBUG 10-14 21:17:35 lpllm.py:1200] one decoder loop j: 26 cur_layer_idx: 13
DEBUG 10-14 21:17:35 lpllm.py:1205] start load next layer cur_layer_idx: 14
DEBUG 10-14 21:17:35 lpllm.py:1801] cuda memory handles: {1: <capsule object NULL at 0x7fdf3c6d9f80>} {1: b'\xe0\xaa29\x00\x00\x00\x00\xe8&\x1d\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\x86\x00\x00\x00\xff\x00\x00/\x00\x00\x00\x00\x00\x00\x00)\xd8\xd5\xc1\xa2\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-14 21:17:35 client.py:72] load_into_gpu: Mixtral-8x7B, 4cf88303-f031-4bcc-95fd-14314f8ef249
INFO 10-14 21:17:35 client.py:113] Model loaded: Mixtral-8x7B, 4cf88303-f031-4bcc-95fd-14314f8ef249
DEBUG 10-14 21:17:35 lpllm.py:1207] start decoder qkv layer_attn 13 layer_mlp 12
DEBUG 10-14 21:17:35 lpmodule.py:526] hidden_states.shape torch.Size([10, 1, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:35 lpmodule.py:541] kv_seq_len 1 past_key_value 512
DEBUG 10-14 21:17:35 lpmodule.py:545] inv_freq device cpu
DEBUG 10-14 21:17:35 lpmodule.py:546] kv_seq_len 513 position_ids torch.Size([10, 1]) value tensor([512])
DEBUG 10-14 21:17:35 lpmodule.py:551] query_states device cuda:1
DEBUG 10-14 21:17:35 lpmodule.py:552] key_states device cuda:1
DEBUG 10-14 21:17:35 lpmodule.py:553] value_states device cuda:1
DEBUG 10-14 21:17:35 lpmodule.py:554] position_ids device cpu
DEBUG 10-14 21:17:35 lpmodule.py:555] cos device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:35 lpmodule.py:556] sin device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:35 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:35 lpmodule.py:162] call mlp prepare
DEBUG 10-14 21:17:35 lpmodule.py:163] layer idx 12
DEBUG 10-14 21:17:35 lpmodule.py:123] Tokens per expert: [0, 0, 9, 1, 0, 0, 10, 0]
DEBUG 10-14 21:17:35 lpmodule.py:124] Number of active experts: 3
DEBUG 10-14 21:17:35 lpmodule.py:195] call mlp post
DEBUG 10-14 21:17:35 lpllm.py:2262] GPU2CPU move cost 0.000333 seconds
DEBUG 10-14 21:17:35 lpmodule.py:340] layer_idx 13 attn layer_idx: 13
DEBUG 10-14 21:17:35 lpmodule.py:352] get seq_length 512 in decoder_attn_batch
WARNING 10-14 21:17:35 lpmodule.py:355] generally decoder_attn_batch called with q_len == 1
DEBUG 10-14 21:17:35 lpmodule.py:364] decoder_attn_batch update batch_dim 0-10
DEBUG 10-14 21:17:35 StaticCacheLen.py:84] static cache update layer_idx: 13, start_batch: 0, end_batch: 10
DEBUG 10-14 21:17:35 lpmodule.py:368] update for kv cache 0-10 kv_seq_len 513 in decoder_attn_batch
DEBUG 10-14 21:17:35 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:35 lpmodule.py:374] update past key value cost 0.002586 seconds
DEBUG 10-14 21:17:35 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 513, 128]), value shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:35 lpmodule.py:399] repeat qkv cost 0.000001 seconds
DEBUG 10-14 21:17:35 lpmodule.py:400] q shape torch.Size([10, 32, 1, 128]), k shape torch.Size([10, 8, 513, 128]), v shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:35 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:35 lpllm.py:503] reset update experts
DEBUG 10-14 21:17:35 lpllm.py:1771] update state cost 2.5033950805664062e-05 s
DEBUG 10-14 21:17:35 lpllm.py:1740] restore layer func cost 0.001470804214477539 s
DEBUG 10-14 21:17:35 lpllm.py:511] restore layer cost 0.0017352104187011719 s
DEBUG 10-14 21:17:35 lpllm.py:512] reset_next_layer_need: layer_attn_idx=13, layer_mlp_idx=13, j_loc=26
DEBUG 10-14 21:17:35 lpllm.py:1280] reset layer cost 0.0018575191497802734 s
DEBUG 10-14 21:17:35 lpllm.py:1281] have reset next layer layer_attn 13 layer_mlp 12 
DEBUG 10-14 21:17:35 lpllm.py:1200] 
DEBUG 10-14 21:17:35 lpllm.py:1200] one decoder loop j: 27 cur_layer_idx: 13
DEBUG 10-14 21:17:35 lpllm.py:1207] start decoder qkv layer_attn 13 layer_mlp 13
DEBUG 10-14 21:17:35 lpmodule.py:526] hidden_states.shape torch.Size([10, 1, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:35 tutils.py:81] single group 0 real attn out cost 0.006245613098144531 s
DEBUG 10-14 21:17:35 lpmodule.py:541] kv_seq_len 1 past_key_value 512
DEBUG 10-14 21:17:35 lpmodule.py:545] inv_freq device cpu
DEBUG 10-14 21:17:35 lpmodule.py:546] kv_seq_len 513 position_ids torch.Size([10, 1]) value tensor([512])
DEBUG 10-14 21:17:35 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:35 lpmodule.py:551] query_states device cuda:1
DEBUG 10-14 21:17:35 lpmodule.py:552] key_states device cuda:1
DEBUG 10-14 21:17:35 lpmodule.py:553] value_states device cuda:1
DEBUG 10-14 21:17:35 lpmodule.py:554] position_ids device cpu
DEBUG 10-14 21:17:35 lpmodule.py:555] cos device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:35 lpmodule.py:556] sin device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:35 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:35 tutils.py:81] single group 1 real attn out cost 0.0031447410583496094 s
DEBUG 10-14 21:17:35 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:35 tutils.py:81] single group 2 real attn out cost 0.004558563232421875 s
DEBUG 10-14 21:17:35 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:35 tutils.py:81] single group 3 real attn out cost 0.0033159255981445312 s
DEBUG 10-14 21:17:35 tutils.py:88] dot attn help cost 0.023473 seconds
DEBUG 10-14 21:17:35 lpmodule.py:433] dot attn cost 0.023603 seconds
DEBUG 10-14 21:17:35 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.0007283687591552734 s
DEBUG 10-14 21:17:35 lpllm.py:2280] CPU attn cost 0.027611 seconds if batch True
DEBUG 10-14 21:17:35 lpllm.py:2289] deal attn result cost 0.000003 seconds
DEBUG 10-14 21:17:35 lpllm.py:2291] CPU compute cost 0.028306 seconds
DEBUG 10-14 21:17:35 lpllm.py:2309] free cost 0.000180 seconds
DEBUG 10-14 21:17:35 lpmodule.py:162] call mlp prepare
DEBUG 10-14 21:17:35 lpmodule.py:163] layer idx 13
DEBUG 10-14 21:17:35 lpmodule.py:123] Tokens per expert: [5, 2, 0, 10, 3, 0, 0, 0]
DEBUG 10-14 21:17:35 lpmodule.py:124] Number of active experts: 4
DEBUG 10-14 21:17:35 lpmodule.py:195] call mlp post
DEBUG 10-14 21:17:35 lpllm.py:2262] GPU2CPU move cost 0.000408 seconds
DEBUG 10-14 21:17:35 lpmodule.py:340] layer_idx 13 attn layer_idx: 13
DEBUG 10-14 21:17:35 lpmodule.py:352] get seq_length 512 in decoder_attn_batch
WARNING 10-14 21:17:35 lpmodule.py:355] generally decoder_attn_batch called with q_len == 1
DEBUG 10-14 21:17:35 lpmodule.py:364] decoder_attn_batch update batch_dim 10-20
DEBUG 10-14 21:17:35 StaticCacheLen.py:84] static cache update layer_idx: 13, start_batch: 10, end_batch: 20
DEBUG 10-14 21:17:35 StaticCacheLen.py:90] static cache update layer_idx: 13, update seq_length to 513
DEBUG 10-14 21:17:35 lpmodule.py:368] update for kv cache 10-20 kv_seq_len 513 in decoder_attn_batch
DEBUG 10-14 21:17:35 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:35 lpmodule.py:374] update past key value cost 0.002270 seconds
DEBUG 10-14 21:17:35 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 513, 128]), value shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:35 lpmodule.py:399] repeat qkv cost 0.000001 seconds
DEBUG 10-14 21:17:35 lpmodule.py:400] q shape torch.Size([10, 32, 1, 128]), k shape torch.Size([10, 8, 513, 128]), v shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:35 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:35 lpllm.py:498] reset update attn
DEBUG 10-14 21:17:35 lpllm.py:1771] update state cost 5.459785461425781e-05 s
DEBUG 10-14 21:17:35 tutils.py:81] single group 0 real attn out cost 0.005365848541259766 s
DEBUG 10-14 21:17:35 lpllm.py:1740] restore layer func cost 0.0013477802276611328 s
DEBUG 10-14 21:17:35 lpllm.py:511] restore layer cost 0.0017735958099365234 s
DEBUG 10-14 21:17:35 lpllm.py:512] reset_next_layer_need: layer_attn_idx=14, layer_mlp_idx=13, j_loc=27
DEBUG 10-14 21:17:35 lpllm.py:1280] reset layer cost 0.0020377635955810547 s
DEBUG 10-14 21:17:35 lpllm.py:1281] have reset next layer layer_attn 13 layer_mlp 13 
DEBUG 10-14 21:17:35 lpllm.py:1287] j: 27 waiting the layer with layer_idx 14 before wait time 0.06463289260864258 s
INFO 10-14 21:17:35 client.py:117] confirm_model_loaded: Mixtral-8x7B, 4cf88303-f031-4bcc-95fd-14314f8ef249
DEBUG 10-14 21:17:35 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:35 tutils.py:81] single group 1 real attn out cost 0.0035674571990966797 s
DEBUG 10-14 21:17:35 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:35 tutils.py:81] single group 2 real attn out cost 0.005004405975341797 s
DEBUG 10-14 21:17:35 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:35 tutils.py:81] single group 3 real attn out cost 0.0036423206329345703 s
DEBUG 10-14 21:17:35 tutils.py:88] dot attn help cost 0.025112 seconds
DEBUG 10-14 21:17:35 lpmodule.py:433] dot attn cost 0.025258 seconds
DEBUG 10-14 21:17:35 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.00043702125549316406 s
DEBUG 10-14 21:17:35 lpllm.py:2280] CPU attn cost 0.028606 seconds if batch True
DEBUG 10-14 21:17:35 lpllm.py:2289] deal attn result cost 0.000004 seconds
DEBUG 10-14 21:17:35 lpllm.py:2291] CPU compute cost 0.029387 seconds
DEBUG 10-14 21:17:35 lpllm.py:2309] free cost 0.000169 seconds
INFO 10-14 21:17:35 client.py:125] Model loaded
DEBUG 10-14 21:17:35 lpllm.py:1291] j: load cost 0.294358491897583 s waiting cost 0.22966933250427246 s
DEBUG 10-14 21:17:35 lpllm.py:1200] 
DEBUG 10-14 21:17:35 lpllm.py:1200] one decoder loop j: 28 cur_layer_idx: 14
DEBUG 10-14 21:17:35 lpllm.py:1205] start load next layer cur_layer_idx: 15
DEBUG 10-14 21:17:35 lpllm.py:1801] cuda memory handles: {1: <capsule object NULL at 0x7fdf3c6da010>} {1: b'\xe0\xaa29\x00\x00\x00\x00\xe8&\x1d\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\x86\x00\x00\x00\xff\x00\x000\x00\x00\x00\x00\x00\x00\x00)\xd8\xd5\xc1\xa4\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-14 21:17:35 client.py:72] load_into_gpu: Mixtral-8x7B, 3367278a-0f8d-4d14-aef4-0be7ecdd0fef
INFO 10-14 21:17:35 client.py:113] Model loaded: Mixtral-8x7B, 3367278a-0f8d-4d14-aef4-0be7ecdd0fef
DEBUG 10-14 21:17:35 lpllm.py:1207] start decoder qkv layer_attn 14 layer_mlp 13
DEBUG 10-14 21:17:35 lpmodule.py:526] hidden_states.shape torch.Size([10, 1, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:35 lpmodule.py:541] kv_seq_len 1 past_key_value 512
DEBUG 10-14 21:17:35 lpmodule.py:545] inv_freq device cpu
DEBUG 10-14 21:17:35 lpmodule.py:546] kv_seq_len 513 position_ids torch.Size([10, 1]) value tensor([512])
DEBUG 10-14 21:17:35 lpmodule.py:551] query_states device cuda:1
DEBUG 10-14 21:17:35 lpmodule.py:552] key_states device cuda:1
DEBUG 10-14 21:17:35 lpmodule.py:553] value_states device cuda:1
DEBUG 10-14 21:17:35 lpmodule.py:554] position_ids device cpu
DEBUG 10-14 21:17:35 lpmodule.py:555] cos device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:35 lpmodule.py:556] sin device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:35 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:35 lpmodule.py:162] call mlp prepare
DEBUG 10-14 21:17:35 lpmodule.py:163] layer idx 13
DEBUG 10-14 21:17:35 lpmodule.py:123] Tokens per expert: [0, 9, 0, 0, 1, 9, 0, 1]
DEBUG 10-14 21:17:35 lpmodule.py:124] Number of active experts: 4
DEBUG 10-14 21:17:35 lpmodule.py:195] call mlp post
DEBUG 10-14 21:17:35 lpllm.py:2262] GPU2CPU move cost 0.000360 seconds
DEBUG 10-14 21:17:35 lpmodule.py:340] layer_idx 14 attn layer_idx: 14
DEBUG 10-14 21:17:35 lpmodule.py:352] get seq_length 512 in decoder_attn_batch
WARNING 10-14 21:17:35 lpmodule.py:355] generally decoder_attn_batch called with q_len == 1
DEBUG 10-14 21:17:35 lpmodule.py:364] decoder_attn_batch update batch_dim 0-10
DEBUG 10-14 21:17:35 StaticCacheLen.py:84] static cache update layer_idx: 14, start_batch: 0, end_batch: 10
DEBUG 10-14 21:17:35 lpmodule.py:368] update for kv cache 0-10 kv_seq_len 513 in decoder_attn_batch
DEBUG 10-14 21:17:35 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:35 lpmodule.py:374] update past key value cost 0.002627 seconds
DEBUG 10-14 21:17:35 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 513, 128]), value shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:35 lpmodule.py:399] repeat qkv cost 0.000001 seconds
DEBUG 10-14 21:17:35 lpmodule.py:400] q shape torch.Size([10, 32, 1, 128]), k shape torch.Size([10, 8, 513, 128]), v shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:35 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:35 lpllm.py:503] reset update experts
DEBUG 10-14 21:17:35 lpllm.py:1771] update state cost 3.1948089599609375e-05 s
DEBUG 10-14 21:17:35 lpllm.py:1740] restore layer func cost 0.0014522075653076172 s
DEBUG 10-14 21:17:35 lpllm.py:511] restore layer cost 0.0017368793487548828 s
DEBUG 10-14 21:17:35 lpllm.py:512] reset_next_layer_need: layer_attn_idx=14, layer_mlp_idx=14, j_loc=28
DEBUG 10-14 21:17:35 lpllm.py:1280] reset layer cost 0.0018820762634277344 s
DEBUG 10-14 21:17:35 lpllm.py:1281] have reset next layer layer_attn 14 layer_mlp 13 
DEBUG 10-14 21:17:35 lpllm.py:1200] 
DEBUG 10-14 21:17:35 lpllm.py:1200] one decoder loop j: 29 cur_layer_idx: 14
DEBUG 10-14 21:17:35 lpllm.py:1207] start decoder qkv layer_attn 14 layer_mlp 14
DEBUG 10-14 21:17:35 lpmodule.py:526] hidden_states.shape torch.Size([10, 1, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:35 tutils.py:81] single group 0 real attn out cost 0.005774736404418945 s
DEBUG 10-14 21:17:35 lpmodule.py:541] kv_seq_len 1 past_key_value 512
DEBUG 10-14 21:17:35 lpmodule.py:545] inv_freq device cpu
DEBUG 10-14 21:17:35 lpmodule.py:546] kv_seq_len 513 position_ids torch.Size([10, 1]) value tensor([512])
DEBUG 10-14 21:17:35 lpmodule.py:551] query_states device cuda:1
DEBUG 10-14 21:17:35 lpmodule.py:552] key_states device cuda:1
DEBUG 10-14 21:17:35 lpmodule.py:553] value_states device cuda:1
DEBUG 10-14 21:17:35 lpmodule.py:554] position_ids device cpu
DEBUG 10-14 21:17:35 lpmodule.py:555] cos device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:35 lpmodule.py:556] sin device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:35 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:35 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:35 tutils.py:81] single group 1 real attn out cost 0.002793550491333008 s
DEBUG 10-14 21:17:35 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:35 tutils.py:81] single group 2 real attn out cost 0.006621599197387695 s
DEBUG 10-14 21:17:35 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:35 tutils.py:81] single group 3 real attn out cost 0.004953861236572266 s
DEBUG 10-14 21:17:35 tutils.py:88] dot attn help cost 0.026650 seconds
DEBUG 10-14 21:17:35 lpmodule.py:433] dot attn cost 0.026733 seconds
DEBUG 10-14 21:17:35 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.0002522468566894531 s
DEBUG 10-14 21:17:35 lpllm.py:2280] CPU attn cost 0.030414 seconds if batch True
DEBUG 10-14 21:17:35 lpllm.py:2289] deal attn result cost 0.000003 seconds
DEBUG 10-14 21:17:35 lpllm.py:2291] CPU compute cost 0.031113 seconds
DEBUG 10-14 21:17:35 lpllm.py:2309] free cost 0.000099 seconds
DEBUG 10-14 21:17:35 lpmodule.py:162] call mlp prepare
DEBUG 10-14 21:17:35 lpmodule.py:163] layer idx 14
DEBUG 10-14 21:17:35 lpmodule.py:123] Tokens per expert: [4, 4, 5, 0, 6, 0, 0, 1]
DEBUG 10-14 21:17:35 lpmodule.py:124] Number of active experts: 5
DEBUG 10-14 21:17:35 lpmodule.py:195] call mlp post
DEBUG 10-14 21:17:35 lpllm.py:2262] GPU2CPU move cost 0.000206 seconds
DEBUG 10-14 21:17:35 lpmodule.py:340] layer_idx 14 attn layer_idx: 14
DEBUG 10-14 21:17:35 lpmodule.py:352] get seq_length 512 in decoder_attn_batch
WARNING 10-14 21:17:35 lpmodule.py:355] generally decoder_attn_batch called with q_len == 1
DEBUG 10-14 21:17:35 lpmodule.py:364] decoder_attn_batch update batch_dim 10-20
DEBUG 10-14 21:17:35 StaticCacheLen.py:84] static cache update layer_idx: 14, start_batch: 10, end_batch: 20
DEBUG 10-14 21:17:35 StaticCacheLen.py:90] static cache update layer_idx: 14, update seq_length to 513
DEBUG 10-14 21:17:35 lpmodule.py:368] update for kv cache 10-20 kv_seq_len 513 in decoder_attn_batch
DEBUG 10-14 21:17:35 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:35 lpmodule.py:374] update past key value cost 0.002654 seconds
DEBUG 10-14 21:17:35 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 513, 128]), value shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:35 lpmodule.py:399] repeat qkv cost 0.000001 seconds
DEBUG 10-14 21:17:35 lpmodule.py:400] q shape torch.Size([10, 32, 1, 128]), k shape torch.Size([10, 8, 513, 128]), v shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:35 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:35 tutils.py:81] single group 0 real attn out cost 0.00529170036315918 s
DEBUG 10-14 21:17:35 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:35 lpllm.py:498] reset update attn
DEBUG 10-14 21:17:35 lpllm.py:1771] update state cost 4.863739013671875e-05 s
DEBUG 10-14 21:17:35 lpllm.py:1740] restore layer func cost 0.0009582042694091797 s
DEBUG 10-14 21:17:35 lpllm.py:511] restore layer cost 0.0014941692352294922 s
DEBUG 10-14 21:17:35 lpllm.py:512] reset_next_layer_need: layer_attn_idx=15, layer_mlp_idx=14, j_loc=29
DEBUG 10-14 21:17:35 lpllm.py:1280] reset layer cost 0.0016770362854003906 s
DEBUG 10-14 21:17:35 lpllm.py:1281] have reset next layer layer_attn 14 layer_mlp 14 
DEBUG 10-14 21:17:35 lpllm.py:1287] j: 29 waiting the layer with layer_idx 15 before wait time 0.07060098648071289 s
INFO 10-14 21:17:35 client.py:117] confirm_model_loaded: Mixtral-8x7B, 3367278a-0f8d-4d14-aef4-0be7ecdd0fef
DEBUG 10-14 21:17:35 tutils.py:81] single group 1 real attn out cost 0.003703594207763672 s
DEBUG 10-14 21:17:35 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:35 tutils.py:81] single group 2 real attn out cost 0.0031960010528564453 s
DEBUG 10-14 21:17:35 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:35 tutils.py:81] single group 3 real attn out cost 0.0030045509338378906 s
DEBUG 10-14 21:17:35 tutils.py:88] dot attn help cost 0.022801 seconds
DEBUG 10-14 21:17:35 lpmodule.py:433] dot attn cost 0.022940 seconds
DEBUG 10-14 21:17:35 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.0009946823120117188 s
DEBUG 10-14 21:17:35 lpllm.py:2280] CPU attn cost 0.027222 seconds if batch True
DEBUG 10-14 21:17:35 lpllm.py:2289] deal attn result cost 0.000003 seconds
DEBUG 10-14 21:17:35 lpllm.py:2291] CPU compute cost 0.027730 seconds
DEBUG 10-14 21:17:35 lpllm.py:2309] free cost 0.000239 seconds
INFO 10-14 21:17:35 client.py:125] Model loaded
DEBUG 10-14 21:17:35 lpllm.py:1291] j: load cost 0.31087183952331543 s waiting cost 0.24023938179016113 s
DEBUG 10-14 21:17:35 lpllm.py:1200] 
DEBUG 10-14 21:17:35 lpllm.py:1200] one decoder loop j: 30 cur_layer_idx: 15
DEBUG 10-14 21:17:35 lpllm.py:1205] start load next layer cur_layer_idx: 16
DEBUG 10-14 21:17:35 lpllm.py:1801] cuda memory handles: {1: <capsule object NULL at 0x7fdf3c6d9ec0>} {1: b'\xe0\xaa29\x00\x00\x00\x00\xe8&\x1d\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\x86\x00\x00\x00\xff\x00\x00.\x00\x00\x00\x00\x00\x00\x00)\xd8\xd5\xc1\xa0\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-14 21:17:35 client.py:72] load_into_gpu: Mixtral-8x7B, bb0ffca0-1d57-4607-a7b3-efa77b745003
INFO 10-14 21:17:35 client.py:113] Model loaded: Mixtral-8x7B, bb0ffca0-1d57-4607-a7b3-efa77b745003
DEBUG 10-14 21:17:35 lpllm.py:1207] start decoder qkv layer_attn 15 layer_mlp 14
DEBUG 10-14 21:17:35 lpmodule.py:526] hidden_states.shape torch.Size([10, 1, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:35 lpmodule.py:541] kv_seq_len 1 past_key_value 512
DEBUG 10-14 21:17:35 lpmodule.py:545] inv_freq device cpu
DEBUG 10-14 21:17:35 lpmodule.py:546] kv_seq_len 513 position_ids torch.Size([10, 1]) value tensor([512])
DEBUG 10-14 21:17:35 lpmodule.py:551] query_states device cuda:1
DEBUG 10-14 21:17:35 lpmodule.py:552] key_states device cuda:1
DEBUG 10-14 21:17:35 lpmodule.py:553] value_states device cuda:1
DEBUG 10-14 21:17:35 lpmodule.py:554] position_ids device cpu
DEBUG 10-14 21:17:35 lpmodule.py:555] cos device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:35 lpmodule.py:556] sin device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:35 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:35 lpmodule.py:162] call mlp prepare
DEBUG 10-14 21:17:35 lpmodule.py:163] layer idx 14
DEBUG 10-14 21:17:35 lpmodule.py:123] Tokens per expert: [1, 1, 0, 0, 0, 9, 9, 0]
DEBUG 10-14 21:17:35 lpmodule.py:124] Number of active experts: 4
DEBUG 10-14 21:17:35 lpmodule.py:195] call mlp post
DEBUG 10-14 21:17:35 lpllm.py:2262] GPU2CPU move cost 0.000414 seconds
DEBUG 10-14 21:17:35 lpmodule.py:340] layer_idx 15 attn layer_idx: 15
DEBUG 10-14 21:17:35 lpmodule.py:352] get seq_length 512 in decoder_attn_batch
WARNING 10-14 21:17:35 lpmodule.py:355] generally decoder_attn_batch called with q_len == 1
DEBUG 10-14 21:17:35 lpmodule.py:364] decoder_attn_batch update batch_dim 0-10
DEBUG 10-14 21:17:35 StaticCacheLen.py:84] static cache update layer_idx: 15, start_batch: 0, end_batch: 10
DEBUG 10-14 21:17:35 lpmodule.py:368] update for kv cache 0-10 kv_seq_len 513 in decoder_attn_batch
DEBUG 10-14 21:17:35 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:35 lpmodule.py:374] update past key value cost 0.002411 seconds
DEBUG 10-14 21:17:35 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 513, 128]), value shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:35 lpmodule.py:399] repeat qkv cost 0.000001 seconds
DEBUG 10-14 21:17:35 lpmodule.py:400] q shape torch.Size([10, 32, 1, 128]), k shape torch.Size([10, 8, 513, 128]), v shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:35 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:35 lpllm.py:503] reset update experts
DEBUG 10-14 21:17:35 lpllm.py:1771] update state cost 2.7894973754882812e-05 s
DEBUG 10-14 21:17:35 lpllm.py:1740] restore layer func cost 0.0011513233184814453 s
DEBUG 10-14 21:17:35 lpllm.py:511] restore layer cost 0.001382589340209961 s
DEBUG 10-14 21:17:35 lpllm.py:512] reset_next_layer_need: layer_attn_idx=15, layer_mlp_idx=15, j_loc=30
DEBUG 10-14 21:17:35 lpllm.py:1280] reset layer cost 0.0014996528625488281 s
DEBUG 10-14 21:17:35 lpllm.py:1281] have reset next layer layer_attn 15 layer_mlp 14 
DEBUG 10-14 21:17:35 lpllm.py:1200] 
DEBUG 10-14 21:17:35 lpllm.py:1200] one decoder loop j: 31 cur_layer_idx: 15
DEBUG 10-14 21:17:35 lpllm.py:1207] start decoder qkv layer_attn 15 layer_mlp 15
DEBUG 10-14 21:17:35 lpmodule.py:526] hidden_states.shape torch.Size([10, 1, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:35 tutils.py:81] single group 0 real attn out cost 0.007199525833129883 s
DEBUG 10-14 21:17:35 lpmodule.py:541] kv_seq_len 1 past_key_value 512
DEBUG 10-14 21:17:35 lpmodule.py:545] inv_freq device cpu
DEBUG 10-14 21:17:35 lpmodule.py:546] kv_seq_len 513 position_ids torch.Size([10, 1]) value tensor([512])
DEBUG 10-14 21:17:35 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:35 lpmodule.py:551] query_states device cuda:1
DEBUG 10-14 21:17:35 lpmodule.py:552] key_states device cuda:1
DEBUG 10-14 21:17:35 lpmodule.py:553] value_states device cuda:1
DEBUG 10-14 21:17:35 lpmodule.py:554] position_ids device cpu
DEBUG 10-14 21:17:35 lpmodule.py:555] cos device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:35 lpmodule.py:556] sin device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:35 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:35 tutils.py:81] single group 1 real attn out cost 0.003536701202392578 s
DEBUG 10-14 21:17:35 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:35 tutils.py:81] single group 2 real attn out cost 0.006370067596435547 s
DEBUG 10-14 21:17:35 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:35 tutils.py:81] single group 3 real attn out cost 0.005391359329223633 s
DEBUG 10-14 21:17:35 tutils.py:88] dot attn help cost 0.029677 seconds
DEBUG 10-14 21:17:35 lpmodule.py:433] dot attn cost 0.029776 seconds
DEBUG 10-14 21:17:35 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.001237630844116211 s
DEBUG 10-14 21:17:35 lpllm.py:2280] CPU attn cost 0.034062 seconds if batch True
DEBUG 10-14 21:17:35 lpllm.py:2289] deal attn result cost 0.000003 seconds
DEBUG 10-14 21:17:35 lpllm.py:2291] CPU compute cost 0.034878 seconds
DEBUG 10-14 21:17:35 lpllm.py:2309] free cost 0.000137 seconds
DEBUG 10-14 21:17:35 lpmodule.py:162] call mlp prepare
DEBUG 10-14 21:17:35 lpmodule.py:163] layer idx 15
DEBUG 10-14 21:17:35 lpmodule.py:123] Tokens per expert: [0, 2, 4, 5, 2, 7, 0, 0]
DEBUG 10-14 21:17:35 lpmodule.py:124] Number of active experts: 5
DEBUG 10-14 21:17:35 lpmodule.py:195] call mlp post
DEBUG 10-14 21:17:35 lpllm.py:2262] GPU2CPU move cost 0.000212 seconds
DEBUG 10-14 21:17:35 lpmodule.py:340] layer_idx 15 attn layer_idx: 15
DEBUG 10-14 21:17:35 lpmodule.py:352] get seq_length 512 in decoder_attn_batch
WARNING 10-14 21:17:35 lpmodule.py:355] generally decoder_attn_batch called with q_len == 1
DEBUG 10-14 21:17:35 lpmodule.py:364] decoder_attn_batch update batch_dim 10-20
DEBUG 10-14 21:17:35 StaticCacheLen.py:84] static cache update layer_idx: 15, start_batch: 10, end_batch: 20
DEBUG 10-14 21:17:35 StaticCacheLen.py:90] static cache update layer_idx: 15, update seq_length to 513
DEBUG 10-14 21:17:35 lpmodule.py:368] update for kv cache 10-20 kv_seq_len 513 in decoder_attn_batch
DEBUG 10-14 21:17:35 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:35 lpmodule.py:374] update past key value cost 0.002321 seconds
DEBUG 10-14 21:17:35 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 513, 128]), value shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:35 lpmodule.py:399] repeat qkv cost 0.000002 seconds
DEBUG 10-14 21:17:35 lpmodule.py:400] q shape torch.Size([10, 32, 1, 128]), k shape torch.Size([10, 8, 513, 128]), v shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:35 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:35 tutils.py:81] single group 0 real attn out cost 0.005715131759643555 s
DEBUG 10-14 21:17:35 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:35 lpllm.py:498] reset update attn
DEBUG 10-14 21:17:35 lpllm.py:1771] update state cost 3.814697265625e-05 s
DEBUG 10-14 21:17:35 tutils.py:81] single group 1 real attn out cost 0.0032050609588623047 s
DEBUG 10-14 21:17:35 lpllm.py:1740] restore layer func cost 0.0013616085052490234 s
DEBUG 10-14 21:17:35 lpllm.py:511] restore layer cost 0.0017592906951904297 s
DEBUG 10-14 21:17:35 lpllm.py:512] reset_next_layer_need: layer_attn_idx=16, layer_mlp_idx=15, j_loc=31
DEBUG 10-14 21:17:35 lpllm.py:1280] reset layer cost 0.0019307136535644531 s
DEBUG 10-14 21:17:35 lpllm.py:1281] have reset next layer layer_attn 15 layer_mlp 15 
DEBUG 10-14 21:17:35 lpllm.py:1287] j: 31 waiting the layer with layer_idx 16 before wait time 0.0794823169708252 s
INFO 10-14 21:17:35 client.py:117] confirm_model_loaded: Mixtral-8x7B, bb0ffca0-1d57-4607-a7b3-efa77b745003
DEBUG 10-14 21:17:35 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:35 tutils.py:81] single group 2 real attn out cost 0.0030510425567626953 s
DEBUG 10-14 21:17:35 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:35 tutils.py:81] single group 3 real attn out cost 0.0026946067810058594 s
DEBUG 10-14 21:17:35 tutils.py:88] dot attn help cost 0.021551 seconds
DEBUG 10-14 21:17:35 lpmodule.py:433] dot attn cost 0.021681 seconds
DEBUG 10-14 21:17:35 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.0010764598846435547 s
DEBUG 10-14 21:17:35 lpllm.py:2280] CPU attn cost 0.025696 seconds if batch True
DEBUG 10-14 21:17:35 lpllm.py:2289] deal attn result cost 0.000003 seconds
DEBUG 10-14 21:17:35 lpllm.py:2291] CPU compute cost 0.026213 seconds
DEBUG 10-14 21:17:35 lpllm.py:2309] free cost 0.000198 seconds
INFO 10-14 21:17:36 client.py:125] Model loaded
DEBUG 10-14 21:17:36 lpllm.py:1291] j: load cost 0.30089712142944336 s waiting cost 0.2213733196258545 s
DEBUG 10-14 21:17:36 lpllm.py:1200] 
DEBUG 10-14 21:17:36 lpllm.py:1200] one decoder loop j: 32 cur_layer_idx: 16
DEBUG 10-14 21:17:36 lpllm.py:1205] start load next layer cur_layer_idx: 17
DEBUG 10-14 21:17:36 lpllm.py:1801] cuda memory handles: {1: <capsule object NULL at 0x7fdf3c6d9f80>} {1: b'\xe0\xaa29\x00\x00\x00\x00\xe8&\x1d\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\x86\x00\x00\x00\xff\x00\x00/\x00\x00\x00\x00\x00\x00\x00)\xd8\xd5\xc1\xa2\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-14 21:17:36 client.py:72] load_into_gpu: Mixtral-8x7B, 9bb99201-b6e3-4b3e-af8b-2ba3bab33691
INFO 10-14 21:17:36 client.py:113] Model loaded: Mixtral-8x7B, 9bb99201-b6e3-4b3e-af8b-2ba3bab33691
DEBUG 10-14 21:17:36 lpllm.py:1207] start decoder qkv layer_attn 16 layer_mlp 15
DEBUG 10-14 21:17:36 lpmodule.py:526] hidden_states.shape torch.Size([10, 1, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:36 lpmodule.py:541] kv_seq_len 1 past_key_value 512
DEBUG 10-14 21:17:36 lpmodule.py:545] inv_freq device cpu
DEBUG 10-14 21:17:36 lpmodule.py:546] kv_seq_len 513 position_ids torch.Size([10, 1]) value tensor([512])
DEBUG 10-14 21:17:36 lpmodule.py:551] query_states device cuda:1
DEBUG 10-14 21:17:36 lpmodule.py:552] key_states device cuda:1
DEBUG 10-14 21:17:36 lpmodule.py:553] value_states device cuda:1
DEBUG 10-14 21:17:36 lpmodule.py:554] position_ids device cpu
DEBUG 10-14 21:17:36 lpmodule.py:555] cos device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:36 lpmodule.py:556] sin device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:36 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:36 lpmodule.py:162] call mlp prepare
DEBUG 10-14 21:17:36 lpmodule.py:163] layer idx 15
DEBUG 10-14 21:17:36 lpmodule.py:123] Tokens per expert: [1, 9, 0, 0, 0, 10, 0, 0]
DEBUG 10-14 21:17:36 lpmodule.py:124] Number of active experts: 3
DEBUG 10-14 21:17:36 lpmodule.py:195] call mlp post
DEBUG 10-14 21:17:36 lpllm.py:2262] GPU2CPU move cost 0.000522 seconds
DEBUG 10-14 21:17:36 lpmodule.py:340] layer_idx 16 attn layer_idx: 16
DEBUG 10-14 21:17:36 lpmodule.py:352] get seq_length 512 in decoder_attn_batch
WARNING 10-14 21:17:36 lpmodule.py:355] generally decoder_attn_batch called with q_len == 1
DEBUG 10-14 21:17:36 lpmodule.py:364] decoder_attn_batch update batch_dim 0-10
DEBUG 10-14 21:17:36 StaticCacheLen.py:84] static cache update layer_idx: 16, start_batch: 0, end_batch: 10
DEBUG 10-14 21:17:36 lpmodule.py:368] update for kv cache 0-10 kv_seq_len 513 in decoder_attn_batch
DEBUG 10-14 21:17:36 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:36 lpmodule.py:374] update past key value cost 0.002878 seconds
DEBUG 10-14 21:17:36 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 513, 128]), value shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:36 lpmodule.py:399] repeat qkv cost 0.000001 seconds
DEBUG 10-14 21:17:36 lpmodule.py:400] q shape torch.Size([10, 32, 1, 128]), k shape torch.Size([10, 8, 513, 128]), v shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:36 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:36 lpllm.py:503] reset update experts
DEBUG 10-14 21:17:36 lpllm.py:1771] update state cost 2.4557113647460938e-05 s
DEBUG 10-14 21:17:36 lpllm.py:1740] restore layer func cost 0.0015838146209716797 s
DEBUG 10-14 21:17:36 lpllm.py:511] restore layer cost 0.0018858909606933594 s
DEBUG 10-14 21:17:36 lpllm.py:512] reset_next_layer_need: layer_attn_idx=16, layer_mlp_idx=16, j_loc=32
DEBUG 10-14 21:17:36 lpllm.py:1280] reset layer cost 0.002023458480834961 s
DEBUG 10-14 21:17:36 lpllm.py:1281] have reset next layer layer_attn 16 layer_mlp 15 
DEBUG 10-14 21:17:36 lpllm.py:1200] 
DEBUG 10-14 21:17:36 lpllm.py:1200] one decoder loop j: 33 cur_layer_idx: 16
DEBUG 10-14 21:17:36 lpllm.py:1207] start decoder qkv layer_attn 16 layer_mlp 16
DEBUG 10-14 21:17:36 lpmodule.py:526] hidden_states.shape torch.Size([10, 1, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:36 tutils.py:81] single group 0 real attn out cost 0.00546717643737793 s
DEBUG 10-14 21:17:36 lpmodule.py:541] kv_seq_len 1 past_key_value 512
DEBUG 10-14 21:17:36 lpmodule.py:545] inv_freq device cpu
DEBUG 10-14 21:17:36 lpmodule.py:546] kv_seq_len 513 position_ids torch.Size([10, 1]) value tensor([512])
DEBUG 10-14 21:17:36 lpmodule.py:551] query_states device cuda:1
DEBUG 10-14 21:17:36 lpmodule.py:552] key_states device cuda:1
DEBUG 10-14 21:17:36 lpmodule.py:553] value_states device cuda:1
DEBUG 10-14 21:17:36 lpmodule.py:554] position_ids device cpu
DEBUG 10-14 21:17:36 lpmodule.py:555] cos device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:36 lpmodule.py:556] sin device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:36 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:36 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:36 tutils.py:81] single group 1 real attn out cost 0.003942728042602539 s
DEBUG 10-14 21:17:36 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:36 tutils.py:81] single group 2 real attn out cost 0.0028018951416015625 s
DEBUG 10-14 21:17:36 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:36 tutils.py:81] single group 3 real attn out cost 0.0035369396209716797 s
DEBUG 10-14 21:17:36 tutils.py:88] dot attn help cost 0.022920 seconds
DEBUG 10-14 21:17:36 lpmodule.py:433] dot attn cost 0.023055 seconds
DEBUG 10-14 21:17:36 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.0007750988006591797 s
DEBUG 10-14 21:17:36 lpllm.py:2280] CPU attn cost 0.027518 seconds if batch True
DEBUG 10-14 21:17:36 lpllm.py:2289] deal attn result cost 0.000004 seconds
DEBUG 10-14 21:17:36 lpllm.py:2291] CPU compute cost 0.028440 seconds
DEBUG 10-14 21:17:36 lpllm.py:2309] free cost 0.000181 seconds
DEBUG 10-14 21:17:36 lpmodule.py:162] call mlp prepare
DEBUG 10-14 21:17:36 lpmodule.py:163] layer idx 16
DEBUG 10-14 21:17:36 lpmodule.py:123] Tokens per expert: [0, 0, 0, 0, 5, 9, 1, 5]
DEBUG 10-14 21:17:36 lpmodule.py:124] Number of active experts: 4
DEBUG 10-14 21:17:36 lpmodule.py:195] call mlp post
DEBUG 10-14 21:17:36 lpllm.py:2262] GPU2CPU move cost 0.000292 seconds
DEBUG 10-14 21:17:36 lpmodule.py:340] layer_idx 16 attn layer_idx: 16
DEBUG 10-14 21:17:36 lpmodule.py:352] get seq_length 512 in decoder_attn_batch
WARNING 10-14 21:17:36 lpmodule.py:355] generally decoder_attn_batch called with q_len == 1
DEBUG 10-14 21:17:36 lpmodule.py:364] decoder_attn_batch update batch_dim 10-20
DEBUG 10-14 21:17:36 StaticCacheLen.py:84] static cache update layer_idx: 16, start_batch: 10, end_batch: 20
DEBUG 10-14 21:17:36 StaticCacheLen.py:90] static cache update layer_idx: 16, update seq_length to 513
DEBUG 10-14 21:17:36 lpmodule.py:368] update for kv cache 10-20 kv_seq_len 513 in decoder_attn_batch
DEBUG 10-14 21:17:36 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:36 lpmodule.py:374] update past key value cost 0.002494 seconds
DEBUG 10-14 21:17:36 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 513, 128]), value shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:36 lpmodule.py:399] repeat qkv cost 0.000001 seconds
DEBUG 10-14 21:17:36 lpmodule.py:400] q shape torch.Size([10, 32, 1, 128]), k shape torch.Size([10, 8, 513, 128]), v shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:36 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:36 lpllm.py:498] reset update attn
DEBUG 10-14 21:17:36 lpllm.py:1771] update state cost 4.220008850097656e-05 s
DEBUG 10-14 21:17:36 tutils.py:81] single group 0 real attn out cost 0.0052642822265625 s
DEBUG 10-14 21:17:36 lpllm.py:1740] restore layer func cost 0.0012440681457519531 s
DEBUG 10-14 21:17:36 lpllm.py:511] restore layer cost 0.001642465591430664 s
DEBUG 10-14 21:17:36 lpllm.py:512] reset_next_layer_need: layer_attn_idx=17, layer_mlp_idx=16, j_loc=33
DEBUG 10-14 21:17:36 lpllm.py:1280] reset layer cost 0.0018146038055419922 s
DEBUG 10-14 21:17:36 lpllm.py:1281] have reset next layer layer_attn 16 layer_mlp 16 
DEBUG 10-14 21:17:36 lpllm.py:1287] j: 33 waiting the layer with layer_idx 17 before wait time 0.06529903411865234 s
INFO 10-14 21:17:36 client.py:117] confirm_model_loaded: Mixtral-8x7B, 9bb99201-b6e3-4b3e-af8b-2ba3bab33691
DEBUG 10-14 21:17:36 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:36 tutils.py:81] single group 1 real attn out cost 0.002819538116455078 s
DEBUG 10-14 21:17:36 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:36 tutils.py:81] single group 2 real attn out cost 0.0027511119842529297 s
DEBUG 10-14 21:17:36 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:36 tutils.py:81] single group 3 real attn out cost 0.0027680397033691406 s
DEBUG 10-14 21:17:36 tutils.py:88] dot attn help cost 0.020989 seconds
DEBUG 10-14 21:17:36 lpmodule.py:433] dot attn cost 0.021106 seconds
DEBUG 10-14 21:17:36 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.0007572174072265625 s
DEBUG 10-14 21:17:36 lpllm.py:2280] CPU attn cost 0.025014 seconds if batch True
DEBUG 10-14 21:17:36 lpllm.py:2289] deal attn result cost 0.000004 seconds
DEBUG 10-14 21:17:36 lpllm.py:2291] CPU compute cost 0.025657 seconds
DEBUG 10-14 21:17:36 lpllm.py:2309] free cost 0.000163 seconds
INFO 10-14 21:17:36 client.py:125] Model loaded
DEBUG 10-14 21:17:36 lpllm.py:1291] j: load cost 0.293917179107666 s waiting cost 0.22857213020324707 s
DEBUG 10-14 21:17:36 lpllm.py:1200] 
DEBUG 10-14 21:17:36 lpllm.py:1200] one decoder loop j: 34 cur_layer_idx: 17
DEBUG 10-14 21:17:36 lpllm.py:1205] start load next layer cur_layer_idx: 18
DEBUG 10-14 21:17:36 lpllm.py:1801] cuda memory handles: {1: <capsule object NULL at 0x7fdf3c6da010>} {1: b'\xe0\xaa29\x00\x00\x00\x00\xe8&\x1d\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\x86\x00\x00\x00\xff\x00\x000\x00\x00\x00\x00\x00\x00\x00)\xd8\xd5\xc1\xa4\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-14 21:17:36 client.py:72] load_into_gpu: Mixtral-8x7B, dc8e5e21-9112-424b-a321-e84294fe4663
INFO 10-14 21:17:36 client.py:113] Model loaded: Mixtral-8x7B, dc8e5e21-9112-424b-a321-e84294fe4663
DEBUG 10-14 21:17:36 lpllm.py:1207] start decoder qkv layer_attn 17 layer_mlp 16
DEBUG 10-14 21:17:36 lpmodule.py:526] hidden_states.shape torch.Size([10, 1, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:36 lpmodule.py:541] kv_seq_len 1 past_key_value 512
DEBUG 10-14 21:17:36 lpmodule.py:545] inv_freq device cpu
DEBUG 10-14 21:17:36 lpmodule.py:546] kv_seq_len 513 position_ids torch.Size([10, 1]) value tensor([512])
DEBUG 10-14 21:17:36 lpmodule.py:551] query_states device cuda:1
DEBUG 10-14 21:17:36 lpmodule.py:552] key_states device cuda:1
DEBUG 10-14 21:17:36 lpmodule.py:553] value_states device cuda:1
DEBUG 10-14 21:17:36 lpmodule.py:554] position_ids device cpu
DEBUG 10-14 21:17:36 lpmodule.py:555] cos device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:36 lpmodule.py:556] sin device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:36 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:36 lpmodule.py:162] call mlp prepare
DEBUG 10-14 21:17:36 lpmodule.py:163] layer idx 16
DEBUG 10-14 21:17:36 lpmodule.py:123] Tokens per expert: [0, 0, 1, 0, 1, 0, 9, 9]
DEBUG 10-14 21:17:36 lpmodule.py:124] Number of active experts: 4
DEBUG 10-14 21:17:36 lpmodule.py:195] call mlp post
DEBUG 10-14 21:17:36 lpllm.py:2262] GPU2CPU move cost 0.000632 seconds
DEBUG 10-14 21:17:36 lpmodule.py:340] layer_idx 17 attn layer_idx: 17
DEBUG 10-14 21:17:36 lpmodule.py:352] get seq_length 512 in decoder_attn_batch
WARNING 10-14 21:17:36 lpmodule.py:355] generally decoder_attn_batch called with q_len == 1
DEBUG 10-14 21:17:36 lpmodule.py:364] decoder_attn_batch update batch_dim 0-10
DEBUG 10-14 21:17:36 StaticCacheLen.py:84] static cache update layer_idx: 17, start_batch: 0, end_batch: 10
DEBUG 10-14 21:17:36 lpmodule.py:368] update for kv cache 0-10 kv_seq_len 513 in decoder_attn_batch
DEBUG 10-14 21:17:36 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:36 lpmodule.py:374] update past key value cost 0.002584 seconds
DEBUG 10-14 21:17:36 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 513, 128]), value shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:36 lpmodule.py:399] repeat qkv cost 0.000001 seconds
DEBUG 10-14 21:17:36 lpmodule.py:400] q shape torch.Size([10, 32, 1, 128]), k shape torch.Size([10, 8, 513, 128]), v shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:36 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:36 lpllm.py:503] reset update experts
DEBUG 10-14 21:17:36 lpllm.py:1771] update state cost 5.412101745605469e-05 s
DEBUG 10-14 21:17:36 lpllm.py:1740] restore layer func cost 0.0016107559204101562 s
DEBUG 10-14 21:17:36 lpllm.py:511] restore layer cost 0.001987457275390625 s
DEBUG 10-14 21:17:36 lpllm.py:512] reset_next_layer_need: layer_attn_idx=17, layer_mlp_idx=17, j_loc=34
DEBUG 10-14 21:17:36 lpllm.py:1280] reset layer cost 0.0022172927856445312 s
DEBUG 10-14 21:17:36 lpllm.py:1281] have reset next layer layer_attn 17 layer_mlp 16 
DEBUG 10-14 21:17:36 lpllm.py:1200] 
DEBUG 10-14 21:17:36 lpllm.py:1200] one decoder loop j: 35 cur_layer_idx: 17
DEBUG 10-14 21:17:36 lpllm.py:1207] start decoder qkv layer_attn 17 layer_mlp 17
DEBUG 10-14 21:17:36 lpmodule.py:526] hidden_states.shape torch.Size([10, 1, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:36 lpmodule.py:541] kv_seq_len 1 past_key_value 512
DEBUG 10-14 21:17:36 lpmodule.py:545] inv_freq device cpu
DEBUG 10-14 21:17:36 lpmodule.py:546] kv_seq_len 513 position_ids torch.Size([10, 1]) value tensor([512])
DEBUG 10-14 21:17:36 tutils.py:81] single group 0 real attn out cost 0.006022930145263672 s
DEBUG 10-14 21:17:36 lpmodule.py:551] query_states device cuda:1
DEBUG 10-14 21:17:36 lpmodule.py:552] key_states device cuda:1
DEBUG 10-14 21:17:36 lpmodule.py:553] value_states device cuda:1
DEBUG 10-14 21:17:36 lpmodule.py:554] position_ids device cpu
DEBUG 10-14 21:17:36 lpmodule.py:555] cos device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:36 lpmodule.py:556] sin device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:36 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:36 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:36 tutils.py:81] single group 1 real attn out cost 0.003096342086791992 s
DEBUG 10-14 21:17:36 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:36 tutils.py:81] single group 2 real attn out cost 0.0029273033142089844 s
DEBUG 10-14 21:17:36 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:36 tutils.py:81] single group 3 real attn out cost 0.003834962844848633 s
DEBUG 10-14 21:17:36 tutils.py:88] dot attn help cost 0.021539 seconds
DEBUG 10-14 21:17:36 lpmodule.py:433] dot attn cost 0.021611 seconds
DEBUG 10-14 21:17:36 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.00044345855712890625 s
DEBUG 10-14 21:17:36 lpllm.py:2280] CPU attn cost 0.025383 seconds if batch True
DEBUG 10-14 21:17:36 lpllm.py:2289] deal attn result cost 0.000002 seconds
DEBUG 10-14 21:17:36 lpllm.py:2291] CPU compute cost 0.026294 seconds
DEBUG 10-14 21:17:36 lpllm.py:2309] free cost 0.000133 seconds
DEBUG 10-14 21:17:36 lpmodule.py:162] call mlp prepare
DEBUG 10-14 21:17:36 lpmodule.py:163] layer idx 17
DEBUG 10-14 21:17:36 lpmodule.py:123] Tokens per expert: [5, 1, 3, 5, 0, 0, 5, 1]
DEBUG 10-14 21:17:36 lpmodule.py:124] Number of active experts: 6
DEBUG 10-14 21:17:36 lpmodule.py:195] call mlp post
DEBUG 10-14 21:17:36 lpllm.py:2262] GPU2CPU move cost 0.000343 seconds
DEBUG 10-14 21:17:36 lpmodule.py:340] layer_idx 17 attn layer_idx: 17
DEBUG 10-14 21:17:36 lpmodule.py:352] get seq_length 512 in decoder_attn_batch
WARNING 10-14 21:17:36 lpmodule.py:355] generally decoder_attn_batch called with q_len == 1
DEBUG 10-14 21:17:36 lpmodule.py:364] decoder_attn_batch update batch_dim 10-20
DEBUG 10-14 21:17:36 StaticCacheLen.py:84] static cache update layer_idx: 17, start_batch: 10, end_batch: 20
DEBUG 10-14 21:17:36 StaticCacheLen.py:90] static cache update layer_idx: 17, update seq_length to 513
DEBUG 10-14 21:17:36 lpmodule.py:368] update for kv cache 10-20 kv_seq_len 513 in decoder_attn_batch
DEBUG 10-14 21:17:36 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:36 lpmodule.py:374] update past key value cost 0.002204 seconds
DEBUG 10-14 21:17:36 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 513, 128]), value shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:36 lpmodule.py:399] repeat qkv cost 0.000001 seconds
DEBUG 10-14 21:17:36 lpmodule.py:400] q shape torch.Size([10, 32, 1, 128]), k shape torch.Size([10, 8, 513, 128]), v shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:36 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:36 lpllm.py:498] reset update attn
DEBUG 10-14 21:17:36 lpllm.py:1771] update state cost 3.8623809814453125e-05 s
DEBUG 10-14 21:17:36 tutils.py:81] single group 0 real attn out cost 0.0055882930755615234 s
DEBUG 10-14 21:17:36 lpllm.py:1740] restore layer func cost 0.0010645389556884766 s
DEBUG 10-14 21:17:36 lpllm.py:511] restore layer cost 0.0013971328735351562 s
DEBUG 10-14 21:17:36 lpllm.py:512] reset_next_layer_need: layer_attn_idx=18, layer_mlp_idx=17, j_loc=35
DEBUG 10-14 21:17:36 lpllm.py:1280] reset layer cost 0.0015451908111572266 s
DEBUG 10-14 21:17:36 lpllm.py:1281] have reset next layer layer_attn 17 layer_mlp 17 
DEBUG 10-14 21:17:36 lpllm.py:1287] j: 35 waiting the layer with layer_idx 18 before wait time 0.06324386596679688 s
INFO 10-14 21:17:36 client.py:117] confirm_model_loaded: Mixtral-8x7B, dc8e5e21-9112-424b-a321-e84294fe4663
DEBUG 10-14 21:17:36 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:36 tutils.py:81] single group 1 real attn out cost 0.0032389163970947266 s
DEBUG 10-14 21:17:36 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:36 tutils.py:81] single group 2 real attn out cost 0.003329753875732422 s
DEBUG 10-14 21:17:36 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:36 tutils.py:81] single group 3 real attn out cost 0.0030510425567626953 s
DEBUG 10-14 21:17:36 tutils.py:88] dot attn help cost 0.023327 seconds
DEBUG 10-14 21:17:36 lpmodule.py:433] dot attn cost 0.023437 seconds
DEBUG 10-14 21:17:36 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.0013575553894042969 s
DEBUG 10-14 21:17:36 lpllm.py:2280] CPU attn cost 0.027584 seconds if batch True
DEBUG 10-14 21:17:36 lpllm.py:2289] deal attn result cost 0.000003 seconds
DEBUG 10-14 21:17:36 lpllm.py:2291] CPU compute cost 0.028179 seconds
DEBUG 10-14 21:17:36 lpllm.py:2309] free cost 0.000153 seconds
INFO 10-14 21:17:36 client.py:125] Model loaded
DEBUG 10-14 21:17:36 lpllm.py:1291] j: load cost 0.29145240783691406 s waiting cost 0.22817730903625488 s
DEBUG 10-14 21:17:36 lpllm.py:1200] 
DEBUG 10-14 21:17:36 lpllm.py:1200] one decoder loop j: 36 cur_layer_idx: 18
DEBUG 10-14 21:17:36 lpllm.py:1205] start load next layer cur_layer_idx: 19
DEBUG 10-14 21:17:36 lpllm.py:1801] cuda memory handles: {1: <capsule object NULL at 0x7fdf3c6d9ec0>} {1: b'\xe0\xaa29\x00\x00\x00\x00\xe8&\x1d\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\x86\x00\x00\x00\xff\x00\x00.\x00\x00\x00\x00\x00\x00\x00)\xd8\xd5\xc1\xa0\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-14 21:17:36 client.py:72] load_into_gpu: Mixtral-8x7B, 1a97b951-6b87-44a0-a5e8-6cef0dbcf0ff
INFO 10-14 21:17:36 client.py:113] Model loaded: Mixtral-8x7B, 1a97b951-6b87-44a0-a5e8-6cef0dbcf0ff
DEBUG 10-14 21:17:36 lpllm.py:1207] start decoder qkv layer_attn 18 layer_mlp 17
DEBUG 10-14 21:17:36 lpmodule.py:526] hidden_states.shape torch.Size([10, 1, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:36 lpmodule.py:541] kv_seq_len 1 past_key_value 512
DEBUG 10-14 21:17:36 lpmodule.py:545] inv_freq device cpu
DEBUG 10-14 21:17:36 lpmodule.py:546] kv_seq_len 513 position_ids torch.Size([10, 1]) value tensor([512])
DEBUG 10-14 21:17:36 lpmodule.py:551] query_states device cuda:1
DEBUG 10-14 21:17:36 lpmodule.py:552] key_states device cuda:1
DEBUG 10-14 21:17:36 lpmodule.py:553] value_states device cuda:1
DEBUG 10-14 21:17:36 lpmodule.py:554] position_ids device cpu
DEBUG 10-14 21:17:36 lpmodule.py:555] cos device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:36 lpmodule.py:556] sin device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:36 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:36 lpmodule.py:162] call mlp prepare
DEBUG 10-14 21:17:36 lpmodule.py:163] layer idx 17
DEBUG 10-14 21:17:36 lpmodule.py:123] Tokens per expert: [0, 0, 9, 10, 0, 0, 1, 0]
DEBUG 10-14 21:17:36 lpmodule.py:124] Number of active experts: 3
DEBUG 10-14 21:17:36 lpmodule.py:195] call mlp post
DEBUG 10-14 21:17:36 lpllm.py:2262] GPU2CPU move cost 0.000350 seconds
DEBUG 10-14 21:17:36 lpmodule.py:340] layer_idx 18 attn layer_idx: 18
DEBUG 10-14 21:17:36 lpmodule.py:352] get seq_length 512 in decoder_attn_batch
WARNING 10-14 21:17:36 lpmodule.py:355] generally decoder_attn_batch called with q_len == 1
DEBUG 10-14 21:17:36 lpmodule.py:364] decoder_attn_batch update batch_dim 0-10
DEBUG 10-14 21:17:36 StaticCacheLen.py:84] static cache update layer_idx: 18, start_batch: 0, end_batch: 10
DEBUG 10-14 21:17:36 lpmodule.py:368] update for kv cache 0-10 kv_seq_len 513 in decoder_attn_batch
DEBUG 10-14 21:17:36 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:36 lpmodule.py:374] update past key value cost 0.002610 seconds
DEBUG 10-14 21:17:36 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 513, 128]), value shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:36 lpmodule.py:399] repeat qkv cost 0.000001 seconds
DEBUG 10-14 21:17:36 lpmodule.py:400] q shape torch.Size([10, 32, 1, 128]), k shape torch.Size([10, 8, 513, 128]), v shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:36 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:36 lpllm.py:503] reset update experts
DEBUG 10-14 21:17:36 lpllm.py:1771] update state cost 3.0279159545898438e-05 s
DEBUG 10-14 21:17:36 lpllm.py:1740] restore layer func cost 0.0025691986083984375 s
DEBUG 10-14 21:17:36 lpllm.py:511] restore layer cost 0.0029959678649902344 s
DEBUG 10-14 21:17:36 lpllm.py:512] reset_next_layer_need: layer_attn_idx=18, layer_mlp_idx=18, j_loc=36
DEBUG 10-14 21:17:36 lpllm.py:1280] reset layer cost 0.0031108856201171875 s
DEBUG 10-14 21:17:36 lpllm.py:1281] have reset next layer layer_attn 18 layer_mlp 17 
DEBUG 10-14 21:17:36 lpllm.py:1200] 
DEBUG 10-14 21:17:36 lpllm.py:1200] one decoder loop j: 37 cur_layer_idx: 18
DEBUG 10-14 21:17:36 lpllm.py:1207] start decoder qkv layer_attn 18 layer_mlp 18
DEBUG 10-14 21:17:36 lpmodule.py:526] hidden_states.shape torch.Size([10, 1, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:36 tutils.py:81] single group 0 real attn out cost 0.006503582000732422 s
DEBUG 10-14 21:17:36 lpmodule.py:541] kv_seq_len 1 past_key_value 512
DEBUG 10-14 21:17:36 lpmodule.py:545] inv_freq device cpu
DEBUG 10-14 21:17:36 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:36 lpmodule.py:546] kv_seq_len 513 position_ids torch.Size([10, 1]) value tensor([512])
DEBUG 10-14 21:17:36 lpmodule.py:551] query_states device cuda:1
DEBUG 10-14 21:17:36 lpmodule.py:552] key_states device cuda:1
DEBUG 10-14 21:17:36 lpmodule.py:553] value_states device cuda:1
DEBUG 10-14 21:17:36 lpmodule.py:554] position_ids device cpu
DEBUG 10-14 21:17:36 lpmodule.py:555] cos device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:36 lpmodule.py:556] sin device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:36 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:36 tutils.py:81] single group 1 real attn out cost 0.003358602523803711 s
DEBUG 10-14 21:17:36 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:36 tutils.py:81] single group 2 real attn out cost 0.0027213096618652344 s
DEBUG 10-14 21:17:36 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:36 tutils.py:81] single group 3 real attn out cost 0.004069328308105469 s
DEBUG 10-14 21:17:36 tutils.py:88] dot attn help cost 0.023045 seconds
DEBUG 10-14 21:17:36 lpmodule.py:433] dot attn cost 0.023176 seconds
DEBUG 10-14 21:17:36 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.0003285408020019531 s
DEBUG 10-14 21:17:36 lpllm.py:2280] CPU attn cost 0.026966 seconds if batch True
DEBUG 10-14 21:17:36 lpllm.py:2289] deal attn result cost 0.000004 seconds
DEBUG 10-14 21:17:36 lpllm.py:2291] CPU compute cost 0.027691 seconds
DEBUG 10-14 21:17:36 lpllm.py:2309] free cost 0.000208 seconds
DEBUG 10-14 21:17:36 lpmodule.py:162] call mlp prepare
DEBUG 10-14 21:17:36 lpmodule.py:163] layer idx 18
DEBUG 10-14 21:17:36 lpmodule.py:123] Tokens per expert: [3, 0, 0, 1, 4, 3, 3, 6]
DEBUG 10-14 21:17:36 lpmodule.py:124] Number of active experts: 6
DEBUG 10-14 21:17:36 lpmodule.py:195] call mlp post
DEBUG 10-14 21:17:36 lpllm.py:2262] GPU2CPU move cost 0.000311 seconds
DEBUG 10-14 21:17:36 lpmodule.py:340] layer_idx 18 attn layer_idx: 18
DEBUG 10-14 21:17:36 lpmodule.py:352] get seq_length 512 in decoder_attn_batch
WARNING 10-14 21:17:36 lpmodule.py:355] generally decoder_attn_batch called with q_len == 1
DEBUG 10-14 21:17:36 lpmodule.py:364] decoder_attn_batch update batch_dim 10-20
DEBUG 10-14 21:17:36 StaticCacheLen.py:84] static cache update layer_idx: 18, start_batch: 10, end_batch: 20
DEBUG 10-14 21:17:36 StaticCacheLen.py:90] static cache update layer_idx: 18, update seq_length to 513
DEBUG 10-14 21:17:36 lpmodule.py:368] update for kv cache 10-20 kv_seq_len 513 in decoder_attn_batch
DEBUG 10-14 21:17:36 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:36 lpmodule.py:374] update past key value cost 0.002502 seconds
DEBUG 10-14 21:17:36 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 513, 128]), value shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:36 lpmodule.py:399] repeat qkv cost 0.000002 seconds
DEBUG 10-14 21:17:36 lpmodule.py:400] q shape torch.Size([10, 32, 1, 128]), k shape torch.Size([10, 8, 513, 128]), v shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:36 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:36 tutils.py:81] single group 0 real attn out cost 0.0052337646484375 s
DEBUG 10-14 21:17:36 lpllm.py:498] reset update attn
DEBUG 10-14 21:17:36 lpllm.py:1771] update state cost 3.0994415283203125e-05 s
DEBUG 10-14 21:17:36 lpllm.py:1740] restore layer func cost 0.0008680820465087891 s
DEBUG 10-14 21:17:36 lpllm.py:511] restore layer cost 0.0011522769927978516 s
DEBUG 10-14 21:17:36 lpllm.py:512] reset_next_layer_need: layer_attn_idx=19, layer_mlp_idx=18, j_loc=37
DEBUG 10-14 21:17:36 lpllm.py:1280] reset layer cost 0.0013110637664794922 s
DEBUG 10-14 21:17:36 lpllm.py:1281] have reset next layer layer_attn 18 layer_mlp 18 
DEBUG 10-14 21:17:36 lpllm.py:1287] j: 37 waiting the layer with layer_idx 19 before wait time 0.06585288047790527 s
INFO 10-14 21:17:36 client.py:117] confirm_model_loaded: Mixtral-8x7B, 1a97b951-6b87-44a0-a5e8-6cef0dbcf0ff
DEBUG 10-14 21:17:36 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:36 tutils.py:81] single group 1 real attn out cost 0.002777576446533203 s
DEBUG 10-14 21:17:36 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:36 tutils.py:81] single group 2 real attn out cost 0.0029764175415039062 s
DEBUG 10-14 21:17:36 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:36 tutils.py:81] single group 3 real attn out cost 0.0035071372985839844 s
DEBUG 10-14 21:17:36 tutils.py:88] dot attn help cost 0.021443 seconds
DEBUG 10-14 21:17:36 lpmodule.py:433] dot attn cost 0.021579 seconds
DEBUG 10-14 21:17:36 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.0008890628814697266 s
DEBUG 10-14 21:17:36 lpllm.py:2280] CPU attn cost 0.025655 seconds if batch True
DEBUG 10-14 21:17:36 lpllm.py:2289] deal attn result cost 0.000003 seconds
DEBUG 10-14 21:17:36 lpllm.py:2291] CPU compute cost 0.026258 seconds
DEBUG 10-14 21:17:36 lpllm.py:2309] free cost 0.000158 seconds
INFO 10-14 21:17:36 client.py:125] Model loaded
DEBUG 10-14 21:17:36 lpllm.py:1291] j: load cost 0.30292248725891113 s waiting cost 0.23703837394714355 s
DEBUG 10-14 21:17:36 lpllm.py:1200] 
DEBUG 10-14 21:17:36 lpllm.py:1200] one decoder loop j: 38 cur_layer_idx: 19
DEBUG 10-14 21:17:36 lpllm.py:1205] start load next layer cur_layer_idx: 20
DEBUG 10-14 21:17:36 lpllm.py:1801] cuda memory handles: {1: <capsule object NULL at 0x7fdf3c6d9f80>} {1: b'\xe0\xaa29\x00\x00\x00\x00\xe8&\x1d\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\x86\x00\x00\x00\xff\x00\x00/\x00\x00\x00\x00\x00\x00\x00)\xd8\xd5\xc1\xa2\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-14 21:17:36 client.py:72] load_into_gpu: Mixtral-8x7B, a4c98148-25db-4896-a5a9-53307280152c
INFO 10-14 21:17:36 client.py:113] Model loaded: Mixtral-8x7B, a4c98148-25db-4896-a5a9-53307280152c
DEBUG 10-14 21:17:36 lpllm.py:1207] start decoder qkv layer_attn 19 layer_mlp 18
DEBUG 10-14 21:17:36 lpmodule.py:526] hidden_states.shape torch.Size([10, 1, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:36 lpmodule.py:541] kv_seq_len 1 past_key_value 512
DEBUG 10-14 21:17:36 lpmodule.py:545] inv_freq device cpu
DEBUG 10-14 21:17:36 lpmodule.py:546] kv_seq_len 513 position_ids torch.Size([10, 1]) value tensor([512])
DEBUG 10-14 21:17:36 lpmodule.py:551] query_states device cuda:1
DEBUG 10-14 21:17:36 lpmodule.py:552] key_states device cuda:1
DEBUG 10-14 21:17:36 lpmodule.py:553] value_states device cuda:1
DEBUG 10-14 21:17:36 lpmodule.py:554] position_ids device cpu
DEBUG 10-14 21:17:36 lpmodule.py:555] cos device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:36 lpmodule.py:556] sin device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:36 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:36 lpmodule.py:162] call mlp prepare
DEBUG 10-14 21:17:36 lpmodule.py:163] layer idx 18
DEBUG 10-14 21:17:36 lpmodule.py:123] Tokens per expert: [1, 0, 1, 2, 0, 7, 0, 9]
DEBUG 10-14 21:17:36 lpmodule.py:124] Number of active experts: 5
DEBUG 10-14 21:17:36 lpmodule.py:195] call mlp post
DEBUG 10-14 21:17:36 lpllm.py:2262] GPU2CPU move cost 0.000610 seconds
DEBUG 10-14 21:17:36 lpmodule.py:340] layer_idx 19 attn layer_idx: 19
DEBUG 10-14 21:17:36 lpmodule.py:352] get seq_length 512 in decoder_attn_batch
WARNING 10-14 21:17:36 lpmodule.py:355] generally decoder_attn_batch called with q_len == 1
DEBUG 10-14 21:17:36 lpmodule.py:364] decoder_attn_batch update batch_dim 0-10
DEBUG 10-14 21:17:36 StaticCacheLen.py:84] static cache update layer_idx: 19, start_batch: 0, end_batch: 10
DEBUG 10-14 21:17:36 lpmodule.py:368] update for kv cache 0-10 kv_seq_len 513 in decoder_attn_batch
DEBUG 10-14 21:17:36 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:36 lpmodule.py:374] update past key value cost 0.002951 seconds
DEBUG 10-14 21:17:36 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 513, 128]), value shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:36 lpmodule.py:399] repeat qkv cost 0.000001 seconds
DEBUG 10-14 21:17:36 lpmodule.py:400] q shape torch.Size([10, 32, 1, 128]), k shape torch.Size([10, 8, 513, 128]), v shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:36 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:36 lpllm.py:503] reset update experts
DEBUG 10-14 21:17:36 lpllm.py:1771] update state cost 3.218650817871094e-05 s
DEBUG 10-14 21:17:36 lpllm.py:1740] restore layer func cost 0.0020394325256347656 s
DEBUG 10-14 21:17:36 lpllm.py:511] restore layer cost 0.002428770065307617 s
DEBUG 10-14 21:17:36 lpllm.py:512] reset_next_layer_need: layer_attn_idx=19, layer_mlp_idx=19, j_loc=38
DEBUG 10-14 21:17:36 lpllm.py:1280] reset layer cost 0.002588033676147461 s
DEBUG 10-14 21:17:36 lpllm.py:1281] have reset next layer layer_attn 19 layer_mlp 18 
DEBUG 10-14 21:17:36 lpllm.py:1200] 
DEBUG 10-14 21:17:36 lpllm.py:1200] one decoder loop j: 39 cur_layer_idx: 19
DEBUG 10-14 21:17:36 lpllm.py:1207] start decoder qkv layer_attn 19 layer_mlp 19
DEBUG 10-14 21:17:36 lpmodule.py:526] hidden_states.shape torch.Size([10, 1, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:36 lpmodule.py:541] kv_seq_len 1 past_key_value 512
DEBUG 10-14 21:17:36 lpmodule.py:545] inv_freq device cpu
DEBUG 10-14 21:17:36 lpmodule.py:546] kv_seq_len 513 position_ids torch.Size([10, 1]) value tensor([512])
DEBUG 10-14 21:17:36 lpmodule.py:551] query_states device cuda:1
DEBUG 10-14 21:17:36 lpmodule.py:552] key_states device cuda:1
DEBUG 10-14 21:17:36 lpmodule.py:553] value_states device cuda:1
DEBUG 10-14 21:17:36 lpmodule.py:554] position_ids device cpu
DEBUG 10-14 21:17:36 lpmodule.py:555] cos device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:36 lpmodule.py:556] sin device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:36 tutils.py:81] single group 0 real attn out cost 0.00693058967590332 s
DEBUG 10-14 21:17:36 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:36 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:36 tutils.py:81] single group 1 real attn out cost 0.003088712692260742 s
DEBUG 10-14 21:17:36 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:36 tutils.py:81] single group 2 real attn out cost 0.003160238265991211 s
DEBUG 10-14 21:17:36 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:36 tutils.py:81] single group 3 real attn out cost 0.003220796585083008 s
DEBUG 10-14 21:17:36 tutils.py:88] dot attn help cost 0.023193 seconds
DEBUG 10-14 21:17:36 lpmodule.py:433] dot attn cost 0.023349 seconds
DEBUG 10-14 21:17:36 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.00037980079650878906 s
DEBUG 10-14 21:17:36 lpllm.py:2280] CPU attn cost 0.027692 seconds if batch True
DEBUG 10-14 21:17:36 lpllm.py:2289] deal attn result cost 0.000004 seconds
DEBUG 10-14 21:17:36 lpllm.py:2291] CPU compute cost 0.028739 seconds
DEBUG 10-14 21:17:36 lpllm.py:2309] free cost 0.000246 seconds
DEBUG 10-14 21:17:36 lpmodule.py:162] call mlp prepare
DEBUG 10-14 21:17:36 lpmodule.py:163] layer idx 19
DEBUG 10-14 21:17:36 lpmodule.py:123] Tokens per expert: [0, 5, 3, 4, 2, 3, 2, 1]
DEBUG 10-14 21:17:36 lpmodule.py:124] Number of active experts: 7
DEBUG 10-14 21:17:36 lpmodule.py:195] call mlp post
DEBUG 10-14 21:17:36 lpllm.py:2262] GPU2CPU move cost 0.000664 seconds
DEBUG 10-14 21:17:36 lpmodule.py:340] layer_idx 19 attn layer_idx: 19
DEBUG 10-14 21:17:36 lpmodule.py:352] get seq_length 512 in decoder_attn_batch
WARNING 10-14 21:17:36 lpmodule.py:355] generally decoder_attn_batch called with q_len == 1
DEBUG 10-14 21:17:36 lpmodule.py:364] decoder_attn_batch update batch_dim 10-20
DEBUG 10-14 21:17:36 StaticCacheLen.py:84] static cache update layer_idx: 19, start_batch: 10, end_batch: 20
DEBUG 10-14 21:17:36 StaticCacheLen.py:90] static cache update layer_idx: 19, update seq_length to 513
DEBUG 10-14 21:17:36 lpmodule.py:368] update for kv cache 10-20 kv_seq_len 513 in decoder_attn_batch
DEBUG 10-14 21:17:36 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:36 lpmodule.py:374] update past key value cost 0.002653 seconds
DEBUG 10-14 21:17:36 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 513, 128]), value shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:36 lpmodule.py:399] repeat qkv cost 0.000001 seconds
DEBUG 10-14 21:17:36 lpmodule.py:400] q shape torch.Size([10, 32, 1, 128]), k shape torch.Size([10, 8, 513, 128]), v shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:36 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:36 lpllm.py:498] reset update attn
DEBUG 10-14 21:17:36 lpllm.py:1771] update state cost 2.4557113647460938e-05 s
DEBUG 10-14 21:17:36 lpllm.py:1740] restore layer func cost 0.0005486011505126953 s
DEBUG 10-14 21:17:36 lpllm.py:511] restore layer cost 0.0007596015930175781 s
DEBUG 10-14 21:17:36 lpllm.py:512] reset_next_layer_need: layer_attn_idx=20, layer_mlp_idx=19, j_loc=39
DEBUG 10-14 21:17:36 lpllm.py:1280] reset layer cost 0.0008723735809326172 s
DEBUG 10-14 21:17:36 lpllm.py:1281] have reset next layer layer_attn 19 layer_mlp 19 
DEBUG 10-14 21:17:36 lpllm.py:1287] j: 39 waiting the layer with layer_idx 20 before wait time 0.06547164916992188 s
INFO 10-14 21:17:36 client.py:117] confirm_model_loaded: Mixtral-8x7B, a4c98148-25db-4896-a5a9-53307280152c
DEBUG 10-14 21:17:36 tutils.py:81] single group 0 real attn out cost 0.005944967269897461 s
DEBUG 10-14 21:17:36 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:36 tutils.py:81] single group 1 real attn out cost 0.0030341148376464844 s
DEBUG 10-14 21:17:36 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:36 tutils.py:81] single group 2 real attn out cost 0.0031442642211914062 s
DEBUG 10-14 21:17:36 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:36 tutils.py:81] single group 3 real attn out cost 0.003262042999267578 s
DEBUG 10-14 21:17:36 tutils.py:88] dot attn help cost 0.021650 seconds
DEBUG 10-14 21:17:36 lpmodule.py:433] dot attn cost 0.021826 seconds
DEBUG 10-14 21:17:36 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.0013477802276611328 s
DEBUG 10-14 21:17:36 lpllm.py:2280] CPU attn cost 0.026677 seconds if batch True
DEBUG 10-14 21:17:36 lpllm.py:2289] deal attn result cost 0.000004 seconds
DEBUG 10-14 21:17:36 lpllm.py:2291] CPU compute cost 0.027683 seconds
DEBUG 10-14 21:17:36 lpllm.py:2309] free cost 0.000170 seconds
INFO 10-14 21:17:37 client.py:125] Model loaded
DEBUG 10-14 21:17:37 lpllm.py:1291] j: load cost 0.30220794677734375 s waiting cost 0.23671793937683105 s
DEBUG 10-14 21:17:37 lpllm.py:1200] 
DEBUG 10-14 21:17:37 lpllm.py:1200] one decoder loop j: 40 cur_layer_idx: 20
DEBUG 10-14 21:17:37 lpllm.py:1205] start load next layer cur_layer_idx: 21
DEBUG 10-14 21:17:37 lpllm.py:1801] cuda memory handles: {1: <capsule object NULL at 0x7fdf3c6da010>} {1: b'\xe0\xaa29\x00\x00\x00\x00\xe8&\x1d\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\x86\x00\x00\x00\xff\x00\x000\x00\x00\x00\x00\x00\x00\x00)\xd8\xd5\xc1\xa4\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-14 21:17:37 client.py:72] load_into_gpu: Mixtral-8x7B, 0f083770-2525-47f8-af73-8d1f6a391f7d
INFO 10-14 21:17:37 client.py:113] Model loaded: Mixtral-8x7B, 0f083770-2525-47f8-af73-8d1f6a391f7d
DEBUG 10-14 21:17:37 lpllm.py:1207] start decoder qkv layer_attn 20 layer_mlp 19
DEBUG 10-14 21:17:37 lpmodule.py:526] hidden_states.shape torch.Size([10, 1, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:37 lpmodule.py:541] kv_seq_len 1 past_key_value 512
DEBUG 10-14 21:17:37 lpmodule.py:545] inv_freq device cpu
DEBUG 10-14 21:17:37 lpmodule.py:546] kv_seq_len 513 position_ids torch.Size([10, 1]) value tensor([512])
DEBUG 10-14 21:17:37 lpmodule.py:551] query_states device cuda:1
DEBUG 10-14 21:17:37 lpmodule.py:552] key_states device cuda:1
DEBUG 10-14 21:17:37 lpmodule.py:553] value_states device cuda:1
DEBUG 10-14 21:17:37 lpmodule.py:554] position_ids device cpu
DEBUG 10-14 21:17:37 lpmodule.py:555] cos device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:37 lpmodule.py:556] sin device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:37 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:37 lpmodule.py:162] call mlp prepare
DEBUG 10-14 21:17:37 lpmodule.py:163] layer idx 19
DEBUG 10-14 21:17:37 lpmodule.py:123] Tokens per expert: [0, 1, 5, 8, 0, 0, 0, 6]
DEBUG 10-14 21:17:37 lpmodule.py:124] Number of active experts: 4
DEBUG 10-14 21:17:37 lpmodule.py:195] call mlp post
DEBUG 10-14 21:17:37 lpllm.py:2262] GPU2CPU move cost 0.000659 seconds
DEBUG 10-14 21:17:37 lpmodule.py:340] layer_idx 20 attn layer_idx: 20
DEBUG 10-14 21:17:37 lpmodule.py:352] get seq_length 512 in decoder_attn_batch
WARNING 10-14 21:17:37 lpmodule.py:355] generally decoder_attn_batch called with q_len == 1
DEBUG 10-14 21:17:37 lpmodule.py:364] decoder_attn_batch update batch_dim 0-10
DEBUG 10-14 21:17:37 StaticCacheLen.py:84] static cache update layer_idx: 20, start_batch: 0, end_batch: 10
DEBUG 10-14 21:17:37 lpmodule.py:368] update for kv cache 0-10 kv_seq_len 513 in decoder_attn_batch
DEBUG 10-14 21:17:37 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:37 lpmodule.py:374] update past key value cost 0.002570 seconds
DEBUG 10-14 21:17:37 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 513, 128]), value shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:37 lpmodule.py:399] repeat qkv cost 0.000001 seconds
DEBUG 10-14 21:17:37 lpmodule.py:400] q shape torch.Size([10, 32, 1, 128]), k shape torch.Size([10, 8, 513, 128]), v shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:37 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:37 lpllm.py:503] reset update experts
DEBUG 10-14 21:17:37 lpllm.py:1771] update state cost 2.7179718017578125e-05 s
DEBUG 10-14 21:17:37 lpllm.py:1740] restore layer func cost 0.0020651817321777344 s
DEBUG 10-14 21:17:37 lpllm.py:511] restore layer cost 0.0023682117462158203 s
DEBUG 10-14 21:17:37 lpllm.py:512] reset_next_layer_need: layer_attn_idx=20, layer_mlp_idx=20, j_loc=40
DEBUG 10-14 21:17:37 lpllm.py:1280] reset layer cost 0.0025243759155273438 s
DEBUG 10-14 21:17:37 lpllm.py:1281] have reset next layer layer_attn 20 layer_mlp 19 
DEBUG 10-14 21:17:37 lpllm.py:1200] 
DEBUG 10-14 21:17:37 lpllm.py:1200] one decoder loop j: 41 cur_layer_idx: 20
DEBUG 10-14 21:17:37 lpllm.py:1207] start decoder qkv layer_attn 20 layer_mlp 20
DEBUG 10-14 21:17:37 lpmodule.py:526] hidden_states.shape torch.Size([10, 1, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:37 lpmodule.py:541] kv_seq_len 1 past_key_value 512
DEBUG 10-14 21:17:37 lpmodule.py:545] inv_freq device cpu
DEBUG 10-14 21:17:37 lpmodule.py:546] kv_seq_len 513 position_ids torch.Size([10, 1]) value tensor([512])
DEBUG 10-14 21:17:37 lpmodule.py:551] query_states device cuda:1
DEBUG 10-14 21:17:37 lpmodule.py:552] key_states device cuda:1
DEBUG 10-14 21:17:37 lpmodule.py:553] value_states device cuda:1
DEBUG 10-14 21:17:37 lpmodule.py:554] position_ids device cpu
DEBUG 10-14 21:17:37 lpmodule.py:555] cos device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:37 lpmodule.py:556] sin device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:37 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:37 tutils.py:81] single group 0 real attn out cost 0.009787559509277344 s
DEBUG 10-14 21:17:37 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:37 tutils.py:81] single group 1 real attn out cost 0.003893136978149414 s
DEBUG 10-14 21:17:37 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:37 tutils.py:81] single group 2 real attn out cost 0.003911256790161133 s
DEBUG 10-14 21:17:37 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:37 tutils.py:81] single group 3 real attn out cost 0.003058195114135742 s
DEBUG 10-14 21:17:37 tutils.py:88] dot attn help cost 0.026466 seconds
DEBUG 10-14 21:17:37 lpmodule.py:433] dot attn cost 0.026564 seconds
DEBUG 10-14 21:17:37 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.00023984909057617188 s
DEBUG 10-14 21:17:37 lpllm.py:2280] CPU attn cost 0.030217 seconds if batch True
DEBUG 10-14 21:17:37 lpllm.py:2289] deal attn result cost 0.000003 seconds
DEBUG 10-14 21:17:37 lpllm.py:2291] CPU compute cost 0.031276 seconds
DEBUG 10-14 21:17:37 lpllm.py:2309] free cost 0.000184 seconds
DEBUG 10-14 21:17:37 lpmodule.py:162] call mlp prepare
DEBUG 10-14 21:17:37 lpmodule.py:163] layer idx 20
DEBUG 10-14 21:17:37 lpmodule.py:123] Tokens per expert: [6, 1, 8, 0, 3, 1, 0, 1]
DEBUG 10-14 21:17:37 lpmodule.py:124] Number of active experts: 6
DEBUG 10-14 21:17:37 lpmodule.py:195] call mlp post
DEBUG 10-14 21:17:37 lpllm.py:2262] GPU2CPU move cost 0.000431 seconds
DEBUG 10-14 21:17:37 lpmodule.py:340] layer_idx 20 attn layer_idx: 20
DEBUG 10-14 21:17:37 lpmodule.py:352] get seq_length 512 in decoder_attn_batch
WARNING 10-14 21:17:37 lpmodule.py:355] generally decoder_attn_batch called with q_len == 1
DEBUG 10-14 21:17:37 lpmodule.py:364] decoder_attn_batch update batch_dim 10-20
DEBUG 10-14 21:17:37 StaticCacheLen.py:84] static cache update layer_idx: 20, start_batch: 10, end_batch: 20
DEBUG 10-14 21:17:37 StaticCacheLen.py:90] static cache update layer_idx: 20, update seq_length to 513
DEBUG 10-14 21:17:37 lpmodule.py:368] update for kv cache 10-20 kv_seq_len 513 in decoder_attn_batch
DEBUG 10-14 21:17:37 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:37 lpmodule.py:374] update past key value cost 0.002966 seconds
DEBUG 10-14 21:17:37 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 513, 128]), value shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:37 lpmodule.py:399] repeat qkv cost 0.000001 seconds
DEBUG 10-14 21:17:37 lpmodule.py:400] q shape torch.Size([10, 32, 1, 128]), k shape torch.Size([10, 8, 513, 128]), v shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:37 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:37 lpllm.py:498] reset update attn
DEBUG 10-14 21:17:37 lpllm.py:1771] update state cost 3.147125244140625e-05 s
DEBUG 10-14 21:17:37 lpllm.py:1740] restore layer func cost 0.0007617473602294922 s
DEBUG 10-14 21:17:37 lpllm.py:511] restore layer cost 0.001094818115234375 s
DEBUG 10-14 21:17:37 lpllm.py:512] reset_next_layer_need: layer_attn_idx=21, layer_mlp_idx=20, j_loc=41
DEBUG 10-14 21:17:37 lpllm.py:1280] reset layer cost 0.0012326240539550781 s
DEBUG 10-14 21:17:37 lpllm.py:1281] have reset next layer layer_attn 20 layer_mlp 20 
DEBUG 10-14 21:17:37 lpllm.py:1287] j: 41 waiting the layer with layer_idx 21 before wait time 0.0672299861907959 s
INFO 10-14 21:17:37 client.py:117] confirm_model_loaded: Mixtral-8x7B, 0f083770-2525-47f8-af73-8d1f6a391f7d
DEBUG 10-14 21:17:37 tutils.py:81] single group 0 real attn out cost 0.00646662712097168 s
DEBUG 10-14 21:17:37 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:37 tutils.py:81] single group 1 real attn out cost 0.003174304962158203 s
DEBUG 10-14 21:17:37 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:37 tutils.py:81] single group 2 real attn out cost 0.003252744674682617 s
DEBUG 10-14 21:17:37 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:37 tutils.py:81] single group 3 real attn out cost 0.003795146942138672 s
DEBUG 10-14 21:17:37 tutils.py:88] dot attn help cost 0.023368 seconds
DEBUG 10-14 21:17:37 lpmodule.py:433] dot attn cost 0.023567 seconds
DEBUG 10-14 21:17:37 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.000621795654296875 s
DEBUG 10-14 21:17:37 lpllm.py:2280] CPU attn cost 0.027908 seconds if batch True
DEBUG 10-14 21:17:37 lpllm.py:2289] deal attn result cost 0.000005 seconds
DEBUG 10-14 21:17:37 lpllm.py:2291] CPU compute cost 0.028780 seconds
DEBUG 10-14 21:17:37 lpllm.py:2309] free cost 0.000175 seconds
INFO 10-14 21:17:37 client.py:125] Model loaded
DEBUG 10-14 21:17:37 lpllm.py:1291] j: load cost 0.30025243759155273 s waiting cost 0.23298859596252441 s
DEBUG 10-14 21:17:37 lpllm.py:1200] 
DEBUG 10-14 21:17:37 lpllm.py:1200] one decoder loop j: 42 cur_layer_idx: 21
DEBUG 10-14 21:17:37 lpllm.py:1205] start load next layer cur_layer_idx: 22
DEBUG 10-14 21:17:37 lpllm.py:1801] cuda memory handles: {1: <capsule object NULL at 0x7fdf3c6d9ec0>} {1: b'\xe0\xaa29\x00\x00\x00\x00\xe8&\x1d\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\x86\x00\x00\x00\xff\x00\x00.\x00\x00\x00\x00\x00\x00\x00)\xd8\xd5\xc1\xa0\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-14 21:17:37 client.py:72] load_into_gpu: Mixtral-8x7B, 997b3837-aaaa-4b15-9d5a-99ab9dd6c056
INFO 10-14 21:17:37 client.py:113] Model loaded: Mixtral-8x7B, 997b3837-aaaa-4b15-9d5a-99ab9dd6c056
DEBUG 10-14 21:17:37 lpllm.py:1207] start decoder qkv layer_attn 21 layer_mlp 20
DEBUG 10-14 21:17:37 lpmodule.py:526] hidden_states.shape torch.Size([10, 1, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:37 lpmodule.py:541] kv_seq_len 1 past_key_value 512
DEBUG 10-14 21:17:37 lpmodule.py:545] inv_freq device cpu
DEBUG 10-14 21:17:37 lpmodule.py:546] kv_seq_len 513 position_ids torch.Size([10, 1]) value tensor([512])
DEBUG 10-14 21:17:37 lpmodule.py:551] query_states device cuda:1
DEBUG 10-14 21:17:37 lpmodule.py:552] key_states device cuda:1
DEBUG 10-14 21:17:37 lpmodule.py:553] value_states device cuda:1
DEBUG 10-14 21:17:37 lpmodule.py:554] position_ids device cpu
DEBUG 10-14 21:17:37 lpmodule.py:555] cos device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:37 lpmodule.py:556] sin device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:37 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:37 lpmodule.py:162] call mlp prepare
DEBUG 10-14 21:17:37 lpmodule.py:163] layer idx 20
DEBUG 10-14 21:17:37 lpmodule.py:123] Tokens per expert: [1, 0, 1, 0, 9, 9, 0, 0]
DEBUG 10-14 21:17:37 lpmodule.py:124] Number of active experts: 4
DEBUG 10-14 21:17:37 lpmodule.py:195] call mlp post
DEBUG 10-14 21:17:37 lpllm.py:2262] GPU2CPU move cost 0.000368 seconds
DEBUG 10-14 21:17:37 lpmodule.py:340] layer_idx 21 attn layer_idx: 21
DEBUG 10-14 21:17:37 lpmodule.py:352] get seq_length 512 in decoder_attn_batch
WARNING 10-14 21:17:37 lpmodule.py:355] generally decoder_attn_batch called with q_len == 1
DEBUG 10-14 21:17:37 lpmodule.py:364] decoder_attn_batch update batch_dim 0-10
DEBUG 10-14 21:17:37 StaticCacheLen.py:84] static cache update layer_idx: 21, start_batch: 0, end_batch: 10
DEBUG 10-14 21:17:37 lpmodule.py:368] update for kv cache 0-10 kv_seq_len 513 in decoder_attn_batch
DEBUG 10-14 21:17:37 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:37 lpmodule.py:374] update past key value cost 0.002681 seconds
DEBUG 10-14 21:17:37 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 513, 128]), value shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:37 lpmodule.py:399] repeat qkv cost 0.000001 seconds
DEBUG 10-14 21:17:37 lpmodule.py:400] q shape torch.Size([10, 32, 1, 128]), k shape torch.Size([10, 8, 513, 128]), v shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:37 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:37 lpllm.py:503] reset update experts
DEBUG 10-14 21:17:37 lpllm.py:1771] update state cost 2.5033950805664062e-05 s
DEBUG 10-14 21:17:37 lpllm.py:1740] restore layer func cost 0.0015087127685546875 s
DEBUG 10-14 21:17:37 lpllm.py:511] restore layer cost 0.0017848014831542969 s
DEBUG 10-14 21:17:37 lpllm.py:512] reset_next_layer_need: layer_attn_idx=21, layer_mlp_idx=21, j_loc=42
DEBUG 10-14 21:17:37 lpllm.py:1280] reset layer cost 0.0019032955169677734 s
DEBUG 10-14 21:17:37 lpllm.py:1281] have reset next layer layer_attn 21 layer_mlp 20 
DEBUG 10-14 21:17:37 lpllm.py:1200] 
DEBUG 10-14 21:17:37 lpllm.py:1200] one decoder loop j: 43 cur_layer_idx: 21
DEBUG 10-14 21:17:37 lpllm.py:1207] start decoder qkv layer_attn 21 layer_mlp 21
DEBUG 10-14 21:17:37 lpmodule.py:526] hidden_states.shape torch.Size([10, 1, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:37 tutils.py:81] single group 0 real attn out cost 0.006256580352783203 s
DEBUG 10-14 21:17:37 lpmodule.py:541] kv_seq_len 1 past_key_value 512
DEBUG 10-14 21:17:37 lpmodule.py:545] inv_freq device cpu
DEBUG 10-14 21:17:37 lpmodule.py:546] kv_seq_len 513 position_ids torch.Size([10, 1]) value tensor([512])
DEBUG 10-14 21:17:37 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:37 lpmodule.py:551] query_states device cuda:1
DEBUG 10-14 21:17:37 lpmodule.py:552] key_states device cuda:1
DEBUG 10-14 21:17:37 lpmodule.py:553] value_states device cuda:1
DEBUG 10-14 21:17:37 lpmodule.py:554] position_ids device cpu
DEBUG 10-14 21:17:37 lpmodule.py:555] cos device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:37 lpmodule.py:556] sin device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:37 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:37 tutils.py:81] single group 1 real attn out cost 0.003645658493041992 s
DEBUG 10-14 21:17:37 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:37 tutils.py:81] single group 2 real attn out cost 0.003560781478881836 s
DEBUG 10-14 21:17:37 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:37 tutils.py:81] single group 3 real attn out cost 0.0035495758056640625 s
DEBUG 10-14 21:17:37 tutils.py:88] dot attn help cost 0.026827 seconds
DEBUG 10-14 21:17:37 lpmodule.py:433] dot attn cost 0.026987 seconds
DEBUG 10-14 21:17:37 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.0011687278747558594 s
DEBUG 10-14 21:17:37 lpllm.py:2280] CPU attn cost 0.031791 seconds if batch True
DEBUG 10-14 21:17:37 lpllm.py:2289] deal attn result cost 0.000003 seconds
DEBUG 10-14 21:17:37 lpllm.py:2291] CPU compute cost 0.032614 seconds
DEBUG 10-14 21:17:37 lpllm.py:2309] free cost 0.000188 seconds
DEBUG 10-14 21:17:37 lpmodule.py:162] call mlp prepare
DEBUG 10-14 21:17:37 lpmodule.py:163] layer idx 21
DEBUG 10-14 21:17:37 lpmodule.py:123] Tokens per expert: [0, 0, 4, 0, 5, 5, 5, 1]
DEBUG 10-14 21:17:37 lpmodule.py:124] Number of active experts: 5
DEBUG 10-14 21:17:37 lpmodule.py:195] call mlp post
DEBUG 10-14 21:17:37 lpllm.py:2262] GPU2CPU move cost 0.000514 seconds
DEBUG 10-14 21:17:37 lpmodule.py:340] layer_idx 21 attn layer_idx: 21
DEBUG 10-14 21:17:37 lpmodule.py:352] get seq_length 512 in decoder_attn_batch
WARNING 10-14 21:17:37 lpmodule.py:355] generally decoder_attn_batch called with q_len == 1
DEBUG 10-14 21:17:37 lpmodule.py:364] decoder_attn_batch update batch_dim 10-20
DEBUG 10-14 21:17:37 StaticCacheLen.py:84] static cache update layer_idx: 21, start_batch: 10, end_batch: 20
DEBUG 10-14 21:17:37 StaticCacheLen.py:90] static cache update layer_idx: 21, update seq_length to 513
DEBUG 10-14 21:17:37 lpmodule.py:368] update for kv cache 10-20 kv_seq_len 513 in decoder_attn_batch
DEBUG 10-14 21:17:37 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:37 lpmodule.py:374] update past key value cost 0.002726 seconds
DEBUG 10-14 21:17:37 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 513, 128]), value shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:37 lpmodule.py:399] repeat qkv cost 0.000001 seconds
DEBUG 10-14 21:17:37 lpmodule.py:400] q shape torch.Size([10, 32, 1, 128]), k shape torch.Size([10, 8, 513, 128]), v shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:37 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:37 lpllm.py:498] reset update attn
DEBUG 10-14 21:17:37 lpllm.py:1771] update state cost 3.9577484130859375e-05 s
DEBUG 10-14 21:17:37 lpllm.py:1740] restore layer func cost 0.0009415149688720703 s
DEBUG 10-14 21:17:37 lpllm.py:511] restore layer cost 0.0012857913970947266 s
DEBUG 10-14 21:17:37 lpllm.py:512] reset_next_layer_need: layer_attn_idx=22, layer_mlp_idx=21, j_loc=43
DEBUG 10-14 21:17:37 lpllm.py:1280] reset layer cost 0.0014367103576660156 s
DEBUG 10-14 21:17:37 lpllm.py:1281] have reset next layer layer_attn 21 layer_mlp 21 
DEBUG 10-14 21:17:37 lpllm.py:1287] j: 43 waiting the layer with layer_idx 22 before wait time 0.06951022148132324 s
INFO 10-14 21:17:37 client.py:117] confirm_model_loaded: Mixtral-8x7B, 997b3837-aaaa-4b15-9d5a-99ab9dd6c056
DEBUG 10-14 21:17:37 tutils.py:81] single group 0 real attn out cost 0.004951000213623047 s
DEBUG 10-14 21:17:37 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:37 tutils.py:81] single group 1 real attn out cost 0.003397703170776367 s
DEBUG 10-14 21:17:37 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:37 tutils.py:81] single group 2 real attn out cost 0.0025577545166015625 s
DEBUG 10-14 21:17:37 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:37 tutils.py:81] single group 3 real attn out cost 0.003021240234375 s
DEBUG 10-14 21:17:37 tutils.py:88] dot attn help cost 0.024391 seconds
DEBUG 10-14 21:17:37 lpmodule.py:433] dot attn cost 0.024512 seconds
DEBUG 10-14 21:17:37 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.0004012584686279297 s
DEBUG 10-14 21:17:37 lpllm.py:2280] CPU attn cost 0.028613 seconds if batch True
DEBUG 10-14 21:17:37 lpllm.py:2289] deal attn result cost 0.000004 seconds
DEBUG 10-14 21:17:37 lpllm.py:2291] CPU compute cost 0.029498 seconds
DEBUG 10-14 21:17:37 lpllm.py:2309] free cost 0.000167 seconds
INFO 10-14 21:17:37 client.py:125] Model loaded
DEBUG 10-14 21:17:37 lpllm.py:1291] j: load cost 0.30149102210998535 s waiting cost 0.23195123672485352 s
DEBUG 10-14 21:17:37 lpllm.py:1200] 
DEBUG 10-14 21:17:37 lpllm.py:1200] one decoder loop j: 44 cur_layer_idx: 22
DEBUG 10-14 21:17:37 lpllm.py:1205] start load next layer cur_layer_idx: 23
DEBUG 10-14 21:17:37 lpllm.py:1801] cuda memory handles: {1: <capsule object NULL at 0x7fdf3c6d9f80>} {1: b'\xe0\xaa29\x00\x00\x00\x00\xe8&\x1d\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\x86\x00\x00\x00\xff\x00\x00/\x00\x00\x00\x00\x00\x00\x00)\xd8\xd5\xc1\xa2\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-14 21:17:37 client.py:72] load_into_gpu: Mixtral-8x7B, 185b7706-39b6-45bd-82b9-81aea0101a3d
INFO 10-14 21:17:37 client.py:113] Model loaded: Mixtral-8x7B, 185b7706-39b6-45bd-82b9-81aea0101a3d
DEBUG 10-14 21:17:37 lpllm.py:1207] start decoder qkv layer_attn 22 layer_mlp 21
DEBUG 10-14 21:17:37 lpmodule.py:526] hidden_states.shape torch.Size([10, 1, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:37 lpmodule.py:541] kv_seq_len 1 past_key_value 512
DEBUG 10-14 21:17:37 lpmodule.py:545] inv_freq device cpu
DEBUG 10-14 21:17:37 lpmodule.py:546] kv_seq_len 513 position_ids torch.Size([10, 1]) value tensor([512])
DEBUG 10-14 21:17:37 lpmodule.py:551] query_states device cuda:1
DEBUG 10-14 21:17:37 lpmodule.py:552] key_states device cuda:1
DEBUG 10-14 21:17:37 lpmodule.py:553] value_states device cuda:1
DEBUG 10-14 21:17:37 lpmodule.py:554] position_ids device cpu
DEBUG 10-14 21:17:37 lpmodule.py:555] cos device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:37 lpmodule.py:556] sin device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:37 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:37 lpmodule.py:162] call mlp prepare
DEBUG 10-14 21:17:37 lpmodule.py:163] layer idx 21
DEBUG 10-14 21:17:37 lpmodule.py:123] Tokens per expert: [0, 1, 9, 0, 1, 6, 1, 2]
DEBUG 10-14 21:17:37 lpmodule.py:124] Number of active experts: 6
DEBUG 10-14 21:17:37 lpmodule.py:195] call mlp post
DEBUG 10-14 21:17:37 lpllm.py:2262] GPU2CPU move cost 0.000455 seconds
DEBUG 10-14 21:17:37 lpmodule.py:340] layer_idx 22 attn layer_idx: 22
DEBUG 10-14 21:17:37 lpmodule.py:352] get seq_length 512 in decoder_attn_batch
WARNING 10-14 21:17:37 lpmodule.py:355] generally decoder_attn_batch called with q_len == 1
DEBUG 10-14 21:17:37 lpmodule.py:364] decoder_attn_batch update batch_dim 0-10
DEBUG 10-14 21:17:37 StaticCacheLen.py:84] static cache update layer_idx: 22, start_batch: 0, end_batch: 10
DEBUG 10-14 21:17:37 lpmodule.py:368] update for kv cache 0-10 kv_seq_len 513 in decoder_attn_batch
DEBUG 10-14 21:17:37 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:37 lpmodule.py:374] update past key value cost 0.002631 seconds
DEBUG 10-14 21:17:37 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 513, 128]), value shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:37 lpmodule.py:399] repeat qkv cost 0.000001 seconds
DEBUG 10-14 21:17:37 lpmodule.py:400] q shape torch.Size([10, 32, 1, 128]), k shape torch.Size([10, 8, 513, 128]), v shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:37 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:37 lpllm.py:503] reset update experts
DEBUG 10-14 21:17:37 lpllm.py:1771] update state cost 3.0517578125e-05 s
DEBUG 10-14 21:17:37 lpllm.py:1740] restore layer func cost 0.0014998912811279297 s
DEBUG 10-14 21:17:37 lpllm.py:511] restore layer cost 0.0018177032470703125 s
DEBUG 10-14 21:17:37 lpllm.py:512] reset_next_layer_need: layer_attn_idx=22, layer_mlp_idx=22, j_loc=44
DEBUG 10-14 21:17:37 lpllm.py:1280] reset layer cost 0.0019478797912597656 s
DEBUG 10-14 21:17:37 lpllm.py:1281] have reset next layer layer_attn 22 layer_mlp 21 
DEBUG 10-14 21:17:37 lpllm.py:1200] 
DEBUG 10-14 21:17:37 lpllm.py:1200] one decoder loop j: 45 cur_layer_idx: 22
DEBUG 10-14 21:17:37 lpllm.py:1207] start decoder qkv layer_attn 22 layer_mlp 22
DEBUG 10-14 21:17:37 lpmodule.py:526] hidden_states.shape torch.Size([10, 1, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:37 tutils.py:81] single group 0 real attn out cost 0.005885601043701172 s
DEBUG 10-14 21:17:37 lpmodule.py:541] kv_seq_len 1 past_key_value 512
DEBUG 10-14 21:17:37 lpmodule.py:545] inv_freq device cpu
DEBUG 10-14 21:17:37 lpmodule.py:546] kv_seq_len 513 position_ids torch.Size([10, 1]) value tensor([512])
DEBUG 10-14 21:17:37 lpmodule.py:551] query_states device cuda:1
DEBUG 10-14 21:17:37 lpmodule.py:552] key_states device cuda:1
DEBUG 10-14 21:17:37 lpmodule.py:553] value_states device cuda:1
DEBUG 10-14 21:17:37 lpmodule.py:554] position_ids device cpu
DEBUG 10-14 21:17:37 lpmodule.py:555] cos device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:37 lpmodule.py:556] sin device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:37 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:37 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:37 tutils.py:81] single group 1 real attn out cost 0.003106355667114258 s
DEBUG 10-14 21:17:37 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:37 tutils.py:81] single group 2 real attn out cost 0.0031080245971679688 s
DEBUG 10-14 21:17:37 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:37 tutils.py:81] single group 3 real attn out cost 0.003607034683227539 s
DEBUG 10-14 21:17:37 tutils.py:88] dot attn help cost 0.022133 seconds
DEBUG 10-14 21:17:37 lpmodule.py:433] dot attn cost 0.022274 seconds
DEBUG 10-14 21:17:37 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.0003674030303955078 s
DEBUG 10-14 21:17:37 lpllm.py:2280] CPU attn cost 0.026127 seconds if batch True
DEBUG 10-14 21:17:37 lpllm.py:2289] deal attn result cost 0.000004 seconds
DEBUG 10-14 21:17:37 lpllm.py:2291] CPU compute cost 0.026988 seconds
DEBUG 10-14 21:17:37 lpllm.py:2309] free cost 0.000188 seconds
DEBUG 10-14 21:17:37 lpmodule.py:162] call mlp prepare
DEBUG 10-14 21:17:37 lpmodule.py:163] layer idx 22
DEBUG 10-14 21:17:37 lpmodule.py:123] Tokens per expert: [4, 5, 2, 0, 0, 0, 7, 2]
DEBUG 10-14 21:17:37 lpmodule.py:124] Number of active experts: 5
DEBUG 10-14 21:17:37 lpmodule.py:195] call mlp post
DEBUG 10-14 21:17:37 lpllm.py:2262] GPU2CPU move cost 0.000304 seconds
DEBUG 10-14 21:17:37 lpmodule.py:340] layer_idx 22 attn layer_idx: 22
DEBUG 10-14 21:17:37 lpmodule.py:352] get seq_length 512 in decoder_attn_batch
WARNING 10-14 21:17:37 lpmodule.py:355] generally decoder_attn_batch called with q_len == 1
DEBUG 10-14 21:17:37 lpmodule.py:364] decoder_attn_batch update batch_dim 10-20
DEBUG 10-14 21:17:37 StaticCacheLen.py:84] static cache update layer_idx: 22, start_batch: 10, end_batch: 20
DEBUG 10-14 21:17:37 StaticCacheLen.py:90] static cache update layer_idx: 22, update seq_length to 513
DEBUG 10-14 21:17:37 lpmodule.py:368] update for kv cache 10-20 kv_seq_len 513 in decoder_attn_batch
DEBUG 10-14 21:17:37 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:37 lpmodule.py:374] update past key value cost 0.002550 seconds
DEBUG 10-14 21:17:37 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 513, 128]), value shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:37 lpmodule.py:399] repeat qkv cost 0.000002 seconds
DEBUG 10-14 21:17:37 lpmodule.py:400] q shape torch.Size([10, 32, 1, 128]), k shape torch.Size([10, 8, 513, 128]), v shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:37 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:37 lpllm.py:498] reset update attn
DEBUG 10-14 21:17:37 lpllm.py:1771] update state cost 3.886222839355469e-05 s
DEBUG 10-14 21:17:37 lpllm.py:1740] restore layer func cost 0.0008380413055419922 s
DEBUG 10-14 21:17:37 lpllm.py:511] restore layer cost 0.0011816024780273438 s
DEBUG 10-14 21:17:37 lpllm.py:512] reset_next_layer_need: layer_attn_idx=23, layer_mlp_idx=22, j_loc=45
DEBUG 10-14 21:17:37 lpllm.py:1280] reset layer cost 0.0013599395751953125 s
DEBUG 10-14 21:17:37 lpllm.py:1281] have reset next layer layer_attn 22 layer_mlp 22 
DEBUG 10-14 21:17:37 lpllm.py:1287] j: 45 waiting the layer with layer_idx 23 before wait time 0.06346607208251953 s
INFO 10-14 21:17:37 client.py:117] confirm_model_loaded: Mixtral-8x7B, 185b7706-39b6-45bd-82b9-81aea0101a3d
DEBUG 10-14 21:17:37 tutils.py:81] single group 0 real attn out cost 0.005747795104980469 s
DEBUG 10-14 21:17:37 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:37 tutils.py:81] single group 1 real attn out cost 0.002864360809326172 s
DEBUG 10-14 21:17:37 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:37 tutils.py:81] single group 2 real attn out cost 0.003413677215576172 s
DEBUG 10-14 21:17:37 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:37 tutils.py:81] single group 3 real attn out cost 0.002955913543701172 s
DEBUG 10-14 21:17:37 tutils.py:88] dot attn help cost 0.021024 seconds
DEBUG 10-14 21:17:37 lpmodule.py:433] dot attn cost 0.021121 seconds
DEBUG 10-14 21:17:37 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.0008573532104492188 s
DEBUG 10-14 21:17:37 lpllm.py:2280] CPU attn cost 0.025195 seconds if batch True
DEBUG 10-14 21:17:37 lpllm.py:2289] deal attn result cost 0.000003 seconds
DEBUG 10-14 21:17:37 lpllm.py:2291] CPU compute cost 0.025843 seconds
DEBUG 10-14 21:17:37 lpllm.py:2309] free cost 0.000144 seconds
INFO 10-14 21:17:38 client.py:125] Model loaded
DEBUG 10-14 21:17:38 lpllm.py:1291] j: load cost 0.29538393020629883 s waiting cost 0.23188257217407227 s
DEBUG 10-14 21:17:38 lpllm.py:1200] 
DEBUG 10-14 21:17:38 lpllm.py:1200] one decoder loop j: 46 cur_layer_idx: 23
DEBUG 10-14 21:17:38 lpllm.py:1205] start load next layer cur_layer_idx: 24
DEBUG 10-14 21:17:38 lpllm.py:1801] cuda memory handles: {1: <capsule object NULL at 0x7fdf3c6da010>} {1: b'\xe0\xaa29\x00\x00\x00\x00\xe8&\x1d\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\x86\x00\x00\x00\xff\x00\x000\x00\x00\x00\x00\x00\x00\x00)\xd8\xd5\xc1\xa4\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-14 21:17:38 client.py:72] load_into_gpu: Mixtral-8x7B, 5a36bbfd-c257-437c-a55b-5146f5bde70c
INFO 10-14 21:17:38 client.py:113] Model loaded: Mixtral-8x7B, 5a36bbfd-c257-437c-a55b-5146f5bde70c
DEBUG 10-14 21:17:38 lpllm.py:1207] start decoder qkv layer_attn 23 layer_mlp 22
DEBUG 10-14 21:17:38 lpmodule.py:526] hidden_states.shape torch.Size([10, 1, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:38 lpmodule.py:541] kv_seq_len 1 past_key_value 512
DEBUG 10-14 21:17:38 lpmodule.py:545] inv_freq device cpu
DEBUG 10-14 21:17:38 lpmodule.py:546] kv_seq_len 513 position_ids torch.Size([10, 1]) value tensor([512])
DEBUG 10-14 21:17:38 lpmodule.py:551] query_states device cuda:1
DEBUG 10-14 21:17:38 lpmodule.py:552] key_states device cuda:1
DEBUG 10-14 21:17:38 lpmodule.py:553] value_states device cuda:1
DEBUG 10-14 21:17:38 lpmodule.py:554] position_ids device cpu
DEBUG 10-14 21:17:38 lpmodule.py:555] cos device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:38 lpmodule.py:556] sin device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:38 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:38 lpmodule.py:162] call mlp prepare
DEBUG 10-14 21:17:38 lpmodule.py:163] layer idx 22
DEBUG 10-14 21:17:38 lpmodule.py:123] Tokens per expert: [1, 6, 9, 0, 0, 0, 2, 2]
DEBUG 10-14 21:17:38 lpmodule.py:124] Number of active experts: 5
DEBUG 10-14 21:17:38 lpmodule.py:195] call mlp post
DEBUG 10-14 21:17:38 lpllm.py:2262] GPU2CPU move cost 0.000418 seconds
DEBUG 10-14 21:17:38 lpmodule.py:340] layer_idx 23 attn layer_idx: 23
DEBUG 10-14 21:17:38 lpmodule.py:352] get seq_length 512 in decoder_attn_batch
WARNING 10-14 21:17:38 lpmodule.py:355] generally decoder_attn_batch called with q_len == 1
DEBUG 10-14 21:17:38 lpmodule.py:364] decoder_attn_batch update batch_dim 0-10
DEBUG 10-14 21:17:38 StaticCacheLen.py:84] static cache update layer_idx: 23, start_batch: 0, end_batch: 10
DEBUG 10-14 21:17:38 lpmodule.py:368] update for kv cache 0-10 kv_seq_len 513 in decoder_attn_batch
DEBUG 10-14 21:17:38 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:38 lpmodule.py:374] update past key value cost 0.002976 seconds
DEBUG 10-14 21:17:38 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 513, 128]), value shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:38 lpmodule.py:399] repeat qkv cost 0.000001 seconds
DEBUG 10-14 21:17:38 lpmodule.py:400] q shape torch.Size([10, 32, 1, 128]), k shape torch.Size([10, 8, 513, 128]), v shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:38 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:38 lpllm.py:503] reset update experts
DEBUG 10-14 21:17:38 lpllm.py:1771] update state cost 2.4557113647460938e-05 s
DEBUG 10-14 21:17:38 lpllm.py:1740] restore layer func cost 0.0014872550964355469 s
DEBUG 10-14 21:17:38 lpllm.py:511] restore layer cost 0.0017452239990234375 s
DEBUG 10-14 21:17:38 lpllm.py:512] reset_next_layer_need: layer_attn_idx=23, layer_mlp_idx=23, j_loc=46
DEBUG 10-14 21:17:38 lpllm.py:1280] reset layer cost 0.0018644332885742188 s
DEBUG 10-14 21:17:38 lpllm.py:1281] have reset next layer layer_attn 23 layer_mlp 22 
DEBUG 10-14 21:17:38 lpllm.py:1200] 
DEBUG 10-14 21:17:38 lpllm.py:1200] one decoder loop j: 47 cur_layer_idx: 23
DEBUG 10-14 21:17:38 lpllm.py:1207] start decoder qkv layer_attn 23 layer_mlp 23
DEBUG 10-14 21:17:38 lpmodule.py:526] hidden_states.shape torch.Size([10, 1, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:38 tutils.py:81] single group 0 real attn out cost 0.005470752716064453 s
DEBUG 10-14 21:17:38 lpmodule.py:541] kv_seq_len 1 past_key_value 512
DEBUG 10-14 21:17:38 lpmodule.py:545] inv_freq device cpu
DEBUG 10-14 21:17:38 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:38 lpmodule.py:546] kv_seq_len 513 position_ids torch.Size([10, 1]) value tensor([512])
DEBUG 10-14 21:17:38 lpmodule.py:551] query_states device cuda:1
DEBUG 10-14 21:17:38 lpmodule.py:552] key_states device cuda:1
DEBUG 10-14 21:17:38 lpmodule.py:553] value_states device cuda:1
DEBUG 10-14 21:17:38 lpmodule.py:554] position_ids device cpu
DEBUG 10-14 21:17:38 lpmodule.py:555] cos device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:38 lpmodule.py:556] sin device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:38 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:38 tutils.py:81] single group 1 real attn out cost 0.0035309791564941406 s
DEBUG 10-14 21:17:38 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:38 tutils.py:81] single group 2 real attn out cost 0.003379344940185547 s
DEBUG 10-14 21:17:38 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:38 tutils.py:81] single group 3 real attn out cost 0.00356292724609375 s
DEBUG 10-14 21:17:38 tutils.py:88] dot attn help cost 0.022541 seconds
DEBUG 10-14 21:17:38 lpmodule.py:433] dot attn cost 0.022679 seconds
DEBUG 10-14 21:17:38 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.0003559589385986328 s
DEBUG 10-14 21:17:38 lpllm.py:2280] CPU attn cost 0.026855 seconds if batch True
DEBUG 10-14 21:17:38 lpllm.py:2289] deal attn result cost 0.000004 seconds
DEBUG 10-14 21:17:38 lpllm.py:2291] CPU compute cost 0.027703 seconds
DEBUG 10-14 21:17:38 lpllm.py:2309] free cost 0.000188 seconds
DEBUG 10-14 21:17:38 lpmodule.py:162] call mlp prepare
DEBUG 10-14 21:17:38 lpmodule.py:163] layer idx 23
DEBUG 10-14 21:17:38 lpmodule.py:123] Tokens per expert: [4, 1, 0, 9, 0, 0, 5, 1]
DEBUG 10-14 21:17:38 lpmodule.py:124] Number of active experts: 5
DEBUG 10-14 21:17:38 lpmodule.py:195] call mlp post
DEBUG 10-14 21:17:38 lpllm.py:2262] GPU2CPU move cost 0.000457 seconds
DEBUG 10-14 21:17:38 lpmodule.py:340] layer_idx 23 attn layer_idx: 23
DEBUG 10-14 21:17:38 lpmodule.py:352] get seq_length 512 in decoder_attn_batch
WARNING 10-14 21:17:38 lpmodule.py:355] generally decoder_attn_batch called with q_len == 1
DEBUG 10-14 21:17:38 lpmodule.py:364] decoder_attn_batch update batch_dim 10-20
DEBUG 10-14 21:17:38 StaticCacheLen.py:84] static cache update layer_idx: 23, start_batch: 10, end_batch: 20
DEBUG 10-14 21:17:38 StaticCacheLen.py:90] static cache update layer_idx: 23, update seq_length to 513
DEBUG 10-14 21:17:38 lpmodule.py:368] update for kv cache 10-20 kv_seq_len 513 in decoder_attn_batch
DEBUG 10-14 21:17:38 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:38 lpmodule.py:374] update past key value cost 0.002595 seconds
DEBUG 10-14 21:17:38 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 513, 128]), value shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:38 lpmodule.py:399] repeat qkv cost 0.000001 seconds
DEBUG 10-14 21:17:38 lpmodule.py:400] q shape torch.Size([10, 32, 1, 128]), k shape torch.Size([10, 8, 513, 128]), v shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:38 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:38 lpllm.py:498] reset update attn
DEBUG 10-14 21:17:38 tutils.py:81] single group 0 real attn out cost 0.005765676498413086 s
DEBUG 10-14 21:17:38 lpllm.py:1771] update state cost 3.2901763916015625e-05 s
DEBUG 10-14 21:17:38 lpllm.py:1740] restore layer func cost 0.0007517337799072266 s
DEBUG 10-14 21:17:38 lpllm.py:511] restore layer cost 0.0013456344604492188 s
DEBUG 10-14 21:17:38 lpllm.py:512] reset_next_layer_need: layer_attn_idx=24, layer_mlp_idx=23, j_loc=47
DEBUG 10-14 21:17:38 lpllm.py:1280] reset layer cost 0.001493215560913086 s
DEBUG 10-14 21:17:38 lpllm.py:1281] have reset next layer layer_attn 23 layer_mlp 23 
DEBUG 10-14 21:17:38 lpllm.py:1287] j: 47 waiting the layer with layer_idx 24 before wait time 0.06580615043640137 s
INFO 10-14 21:17:38 client.py:117] confirm_model_loaded: Mixtral-8x7B, 5a36bbfd-c257-437c-a55b-5146f5bde70c
DEBUG 10-14 21:17:38 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:38 tutils.py:81] single group 1 real attn out cost 0.0054168701171875 s
DEBUG 10-14 21:17:38 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:38 tutils.py:81] single group 2 real attn out cost 0.0032024383544921875 s
DEBUG 10-14 21:17:38 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:38 tutils.py:81] single group 3 real attn out cost 0.002759695053100586 s
DEBUG 10-14 21:17:38 tutils.py:88] dot attn help cost 0.023486 seconds
DEBUG 10-14 21:17:38 lpmodule.py:433] dot attn cost 0.023576 seconds
DEBUG 10-14 21:17:38 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.001146554946899414 s
DEBUG 10-14 21:17:38 lpllm.py:2280] CPU attn cost 0.027900 seconds if batch True
DEBUG 10-14 21:17:38 lpllm.py:2289] deal attn result cost 0.000003 seconds
DEBUG 10-14 21:17:38 lpllm.py:2291] CPU compute cost 0.028683 seconds
DEBUG 10-14 21:17:38 lpllm.py:2309] free cost 0.000150 seconds
INFO 10-14 21:17:38 client.py:125] Model loaded
DEBUG 10-14 21:17:38 lpllm.py:1291] j: load cost 0.30501246452331543 s waiting cost 0.23917651176452637 s
DEBUG 10-14 21:17:38 lpllm.py:1200] 
DEBUG 10-14 21:17:38 lpllm.py:1200] one decoder loop j: 48 cur_layer_idx: 24
DEBUG 10-14 21:17:38 lpllm.py:1205] start load next layer cur_layer_idx: 25
DEBUG 10-14 21:17:38 lpllm.py:1801] cuda memory handles: {1: <capsule object NULL at 0x7fdf3c6d9ec0>} {1: b'\xe0\xaa29\x00\x00\x00\x00\xe8&\x1d\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\x86\x00\x00\x00\xff\x00\x00.\x00\x00\x00\x00\x00\x00\x00)\xd8\xd5\xc1\xa0\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-14 21:17:38 client.py:72] load_into_gpu: Mixtral-8x7B, 67dc30b8-9a16-4a90-931a-837e8421cb0b
INFO 10-14 21:17:38 client.py:113] Model loaded: Mixtral-8x7B, 67dc30b8-9a16-4a90-931a-837e8421cb0b
DEBUG 10-14 21:17:38 lpllm.py:1207] start decoder qkv layer_attn 24 layer_mlp 23
DEBUG 10-14 21:17:38 lpmodule.py:526] hidden_states.shape torch.Size([10, 1, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:38 lpmodule.py:541] kv_seq_len 1 past_key_value 512
DEBUG 10-14 21:17:38 lpmodule.py:545] inv_freq device cpu
DEBUG 10-14 21:17:38 lpmodule.py:546] kv_seq_len 513 position_ids torch.Size([10, 1]) value tensor([512])
DEBUG 10-14 21:17:38 lpmodule.py:551] query_states device cuda:1
DEBUG 10-14 21:17:38 lpmodule.py:552] key_states device cuda:1
DEBUG 10-14 21:17:38 lpmodule.py:553] value_states device cuda:1
DEBUG 10-14 21:17:38 lpmodule.py:554] position_ids device cpu
DEBUG 10-14 21:17:38 lpmodule.py:555] cos device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:38 lpmodule.py:556] sin device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:38 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:38 lpmodule.py:162] call mlp prepare
DEBUG 10-14 21:17:38 lpmodule.py:163] layer idx 23
DEBUG 10-14 21:17:38 lpmodule.py:123] Tokens per expert: [1, 0, 9, 0, 3, 0, 6, 1]
DEBUG 10-14 21:17:38 lpmodule.py:124] Number of active experts: 5
DEBUG 10-14 21:17:38 lpmodule.py:195] call mlp post
DEBUG 10-14 21:17:38 lpllm.py:2262] GPU2CPU move cost 0.000666 seconds
DEBUG 10-14 21:17:38 lpmodule.py:340] layer_idx 24 attn layer_idx: 24
DEBUG 10-14 21:17:38 lpmodule.py:352] get seq_length 512 in decoder_attn_batch
WARNING 10-14 21:17:38 lpmodule.py:355] generally decoder_attn_batch called with q_len == 1
DEBUG 10-14 21:17:38 lpmodule.py:364] decoder_attn_batch update batch_dim 0-10
DEBUG 10-14 21:17:38 StaticCacheLen.py:84] static cache update layer_idx: 24, start_batch: 0, end_batch: 10
DEBUG 10-14 21:17:38 lpmodule.py:368] update for kv cache 0-10 kv_seq_len 513 in decoder_attn_batch
DEBUG 10-14 21:17:38 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:38 lpmodule.py:374] update past key value cost 0.002794 seconds
DEBUG 10-14 21:17:38 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 513, 128]), value shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:38 lpmodule.py:399] repeat qkv cost 0.000001 seconds
DEBUG 10-14 21:17:38 lpmodule.py:400] q shape torch.Size([10, 32, 1, 128]), k shape torch.Size([10, 8, 513, 128]), v shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:38 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:38 lpllm.py:503] reset update experts
DEBUG 10-14 21:17:38 lpllm.py:1771] update state cost 2.384185791015625e-05 s
DEBUG 10-14 21:17:38 lpllm.py:1740] restore layer func cost 0.0013551712036132812 s
DEBUG 10-14 21:17:38 lpllm.py:511] restore layer cost 0.0015931129455566406 s
DEBUG 10-14 21:17:38 lpllm.py:512] reset_next_layer_need: layer_attn_idx=24, layer_mlp_idx=24, j_loc=48
DEBUG 10-14 21:17:38 lpllm.py:1280] reset layer cost 0.0017061233520507812 s
DEBUG 10-14 21:17:38 lpllm.py:1281] have reset next layer layer_attn 24 layer_mlp 23 
DEBUG 10-14 21:17:38 lpllm.py:1200] 
DEBUG 10-14 21:17:38 lpllm.py:1200] one decoder loop j: 49 cur_layer_idx: 24
DEBUG 10-14 21:17:38 lpllm.py:1207] start decoder qkv layer_attn 24 layer_mlp 24
DEBUG 10-14 21:17:38 lpmodule.py:526] hidden_states.shape torch.Size([10, 1, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:38 tutils.py:81] single group 0 real attn out cost 0.005677938461303711 s
DEBUG 10-14 21:17:38 lpmodule.py:541] kv_seq_len 1 past_key_value 512
DEBUG 10-14 21:17:38 lpmodule.py:545] inv_freq device cpu
DEBUG 10-14 21:17:38 lpmodule.py:546] kv_seq_len 513 position_ids torch.Size([10, 1]) value tensor([512])
DEBUG 10-14 21:17:38 lpmodule.py:551] query_states device cuda:1
DEBUG 10-14 21:17:38 lpmodule.py:552] key_states device cuda:1
DEBUG 10-14 21:17:38 lpmodule.py:553] value_states device cuda:1
DEBUG 10-14 21:17:38 lpmodule.py:554] position_ids device cpu
DEBUG 10-14 21:17:38 lpmodule.py:555] cos device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:38 lpmodule.py:556] sin device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:38 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:38 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:38 tutils.py:81] single group 1 real attn out cost 0.003167867660522461 s
DEBUG 10-14 21:17:38 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:38 tutils.py:81] single group 2 real attn out cost 0.003165721893310547 s
DEBUG 10-14 21:17:38 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:38 tutils.py:81] single group 3 real attn out cost 0.0031113624572753906 s
DEBUG 10-14 21:17:38 tutils.py:88] dot attn help cost 0.021907 seconds
DEBUG 10-14 21:17:38 lpmodule.py:433] dot attn cost 0.022042 seconds
DEBUG 10-14 21:17:38 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.00033736228942871094 s
DEBUG 10-14 21:17:38 lpllm.py:2280] CPU attn cost 0.026092 seconds if batch True
DEBUG 10-14 21:17:38 lpllm.py:2289] deal attn result cost 0.000003 seconds
DEBUG 10-14 21:17:38 lpllm.py:2291] CPU compute cost 0.027179 seconds
DEBUG 10-14 21:17:38 lpllm.py:2309] free cost 0.000190 seconds
DEBUG 10-14 21:17:38 lpmodule.py:162] call mlp prepare
DEBUG 10-14 21:17:38 lpmodule.py:163] layer idx 24
DEBUG 10-14 21:17:38 lpmodule.py:123] Tokens per expert: [2, 2, 1, 4, 3, 3, 0, 5]
DEBUG 10-14 21:17:38 lpmodule.py:124] Number of active experts: 7
DEBUG 10-14 21:17:38 lpmodule.py:195] call mlp post
DEBUG 10-14 21:17:38 lpllm.py:2262] GPU2CPU move cost 0.000301 seconds
DEBUG 10-14 21:17:38 lpmodule.py:340] layer_idx 24 attn layer_idx: 24
DEBUG 10-14 21:17:38 lpmodule.py:352] get seq_length 512 in decoder_attn_batch
WARNING 10-14 21:17:38 lpmodule.py:355] generally decoder_attn_batch called with q_len == 1
DEBUG 10-14 21:17:38 lpmodule.py:364] decoder_attn_batch update batch_dim 10-20
DEBUG 10-14 21:17:38 StaticCacheLen.py:84] static cache update layer_idx: 24, start_batch: 10, end_batch: 20
DEBUG 10-14 21:17:38 StaticCacheLen.py:90] static cache update layer_idx: 24, update seq_length to 513
DEBUG 10-14 21:17:38 lpmodule.py:368] update for kv cache 10-20 kv_seq_len 513 in decoder_attn_batch
DEBUG 10-14 21:17:38 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:38 lpmodule.py:374] update past key value cost 0.002542 seconds
DEBUG 10-14 21:17:38 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 513, 128]), value shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:38 lpmodule.py:399] repeat qkv cost 0.000001 seconds
DEBUG 10-14 21:17:38 lpmodule.py:400] q shape torch.Size([10, 32, 1, 128]), k shape torch.Size([10, 8, 513, 128]), v shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:38 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:38 lpllm.py:498] reset update attn
DEBUG 10-14 21:17:38 lpllm.py:1771] update state cost 3.7670135498046875e-05 s
DEBUG 10-14 21:17:38 lpllm.py:1740] restore layer func cost 0.0006318092346191406 s
DEBUG 10-14 21:17:38 lpllm.py:511] restore layer cost 0.0009109973907470703 s
DEBUG 10-14 21:17:38 lpllm.py:512] reset_next_layer_need: layer_attn_idx=25, layer_mlp_idx=24, j_loc=49
DEBUG 10-14 21:17:38 lpllm.py:1280] reset layer cost 0.0010552406311035156 s
DEBUG 10-14 21:17:38 lpllm.py:1281] have reset next layer layer_attn 24 layer_mlp 24 
DEBUG 10-14 21:17:38 lpllm.py:1287] j: 49 waiting the layer with layer_idx 25 before wait time 0.06165051460266113 s
INFO 10-14 21:17:38 client.py:117] confirm_model_loaded: Mixtral-8x7B, 67dc30b8-9a16-4a90-931a-837e8421cb0b
DEBUG 10-14 21:17:38 tutils.py:81] single group 0 real attn out cost 0.007277011871337891 s
DEBUG 10-14 21:17:38 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:38 tutils.py:81] single group 1 real attn out cost 0.0031599998474121094 s
DEBUG 10-14 21:17:38 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:38 tutils.py:81] single group 2 real attn out cost 0.0031080245971679688 s
DEBUG 10-14 21:17:38 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:38 tutils.py:81] single group 3 real attn out cost 0.003526926040649414 s
DEBUG 10-14 21:17:38 tutils.py:88] dot attn help cost 0.023045 seconds
DEBUG 10-14 21:17:38 lpmodule.py:433] dot attn cost 0.023176 seconds
DEBUG 10-14 21:17:38 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.0008292198181152344 s
DEBUG 10-14 21:17:38 lpllm.py:2280] CPU attn cost 0.027183 seconds if batch True
DEBUG 10-14 21:17:38 lpllm.py:2289] deal attn result cost 0.000004 seconds
DEBUG 10-14 21:17:38 lpllm.py:2291] CPU compute cost 0.027857 seconds
DEBUG 10-14 21:17:38 lpllm.py:2309] free cost 0.000168 seconds
INFO 10-14 21:17:38 client.py:125] Model loaded
DEBUG 10-14 21:17:38 lpllm.py:1291] j: load cost 0.2935962677001953 s waiting cost 0.23191523551940918 s
DEBUG 10-14 21:17:38 lpllm.py:1200] 
DEBUG 10-14 21:17:38 lpllm.py:1200] one decoder loop j: 50 cur_layer_idx: 25
DEBUG 10-14 21:17:38 lpllm.py:1205] start load next layer cur_layer_idx: 26
DEBUG 10-14 21:17:38 lpllm.py:1801] cuda memory handles: {1: <capsule object NULL at 0x7fdf3c6d9f80>} {1: b'\xe0\xaa29\x00\x00\x00\x00\xe8&\x1d\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\x86\x00\x00\x00\xff\x00\x00/\x00\x00\x00\x00\x00\x00\x00)\xd8\xd5\xc1\xa2\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-14 21:17:38 client.py:72] load_into_gpu: Mixtral-8x7B, 7022ee6e-cb23-4971-afa4-ce35381363a1
INFO 10-14 21:17:38 client.py:113] Model loaded: Mixtral-8x7B, 7022ee6e-cb23-4971-afa4-ce35381363a1
DEBUG 10-14 21:17:38 lpllm.py:1207] start decoder qkv layer_attn 25 layer_mlp 24
DEBUG 10-14 21:17:38 lpmodule.py:526] hidden_states.shape torch.Size([10, 1, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:38 lpmodule.py:541] kv_seq_len 1 past_key_value 512
DEBUG 10-14 21:17:38 lpmodule.py:545] inv_freq device cpu
DEBUG 10-14 21:17:38 lpmodule.py:546] kv_seq_len 513 position_ids torch.Size([10, 1]) value tensor([512])
DEBUG 10-14 21:17:38 lpmodule.py:551] query_states device cuda:1
DEBUG 10-14 21:17:38 lpmodule.py:552] key_states device cuda:1
DEBUG 10-14 21:17:38 lpmodule.py:553] value_states device cuda:1
DEBUG 10-14 21:17:38 lpmodule.py:554] position_ids device cpu
DEBUG 10-14 21:17:38 lpmodule.py:555] cos device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:38 lpmodule.py:556] sin device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:38 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:38 lpmodule.py:162] call mlp prepare
DEBUG 10-14 21:17:38 lpmodule.py:163] layer idx 24
DEBUG 10-14 21:17:38 lpmodule.py:123] Tokens per expert: [10, 5, 0, 0, 4, 0, 0, 1]
DEBUG 10-14 21:17:38 lpmodule.py:124] Number of active experts: 4
DEBUG 10-14 21:17:38 lpmodule.py:195] call mlp post
DEBUG 10-14 21:17:38 lpllm.py:2262] GPU2CPU move cost 0.000338 seconds
DEBUG 10-14 21:17:38 lpmodule.py:340] layer_idx 25 attn layer_idx: 25
DEBUG 10-14 21:17:38 lpmodule.py:352] get seq_length 512 in decoder_attn_batch
WARNING 10-14 21:17:38 lpmodule.py:355] generally decoder_attn_batch called with q_len == 1
DEBUG 10-14 21:17:38 lpmodule.py:364] decoder_attn_batch update batch_dim 0-10
DEBUG 10-14 21:17:38 StaticCacheLen.py:84] static cache update layer_idx: 25, start_batch: 0, end_batch: 10
DEBUG 10-14 21:17:38 lpmodule.py:368] update for kv cache 0-10 kv_seq_len 513 in decoder_attn_batch
DEBUG 10-14 21:17:38 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:38 lpmodule.py:374] update past key value cost 0.002677 seconds
DEBUG 10-14 21:17:38 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 513, 128]), value shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:38 lpmodule.py:399] repeat qkv cost 0.000001 seconds
DEBUG 10-14 21:17:38 lpmodule.py:400] q shape torch.Size([10, 32, 1, 128]), k shape torch.Size([10, 8, 513, 128]), v shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:38 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:38 lpllm.py:503] reset update experts
DEBUG 10-14 21:17:38 lpllm.py:1771] update state cost 2.6226043701171875e-05 s
DEBUG 10-14 21:17:38 lpllm.py:1740] restore layer func cost 0.0017359256744384766 s
DEBUG 10-14 21:17:38 lpllm.py:511] restore layer cost 0.002031564712524414 s
DEBUG 10-14 21:17:38 lpllm.py:512] reset_next_layer_need: layer_attn_idx=25, layer_mlp_idx=25, j_loc=50
DEBUG 10-14 21:17:38 lpllm.py:1280] reset layer cost 0.0021927356719970703 s
DEBUG 10-14 21:17:38 lpllm.py:1281] have reset next layer layer_attn 25 layer_mlp 24 
DEBUG 10-14 21:17:38 lpllm.py:1200] 
DEBUG 10-14 21:17:38 lpllm.py:1200] one decoder loop j: 51 cur_layer_idx: 25
DEBUG 10-14 21:17:38 lpllm.py:1207] start decoder qkv layer_attn 25 layer_mlp 25
DEBUG 10-14 21:17:38 lpmodule.py:526] hidden_states.shape torch.Size([10, 1, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:38 tutils.py:81] single group 0 real attn out cost 0.005547285079956055 s
DEBUG 10-14 21:17:38 lpmodule.py:541] kv_seq_len 1 past_key_value 512
DEBUG 10-14 21:17:38 lpmodule.py:545] inv_freq device cpu
DEBUG 10-14 21:17:38 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:38 lpmodule.py:546] kv_seq_len 513 position_ids torch.Size([10, 1]) value tensor([512])
DEBUG 10-14 21:17:38 lpmodule.py:551] query_states device cuda:1
DEBUG 10-14 21:17:38 lpmodule.py:552] key_states device cuda:1
DEBUG 10-14 21:17:38 lpmodule.py:553] value_states device cuda:1
DEBUG 10-14 21:17:38 lpmodule.py:554] position_ids device cpu
DEBUG 10-14 21:17:38 lpmodule.py:555] cos device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:38 lpmodule.py:556] sin device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:38 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:38 tutils.py:81] single group 1 real attn out cost 0.0030667781829833984 s
DEBUG 10-14 21:17:38 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:38 tutils.py:81] single group 2 real attn out cost 0.0031058788299560547 s
DEBUG 10-14 21:17:38 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:38 tutils.py:81] single group 3 real attn out cost 0.0035390853881835938 s
DEBUG 10-14 21:17:38 tutils.py:88] dot attn help cost 0.022170 seconds
DEBUG 10-14 21:17:38 lpmodule.py:433] dot attn cost 0.022319 seconds
DEBUG 10-14 21:17:38 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.00033926963806152344 s
DEBUG 10-14 21:17:38 lpllm.py:2280] CPU attn cost 0.026063 seconds if batch True
DEBUG 10-14 21:17:38 lpllm.py:2289] deal attn result cost 0.000004 seconds
DEBUG 10-14 21:17:38 lpllm.py:2291] CPU compute cost 0.026745 seconds
DEBUG 10-14 21:17:38 lpllm.py:2309] free cost 0.000242 seconds
DEBUG 10-14 21:17:38 lpmodule.py:162] call mlp prepare
DEBUG 10-14 21:17:38 lpmodule.py:163] layer idx 25
DEBUG 10-14 21:17:38 lpmodule.py:123] Tokens per expert: [0, 2, 7, 0, 1, 1, 7, 2]
DEBUG 10-14 21:17:38 lpmodule.py:124] Number of active experts: 6
DEBUG 10-14 21:17:38 lpmodule.py:195] call mlp post
DEBUG 10-14 21:17:38 lpllm.py:2262] GPU2CPU move cost 0.000482 seconds
DEBUG 10-14 21:17:38 lpmodule.py:340] layer_idx 25 attn layer_idx: 25
DEBUG 10-14 21:17:38 lpmodule.py:352] get seq_length 512 in decoder_attn_batch
WARNING 10-14 21:17:38 lpmodule.py:355] generally decoder_attn_batch called with q_len == 1
DEBUG 10-14 21:17:38 lpmodule.py:364] decoder_attn_batch update batch_dim 10-20
DEBUG 10-14 21:17:38 StaticCacheLen.py:84] static cache update layer_idx: 25, start_batch: 10, end_batch: 20
DEBUG 10-14 21:17:38 StaticCacheLen.py:90] static cache update layer_idx: 25, update seq_length to 513
DEBUG 10-14 21:17:38 lpmodule.py:368] update for kv cache 10-20 kv_seq_len 513 in decoder_attn_batch
DEBUG 10-14 21:17:38 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:38 lpmodule.py:374] update past key value cost 0.004159 seconds
DEBUG 10-14 21:17:38 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 513, 128]), value shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:38 lpmodule.py:399] repeat qkv cost 0.000002 seconds
DEBUG 10-14 21:17:38 lpmodule.py:400] q shape torch.Size([10, 32, 1, 128]), k shape torch.Size([10, 8, 513, 128]), v shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:38 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:38 lpllm.py:498] reset update attn
DEBUG 10-14 21:17:38 lpllm.py:1771] update state cost 2.5033950805664062e-05 s
DEBUG 10-14 21:17:38 lpllm.py:1740] restore layer func cost 0.0005717277526855469 s
DEBUG 10-14 21:17:38 lpllm.py:511] restore layer cost 0.0007834434509277344 s
DEBUG 10-14 21:17:38 lpllm.py:512] reset_next_layer_need: layer_attn_idx=26, layer_mlp_idx=25, j_loc=51
DEBUG 10-14 21:17:38 lpllm.py:1280] reset layer cost 0.0008797645568847656 s
DEBUG 10-14 21:17:38 lpllm.py:1281] have reset next layer layer_attn 25 layer_mlp 25 
DEBUG 10-14 21:17:38 lpllm.py:1287] j: 51 waiting the layer with layer_idx 26 before wait time 0.06203198432922363 s
INFO 10-14 21:17:38 client.py:117] confirm_model_loaded: Mixtral-8x7B, 7022ee6e-cb23-4971-afa4-ce35381363a1
DEBUG 10-14 21:17:38 tutils.py:81] single group 0 real attn out cost 0.005095005035400391 s
DEBUG 10-14 21:17:38 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:38 tutils.py:81] single group 1 real attn out cost 0.0029838085174560547 s
DEBUG 10-14 21:17:38 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:38 tutils.py:81] single group 2 real attn out cost 0.0035178661346435547 s
DEBUG 10-14 21:17:38 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:38 tutils.py:81] single group 3 real attn out cost 0.003290891647338867 s
DEBUG 10-14 21:17:38 tutils.py:88] dot attn help cost 0.021153 seconds
DEBUG 10-14 21:17:38 lpmodule.py:433] dot attn cost 0.021335 seconds
DEBUG 10-14 21:17:38 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.0008027553558349609 s
DEBUG 10-14 21:17:38 lpllm.py:2280] CPU attn cost 0.027062 seconds if batch True
DEBUG 10-14 21:17:38 lpllm.py:2289] deal attn result cost 0.000004 seconds
DEBUG 10-14 21:17:38 lpllm.py:2291] CPU compute cost 0.027872 seconds
DEBUG 10-14 21:17:38 lpllm.py:2309] free cost 0.000187 seconds
INFO 10-14 21:17:39 client.py:125] Model loaded
DEBUG 10-14 21:17:39 lpllm.py:1291] j: load cost 0.3076794147491455 s waiting cost 0.24563217163085938 s
DEBUG 10-14 21:17:39 lpllm.py:1200] 
DEBUG 10-14 21:17:39 lpllm.py:1200] one decoder loop j: 52 cur_layer_idx: 26
DEBUG 10-14 21:17:39 lpllm.py:1205] start load next layer cur_layer_idx: 27
DEBUG 10-14 21:17:39 lpllm.py:1801] cuda memory handles: {1: <capsule object NULL at 0x7fdf3c6da010>} {1: b'\xe0\xaa29\x00\x00\x00\x00\xe8&\x1d\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\x86\x00\x00\x00\xff\x00\x000\x00\x00\x00\x00\x00\x00\x00)\xd8\xd5\xc1\xa4\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-14 21:17:39 client.py:72] load_into_gpu: Mixtral-8x7B, 4295cd35-f15a-4846-b211-2bc2c21353f4
INFO 10-14 21:17:39 client.py:113] Model loaded: Mixtral-8x7B, 4295cd35-f15a-4846-b211-2bc2c21353f4
DEBUG 10-14 21:17:39 lpllm.py:1207] start decoder qkv layer_attn 26 layer_mlp 25
DEBUG 10-14 21:17:39 lpmodule.py:526] hidden_states.shape torch.Size([10, 1, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:39 lpmodule.py:541] kv_seq_len 1 past_key_value 512
DEBUG 10-14 21:17:39 lpmodule.py:545] inv_freq device cpu
DEBUG 10-14 21:17:39 lpmodule.py:546] kv_seq_len 513 position_ids torch.Size([10, 1]) value tensor([512])
DEBUG 10-14 21:17:39 lpmodule.py:551] query_states device cuda:1
DEBUG 10-14 21:17:39 lpmodule.py:552] key_states device cuda:1
DEBUG 10-14 21:17:39 lpmodule.py:553] value_states device cuda:1
DEBUG 10-14 21:17:39 lpmodule.py:554] position_ids device cpu
DEBUG 10-14 21:17:39 lpmodule.py:555] cos device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:39 lpmodule.py:556] sin device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:39 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:39 lpmodule.py:162] call mlp prepare
DEBUG 10-14 21:17:39 lpmodule.py:163] layer idx 25
DEBUG 10-14 21:17:39 lpmodule.py:123] Tokens per expert: [3, 10, 1, 0, 0, 0, 6, 0]
DEBUG 10-14 21:17:39 lpmodule.py:124] Number of active experts: 4
DEBUG 10-14 21:17:39 lpmodule.py:195] call mlp post
DEBUG 10-14 21:17:39 lpllm.py:2262] GPU2CPU move cost 0.000347 seconds
DEBUG 10-14 21:17:39 lpmodule.py:340] layer_idx 26 attn layer_idx: 26
DEBUG 10-14 21:17:39 lpmodule.py:352] get seq_length 512 in decoder_attn_batch
WARNING 10-14 21:17:39 lpmodule.py:355] generally decoder_attn_batch called with q_len == 1
DEBUG 10-14 21:17:39 lpmodule.py:364] decoder_attn_batch update batch_dim 0-10
DEBUG 10-14 21:17:39 StaticCacheLen.py:84] static cache update layer_idx: 26, start_batch: 0, end_batch: 10
DEBUG 10-14 21:17:39 lpmodule.py:368] update for kv cache 0-10 kv_seq_len 513 in decoder_attn_batch
DEBUG 10-14 21:17:39 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:39 lpmodule.py:374] update past key value cost 0.002702 seconds
DEBUG 10-14 21:17:39 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 513, 128]), value shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:39 lpmodule.py:399] repeat qkv cost 0.000001 seconds
DEBUG 10-14 21:17:39 lpmodule.py:400] q shape torch.Size([10, 32, 1, 128]), k shape torch.Size([10, 8, 513, 128]), v shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:39 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:39 lpllm.py:503] reset update experts
DEBUG 10-14 21:17:39 lpllm.py:1771] update state cost 3.0517578125e-05 s
DEBUG 10-14 21:17:39 lpllm.py:1740] restore layer func cost 0.0014050006866455078 s
DEBUG 10-14 21:17:39 lpllm.py:511] restore layer cost 0.0016717910766601562 s
DEBUG 10-14 21:17:39 lpllm.py:512] reset_next_layer_need: layer_attn_idx=26, layer_mlp_idx=26, j_loc=52
DEBUG 10-14 21:17:39 lpllm.py:1280] reset layer cost 0.0018088817596435547 s
DEBUG 10-14 21:17:39 lpllm.py:1281] have reset next layer layer_attn 26 layer_mlp 25 
DEBUG 10-14 21:17:39 lpllm.py:1200] 
DEBUG 10-14 21:17:39 lpllm.py:1200] one decoder loop j: 53 cur_layer_idx: 26
DEBUG 10-14 21:17:39 lpllm.py:1207] start decoder qkv layer_attn 26 layer_mlp 26
DEBUG 10-14 21:17:39 lpmodule.py:526] hidden_states.shape torch.Size([10, 1, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:39 lpmodule.py:541] kv_seq_len 1 past_key_value 512
DEBUG 10-14 21:17:39 lpmodule.py:545] inv_freq device cpu
DEBUG 10-14 21:17:39 lpmodule.py:546] kv_seq_len 513 position_ids torch.Size([10, 1]) value tensor([512])
DEBUG 10-14 21:17:39 tutils.py:81] single group 0 real attn out cost 0.0051343441009521484 s
DEBUG 10-14 21:17:39 lpmodule.py:551] query_states device cuda:1
DEBUG 10-14 21:17:39 lpmodule.py:552] key_states device cuda:1
DEBUG 10-14 21:17:39 lpmodule.py:553] value_states device cuda:1
DEBUG 10-14 21:17:39 lpmodule.py:554] position_ids device cpu
DEBUG 10-14 21:17:39 lpmodule.py:555] cos device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:39 lpmodule.py:556] sin device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:39 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:39 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:39 tutils.py:81] single group 1 real attn out cost 0.0041120052337646484 s
DEBUG 10-14 21:17:39 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:39 tutils.py:81] single group 2 real attn out cost 0.003176450729370117 s
DEBUG 10-14 21:17:39 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:39 tutils.py:81] single group 3 real attn out cost 0.0032303333282470703 s
DEBUG 10-14 21:17:39 tutils.py:88] dot attn help cost 0.022286 seconds
DEBUG 10-14 21:17:39 lpmodule.py:433] dot attn cost 0.022415 seconds
DEBUG 10-14 21:17:39 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.0007643699645996094 s
DEBUG 10-14 21:17:39 lpllm.py:2280] CPU attn cost 0.026714 seconds if batch True
DEBUG 10-14 21:17:39 lpllm.py:2289] deal attn result cost 0.000004 seconds
DEBUG 10-14 21:17:39 lpllm.py:2291] CPU compute cost 0.027486 seconds
DEBUG 10-14 21:17:39 lpllm.py:2309] free cost 0.000218 seconds
DEBUG 10-14 21:17:39 lpmodule.py:162] call mlp prepare
DEBUG 10-14 21:17:39 lpmodule.py:163] layer idx 26
DEBUG 10-14 21:17:39 lpmodule.py:123] Tokens per expert: [5, 1, 0, 4, 0, 9, 0, 1]
DEBUG 10-14 21:17:39 lpmodule.py:124] Number of active experts: 5
DEBUG 10-14 21:17:39 lpmodule.py:195] call mlp post
DEBUG 10-14 21:17:39 lpllm.py:2262] GPU2CPU move cost 0.000289 seconds
DEBUG 10-14 21:17:39 lpmodule.py:340] layer_idx 26 attn layer_idx: 26
DEBUG 10-14 21:17:39 lpmodule.py:352] get seq_length 512 in decoder_attn_batch
WARNING 10-14 21:17:39 lpmodule.py:355] generally decoder_attn_batch called with q_len == 1
DEBUG 10-14 21:17:39 lpmodule.py:364] decoder_attn_batch update batch_dim 10-20
DEBUG 10-14 21:17:39 StaticCacheLen.py:84] static cache update layer_idx: 26, start_batch: 10, end_batch: 20
DEBUG 10-14 21:17:39 StaticCacheLen.py:90] static cache update layer_idx: 26, update seq_length to 513
DEBUG 10-14 21:17:39 lpmodule.py:368] update for kv cache 10-20 kv_seq_len 513 in decoder_attn_batch
DEBUG 10-14 21:17:39 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:39 lpmodule.py:374] update past key value cost 0.002425 seconds
DEBUG 10-14 21:17:39 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 513, 128]), value shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:39 lpmodule.py:399] repeat qkv cost 0.000001 seconds
DEBUG 10-14 21:17:39 lpmodule.py:400] q shape torch.Size([10, 32, 1, 128]), k shape torch.Size([10, 8, 513, 128]), v shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:39 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:39 tutils.py:81] single group 0 real attn out cost 0.0054361820220947266 s
DEBUG 10-14 21:17:39 lpllm.py:498] reset update attn
DEBUG 10-14 21:17:39 lpllm.py:1771] update state cost 4.00543212890625e-05 s
DEBUG 10-14 21:17:39 lpllm.py:1740] restore layer func cost 0.0008580684661865234 s
DEBUG 10-14 21:17:39 lpllm.py:511] restore layer cost 0.0012001991271972656 s
DEBUG 10-14 21:17:39 lpllm.py:512] reset_next_layer_need: layer_attn_idx=27, layer_mlp_idx=26, j_loc=53
DEBUG 10-14 21:17:39 lpllm.py:1280] reset layer cost 0.001344442367553711 s
DEBUG 10-14 21:17:39 lpllm.py:1281] have reset next layer layer_attn 26 layer_mlp 26 
DEBUG 10-14 21:17:39 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:39 lpllm.py:1287] j: 53 waiting the layer with layer_idx 27 before wait time 0.06284260749816895 s
INFO 10-14 21:17:39 client.py:117] confirm_model_loaded: Mixtral-8x7B, 4295cd35-f15a-4846-b211-2bc2c21353f4
DEBUG 10-14 21:17:39 tutils.py:81] single group 1 real attn out cost 0.003573894500732422 s
DEBUG 10-14 21:17:39 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:39 tutils.py:81] single group 2 real attn out cost 0.003512144088745117 s
DEBUG 10-14 21:17:39 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:39 tutils.py:81] single group 3 real attn out cost 0.003137826919555664 s
DEBUG 10-14 21:17:39 tutils.py:88] dot attn help cost 0.022711 seconds
DEBUG 10-14 21:17:39 lpmodule.py:433] dot attn cost 0.022911 seconds
DEBUG 10-14 21:17:39 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.0011091232299804688 s
DEBUG 10-14 21:17:39 lpllm.py:2280] CPU attn cost 0.027127 seconds if batch True
DEBUG 10-14 21:17:39 lpllm.py:2289] deal attn result cost 0.000004 seconds
DEBUG 10-14 21:17:39 lpllm.py:2291] CPU compute cost 0.027840 seconds
DEBUG 10-14 21:17:39 lpllm.py:2309] free cost 0.000220 seconds
INFO 10-14 21:17:39 client.py:125] Model loaded
DEBUG 10-14 21:17:39 lpllm.py:1291] j: load cost 0.29212069511413574 s waiting cost 0.22911834716796875 s
DEBUG 10-14 21:17:39 lpllm.py:1200] 
DEBUG 10-14 21:17:39 lpllm.py:1200] one decoder loop j: 54 cur_layer_idx: 27
DEBUG 10-14 21:17:39 lpllm.py:1205] start load next layer cur_layer_idx: 28
DEBUG 10-14 21:17:39 lpllm.py:1801] cuda memory handles: {1: <capsule object NULL at 0x7fdf3c6d9ec0>} {1: b'\xe0\xaa29\x00\x00\x00\x00\xe8&\x1d\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\x86\x00\x00\x00\xff\x00\x00.\x00\x00\x00\x00\x00\x00\x00)\xd8\xd5\xc1\xa0\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-14 21:17:39 client.py:72] load_into_gpu: Mixtral-8x7B, 583ae0f3-f59b-4ad6-bc34-42eae26a82b6
INFO 10-14 21:17:39 client.py:113] Model loaded: Mixtral-8x7B, 583ae0f3-f59b-4ad6-bc34-42eae26a82b6
DEBUG 10-14 21:17:39 lpllm.py:1207] start decoder qkv layer_attn 27 layer_mlp 26
DEBUG 10-14 21:17:39 lpmodule.py:526] hidden_states.shape torch.Size([10, 1, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:39 lpmodule.py:541] kv_seq_len 1 past_key_value 512
DEBUG 10-14 21:17:39 lpmodule.py:545] inv_freq device cpu
DEBUG 10-14 21:17:39 lpmodule.py:546] kv_seq_len 513 position_ids torch.Size([10, 1]) value tensor([512])
DEBUG 10-14 21:17:39 lpmodule.py:551] query_states device cuda:1
DEBUG 10-14 21:17:39 lpmodule.py:552] key_states device cuda:1
DEBUG 10-14 21:17:39 lpmodule.py:553] value_states device cuda:1
DEBUG 10-14 21:17:39 lpmodule.py:554] position_ids device cpu
DEBUG 10-14 21:17:39 lpmodule.py:555] cos device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:39 lpmodule.py:556] sin device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:39 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:39 lpmodule.py:162] call mlp prepare
DEBUG 10-14 21:17:39 lpmodule.py:163] layer idx 26
DEBUG 10-14 21:17:39 lpmodule.py:123] Tokens per expert: [0, 9, 3, 7, 0, 1, 0, 0]
DEBUG 10-14 21:17:39 lpmodule.py:124] Number of active experts: 4
DEBUG 10-14 21:17:39 lpmodule.py:195] call mlp post
DEBUG 10-14 21:17:39 lpllm.py:2262] GPU2CPU move cost 0.000615 seconds
DEBUG 10-14 21:17:39 lpmodule.py:340] layer_idx 27 attn layer_idx: 27
DEBUG 10-14 21:17:39 lpmodule.py:352] get seq_length 512 in decoder_attn_batch
WARNING 10-14 21:17:39 lpmodule.py:355] generally decoder_attn_batch called with q_len == 1
DEBUG 10-14 21:17:39 lpmodule.py:364] decoder_attn_batch update batch_dim 0-10
DEBUG 10-14 21:17:39 StaticCacheLen.py:84] static cache update layer_idx: 27, start_batch: 0, end_batch: 10
DEBUG 10-14 21:17:39 lpmodule.py:368] update for kv cache 0-10 kv_seq_len 513 in decoder_attn_batch
DEBUG 10-14 21:17:39 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:39 lpmodule.py:374] update past key value cost 0.002753 seconds
DEBUG 10-14 21:17:39 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 513, 128]), value shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:39 lpmodule.py:399] repeat qkv cost 0.000002 seconds
DEBUG 10-14 21:17:39 lpmodule.py:400] q shape torch.Size([10, 32, 1, 128]), k shape torch.Size([10, 8, 513, 128]), v shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:39 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:39 lpllm.py:503] reset update experts
DEBUG 10-14 21:17:39 lpllm.py:1771] update state cost 2.6226043701171875e-05 s
DEBUG 10-14 21:17:39 lpllm.py:1740] restore layer func cost 0.0014777183532714844 s
DEBUG 10-14 21:17:39 lpllm.py:511] restore layer cost 0.0017845630645751953 s
DEBUG 10-14 21:17:39 lpllm.py:512] reset_next_layer_need: layer_attn_idx=27, layer_mlp_idx=27, j_loc=54
DEBUG 10-14 21:17:39 lpllm.py:1280] reset layer cost 0.0019075870513916016 s
DEBUG 10-14 21:17:39 lpllm.py:1281] have reset next layer layer_attn 27 layer_mlp 26 
DEBUG 10-14 21:17:39 lpllm.py:1200] 
DEBUG 10-14 21:17:39 lpllm.py:1200] one decoder loop j: 55 cur_layer_idx: 27
DEBUG 10-14 21:17:39 lpllm.py:1207] start decoder qkv layer_attn 27 layer_mlp 27
DEBUG 10-14 21:17:39 lpmodule.py:526] hidden_states.shape torch.Size([10, 1, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:39 tutils.py:81] single group 0 real attn out cost 0.006117820739746094 s
DEBUG 10-14 21:17:39 lpmodule.py:541] kv_seq_len 1 past_key_value 512
DEBUG 10-14 21:17:39 lpmodule.py:545] inv_freq device cpu
DEBUG 10-14 21:17:39 lpmodule.py:546] kv_seq_len 513 position_ids torch.Size([10, 1]) value tensor([512])
DEBUG 10-14 21:17:39 lpmodule.py:551] query_states device cuda:1
DEBUG 10-14 21:17:39 lpmodule.py:552] key_states device cuda:1
DEBUG 10-14 21:17:39 lpmodule.py:553] value_states device cuda:1
DEBUG 10-14 21:17:39 lpmodule.py:554] position_ids device cpu
DEBUG 10-14 21:17:39 lpmodule.py:555] cos device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:39 lpmodule.py:556] sin device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:39 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:39 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:39 tutils.py:81] single group 1 real attn out cost 0.006130218505859375 s
DEBUG 10-14 21:17:39 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:39 tutils.py:81] single group 2 real attn out cost 0.0031037330627441406 s
DEBUG 10-14 21:17:39 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:39 tutils.py:81] single group 3 real attn out cost 0.003502368927001953 s
DEBUG 10-14 21:17:39 tutils.py:88] dot attn help cost 0.025634 seconds
DEBUG 10-14 21:17:39 lpmodule.py:433] dot attn cost 0.025830 seconds
DEBUG 10-14 21:17:39 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.0008680820465087891 s
DEBUG 10-14 21:17:39 lpllm.py:2280] CPU attn cost 0.030430 seconds if batch True
DEBUG 10-14 21:17:39 lpllm.py:2289] deal attn result cost 0.000005 seconds
DEBUG 10-14 21:17:39 lpllm.py:2291] CPU compute cost 0.031562 seconds
DEBUG 10-14 21:17:39 lpllm.py:2309] free cost 0.000336 seconds
DEBUG 10-14 21:17:39 lpmodule.py:162] call mlp prepare
DEBUG 10-14 21:17:39 lpmodule.py:163] layer idx 27
DEBUG 10-14 21:17:39 lpmodule.py:123] Tokens per expert: [2, 9, 0, 0, 0, 2, 1, 6]
DEBUG 10-14 21:17:39 lpmodule.py:124] Number of active experts: 5
DEBUG 10-14 21:17:39 lpmodule.py:195] call mlp post
DEBUG 10-14 21:17:39 lpllm.py:2262] GPU2CPU move cost 0.000460 seconds
DEBUG 10-14 21:17:39 lpmodule.py:340] layer_idx 27 attn layer_idx: 27
DEBUG 10-14 21:17:39 lpmodule.py:352] get seq_length 512 in decoder_attn_batch
WARNING 10-14 21:17:39 lpmodule.py:355] generally decoder_attn_batch called with q_len == 1
DEBUG 10-14 21:17:39 lpmodule.py:364] decoder_attn_batch update batch_dim 10-20
DEBUG 10-14 21:17:39 StaticCacheLen.py:84] static cache update layer_idx: 27, start_batch: 10, end_batch: 20
DEBUG 10-14 21:17:39 StaticCacheLen.py:90] static cache update layer_idx: 27, update seq_length to 513
DEBUG 10-14 21:17:39 lpmodule.py:368] update for kv cache 10-20 kv_seq_len 513 in decoder_attn_batch
DEBUG 10-14 21:17:39 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:39 lpmodule.py:374] update past key value cost 0.002815 seconds
DEBUG 10-14 21:17:39 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 513, 128]), value shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:39 lpmodule.py:399] repeat qkv cost 0.000002 seconds
DEBUG 10-14 21:17:39 lpmodule.py:400] q shape torch.Size([10, 32, 1, 128]), k shape torch.Size([10, 8, 513, 128]), v shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:39 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:39 lpllm.py:498] reset update attn
DEBUG 10-14 21:17:39 lpllm.py:1771] update state cost 3.2901763916015625e-05 s
DEBUG 10-14 21:17:39 lpllm.py:1740] restore layer func cost 0.0008673667907714844 s
DEBUG 10-14 21:17:39 lpllm.py:511] restore layer cost 0.0011866092681884766 s
DEBUG 10-14 21:17:39 lpllm.py:512] reset_next_layer_need: layer_attn_idx=28, layer_mlp_idx=27, j_loc=55
DEBUG 10-14 21:17:39 lpllm.py:1280] reset layer cost 0.0013780593872070312 s
DEBUG 10-14 21:17:39 lpllm.py:1281] have reset next layer layer_attn 27 layer_mlp 27 
DEBUG 10-14 21:17:39 lpllm.py:1287] j: 55 waiting the layer with layer_idx 28 before wait time 0.07561397552490234 s
INFO 10-14 21:17:39 client.py:117] confirm_model_loaded: Mixtral-8x7B, 583ae0f3-f59b-4ad6-bc34-42eae26a82b6
DEBUG 10-14 21:17:39 tutils.py:81] single group 0 real attn out cost 0.006470918655395508 s
DEBUG 10-14 21:17:39 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:39 tutils.py:81] single group 1 real attn out cost 0.0031435489654541016 s
DEBUG 10-14 21:17:39 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:39 tutils.py:81] single group 2 real attn out cost 0.003147602081298828 s
DEBUG 10-14 21:17:39 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:39 tutils.py:81] single group 3 real attn out cost 0.002991914749145508 s
DEBUG 10-14 21:17:39 tutils.py:88] dot attn help cost 0.022274 seconds
DEBUG 10-14 21:17:39 lpmodule.py:433] dot attn cost 0.022386 seconds
DEBUG 10-14 21:17:39 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.00045228004455566406 s
DEBUG 10-14 21:17:39 lpllm.py:2280] CPU attn cost 0.026496 seconds if batch True
DEBUG 10-14 21:17:39 lpllm.py:2289] deal attn result cost 0.000004 seconds
DEBUG 10-14 21:17:39 lpllm.py:2291] CPU compute cost 0.027318 seconds
DEBUG 10-14 21:17:39 lpllm.py:2309] free cost 0.000144 seconds
INFO 10-14 21:17:39 client.py:125] Model loaded
DEBUG 10-14 21:17:39 lpllm.py:1291] j: load cost 0.29729700088500977 s waiting cost 0.2216169834136963 s
DEBUG 10-14 21:17:39 lpllm.py:1200] 
DEBUG 10-14 21:17:39 lpllm.py:1200] one decoder loop j: 56 cur_layer_idx: 28
DEBUG 10-14 21:17:39 lpllm.py:1205] start load next layer cur_layer_idx: 29
DEBUG 10-14 21:17:39 lpllm.py:1801] cuda memory handles: {1: <capsule object NULL at 0x7fdf3c6d9f80>} {1: b'\xe0\xaa29\x00\x00\x00\x00\xe8&\x1d\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\x86\x00\x00\x00\xff\x00\x00/\x00\x00\x00\x00\x00\x00\x00)\xd8\xd5\xc1\xa2\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-14 21:17:39 client.py:72] load_into_gpu: Mixtral-8x7B, a46bb1d3-a05b-4bc9-846e-df4201a964e0
INFO 10-14 21:17:39 client.py:113] Model loaded: Mixtral-8x7B, a46bb1d3-a05b-4bc9-846e-df4201a964e0
DEBUG 10-14 21:17:39 lpllm.py:1207] start decoder qkv layer_attn 28 layer_mlp 27
DEBUG 10-14 21:17:39 lpmodule.py:526] hidden_states.shape torch.Size([10, 1, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:39 lpmodule.py:541] kv_seq_len 1 past_key_value 512
DEBUG 10-14 21:17:39 lpmodule.py:545] inv_freq device cpu
DEBUG 10-14 21:17:39 lpmodule.py:546] kv_seq_len 513 position_ids torch.Size([10, 1]) value tensor([512])
DEBUG 10-14 21:17:39 lpmodule.py:551] query_states device cuda:1
DEBUG 10-14 21:17:39 lpmodule.py:552] key_states device cuda:1
DEBUG 10-14 21:17:39 lpmodule.py:553] value_states device cuda:1
DEBUG 10-14 21:17:39 lpmodule.py:554] position_ids device cpu
DEBUG 10-14 21:17:39 lpmodule.py:555] cos device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:39 lpmodule.py:556] sin device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:39 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:39 lpmodule.py:162] call mlp prepare
DEBUG 10-14 21:17:39 lpmodule.py:163] layer idx 27
DEBUG 10-14 21:17:39 lpmodule.py:123] Tokens per expert: [1, 6, 0, 3, 0, 1, 6, 3]
DEBUG 10-14 21:17:39 lpmodule.py:124] Number of active experts: 6
DEBUG 10-14 21:17:39 lpmodule.py:195] call mlp post
DEBUG 10-14 21:17:39 lpllm.py:2262] GPU2CPU move cost 0.000444 seconds
DEBUG 10-14 21:17:39 lpmodule.py:340] layer_idx 28 attn layer_idx: 28
DEBUG 10-14 21:17:39 lpmodule.py:352] get seq_length 512 in decoder_attn_batch
WARNING 10-14 21:17:39 lpmodule.py:355] generally decoder_attn_batch called with q_len == 1
DEBUG 10-14 21:17:39 lpmodule.py:364] decoder_attn_batch update batch_dim 0-10
DEBUG 10-14 21:17:39 StaticCacheLen.py:84] static cache update layer_idx: 28, start_batch: 0, end_batch: 10
DEBUG 10-14 21:17:39 lpmodule.py:368] update for kv cache 0-10 kv_seq_len 513 in decoder_attn_batch
DEBUG 10-14 21:17:39 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:39 lpmodule.py:374] update past key value cost 0.002693 seconds
DEBUG 10-14 21:17:39 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 513, 128]), value shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:39 lpmodule.py:399] repeat qkv cost 0.000002 seconds
DEBUG 10-14 21:17:39 lpmodule.py:400] q shape torch.Size([10, 32, 1, 128]), k shape torch.Size([10, 8, 513, 128]), v shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:39 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:39 lpllm.py:503] reset update experts
DEBUG 10-14 21:17:39 lpllm.py:1771] update state cost 3.361701965332031e-05 s
DEBUG 10-14 21:17:39 lpllm.py:1740] restore layer func cost 0.0015819072723388672 s
DEBUG 10-14 21:17:39 lpllm.py:511] restore layer cost 0.0019648075103759766 s
DEBUG 10-14 21:17:39 lpllm.py:512] reset_next_layer_need: layer_attn_idx=28, layer_mlp_idx=28, j_loc=56
DEBUG 10-14 21:17:39 lpllm.py:1280] reset layer cost 0.0021071434020996094 s
DEBUG 10-14 21:17:39 lpllm.py:1281] have reset next layer layer_attn 28 layer_mlp 27 
DEBUG 10-14 21:17:39 lpllm.py:1200] 
DEBUG 10-14 21:17:39 lpllm.py:1200] one decoder loop j: 57 cur_layer_idx: 28
DEBUG 10-14 21:17:39 lpllm.py:1207] start decoder qkv layer_attn 28 layer_mlp 28
DEBUG 10-14 21:17:39 lpmodule.py:526] hidden_states.shape torch.Size([10, 1, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:39 tutils.py:81] single group 0 real attn out cost 0.0055773258209228516 s
DEBUG 10-14 21:17:39 lpmodule.py:541] kv_seq_len 1 past_key_value 512
DEBUG 10-14 21:17:39 lpmodule.py:545] inv_freq device cpu
DEBUG 10-14 21:17:39 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:39 lpmodule.py:546] kv_seq_len 513 position_ids torch.Size([10, 1]) value tensor([512])
DEBUG 10-14 21:17:39 lpmodule.py:551] query_states device cuda:1
DEBUG 10-14 21:17:39 lpmodule.py:552] key_states device cuda:1
DEBUG 10-14 21:17:39 lpmodule.py:553] value_states device cuda:1
DEBUG 10-14 21:17:39 lpmodule.py:554] position_ids device cpu
DEBUG 10-14 21:17:39 lpmodule.py:555] cos device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:39 lpmodule.py:556] sin device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:39 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:39 tutils.py:81] single group 1 real attn out cost 0.003255128860473633 s
DEBUG 10-14 21:17:39 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:39 tutils.py:81] single group 2 real attn out cost 0.0034666061401367188 s
DEBUG 10-14 21:17:39 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:39 tutils.py:81] single group 3 real attn out cost 0.0034275054931640625 s
DEBUG 10-14 21:17:39 tutils.py:88] dot attn help cost 0.022135 seconds
DEBUG 10-14 21:17:39 lpmodule.py:433] dot attn cost 0.022282 seconds
DEBUG 10-14 21:17:39 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.00035953521728515625 s
DEBUG 10-14 21:17:39 lpllm.py:2280] CPU attn cost 0.026138 seconds if batch True
DEBUG 10-14 21:17:39 lpllm.py:2289] deal attn result cost 0.000004 seconds
DEBUG 10-14 21:17:39 lpllm.py:2291] CPU compute cost 0.027079 seconds
DEBUG 10-14 21:17:39 lpllm.py:2309] free cost 0.000194 seconds
DEBUG 10-14 21:17:39 lpmodule.py:162] call mlp prepare
DEBUG 10-14 21:17:39 lpmodule.py:163] layer idx 28
DEBUG 10-14 21:17:39 lpmodule.py:123] Tokens per expert: [4, 0, 0, 7, 2, 6, 0, 1]
DEBUG 10-14 21:17:39 lpmodule.py:124] Number of active experts: 5
DEBUG 10-14 21:17:39 lpmodule.py:195] call mlp post
DEBUG 10-14 21:17:39 lpllm.py:2262] GPU2CPU move cost 0.000501 seconds
DEBUG 10-14 21:17:39 lpmodule.py:340] layer_idx 28 attn layer_idx: 28
DEBUG 10-14 21:17:39 lpmodule.py:352] get seq_length 512 in decoder_attn_batch
WARNING 10-14 21:17:39 lpmodule.py:355] generally decoder_attn_batch called with q_len == 1
DEBUG 10-14 21:17:39 lpmodule.py:364] decoder_attn_batch update batch_dim 10-20
DEBUG 10-14 21:17:39 StaticCacheLen.py:84] static cache update layer_idx: 28, start_batch: 10, end_batch: 20
DEBUG 10-14 21:17:39 StaticCacheLen.py:90] static cache update layer_idx: 28, update seq_length to 513
DEBUG 10-14 21:17:39 lpmodule.py:368] update for kv cache 10-20 kv_seq_len 513 in decoder_attn_batch
DEBUG 10-14 21:17:39 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:39 lpmodule.py:374] update past key value cost 0.002342 seconds
DEBUG 10-14 21:17:39 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 513, 128]), value shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:39 lpmodule.py:399] repeat qkv cost 0.000001 seconds
DEBUG 10-14 21:17:39 lpmodule.py:400] q shape torch.Size([10, 32, 1, 128]), k shape torch.Size([10, 8, 513, 128]), v shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:39 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:39 tutils.py:81] single group 0 real attn out cost 0.008378267288208008 s
DEBUG 10-14 21:17:39 lpllm.py:498] reset update attn
DEBUG 10-14 21:17:39 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:39 lpllm.py:1771] update state cost 5.459785461425781e-05 s
DEBUG 10-14 21:17:39 lpllm.py:1740] restore layer func cost 0.0019550323486328125 s
DEBUG 10-14 21:17:39 lpllm.py:511] restore layer cost 0.002948284149169922 s
DEBUG 10-14 21:17:39 lpllm.py:512] reset_next_layer_need: layer_attn_idx=29, layer_mlp_idx=28, j_loc=57
DEBUG 10-14 21:17:39 lpllm.py:1280] reset layer cost 0.0032711029052734375 s
DEBUG 10-14 21:17:39 lpllm.py:1281] have reset next layer layer_attn 28 layer_mlp 28 
DEBUG 10-14 21:17:39 lpllm.py:1287] j: 57 waiting the layer with layer_idx 29 before wait time 0.06514263153076172 s
INFO 10-14 21:17:39 client.py:117] confirm_model_loaded: Mixtral-8x7B, a46bb1d3-a05b-4bc9-846e-df4201a964e0
DEBUG 10-14 21:17:39 tutils.py:81] single group 1 real attn out cost 0.003228425979614258 s
DEBUG 10-14 21:17:39 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:39 tutils.py:81] single group 2 real attn out cost 0.003082275390625 s
DEBUG 10-14 21:17:39 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:39 tutils.py:81] single group 3 real attn out cost 0.0038166046142578125 s
DEBUG 10-14 21:17:39 tutils.py:88] dot attn help cost 0.025691 seconds
DEBUG 10-14 21:17:39 lpmodule.py:433] dot attn cost 0.025829 seconds
DEBUG 10-14 21:17:39 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.0005667209625244141 s
DEBUG 10-14 21:17:39 lpllm.py:2280] CPU attn cost 0.029418 seconds if batch True
DEBUG 10-14 21:17:39 lpllm.py:2289] deal attn result cost 0.000005 seconds
DEBUG 10-14 21:17:39 lpllm.py:2291] CPU compute cost 0.030275 seconds
DEBUG 10-14 21:17:39 lpllm.py:2309] free cost 0.000181 seconds
INFO 10-14 21:17:39 client.py:125] Model loaded
DEBUG 10-14 21:17:39 lpllm.py:1291] j: load cost 0.29088377952575684 s waiting cost 0.22567152976989746 s
DEBUG 10-14 21:17:39 lpllm.py:1200] 
DEBUG 10-14 21:17:39 lpllm.py:1200] one decoder loop j: 58 cur_layer_idx: 29
DEBUG 10-14 21:17:39 lpllm.py:1205] start load next layer cur_layer_idx: 30
DEBUG 10-14 21:17:39 lpllm.py:1801] cuda memory handles: {1: <capsule object NULL at 0x7fdf3c6da010>} {1: b'\xe0\xaa29\x00\x00\x00\x00\xe8&\x1d\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\x86\x00\x00\x00\xff\x00\x000\x00\x00\x00\x00\x00\x00\x00)\xd8\xd5\xc1\xa4\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-14 21:17:39 client.py:72] load_into_gpu: Mixtral-8x7B, d9a8b968-7393-44e7-ba2e-69615e4915df
INFO 10-14 21:17:39 client.py:113] Model loaded: Mixtral-8x7B, d9a8b968-7393-44e7-ba2e-69615e4915df
DEBUG 10-14 21:17:39 lpllm.py:1207] start decoder qkv layer_attn 29 layer_mlp 28
DEBUG 10-14 21:17:39 lpmodule.py:526] hidden_states.shape torch.Size([10, 1, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:39 lpmodule.py:541] kv_seq_len 1 past_key_value 512
DEBUG 10-14 21:17:39 lpmodule.py:545] inv_freq device cpu
DEBUG 10-14 21:17:39 lpmodule.py:546] kv_seq_len 513 position_ids torch.Size([10, 1]) value tensor([512])
DEBUG 10-14 21:17:39 lpmodule.py:551] query_states device cuda:1
DEBUG 10-14 21:17:39 lpmodule.py:552] key_states device cuda:1
DEBUG 10-14 21:17:39 lpmodule.py:553] value_states device cuda:1
DEBUG 10-14 21:17:39 lpmodule.py:554] position_ids device cpu
DEBUG 10-14 21:17:39 lpmodule.py:555] cos device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:39 lpmodule.py:556] sin device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:39 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:39 lpmodule.py:162] call mlp prepare
DEBUG 10-14 21:17:39 lpmodule.py:163] layer idx 28
DEBUG 10-14 21:17:39 lpmodule.py:123] Tokens per expert: [1, 0, 1, 9, 0, 1, 3, 5]
DEBUG 10-14 21:17:39 lpmodule.py:124] Number of active experts: 6
DEBUG 10-14 21:17:39 lpmodule.py:195] call mlp post
DEBUG 10-14 21:17:39 lpllm.py:2262] GPU2CPU move cost 0.000480 seconds
DEBUG 10-14 21:17:39 lpmodule.py:340] layer_idx 29 attn layer_idx: 29
DEBUG 10-14 21:17:39 lpmodule.py:352] get seq_length 512 in decoder_attn_batch
WARNING 10-14 21:17:39 lpmodule.py:355] generally decoder_attn_batch called with q_len == 1
DEBUG 10-14 21:17:39 lpmodule.py:364] decoder_attn_batch update batch_dim 0-10
DEBUG 10-14 21:17:39 StaticCacheLen.py:84] static cache update layer_idx: 29, start_batch: 0, end_batch: 10
DEBUG 10-14 21:17:39 lpmodule.py:368] update for kv cache 0-10 kv_seq_len 513 in decoder_attn_batch
DEBUG 10-14 21:17:39 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:39 lpmodule.py:374] update past key value cost 0.002676 seconds
DEBUG 10-14 21:17:39 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 513, 128]), value shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:39 lpmodule.py:399] repeat qkv cost 0.000001 seconds
DEBUG 10-14 21:17:39 lpmodule.py:400] q shape torch.Size([10, 32, 1, 128]), k shape torch.Size([10, 8, 513, 128]), v shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:39 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:39 lpllm.py:503] reset update experts
DEBUG 10-14 21:17:39 lpllm.py:1771] update state cost 2.5510787963867188e-05 s
DEBUG 10-14 21:17:39 tutils.py:81] single group 0 real attn out cost 0.005076885223388672 s
DEBUG 10-14 21:17:39 lpllm.py:1740] restore layer func cost 0.0020058155059814453 s
DEBUG 10-14 21:17:39 lpllm.py:511] restore layer cost 0.002359151840209961 s
DEBUG 10-14 21:17:39 lpllm.py:512] reset_next_layer_need: layer_attn_idx=29, layer_mlp_idx=29, j_loc=58
DEBUG 10-14 21:17:39 lpllm.py:1280] reset layer cost 0.002488374710083008 s
DEBUG 10-14 21:17:39 lpllm.py:1281] have reset next layer layer_attn 29 layer_mlp 28 
DEBUG 10-14 21:17:39 lpllm.py:1200] 
DEBUG 10-14 21:17:39 lpllm.py:1200] one decoder loop j: 59 cur_layer_idx: 29
DEBUG 10-14 21:17:39 lpllm.py:1207] start decoder qkv layer_attn 29 layer_mlp 29
DEBUG 10-14 21:17:39 lpmodule.py:526] hidden_states.shape torch.Size([10, 1, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:39 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:39 lpmodule.py:541] kv_seq_len 1 past_key_value 512
DEBUG 10-14 21:17:39 lpmodule.py:545] inv_freq device cpu
DEBUG 10-14 21:17:39 lpmodule.py:546] kv_seq_len 513 position_ids torch.Size([10, 1]) value tensor([512])
DEBUG 10-14 21:17:39 lpmodule.py:551] query_states device cuda:1
DEBUG 10-14 21:17:39 lpmodule.py:552] key_states device cuda:1
DEBUG 10-14 21:17:39 lpmodule.py:553] value_states device cuda:1
DEBUG 10-14 21:17:39 lpmodule.py:554] position_ids device cpu
DEBUG 10-14 21:17:39 lpmodule.py:555] cos device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:39 lpmodule.py:556] sin device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:39 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:39 tutils.py:81] single group 1 real attn out cost 0.003131389617919922 s
DEBUG 10-14 21:17:39 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:39 tutils.py:81] single group 2 real attn out cost 0.003521442413330078 s
DEBUG 10-14 21:17:39 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:39 tutils.py:81] single group 3 real attn out cost 0.002965211868286133 s
DEBUG 10-14 21:17:39 tutils.py:88] dot attn help cost 0.021945 seconds
DEBUG 10-14 21:17:39 lpmodule.py:433] dot attn cost 0.022076 seconds
DEBUG 10-14 21:17:39 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.0003681182861328125 s
DEBUG 10-14 21:17:39 lpllm.py:2280] CPU attn cost 0.025988 seconds if batch True
DEBUG 10-14 21:17:39 lpllm.py:2289] deal attn result cost 0.000004 seconds
DEBUG 10-14 21:17:39 lpllm.py:2291] CPU compute cost 0.027080 seconds
DEBUG 10-14 21:17:39 lpllm.py:2309] free cost 0.000198 seconds
DEBUG 10-14 21:17:39 lpmodule.py:162] call mlp prepare
DEBUG 10-14 21:17:39 lpmodule.py:163] layer idx 29
DEBUG 10-14 21:17:39 lpmodule.py:123] Tokens per expert: [5, 5, 1, 2, 0, 3, 2, 2]
DEBUG 10-14 21:17:39 lpmodule.py:124] Number of active experts: 7
DEBUG 10-14 21:17:39 lpmodule.py:195] call mlp post
DEBUG 10-14 21:17:39 lpllm.py:2262] GPU2CPU move cost 0.000302 seconds
DEBUG 10-14 21:17:39 lpmodule.py:340] layer_idx 29 attn layer_idx: 29
DEBUG 10-14 21:17:39 lpmodule.py:352] get seq_length 512 in decoder_attn_batch
WARNING 10-14 21:17:39 lpmodule.py:355] generally decoder_attn_batch called with q_len == 1
DEBUG 10-14 21:17:39 lpmodule.py:364] decoder_attn_batch update batch_dim 10-20
DEBUG 10-14 21:17:39 StaticCacheLen.py:84] static cache update layer_idx: 29, start_batch: 10, end_batch: 20
DEBUG 10-14 21:17:39 StaticCacheLen.py:90] static cache update layer_idx: 29, update seq_length to 513
DEBUG 10-14 21:17:39 lpmodule.py:368] update for kv cache 10-20 kv_seq_len 513 in decoder_attn_batch
DEBUG 10-14 21:17:39 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:39 lpmodule.py:374] update past key value cost 0.002310 seconds
DEBUG 10-14 21:17:39 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 513, 128]), value shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:39 lpmodule.py:399] repeat qkv cost 0.000001 seconds
DEBUG 10-14 21:17:39 lpmodule.py:400] q shape torch.Size([10, 32, 1, 128]), k shape torch.Size([10, 8, 513, 128]), v shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:39 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:39 lpllm.py:498] reset update attn
DEBUG 10-14 21:17:39 lpllm.py:1771] update state cost 4.887580871582031e-05 s
DEBUG 10-14 21:17:39 tutils.py:81] single group 0 real attn out cost 0.0062541961669921875 s
DEBUG 10-14 21:17:39 lpllm.py:1740] restore layer func cost 0.0012254714965820312 s
DEBUG 10-14 21:17:39 lpllm.py:511] restore layer cost 0.0016868114471435547 s
DEBUG 10-14 21:17:39 lpllm.py:512] reset_next_layer_need: layer_attn_idx=30, layer_mlp_idx=29, j_loc=59
DEBUG 10-14 21:17:39 lpllm.py:1280] reset layer cost 0.0018906593322753906 s
DEBUG 10-14 21:17:39 lpllm.py:1281] have reset next layer layer_attn 29 layer_mlp 29 
DEBUG 10-14 21:17:39 lpllm.py:1287] j: 59 waiting the layer with layer_idx 30 before wait time 0.06499242782592773 s
INFO 10-14 21:17:39 client.py:117] confirm_model_loaded: Mixtral-8x7B, d9a8b968-7393-44e7-ba2e-69615e4915df
DEBUG 10-14 21:17:39 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:39 tutils.py:81] single group 1 real attn out cost 0.002710103988647461 s
DEBUG 10-14 21:17:39 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:39 tutils.py:81] single group 2 real attn out cost 0.0027730464935302734 s
DEBUG 10-14 21:17:39 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:39 tutils.py:81] single group 3 real attn out cost 0.0030388832092285156 s
DEBUG 10-14 21:17:39 tutils.py:88] dot attn help cost 0.021726 seconds
DEBUG 10-14 21:17:39 lpmodule.py:433] dot attn cost 0.021926 seconds
DEBUG 10-14 21:17:39 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.000469207763671875 s
DEBUG 10-14 21:17:39 lpllm.py:2280] CPU attn cost 0.025368 seconds if batch True
DEBUG 10-14 21:17:39 lpllm.py:2289] deal attn result cost 0.000004 seconds
DEBUG 10-14 21:17:39 lpllm.py:2291] CPU compute cost 0.026079 seconds
DEBUG 10-14 21:17:39 lpllm.py:2309] free cost 0.000178 seconds
INFO 10-14 21:17:40 client.py:125] Model loaded
DEBUG 10-14 21:17:40 lpllm.py:1291] j: load cost 0.2923123836517334 s waiting cost 0.22727131843566895 s
DEBUG 10-14 21:17:40 lpllm.py:1200] 
DEBUG 10-14 21:17:40 lpllm.py:1200] one decoder loop j: 60 cur_layer_idx: 30
DEBUG 10-14 21:17:40 lpllm.py:1205] start load next layer cur_layer_idx: 31
DEBUG 10-14 21:17:40 lpllm.py:1801] cuda memory handles: {1: <capsule object NULL at 0x7fdf3c6d9ec0>} {1: b'\xe0\xaa29\x00\x00\x00\x00\xe8&\x1d\x00\x00\x00\x00\x00\x00@\x01\xad\x00\x00\x00\x00\x14\xd0\n\x00\x00\x00\x00\x00\x00\x86\x00\x00\x00\xff\x00\x00.\x00\x00\x00\x00\x00\x00\x00)\xd8\xd5\xc1\xa0\x00\x00\\\x00\x00\x00\x00\x00\x00\x00\x00'}
DEBUG 10-14 21:17:40 client.py:72] load_into_gpu: Mixtral-8x7B, 3f2907be-42a0-4b6d-8920-97289d1b2822
INFO 10-14 21:17:40 client.py:113] Model loaded: Mixtral-8x7B, 3f2907be-42a0-4b6d-8920-97289d1b2822
DEBUG 10-14 21:17:40 lpllm.py:1207] start decoder qkv layer_attn 30 layer_mlp 29
DEBUG 10-14 21:17:40 lpmodule.py:526] hidden_states.shape torch.Size([10, 1, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:40 lpmodule.py:541] kv_seq_len 1 past_key_value 512
DEBUG 10-14 21:17:40 lpmodule.py:545] inv_freq device cpu
DEBUG 10-14 21:17:40 lpmodule.py:546] kv_seq_len 513 position_ids torch.Size([10, 1]) value tensor([512])
DEBUG 10-14 21:17:40 lpmodule.py:551] query_states device cuda:1
DEBUG 10-14 21:17:40 lpmodule.py:552] key_states device cuda:1
DEBUG 10-14 21:17:40 lpmodule.py:553] value_states device cuda:1
DEBUG 10-14 21:17:40 lpmodule.py:554] position_ids device cpu
DEBUG 10-14 21:17:40 lpmodule.py:555] cos device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:40 lpmodule.py:556] sin device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:40 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:40 lpmodule.py:162] call mlp prepare
DEBUG 10-14 21:17:40 lpmodule.py:163] layer idx 29
DEBUG 10-14 21:17:40 lpmodule.py:123] Tokens per expert: [0, 0, 0, 0, 0, 10, 9, 1]
DEBUG 10-14 21:17:40 lpmodule.py:124] Number of active experts: 3
DEBUG 10-14 21:17:40 lpmodule.py:195] call mlp post
DEBUG 10-14 21:17:40 lpllm.py:2262] GPU2CPU move cost 0.000544 seconds
DEBUG 10-14 21:17:40 lpmodule.py:340] layer_idx 30 attn layer_idx: 30
DEBUG 10-14 21:17:40 lpmodule.py:352] get seq_length 512 in decoder_attn_batch
WARNING 10-14 21:17:40 lpmodule.py:355] generally decoder_attn_batch called with q_len == 1
DEBUG 10-14 21:17:40 lpmodule.py:364] decoder_attn_batch update batch_dim 0-10
DEBUG 10-14 21:17:40 StaticCacheLen.py:84] static cache update layer_idx: 30, start_batch: 0, end_batch: 10
DEBUG 10-14 21:17:40 lpmodule.py:368] update for kv cache 0-10 kv_seq_len 513 in decoder_attn_batch
DEBUG 10-14 21:17:40 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:40 lpmodule.py:374] update past key value cost 0.002479 seconds
DEBUG 10-14 21:17:40 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 513, 128]), value shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:40 lpmodule.py:399] repeat qkv cost 0.000001 seconds
DEBUG 10-14 21:17:40 lpmodule.py:400] q shape torch.Size([10, 32, 1, 128]), k shape torch.Size([10, 8, 513, 128]), v shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:40 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:40 lpllm.py:503] reset update experts
DEBUG 10-14 21:17:40 lpllm.py:1771] update state cost 2.6702880859375e-05 s
DEBUG 10-14 21:17:40 lpllm.py:1740] restore layer func cost 0.001547098159790039 s
DEBUG 10-14 21:17:40 lpllm.py:511] restore layer cost 0.0018646717071533203 s
DEBUG 10-14 21:17:40 lpllm.py:512] reset_next_layer_need: layer_attn_idx=30, layer_mlp_idx=30, j_loc=60
DEBUG 10-14 21:17:40 lpllm.py:1280] reset layer cost 0.001993417739868164 s
DEBUG 10-14 21:17:40 lpllm.py:1281] have reset next layer layer_attn 30 layer_mlp 29 
DEBUG 10-14 21:17:40 lpllm.py:1200] 
DEBUG 10-14 21:17:40 lpllm.py:1200] one decoder loop j: 61 cur_layer_idx: 30
DEBUG 10-14 21:17:40 lpllm.py:1207] start decoder qkv layer_attn 30 layer_mlp 30
DEBUG 10-14 21:17:40 lpmodule.py:526] hidden_states.shape torch.Size([10, 1, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:40 tutils.py:81] single group 0 real attn out cost 0.005354166030883789 s
DEBUG 10-14 21:17:40 lpmodule.py:541] kv_seq_len 1 past_key_value 512
DEBUG 10-14 21:17:40 lpmodule.py:545] inv_freq device cpu
DEBUG 10-14 21:17:40 lpmodule.py:546] kv_seq_len 513 position_ids torch.Size([10, 1]) value tensor([512])
DEBUG 10-14 21:17:40 lpmodule.py:551] query_states device cuda:1
DEBUG 10-14 21:17:40 lpmodule.py:552] key_states device cuda:1
DEBUG 10-14 21:17:40 lpmodule.py:553] value_states device cuda:1
DEBUG 10-14 21:17:40 lpmodule.py:554] position_ids device cpu
DEBUG 10-14 21:17:40 lpmodule.py:555] cos device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:40 lpmodule.py:556] sin device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:40 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:40 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:40 tutils.py:81] single group 1 real attn out cost 0.0031571388244628906 s
DEBUG 10-14 21:17:40 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:40 tutils.py:81] single group 2 real attn out cost 0.004163980484008789 s
DEBUG 10-14 21:17:40 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:40 tutils.py:81] single group 3 real attn out cost 0.00321197509765625 s
DEBUG 10-14 21:17:40 tutils.py:88] dot attn help cost 0.022960 seconds
DEBUG 10-14 21:17:40 lpmodule.py:433] dot attn cost 0.023104 seconds
DEBUG 10-14 21:17:40 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.00036454200744628906 s
DEBUG 10-14 21:17:40 lpllm.py:2280] CPU attn cost 0.026716 seconds if batch True
DEBUG 10-14 21:17:40 lpllm.py:2289] deal attn result cost 0.000005 seconds
DEBUG 10-14 21:17:40 lpllm.py:2291] CPU compute cost 0.027742 seconds
DEBUG 10-14 21:17:40 lpllm.py:2309] free cost 0.000211 seconds
DEBUG 10-14 21:17:40 lpmodule.py:162] call mlp prepare
DEBUG 10-14 21:17:40 lpmodule.py:163] layer idx 30
DEBUG 10-14 21:17:40 lpmodule.py:123] Tokens per expert: [7, 0, 0, 0, 5, 5, 3, 0]
DEBUG 10-14 21:17:40 lpmodule.py:124] Number of active experts: 4
DEBUG 10-14 21:17:40 lpmodule.py:195] call mlp post
DEBUG 10-14 21:17:40 lpllm.py:2262] GPU2CPU move cost 0.000371 seconds
DEBUG 10-14 21:17:40 lpmodule.py:340] layer_idx 30 attn layer_idx: 30
DEBUG 10-14 21:17:40 lpmodule.py:352] get seq_length 512 in decoder_attn_batch
WARNING 10-14 21:17:40 lpmodule.py:355] generally decoder_attn_batch called with q_len == 1
DEBUG 10-14 21:17:40 lpmodule.py:364] decoder_attn_batch update batch_dim 10-20
DEBUG 10-14 21:17:40 StaticCacheLen.py:84] static cache update layer_idx: 30, start_batch: 10, end_batch: 20
DEBUG 10-14 21:17:40 StaticCacheLen.py:90] static cache update layer_idx: 30, update seq_length to 513
DEBUG 10-14 21:17:40 lpmodule.py:368] update for kv cache 10-20 kv_seq_len 513 in decoder_attn_batch
DEBUG 10-14 21:17:40 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:40 lpmodule.py:374] update past key value cost 0.002360 seconds
DEBUG 10-14 21:17:40 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 513, 128]), value shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:40 lpmodule.py:399] repeat qkv cost 0.000001 seconds
DEBUG 10-14 21:17:40 lpmodule.py:400] q shape torch.Size([10, 32, 1, 128]), k shape torch.Size([10, 8, 513, 128]), v shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:40 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:40 tutils.py:81] single group 0 real attn out cost 0.004716396331787109 s
DEBUG 10-14 21:17:40 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:40 lpllm.py:498] reset update attn
DEBUG 10-14 21:17:40 lpllm.py:1771] update state cost 5.0067901611328125e-05 s
DEBUG 10-14 21:17:40 lpllm.py:1740] restore layer func cost 0.0011508464813232422 s
DEBUG 10-14 21:17:40 lpllm.py:511] restore layer cost 0.0017647743225097656 s
DEBUG 10-14 21:17:40 lpllm.py:512] reset_next_layer_need: layer_attn_idx=31, layer_mlp_idx=30, j_loc=61
DEBUG 10-14 21:17:40 lpllm.py:1280] reset layer cost 0.0019185543060302734 s
DEBUG 10-14 21:17:40 lpllm.py:1281] have reset next layer layer_attn 30 layer_mlp 30 
DEBUG 10-14 21:17:40 lpllm.py:1287] j: 61 waiting the layer with layer_idx 31 before wait time 0.06804227828979492 s
INFO 10-14 21:17:40 client.py:117] confirm_model_loaded: Mixtral-8x7B, 3f2907be-42a0-4b6d-8920-97289d1b2822
DEBUG 10-14 21:17:40 tutils.py:81] single group 1 real attn out cost 0.0032951831817626953 s
DEBUG 10-14 21:17:40 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:40 tutils.py:81] single group 2 real attn out cost 0.0030515193939208984 s
DEBUG 10-14 21:17:40 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:40 tutils.py:81] single group 3 real attn out cost 0.0035524368286132812 s
DEBUG 10-14 21:17:40 tutils.py:88] dot attn help cost 0.020857 seconds
DEBUG 10-14 21:17:40 lpmodule.py:433] dot attn cost 0.021075 seconds
DEBUG 10-14 21:17:40 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.0008618831634521484 s
DEBUG 10-14 21:17:40 lpllm.py:2280] CPU attn cost 0.025007 seconds if batch True
DEBUG 10-14 21:17:40 lpllm.py:2289] deal attn result cost 0.000004 seconds
DEBUG 10-14 21:17:40 lpllm.py:2291] CPU compute cost 0.025765 seconds
DEBUG 10-14 21:17:40 lpllm.py:2309] free cost 0.000187 seconds
INFO 10-14 21:17:40 client.py:125] Model loaded
DEBUG 10-14 21:17:40 lpllm.py:1291] j: load cost 0.3053555488586426 s waiting cost 0.23727679252624512 s
DEBUG 10-14 21:17:40 lpllm.py:1200] 
DEBUG 10-14 21:17:40 lpllm.py:1200] one decoder loop j: 62 cur_layer_idx: 31
DEBUG 10-14 21:17:40 lpllm.py:1207] start decoder qkv layer_attn 31 layer_mlp 30
DEBUG 10-14 21:17:40 lpmodule.py:526] hidden_states.shape torch.Size([10, 1, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:40 lpmodule.py:541] kv_seq_len 1 past_key_value 512
DEBUG 10-14 21:17:40 lpmodule.py:545] inv_freq device cpu
DEBUG 10-14 21:17:40 lpmodule.py:546] kv_seq_len 513 position_ids torch.Size([10, 1]) value tensor([512])
DEBUG 10-14 21:17:40 lpmodule.py:551] query_states device cuda:1
DEBUG 10-14 21:17:40 lpmodule.py:552] key_states device cuda:1
DEBUG 10-14 21:17:40 lpmodule.py:553] value_states device cuda:1
DEBUG 10-14 21:17:40 lpmodule.py:554] position_ids device cpu
DEBUG 10-14 21:17:40 lpmodule.py:555] cos device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:40 lpmodule.py:556] sin device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:40 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:40 lpmodule.py:162] call mlp prepare
DEBUG 10-14 21:17:40 lpmodule.py:163] layer idx 30
DEBUG 10-14 21:17:40 lpmodule.py:123] Tokens per expert: [9, 0, 0, 0, 9, 1, 0, 1]
DEBUG 10-14 21:17:40 lpmodule.py:124] Number of active experts: 4
DEBUG 10-14 21:17:40 lpmodule.py:195] call mlp post
DEBUG 10-14 21:17:40 lpllm.py:2262] GPU2CPU move cost 0.000520 seconds
DEBUG 10-14 21:17:40 lpmodule.py:340] layer_idx 31 attn layer_idx: 31
DEBUG 10-14 21:17:40 lpmodule.py:352] get seq_length 512 in decoder_attn_batch
WARNING 10-14 21:17:40 lpmodule.py:355] generally decoder_attn_batch called with q_len == 1
DEBUG 10-14 21:17:40 lpmodule.py:364] decoder_attn_batch update batch_dim 0-10
DEBUG 10-14 21:17:40 StaticCacheLen.py:84] static cache update layer_idx: 31, start_batch: 0, end_batch: 10
DEBUG 10-14 21:17:40 lpmodule.py:368] update for kv cache 0-10 kv_seq_len 513 in decoder_attn_batch
DEBUG 10-14 21:17:40 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:40 lpmodule.py:374] update past key value cost 0.002540 seconds
DEBUG 10-14 21:17:40 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 513, 128]), value shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:40 lpmodule.py:399] repeat qkv cost 0.000001 seconds
DEBUG 10-14 21:17:40 lpmodule.py:400] q shape torch.Size([10, 32, 1, 128]), k shape torch.Size([10, 8, 513, 128]), v shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:40 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:40 lpllm.py:1281] have reset next layer layer_attn 31 layer_mlp 30 
DEBUG 10-14 21:17:40 lpllm.py:1200] 
DEBUG 10-14 21:17:40 lpllm.py:1200] one decoder loop j: 63 cur_layer_idx: 31
DEBUG 10-14 21:17:40 lpllm.py:1207] start decoder qkv layer_attn 31 layer_mlp 31
DEBUG 10-14 21:17:40 lpmodule.py:526] hidden_states.shape torch.Size([10, 1, 4096]) hidden_states.dtype torch.bfloat16 device cuda:1
DEBUG 10-14 21:17:40 lpmodule.py:541] kv_seq_len 1 past_key_value 512
DEBUG 10-14 21:17:40 lpmodule.py:545] inv_freq device cpu
DEBUG 10-14 21:17:40 lpmodule.py:546] kv_seq_len 513 position_ids torch.Size([10, 1]) value tensor([512])
DEBUG 10-14 21:17:40 lpmodule.py:551] query_states device cuda:1
DEBUG 10-14 21:17:40 lpmodule.py:552] key_states device cuda:1
DEBUG 10-14 21:17:40 lpmodule.py:553] value_states device cuda:1
DEBUG 10-14 21:17:40 lpmodule.py:554] position_ids device cpu
DEBUG 10-14 21:17:40 lpmodule.py:555] cos device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:40 lpmodule.py:556] sin device cuda:1 shape torch.Size([513, 128])
DEBUG 10-14 21:17:40 tutils.py:81] single group 0 real attn out cost 0.0054168701171875 s
DEBUG 10-14 21:17:40 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:40 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:40 tutils.py:81] single group 1 real attn out cost 0.003249645233154297 s
DEBUG 10-14 21:17:40 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:40 tutils.py:81] single group 2 real attn out cost 0.0031223297119140625 s
DEBUG 10-14 21:17:40 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:40 tutils.py:81] single group 3 real attn out cost 0.004507780075073242 s
DEBUG 10-14 21:17:40 tutils.py:88] dot attn help cost 0.022398 seconds
DEBUG 10-14 21:17:40 lpmodule.py:433] dot attn cost 0.022515 seconds
DEBUG 10-14 21:17:40 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.0003750324249267578 s
DEBUG 10-14 21:17:40 lpllm.py:2280] CPU attn cost 0.026215 seconds if batch True
DEBUG 10-14 21:17:40 lpllm.py:2289] deal attn result cost 0.000004 seconds
DEBUG 10-14 21:17:40 lpllm.py:2291] CPU compute cost 0.027157 seconds
DEBUG 10-14 21:17:40 lpllm.py:2309] free cost 0.000289 seconds
DEBUG 10-14 21:17:40 lpmodule.py:162] call mlp prepare
DEBUG 10-14 21:17:40 lpmodule.py:163] layer idx 30
DEBUG 10-14 21:17:40 lpmodule.py:123] Tokens per expert: [7, 0, 0, 0, 5, 5, 3, 0]
DEBUG 10-14 21:17:40 lpmodule.py:124] Number of active experts: 4
DEBUG 10-14 21:17:40 lpmodule.py:195] call mlp post
DEBUG 10-14 21:17:40 lpllm.py:2262] GPU2CPU move cost 0.000390 seconds
DEBUG 10-14 21:17:40 lpmodule.py:340] layer_idx 31 attn layer_idx: 31
DEBUG 10-14 21:17:40 lpmodule.py:352] get seq_length 512 in decoder_attn_batch
WARNING 10-14 21:17:40 lpmodule.py:355] generally decoder_attn_batch called with q_len == 1
DEBUG 10-14 21:17:40 lpmodule.py:364] decoder_attn_batch update batch_dim 10-20
DEBUG 10-14 21:17:40 StaticCacheLen.py:84] static cache update layer_idx: 31, start_batch: 10, end_batch: 20
DEBUG 10-14 21:17:40 StaticCacheLen.py:90] static cache update layer_idx: 31, update seq_length to 513
DEBUG 10-14 21:17:40 lpmodule.py:368] update for kv cache 10-20 kv_seq_len 513 in decoder_attn_batch
DEBUG 10-14 21:17:40 lpmodule.py:373] key_states device cpu
DEBUG 10-14 21:17:40 lpmodule.py:374] update past key value cost 0.002267 seconds
DEBUG 10-14 21:17:40 lpmodule.py:389] before repeat qkv key shape torch.Size([10, 8, 513, 128]), value shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:40 lpmodule.py:399] repeat qkv cost 0.000001 seconds
DEBUG 10-14 21:17:40 lpmodule.py:400] q shape torch.Size([10, 32, 1, 128]), k shape torch.Size([10, 8, 513, 128]), v shape torch.Size([10, 8, 513, 128])
DEBUG 10-14 21:17:40 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:40 tutils.py:81] single group 0 real attn out cost 0.005405426025390625 s
DEBUG 10-14 21:17:40 lpllm.py:1281] have reset next layer layer_attn 31 layer_mlp 31 
DEBUG 10-14 21:17:40 lpllm.py:2081] waiting out_queue
DEBUG 10-14 21:17:40 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:40 tutils.py:81] single group 1 real attn out cost 0.003103494644165039 s
DEBUG 10-14 21:17:40 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:40 tutils.py:81] single group 2 real attn out cost 0.003496885299682617 s
DEBUG 10-14 21:17:40 tutils.py:69] torch.Size([10, 8, 1, 128])
DEBUG 10-14 21:17:40 tutils.py:81] single group 3 real attn out cost 0.003096342086791992 s
DEBUG 10-14 21:17:40 tutils.py:88] dot attn help cost 0.021519 seconds
DEBUG 10-14 21:17:40 lpmodule.py:433] dot attn cost 0.021653 seconds
DEBUG 10-14 21:17:40 lpmodule.py:437] layer self attn q_proj device cuda:1
time cost move to cuda:1 0.0003750324249267578 s
DEBUG 10-14 21:17:40 lpllm.py:2280] CPU attn cost 0.025007 seconds if batch True
DEBUG 10-14 21:17:40 lpllm.py:2289] deal attn result cost 0.000004 seconds
DEBUG 10-14 21:17:40 lpllm.py:2291] CPU compute cost 0.025717 seconds
DEBUG 10-14 21:17:40 lpllm.py:2309] free cost 0.000184 seconds
DEBUG 10-14 21:17:40 lpllm.py:1305] start decoder mlp last layer
DEBUG 10-14 21:17:40 lpmodule.py:227] layer idx 0
DEBUG 10-14 21:17:40 lpllm.py:789] decoders non batch cost 9.729030132293701 s
DEBUG 10-14 21:17:40 lpllm.py:670] last_hidden_states shape torch.Size([10, 1, 4096])
DEBUG 10-14 21:17:40 lpllm.py:689] next_token_logits shape torch.Size([10, 32000])
DEBUG 10-14 21:17:40 lpllm.py:793] get_next_token1 non batch cost 0.001255035400390625 s
DEBUG 10-14 21:17:40 lpllm.py:670] last_hidden_states shape torch.Size([10, 1, 4096])
DEBUG 10-14 21:17:40 lpllm.py:689] next_token_logits shape torch.Size([10, 32000])
DEBUG 10-14 21:17:40 lpllm.py:795] get_next_token2 non batch cost 0.002275705337524414 s
DEBUG 10-14 21:17:40 lpllm.py:802] input_ids shape torch.Size([20, 512]) generted_new_tokens.shape torch.Size([20, 1])
DEBUG 10-14 21:17:40 lpllm.py:802] input_ids shape torch.Size([20, 512]) generted_new_tokens.shape torch.Size([20, 1])
INFO 10-14 21:17:40 lpllm.py:830] Generation completed. Final shape: torch.Size([20, 513]) generated_new_tokens torch.Size([20, 1])
generate cost 21.691009521484375 s
torch.Size([20, 513])
batch size 20 seq_len 512 generated_tokens generated_new_tokens len 1
Token throughput: 0.92 tokens/s
INFO 10-14 21:17:40 lpllm.py:1656] Stopping LPLLM...
INFO 10-14 21:17:40 lpllm.py:1929] Stopping AttnManager...
DEBUG 10-14 21:17:41 lpllm.py:2498] CPU2GPU thread exiting
DEBUG 10-14 21:17:41 lpllm.py:2385] CPU compute thread exiting
DEBUG 10-14 21:17:41 lpllm.py:2225] GPU2CPU thread exiting
INFO 10-14 21:17:41 lpllm.py:1943] AttnManager stopped
INFO 10-14 21:17:41 lpllm.py:1662] AttnManager stopped
WARNING 10-14 21:17:41 lpllm.py:375] Force stopping all threads...
INFO 10-14 21:17:41 lpllm.py:1929] Stopping AttnManager...
INFO 10-14 21:17:41 lpllm.py:1943] AttnManager stopped
INFO 10-14 21:17:41 lpllm.py:1673] CUDA memory freed
INFO 10-14 21:17:41 lpllm.py:1677] LPLLM stopped successfully
Collecting data...
Generating '/tmp/nsys-report-e74d.qdstrm'
[1/1] [0%                          ] report6.nsys-rep[1/1] [0%                          ] report6.nsys-rep[1/1] [6%                          ] report6.nsys-rep[1/1] [10%                         ] report6.nsys-rep[1/1] [14%                         ] report6.nsys-rep[1/1] [==18%                       ] report6.nsys-rep[1/1] [===24%                      ] report6.nsys-rep[1/1] [=======36%                  ] report6.nsys-rep[1/1] [==========48%               ] report6.nsys-rep[1/1] [=============60%            ] report6.nsys-rep[1/1] [===============65%          ] report6.nsys-rep[1/1] [========================100%] report6.nsys-rep[1/1] [========================100%] report6.nsys-rep
Generated:
	/mnt/zhengcf3/lpllm/lpllm/tests/report6.nsys-rep
