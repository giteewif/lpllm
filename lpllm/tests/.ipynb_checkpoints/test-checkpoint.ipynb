{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7a449c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "batch_size = 90\n",
    "device=\"cuda:1\"\n",
    "query_states_c = torch.randn(batch_size, 32, 1, 128, dtype=torch.bfloat16, device=\"cpu\")\n",
    "query_states = torch.randn(batch_size, 32, 512, 128, dtype=torch.bfloat16, device=\"cpu\").pin_memory()\n",
    "# 480MB, *4 = 1920MB\n",
    "key_states = torch.randn(batch_size, 8, 512, 128, dtype=torch.bfloat16, device=\"cpu\").pin_memory()\n",
    "value_states = torch.randn(batch_size, 8, 512, 128, dtype=torch.bfloat16, device=\"cpu\").pin_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f944ef40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pin and move q cost 0.003097057342529297 s\n",
      "move q cost 0.031256675720214844 s\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "query_states_cgpu = query_states_c.pin_memory()\n",
    "qgpu_c = query_states_cgpu.to(device=\"cuda:1\")\n",
    "print(f\"pin and move q cost {time.time() - time_start} s\")\n",
    "time_start = time.time()\n",
    "qgpu = query_states.to(device=\"cuda:1\")\n",
    "print(f\"move q cost {time.time() - time_start} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "312ec9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time cost to cuda:1 0.015724897384643555 s\n"
     ]
    }
   ],
   "source": [
    "# 960MB*3 = 2880MB\n",
    "time_start = time.time()\n",
    "for i in range(1):\n",
    "    gpu_key_states = key_states.to(device=\"cuda:1\")\n",
    "    gpu_value_states = value_states.to(device=\"cuda:1\")\n",
    "print(f\"time cost to cuda:1 {time.time() - time_start} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9f5a9a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import math\n",
    "def scaled_dot_product_attention_with_pinned_memory(\n",
    "    query: torch.Tensor,\n",
    "    key: torch.Tensor, \n",
    "    value: torch.Tensor,\n",
    "    output_tensor: Optional[torch.Tensor] = None,\n",
    "    attn_mask: Optional[torch.Tensor] = None,\n",
    "    dropout_p: float = 0.0,\n",
    "    is_causal: bool = False,\n",
    "    scale: Optional[float] = None,\n",
    "    enable_gqa: bool = False\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    修改版的 scaled_dot_product_attention，支持将结果直接写入预分配的 pinned memory。\n",
    "    \n",
    "    Args:\n",
    "        query: Query tensor of shape (..., L, E)\n",
    "        key: Key tensor of shape (..., S, E) \n",
    "        value: Value tensor of shape (..., S, Ev)\n",
    "        output_tensor: 预分配的 pinned memory tensor，用于存储结果。如果为 None，则创建新的 tensor\n",
    "        attn_mask: 可选的注意力掩码\n",
    "        dropout_p: Dropout 概率\n",
    "        is_causal: 是否使用因果掩码\n",
    "        scale: 缩放因子，如果为 None 则使用 1/sqrt(E)\n",
    "        enable_gqa: 是否启用分组查询注意力\n",
    "        \n",
    "    Returns:\n",
    "        注意力输出 tensor，如果提供了 output_tensor 则返回该 tensor\n",
    "    \"\"\"\n",
    "    L, S = query.size(-2), key.size(-2)\n",
    "    scale_factor = 1 / math.sqrt(query.size(-1)) if scale is None else scale\n",
    "    \n",
    "    # 计算输出形状\n",
    "    output_shape = query.shape[:-1] + (value.shape[-1],)\n",
    "    \n",
    "    # 如果提供了预分配的输出 tensor，验证其形状和类型\n",
    "    if output_tensor is not None:\n",
    "        # 注意：对于 GPU tensor，即使原始是 pinned memory，移动到 GPU 后也不再是 pinned\n",
    "        if output_tensor.shape != output_shape:\n",
    "            raise ValueError(f\"output_tensor 形状 {output_tensor.shape} 与期望形状 {output_shape} 不匹配\")\n",
    "        if output_tensor.dtype != query.dtype:\n",
    "            raise ValueError(f\"output_tensor 数据类型 {output_tensor.dtype} 与 query 数据类型 {query.dtype} 不匹配\")\n",
    "        if output_tensor.device != query.device:\n",
    "            raise ValueError(f\"output_tensor 设备 {output_tensor.device} 与 query 设备 {query.device} 不匹配\")\n",
    "    \n",
    "    # 创建注意力偏置\n",
    "    attn_bias = torch.zeros(L, S, dtype=query.dtype, device=query.device)\n",
    "    \n",
    "    if is_causal:\n",
    "        assert attn_mask is None, \"is_causal 和 attn_mask 不能同时使用\"\n",
    "        temp_mask = torch.ones(L, S, dtype=torch.bool, device=query.device).tril(diagonal=0)\n",
    "        attn_bias.masked_fill_(temp_mask.logical_not(), float(\"-inf\"))\n",
    "        attn_bias = attn_bias.to(query.dtype)\n",
    "\n",
    "    if attn_mask is not None:\n",
    "        if attn_mask.dtype == torch.bool:\n",
    "            attn_bias.masked_fill_(attn_mask.logical_not(), float(\"-inf\"))\n",
    "        else:\n",
    "            attn_bias += attn_mask\n",
    "\n",
    "    # 处理分组查询注意力\n",
    "    if enable_gqa:\n",
    "        key = key.repeat_interleave(query.size(-3)//key.size(-3), -3)\n",
    "        value = value.repeat_interleave(query.size(-3)//value.size(-3), -3)\n",
    "\n",
    "    # 计算注意力权重\n",
    "    attn_weight = query @ key.transpose(-2, -1) * scale_factor\n",
    "    attn_weight += attn_bias\n",
    "    attn_weight = torch.softmax(attn_weight, dim=-1)\n",
    "    \n",
    "    # 应用 dropout\n",
    "    if dropout_p > 0.0:\n",
    "        attn_weight = torch.dropout(attn_weight, dropout_p, train=True)\n",
    "    \n",
    "    # 计算最终输出\n",
    "    if output_tensor is not None:\n",
    "        # 直接写入预分配的 pinned memory\n",
    "        torch.matmul(attn_weight, value, out=output_tensor)\n",
    "        return output_tensor\n",
    "    else:\n",
    "        # 创建新的 tensor\n",
    "        return attn_weight @ value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4feb90b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key_states cpu\n",
      "time cost repeat kv 0.11120891571044922 s\n",
      "time cost check nan 0.01766490936279297 s\n",
      "dot attn cost 0.182348 seconds\n",
      "dot attn pin cost 0.702972 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hidden_states_cache = torch.zeros(batch_size, 8, 4, 512, 128, dtype=torch.bfloat16, device=\"cpu\").pin_memory()\n",
    "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n",
    "    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n",
    "    \"\"\"\n",
    "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
    "    if n_rep == 1: \n",
    "        return hidden_states\n",
    "    # 改为将hidden_states 复制到 hidden_states_cache 上\n",
    "    # for i in range(hidden_states_cache.shape[2]):\n",
    "        # 这行代码的作用是：将 hidden_states 的内容复制到 hidden_states_cache 的第 i 个位置（第三个维度）。\n",
    "        # 也就是说，对于 hidden_states_cache 的每个“副本”维度 i，都用同样的 hidden_states 填充，实现了在第3维上重复 hidden_states 的效果。\n",
    "        # hidden_states_cache[:, :, i, :, :].index_copy_(\n",
    "        #     2, torch.arange(hidden_states.shape[2]), hidden_states\n",
    "        # )\n",
    "        # hidden_states_cache[:, :, i, :, :] = hidden_states\n",
    "        # hidden_states_cache[:, :, i, :, :].index_copy_(3, torch.arange(hidden_states.shape[3]), hidden_states)\n",
    "    # hidden_states_cache = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
    "    hidden_states_cache = hidden_states.unsqueeze(2).expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
    "    return hidden_states_cache.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n",
    "print(f\"key_states {key_states.device}\")\n",
    "time_start = time.time()\n",
    "new_key_states = repeat_kv(key_states, 4)\n",
    "new_value_states = repeat_kv(value_states, 4)\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "print(f\"time cost repeat kv {time.time() - time_start} s\")\n",
    "\n",
    "time_start = time.time()\n",
    "if torch.isnan(key_states).any():\n",
    "    print(\"NaN detected in key_states\")\n",
    "if torch.isnan(value_states).any():\n",
    "    print(\"NaN detected in value_states\")\n",
    "print(f\"time cost check nan {time.time() - time_start} s\")\n",
    "attn_output_c = torch.empty_like(query_states, device=\"cpu\", dtype=torch.bfloat16, pin_memory=True)\n",
    "def func_here():\n",
    "    time_start = time.time()\n",
    "    attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
    "        query_states,\n",
    "        new_key_states,\n",
    "        new_value_states,\n",
    "        attn_mask=None,\n",
    "        dropout_p=0.0,\n",
    "        # The q_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create a causal mask in case q_len == 1.\n",
    "        is_causal=True,\n",
    "    )\n",
    "    # print(attn_output[0])\n",
    "    print(f\"dot attn cost {time.time()-time_start:.6f} seconds\")\n",
    "def func_pin():\n",
    "    time_start = time.time()\n",
    "    attn_output = scaled_dot_product_attention_with_pinned_memory(\n",
    "        query_states,\n",
    "        new_key_states,\n",
    "        new_value_states,\n",
    "        output_tensor=attn_output_c,\n",
    "        attn_mask=None,\n",
    "        dropout_p=0.0,\n",
    "        # The q_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create a causal mask in case q_len == 1.\n",
    "        is_causal=True,\n",
    "    )\n",
    "    # print(attn_output[0])\n",
    "    print(f\"dot attn pin cost {time.time()-time_start:.6f} seconds\")\n",
    "func_here()\n",
    "func_pin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "966c9f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time cost to cuda:1 0.2299492359161377 s\n"
     ]
    }
   ],
   "source": [
    "# 测量移动整层layer需要的耗时\n",
    "import torch\n",
    "import time\n",
    "batch_size = 360\n",
    "device=\"cuda:1\"\n",
    "# hd = 16384\n",
    "# hdd= 6144\n",
    "hd = 14336\n",
    "hdd= 4096\n",
    "w0 = torch.randn(hd, hdd, dtype=torch.bfloat16, device=\"cpu\").pin_memory()\n",
    "w1 = torch.randn(hd, hdd, dtype=torch.bfloat16, device=\"cpu\").pin_memory()\n",
    "w2 = torch.randn(hd, hdd, dtype=torch.bfloat16, device=\"cpu\").pin_memory()\n",
    "\n",
    "time_start = time.time()\n",
    "for i in range(8):\n",
    "    w0_gpu = w0.to(device=\"cuda:1\")\n",
    "    w1_gpu = w1.to(device=\"cuda:1\")\n",
    "    w2_gpu = w2.to(device=\"cuda:1\")\n",
    "print(f\"time cost to cuda:1 {time.time() - time_start} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2813583b-b109-47ea-a192-738437282e0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
