{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "701a120f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "batch_size = 90\n",
    "device=\"cuda:0\"\n",
    "query_states = torch.randn(batch_size, 32, 512, 128, dtype=torch.bfloat16, device=\"cpu\").pin_memory()\n",
    "# 480MB, *4 = 1920MB\n",
    "key_states = torch.randn(batch_size, 8, 512, 128, dtype=torch.bfloat16, device=\"cpu\").pin_memory()\n",
    "value_states = torch.randn(batch_size, 8, 512, 128, dtype=torch.bfloat16, device=\"cpu\").pin_memory()\n",
    "# 960MB*3 = 2880MB\n",
    "# time_start = time.time()\n",
    "# for i in range(4):\n",
    "#     key_states.to(device=\"cuda:1\")\n",
    "#     value_states.to(device=\"cuda:1\")\n",
    "# print(f\"time cost to cuda:1 {time.time() - time_start} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6ea51d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key_states cpu\n",
      "time cost repeat kv 0.12833380699157715 s\n",
      "dot attn cost 0.559304 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "hidden_states_cache = torch.zeros(batch_size, 8, 4, 512, 128, dtype=torch.bfloat16, device=\"cpu\").pin_memory()\n",
    "# Copied from transformers.models.llama.modeling_llama.repeat_kv\n",
    "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n",
    "    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n",
    "    \"\"\"\n",
    "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
    "    if n_rep == 1:\n",
    "        return hidden_states\n",
    "    # 改为将hidden_states 复制到 hidden_states_cache 上\n",
    "    # for i in range(hidden_states_cache.shape[2]):\n",
    "        # 这行代码的作用是：将 hidden_states 的内容复制到 hidden_states_cache 的第 i 个位置（第三个维度）。\n",
    "        # 也就是说，对于 hidden_states_cache 的每个“副本”维度 i，都用同样的 hidden_states 填充，实现了在第3维上重复 hidden_states 的效果。\n",
    "        # hidden_states_cache[:, :, i, :, :] = hidden_states\n",
    "        # hidden_states_cache[:, :, i, :, :].index_copy_(3, torch.arange(hidden_states.shape[3]), hidden_states)\n",
    "    hidden_states_cache = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
    "    # hidden_states_cache = hidden_states.unsqueeze(2).expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
    "    return hidden_states_cache.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n",
    "def repeat_kv_optimized_v2(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    使用expand + reshape优化版本\n",
    "    \"\"\"\n",
    "    if n_rep == 1:\n",
    "        return hidden_states\n",
    "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
    "    # 在第1维上扩展n_rep倍\n",
    "    expanded = hidden_states.unsqueeze(2).expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
    "    return expanded.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n",
    "# attn_mask = torch.ones((480,512), dtype=torch.bfloat16, device=\"cpu\")\n",
    "print(f\"key_states {key_states.device}\")\n",
    "time_start = time.time()\n",
    "new_key_states = repeat_kv(key_states, 4)\n",
    "new_value_states = repeat_kv(value_states, 4)\n",
    "print(f\"time cost repeat kv {time.time() - time_start} s\")\n",
    "\n",
    "time_start = time.time()\n",
    "attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
    "    query_states,\n",
    "    new_key_states,\n",
    "    new_value_states,\n",
    "    attn_mask=None,\n",
    "    dropout_p=0.0,\n",
    "    # The q_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create a causal mask in case q_len == 1.\n",
    "    is_causal=False,\n",
    ")\n",
    "print(f\"dot attn cost {time.time()-time_start:.6f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
