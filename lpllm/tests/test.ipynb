{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a449c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "batch_size = 1440\n",
    "device=\"cuda:1\"\n",
    "query_states = torch.randn(batch_size, 32, 1, 128, dtype=torch.bfloat16, device=\"cpu\").pin_memory()\n",
    "# 480MB, *4 = 1920MB\n",
    "key_states = torch.randn(batch_size, 8, 512, 128, dtype=torch.bfloat16, device=\"cpu\").pin_memory()\n",
    "value_states = torch.randn(batch_size, 8, 512, 128, dtype=torch.bfloat16, device=\"cpu\").pin_memory()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b4b98cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time cost repeat kv 1.4284155368804932 s\n",
      "dot attn cost 0.067475 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-2"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n",
    "    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n",
    "    \"\"\"\n",
    "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
    "    if n_rep == 1: \n",
    "        return hidden_states\n",
    "    # 正确的repeat_kv实现\n",
    "    expanded = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
    "    return expanded.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n",
    "time_start = time.time()\n",
    "torch.cuda.nvtx.range_push(\"repeat kv\")\n",
    "new_key_states = repeat_kv(key_states, 4)\n",
    "new_value_states = repeat_kv(value_states, 4)\n",
    "torch.cuda.nvtx.range_pop()\n",
    "print(f\"time cost repeat kv {time.time() - time_start} s\")\n",
    "\n",
    "torch.cuda.nvtx.range_push(\"dot attention\")\n",
    "time_start = time.time()\n",
    "attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
    "    query_states,\n",
    "    new_key_states,\n",
    "    new_value_states,\n",
    "    attn_mask=None,\n",
    "    dropout_p=0.0,\n",
    "    enable_gqa = False,\n",
    "    # The q_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create a causal mask in case q_len == 1.\n",
    "    is_causal=False,\n",
    ")\n",
    "print(f\"dot attn cost {time.time()-time_start:.6f} seconds\")\n",
    "torch.cuda.nvtx.range_pop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "15b32f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1440, 8, 1, 128])\n",
      "real attn out cost 0.039719581604003906 s\n",
      "torch.Size([1440, 8, 1, 128])\n",
      "real attn out cost 0.03912043571472168 s\n",
      "torch.Size([1440, 8, 1, 128])\n",
      "real attn out cost 0.038926124572753906 s\n",
      "torch.Size([1440, 8, 1, 128])\n",
      "real attn out cost 0.039426565170288086 s\n",
      "dot attn cost 0.162191 seconds\n",
      "time cost move to cuda:1 0.0032503604888916016 s\n",
      "attn_output == attn_output_group\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 按多组分别对kv和query分头计算，再汇总结果\n",
    "time_start = time.time()\n",
    "num_groups = 4   # 4 组\n",
    "num_query_heads = query_states.shape[1]     # e.g. 32\n",
    "heads_per_group = num_query_heads // num_groups   # e.g. 8\n",
    "\n",
    "attn_outputs_per_group = []\n",
    "for i in range(1):\n",
    "    for group_idx in range(num_groups):\n",
    "        # 不扩展kv，而是使用整个kv，query取值需变化\n",
    "        # 每组使用整个key/value heads，但query heads按步数提取\n",
    "        # 例如：32个query heads，8个key/value heads\n",
    "        # 按步数提取query heads\n",
    "        query_indices = torch.arange(group_idx, num_query_heads, num_groups)\n",
    "        query_group = query_states[:, query_indices, :, :]  # (batch, heads_per_group, seq_len, head_dim)\n",
    "        print(query_group.shape)\n",
    "        key_group = key_states    # (batch, 8, seq_len, head_dim)\n",
    "        value_group = value_states # (batch, 8, seq_len, head_dim)\n",
    "\n",
    "        time_start_tmp = time.time()\n",
    "        attn_out = torch.nn.functional.scaled_dot_product_attention(\n",
    "            query_group, key_group, value_group,\n",
    "            attn_mask=None,\n",
    "            dropout_p=0.0,\n",
    "            enable_gqa=False,\n",
    "            is_causal=False\n",
    "        )\n",
    "        print(f\"real attn out cost {time.time() - time_start_tmp} s\")\n",
    "        # print(f\"Group {group_idx}: query_indices={query_indices.tolist()}, kv_heads=all, attn_out.shape={attn_out.shape}\")\n",
    "        # instead of simply appending to a list, collect all group results in the correct head positions\n",
    "        # (batch, num_query_heads, seq_len, head_dim) for all groups\n",
    "        if group_idx == 0:\n",
    "            attn_outputs_full = torch.zeros(\n",
    "                query_states.shape, dtype=attn_out.dtype, device=attn_out.device, pin_memory=True\n",
    "            )\n",
    "        attn_outputs_full[:, query_indices, :, :] = attn_out\n",
    "\n",
    "print(f\"dot attn cost {time.time()-time_start:.6f} seconds\")\n",
    "\n",
    "time_start_move = time.time()\n",
    "attn_outputs_full_gpu = attn_outputs_full.to(device=\"cuda:1\")\n",
    "print(f\"time cost move to cuda:1 {time.time() - time_start_move} s\")\n",
    "\n",
    "if torch.allclose(attn_output, attn_outputs_full, atol=1e-6):\n",
    "    print(\"attn_output == attn_output_group\")\n",
    "else:\n",
    "    print(\"attn_output != attn_output_group\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f08154c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Multithreaded group attention ===\n",
      "dot attn multithreaded cost 0.181015 seconds\n",
      "attn_output == attn_output_group_threaded\n"
     ]
    }
   ],
   "source": [
    "#==== 多线程版本 ====\n",
    "import threading\n",
    "\n",
    "print(\"\\n=== Multithreaded group attention ===\")\n",
    "\n",
    "for i in range(1):\n",
    "    time_start = time.time()\n",
    "    attn_outputs_full_threaded = torch.zeros(\n",
    "        query_states.shape, dtype=query_states.dtype, device=query_states.device\n",
    "    )\n",
    "\n",
    "    def compute_group_attn(group_idx):\n",
    "        query_indices = torch.arange(group_idx, num_query_heads, num_groups)\n",
    "        query_group = query_states[:, query_indices, :, :]\n",
    "        key_group = key_states\n",
    "        value_group = value_states\n",
    "        attn_out = torch.nn.functional.scaled_dot_product_attention(\n",
    "            query_group, key_group, value_group,\n",
    "            attn_mask=None,\n",
    "            dropout_p=0.0,\n",
    "            enable_gqa=False,\n",
    "            is_causal=False\n",
    "        )\n",
    "        attn_outputs_full_threaded[:, query_indices, :, :] = attn_out\n",
    "\n",
    "    threads = []\n",
    "    for group_idx in range(num_groups):\n",
    "        t = threading.Thread(target=compute_group_attn, args=(group_idx,))\n",
    "        threads.append(t)\n",
    "        t.start()\n",
    "\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "\n",
    "    print(f\"dot attn multithreaded cost {time.time()-time_start:.6f} seconds\")\n",
    "\n",
    "    if torch.allclose(attn_output, attn_outputs_full_threaded, atol=1e-6):\n",
    "        print(\"attn_output == attn_output_group_threaded\")\n",
    "    else:\n",
    "        print(\"attn_output != attn_output_group_threaded\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
